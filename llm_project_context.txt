--- PROJECT STRUCTURE ---
/Users/latchy/lumiere_semantique
├── .gitignore
├── backend
│   ├── .env
│   ├── api
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   ├── urls.py
│   │   └── views.py
│   ├── db.sqlite3
│   ├── ingestion
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── crawler.py
│   │   ├── indexing.py
│   │   ├── jsonifier.py
│   │   ├── management
│   │   │   ├── __init__.py
│   │   │   └── commands
│   │   │       ├── __init__.py
│   │   │       ├── generate_briefing.py
│   │   │       ├── ingest_repo.py
│   │   │       ├── inspect_graph.py
│   │   │       ├── run_crawler.py
│   │   │       ├── run_indexer.py
│   │   │       └── search.py
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   └── views.py
│   ├── lumiere_core
│   │   ├── __init__.py
│   │   ├── .env
│   │   ├── .gitignore
│   │   ├── asgi.py
│   │   ├── services
│   │   │   ├── __init__.py
│   │   │   ├── ambassador.py
│   │   │   ├── cartographer.py
│   │   │   ├── code_surgery.py
│   │   │   ├── cortex_service.py
│   │   │   ├── crucible.py
│   │   │   ├── diplomat.py
│   │   │   ├── documentation.py
│   │   │   ├── gemini_service.py
│   │   │   ├── github.py
│   │   │   ├── ingestion_service.py
│   │   │   ├── llm_service.py
│   │   │   ├── ollama_service.py
│   │   │   ├── ollama.py
│   │   │   ├── profile_service.py
│   │   │   ├── rca_service.py
│   │   │   ├── review_service.py
│   │   │   ├── scaffolding.py
│   │   │   ├── strategist.py
│   │   │   ├── testing.py
│   │   │   └── utils.py
│   │   ├── settings.py
│   │   ├── urls.py
│   │   └── wsgi.py
│   ├── manage.py
│   ├── requirements.txt
│   └── run_server.sh
├── crawler.sh
├── llm_project_context.txt
├── lumiere.py
└── README.md

10 directories, 64 files

--- END PROJECT STRUCTURE ---


--- FILE_START: lumiere.py ---
# In /Users/latchy/lumiere_semantique/lumiere.py

import typer
import requests
import sys
import re
import shlex
import difflib
import traceback
from typing import Optional, List, Dict, Tuple
from collections import defaultdict
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.markdown import Markdown
from rich.text import Text
from rich.status import Status
from rich.live import Live
from rich.align import Align
from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.shortcuts import confirm
from prompt_toolkit.styles import Style
from pathlib import Path
import json
import time
from datetime import datetime
import textwrap
from rich.tree import Tree

# --- Global Objects & Configuration ---
console = Console()
history_path = Path.home() / ".lumiere" / "history.txt"
config_path = Path.home() / ".lumiere" / "config.json"
history_path.parent.mkdir(parents=True, exist_ok=True)

# --- NEW: Centralized API URL ---
API_BASE_URL = "http://127.0.0.1:8002/api/v1"

# Create command completers for better UX
main_commands = ['analyze', 'a', 'profile', 'p', 'config', 'c', 'help', 'h', 'exit', 'x', 'quit']
analysis_commands = ['list', 'l', 'fix', 'f', 'briefing', 'b', 'rca', 'r', 'details', 'd', 'graph', 'g', 'help', 'h', 'back', 'exit', 'quit']

main_completer = WordCompleter(main_commands, ignore_case=True)
analysis_completer = WordCompleter(analysis_commands, ignore_case=True)

# --- ADDED: Style for prompt_toolkit prompt to match rich colors ---
prompt_style = Style.from_dict({
    'lumiere': 'bold #00ffff',  # bold cyan
    'provider': 'yellow',
    'separator': 'white'
})

prompt_session = PromptSession(
    history=FileHistory(str(history_path)),
    completer=main_completer,
    style=prompt_style
)

# --- Global CLI State ---
cli_state = {
    "model": None,  # Will be populated from config
    "available_models": [],
    "last_repo_url": None,
    "debug_mode": False,
}

def load_config():
    """Load configuration from file if it exists."""
    try:
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                cli_state.update(config)
                console.print(f"[dim]✓ Configuration loaded from {config_path}[/dim]")
    except Exception as e:
        console.print(f"[yellow]Warning: Could not load config: {e}[/yellow]")

def save_config():
    """Save current configuration to file."""
    try:
        with open(config_path, 'w') as f:
            json.dump({
                "model": cli_state["model"],
                "last_repo_url": cli_state["last_repo_url"],
                "debug_mode": cli_state["debug_mode"]
            }, f, indent=2)
    except Exception as e:
        console.print(f"[yellow]Warning: Could not save config: {e}[/yellow]")

def validate_github_url(url: str) -> bool:
    """Validate that the URL is a proper GitHub repository URL."""
    github_pattern = r'^https://github\.com/[\w\-\.]+/[\w\-\.]+/?$'
    return bool(re.match(github_pattern, url))

def format_url(url: str) -> str:
    """Normalize GitHub URL format."""
    url = url.strip().rstrip('/')
    if not url.startswith('https://'):
        if url.startswith('github.com/'):
            url = 'https://' + url
        elif '/' in url and not url.startswith('http'):
            url = 'https://github.com/' + url
    return url

def _insert_docstring_into_code(code: str, docstring: str) -> str:
    """Intelligently inserts a docstring into a Python code snippet."""
    # Find the first function or class definition
    match = re.search(r"^(?P<indent>\s*)(def|class)\s+\w+", code, re.MULTILINE)
    if not match:
        return code # Cannot find where to insert, return original

    indentation = match.group('indent')
    insertion_point = match.end()

    # Prepare the docstring with the correct indentation
    indented_docstring = textwrap.indent(f'"""{docstring}"""', indentation + '    ')

    # Insert the docstring
    return f"{code[:insertion_point]}\n{indented_docstring}{code[insertion_point:]}"

# --- Enhanced API Client ---
class LumiereAPIClient:
    def __init__(self, base_url: str = API_BASE_URL, timeout: int = 600):
        self.base_url = base_url
        self.timeout = timeout
        self.session = requests.Session()  # Reuse connections

    def _request(self, method: str, endpoint: str, **kwargs):
        try:
            if method.upper() in ["POST"]:
                data = kwargs.get("json", {})
                if "model" not in data:
                    # Ensure a model is selected before making a request
                    if not cli_state.get("model"):
                        console.print("\n[bold red]Error: No LLM model selected.[/bold red]")
                        console.print("Please use the [bold cyan]config[/bold cyan] command to choose a model first.")
                        return None # Abort the request
                    data["model"] = cli_state["model"]
                kwargs["json"] = data

            url = f"{self.base_url}/{endpoint}"

            if cli_state["debug_mode"]:
                console.print(f"[dim]DEBUG: {method} {url}[/dim]")
                if "json" in kwargs:
                    console.print(f"[dim]DEBUG: Payload: {kwargs['json']}[/dim]")

            response = self.session.request(method, url, timeout=self.timeout, **kwargs)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.ConnectionError as e:
            console.print(Panel(
                f"[bold red]Cannot connect to Lumière backend[/bold red]\n"
                f"[yellow]Expected URL:[/yellow] {self.base_url}\n"
                f"[yellow]Error:[/yellow] {str(e)}\n\n"
                f"[dim]💡 Make sure the backend server is running:\n"
                f"   • Execute `cd backend && ./run_server.sh`\n"
                f"   • Verify the URL is correct\n"
                f"   • Check firewall settings[/dim]",
                title="[red]Connection Error[/red]",
                border_style="red"
            ))
            return None

        except requests.exceptions.HTTPError as e:
            try:
                # Check for a JSON response, which might contain detailed error info from our backend
                error_json = e.response.json()
                error_details = error_json.get('error', str(error_json))

                console.print(Panel(
                    f"[bold red]API Request Failed[/bold red]\n"
                    f"[yellow]URL:[/yellow] {e.request.url}\n"
                    f"[yellow]Status:[/yellow] {e.response.status_code}\n"
                    f"[yellow]Error:[/yellow] {error_details}",
                    title="[red]API Error[/red]",
                    border_style="red"
                ))

                # If the backend included the raw LLM response for debugging, display it
                llm_response_for_debug = error_json.get('llm_response')
                if llm_response_for_debug:
                    console.print(Panel(
                        Text(llm_response_for_debug, overflow="fold"),
                        title="[bold yellow]🔍 LLM Raw Response (for debugging)[/bold yellow]",
                        border_style="yellow",
                        expand=False
                    ))

            except json.JSONDecodeError:
                # The error response wasn't JSON, so we just display the raw text
                console.print(Panel(
                    f"[bold red]HTTP Error {e.response.status_code}[/bold red]\n"
                    f"[yellow]URL:[/yellow] {e.request.url}\n\n"
                    f"[bold]Response Text:[/bold]\n{e.response.text[:500]}...",
                    title="[red]Non-JSON API Error[/red]",
                    border_style="red"
                ))
            return None

        except requests.exceptions.Timeout:
            console.print(Panel(
                f"[bold red]Request Timeout[/bold red]\n"
                f"The request took longer than {self.timeout} seconds.\n"
                f"[dim]The backend might be processing a large repository.[/dim]",
                title="[red]Timeout Error[/red]"
            ))
            return None

        except requests.exceptions.RequestException as e:
            console.print(Panel(
                f"[bold red]Request Error[/bold red]\n"
                f"[yellow]Error:[/yellow] {str(e)}",
                title="[red]Request Error[/red]"
            ))
            return None

    def health_check(self) -> bool:
        """Check if the backend is healthy."""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False

    def list_models(self): return self._request("GET", "models/list/")
    def get_analysis(self, repo_url: str): return self._request("POST", "strategist/prioritize/", json={"repo_url": repo_url})
    def get_briefing(self, issue_url: str): return self._request("POST", "briefing/", json={"issue_url": issue_url})
    def get_rca(self, repo_url: str, bug_description: str): return self._request("POST", "rca/", json={"repo_url": repo_url, "bug_description": bug_description})
    def get_profile(self, username: str): return self._request("POST", "profile/review/", json={"username": username})
    def get_graph(self, repo_id: str): return self._request("GET", f"graph/?repo_id={repo_id}")

    def generate_docstring(self, repo_id: str, code: str, instruction: str):
        return self._request("POST", "generate-docstring/", json={
            "repo_id": repo_id,
            "new_code": code,
            "instruction": instruction
        })

    # --- CORRECTED: Replaced old `generate_fix` with new `generate_scaffold` ---
    def generate_scaffold(self, repo_id: str, target_files: List[str], instruction: str, rca_report: str, refinement_history: Optional[List[Dict]] = None):
        payload = {
            "repo_id": repo_id,
            "target_files": target_files,
            "instruction": instruction,
            "rca_report": rca_report,
            "refinement_history": refinement_history or []
        }
        return self._request("POST", "scaffold/", json=payload)

    # --- MODIFIED: Updated to handle a dictionary of files ---
    def create_pr(self, issue_url: str, modified_files: Dict[str, str]):
        return self._request("POST", "ambassador/dispatch/", json={"issue_url": issue_url, "modified_files": modified_files})

    def get_diplomat_report(self, issue_title: str, issue_body: str): return self._request("POST", "diplomat/find-similar-issues/", json={"issue_title": issue_title, "issue_body": issue_body})
    def validate_in_crucible(self, repo_url: str, target_file: str, modified_code: str): return self._request("POST", "crucible/validate/", json={"repo_url": repo_url, "target_file": target_file, "modified_code": modified_code})
    def ingest_repository(self, repo_url: str): return self._request("POST", "ingest/", json={"repo_url": repo_url})

# --- MODIFIED: Implemented a two-step provider/model selection process. ---
def handle_model_selection(api: "LumiereAPIClient"):
    """
    Guides the user through a two-step process: first selecting an LLM provider,
    then selecting a model from that provider.
    """
    console.print("\n[bold cyan]🤖 LLM Provider & Model Selection[/bold cyan]")
    with Status("[cyan]Fetching available models from backend...[/cyan]"):
        available_models_data = api.list_models()

    if not available_models_data:
        # Error is printed by the API client
        return

    cli_state["available_models"] = available_models_data

    # Step 1: Group models by provider
    providers = defaultdict(list)
    for model in available_models_data:
        provider = model.get('provider', 'unknown').capitalize()
        providers[provider].append(model)

    if not providers:
        console.print("[red]No providers with available models found.[/red]")
        return

    provider_names = list(providers.keys())

    try:
        # Step 2: Prompt for the provider
        console.print("\n[bold]First, select a provider:[/bold]")
        provider_table = Table(show_header=False, box=None, padding=(0, 2))
        for i, name in enumerate(provider_names, 1):
            provider_table.add_row(f"[cyan]({i})[/cyan]", name)
        console.print(provider_table)

        # Determine default provider choice
        current_provider = ""
        if cli_state.get("model"):
            current_provider = cli_state.get("model").split('/')[0].capitalize()

        default_provider_idx = "1"
        if current_provider in provider_names:
            default_provider_idx = str(provider_names.index(current_provider) + 1)

        provider_choice_str = Prompt.ask(
            "Enter your provider choice",
            choices=[str(i) for i in range(1, len(provider_names) + 1)],
            show_choices=False,
            default=default_provider_idx
        )
        selected_provider_name = provider_names[int(provider_choice_str) - 1]

        # Step 3: Prompt for a model from the selected provider
        models_for_provider = providers[selected_provider_name]

        model_table = Table(title=f"Available Models from [yellow]{selected_provider_name}[/yellow]", border_style="blue")
        model_table.add_column("Choice #", style="dim", justify="center")
        model_table.add_column("Model Name", style="white")
        model_table.add_column("Model ID", style="cyan")

        model_choices_for_prompt = []
        for i, model in enumerate(models_for_provider, 1):
            model_choices_for_prompt.append(str(i))
            model_table.add_row(
                str(i),
                model.get('name', 'N/A'),
                model.get('id', 'N/A')
            )
        console.print(model_table)

        # Find default selection if a model from this provider is already selected
        current_model_index_str = "1"
        current_model_id = cli_state.get("model")
        if current_model_id and current_model_id.startswith(selected_provider_name.lower()):
            for i, model in enumerate(models_for_provider):
                if model['id'] == current_model_id:
                    current_model_index_str = str(i + 1)
                    break

        model_choice_str = Prompt.ask(
            "[bold]Select a model number to use[/bold]",
            choices=model_choices_for_prompt,
            show_choices=False,
            default=current_model_index_str
        )
        selected_model_index = int(model_choice_str) - 1
        selected_model_id = models_for_provider[selected_model_index]['id']

        cli_state["model"] = selected_model_id
        save_config()
        console.print(f"✅ Model set to [bold green]{cli_state['model']}[/bold green]. This will be saved for future sessions.")

    except (ValueError, IndexError):
        console.print("[red]❌ Invalid selection.[/red]")
    except KeyboardInterrupt:
        console.print("\n[yellow]Model selection cancelled.[/yellow]")

# --- MODIFIED: Dynamic prompt text based on selected model/provider ---
def get_prompt_text() -> List[Tuple[str, str]]:
    """
    Builds a prompt_toolkit-compatible formatted text list for the prompt.
    Dynamically displays the provider or model name.
    """
    display_name = "Choose Provider"  # Default text when no model is selected
    model_id = cli_state.get("model")

    if model_id:
        parts = model_id.split('/', 1)
        if len(parts) == 2:
            provider, model_name = parts
            if provider == "ollama":
                display_name = model_name  # Show specific model name for ollama
            else:
                display_name = provider.capitalize()  # Show provider name for others (e.g., Gemini)
        else:
            display_name = model_id  # Fallback for malformed ID

    return [
        ('class:lumiere', 'Lumière'),
        ('class:provider', f' ({display_name})'),
        ('class:separator', ' > '),
    ]

# --- Enhanced Analysis Session Manager ---
class AnalysisSession:
    def __init__(self, repo_url: str):
        self.repo_url = format_url(repo_url)
        if not validate_github_url(self.repo_url):
            raise ValueError(f"Invalid GitHub URL: {self.repo_url}")

        self.repo_id = self.repo_url.replace("https://github.com/", "").replace("/", "_")
        self.issues = []
        # --- State for RCA-to-Fix Pipeline ---
        self.last_rca_report = None
        self.last_rca_issue_num = None
        # ---
        self.api = LumiereAPIClient()
        cli_state["last_repo_url"] = self.repo_url
        save_config()

    def start(self) -> bool:
        """Initialize the analysis session."""
        console.print(Panel(
            f"[bold cyan]🔍 Analysis Session Starting[/bold cyan]\n"
            f"[yellow]Repository:[/yellow] {self.repo_url}\n"
            f"[yellow]Model:[/yellow] {cli_state['model']}",
            border_style="cyan"
        ))

        with Status("[cyan]Checking backend connection...") as status:
            if not self.api.health_check():
                return False # Error already printed by client
            status.update("[green]✓ Backend connection established")
            time.sleep(0.5)

        try:
            do_embed = Confirm.ask(
                "\n[bold]Do you want to clone and embed this repo for full analysis (briefing, rca, fix)?[/bold]\n"
                "[dim](This can take a few minutes for large repos. Choose 'N' for issue listing only.)[/dim]",
                default=True
            )
        except KeyboardInterrupt:
            console.print("\n[yellow]Analysis cancelled.[/yellow]")
            return False

        if do_embed:
            with Status("[cyan]🚀 Beginning ingestion...[/cyan]", spinner="earth") as status:
                status.update("[cyan]Cloning repository and analyzing files...[/cyan]")
                ingest_result = self.api.ingest_repository(self.repo_url)

                if ingest_result and ingest_result.get("status") == "success":
                    status.update("[green]✓ Repository cloned and embedded successfully.[/green]")
                    time.sleep(1)
                else:
                    error_details = ingest_result.get('error', 'Unknown error during ingestion.') if ingest_result else "No response from server."
                    console.print(f"\n[bold red]❌ Ingestion failed:[/bold red] {error_details}")
                    try:
                        if not Confirm.ask("[yellow]Would you like to continue with limited (issue list only) analysis?[/yellow]", default=True):
                            return False
                    except KeyboardInterrupt:
                        return False

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            transient=True
        ) as progress:
            task = progress.add_task("[green]🤖 Contacting The Strategist...", total=100)
            strategist_data = self.api.get_analysis(self.repo_url)
            if not strategist_data:
                return False
            progress.update(task, completed=100)

        self.issues = strategist_data.get("prioritized_issues", [])
        if not self.issues:
            console.print("[yellow]📭 No open issues found in this repository.[/yellow]")
            return False

        console.print(f"✨ The Strategist identified [bold green]{len(self.issues)}[/bold green] open issues for analysis.")
        self.display_issue_table()
        return True

    def display_issue_table(self):
        """Display a formatted table of prioritized issues."""
        table = Table(
            title="[bold blue]🎯 Prioritized Issue Triage[/bold blue]",
            show_header=True,
            header_style="bold magenta",
            border_style="blue"
        )
        table.add_column("Rank", style="dim", justify="center", width=6)
        table.add_column("Score", style="bold", justify="center", width=8)
        table.add_column("Issue #", style="green", justify="center", width=10)
        table.add_column("Title", style="white", no_wrap=False)

        for issue in self.issues:
            score = issue.get('score', 0)
            score_style = "red" if score >= 90 else "yellow" if score >= 70 else "white"
            score_emoji = "🔥" if score >= 90 else "⚡" if score >= 70 else "📝"

            table.add_row(
                f"#{issue['rank']}",
                f"{score_emoji} [{score_style}]{score}[/{score_style}]",
                f"#{issue['number']}",
                issue['title'][:80] + "..." if len(issue['title']) > 80 else issue['title']
            )
        console.print(table)

    def display_graph(self, graph_data: dict, repo_id: str):
        """
        [NEW & IMPROVED] Displays the architectural graph in a summarized, readable format.
        It now groups and counts calls to avoid overwhelming the user.
        """
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph (Summary) ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])

        if not nodes:
            console.print("[yellow]No architectural nodes were mapped for this project.[/yellow]")
            return

        # --- MAJOR IMPROVEMENT: Process edges to group and count calls ---
        edges_by_source = defaultdict(lambda: {'imports': [], 'calls': defaultdict(int)})
        for edge in edges:
            source_id = edge['source']
            edge_type = edge['type']
            target_id = edge['target']

            if edge_type == 'IMPORTS':
                edges_by_source[source_id]['imports'].append(target_id)
            elif edge_type == 'CALLS':
                edges_by_source[source_id]['calls'][target_id] += 1

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]", guide_style="cyan")
        file_tree_nodes = {}

        # First pass: Build the primary structure (Files, Classes, Functions)
        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                lang = node_data.get('language', 'unknown')
                icon = "📄"
                if lang == 'python': icon = "🐍"
                if lang == 'javascript': icon = "🟨"

                file_branch = tree.add(f"{icon} [bold green]{node_id}[/bold green] [dim]({lang})[/dim]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_node_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")

                    # Attach methods to their class
                    for method_name in sorted(nodes.get(class_node_id, {}).get('methods', [])):
                        class_branch.add(f"  -  M [dim]{method_name}()[/dim]")

                # Attach top-level functions to the file
                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"  - F [dim]{func_name}()[/dim]")

        # Second pass: Add the summarized relationships (imports and calls)
        for source_id, relationships in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]

                # Display Imports first
                if relationships['imports']:
                    import_branch = parent_branch.add("📥 [bold]Imports[/bold]")
                    for target in sorted(list(set(relationships['imports']))): # Use set to remove duplicates
                        import_branch.add(f"[yellow]{target}[/yellow]")

                # Display Summarized Calls
                if relationships['calls']:
                    calls_branch = parent_branch.add("📞 [bold]Calls[/bold]")
                    # Sort calls by frequency (most frequent first)
                    sorted_calls = sorted(relationships['calls'].items(), key=lambda item: item[1], reverse=True)

                    # Limit the number of calls displayed to prevent clutter
                    max_calls_to_show = 15
                    for i, (target, count) in enumerate(sorted_calls):
                        if i >= max_calls_to_show:
                            calls_branch.add(f"[dim]... and {len(sorted_calls) - max_calls_to_show} more.[/dim]")
                            break

                        count_str = f" [dim](x{count})[/dim]" if count > 1 else ""
                        calls_branch.add(f"[magenta]{target}[/magenta]{count_str}")


        console.print(tree)
        console.print("\n[bold magenta]--------------------------------------------------------[/bold magenta]")

    def loop(self):
        """Main interactive loop for the analysis session."""
        display_interactive_help('analyze')

        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=analysis_completer,
            style=prompt_style
        )

        while True:
            try:
                prompt_text = get_prompt_text()
                command_str = prompt_session.prompt(prompt_text).strip()

                if not command_str:
                    continue

                if command_str.lower() in ("q", "quit", "exit", "back"):
                    break

                self.handle_analysis_command(command_str)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[cyan]📊 Analysis session ended.[/cyan]")

        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def handle_analysis_command(self, command_str: str):
        """Handle commands within the analysis session."""
        try:
            parts = shlex.split(command_str.lower())
        except ValueError:
            console.print("[red]❌ Invalid command syntax.[/red]")
            return

        command = parts[0] if parts else ""
        args = parts[1:] if len(parts) > 1 else []

        if command in ("h", "help"):
            display_interactive_help('analyze')
            return

        if command in ("l", "list"):
            self.display_issue_table()
            return

        if command in ('g', 'graph'):
            self.execute_action(command, {}) # Pass empty dict, issue not needed
            console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
            return

        if command in ('f', 'fix') and self.last_rca_report:
             issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
             if issue:
                 self.execute_action(command, issue)
                 console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
                 return

        if command not in ('f', 'fix', 'b', 'briefing', 'r', 'rca', 'd', 'details'):
            console.print("[red]❌ Unknown command. Type 'help' for available commands.[/red]")
            return

        issue_num_str = None
        if args and args[0].isdigit():
            issue_num_str = args[0]
        else:
            try:
                prompt_ask_text = f"Which issue # for '[cyan]{command}[/cyan]'?"
                if command in ('f', 'fix') and self.last_rca_report:
                     prompt_ask_text += f"\n[dim](Press Enter to fix issue #{self.last_rca_issue_num} from the last RCA)[/dim]"

                issue_num_str = Prompt.ask(prompt_ask_text).strip()
            except KeyboardInterrupt:
                console.print("\n[yellow]Command cancelled.[/yellow]")
                return

        if not issue_num_str:
            if command in ('f', 'fix') and self.last_rca_report:
                issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
                if issue:
                    self.execute_action(command, issue)
                else:
                    console.print("[red]❌ Could not find issue from last RCA. Please specify an issue number.[/red]")
            else:
                console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        if not issue_num_str.isdigit():
            console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        target_issue = next((iss for iss in self.issues if iss.get('number') == int(issue_num_str)), None)
        if not target_issue:
            console.print(f"[red]❌ Issue #{issue_num_str} not found in the prioritized list.[/red]")
            console.print("[dim]💡 Use 'list' to see available issues.[/dim]")
            return

        self.execute_action(command, target_issue)
        console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")

    def execute_action(self, command: str, issue: Dict):
        """Execute the specified action on an issue."""
        if command in ("f", "fix"):
            self.handle_fix_dialogue(issue)
            return

        if command in ("r", "rca"):
            self.handle_rca_command(issue)
            return

        if command in ('g', 'graph'):
            with Status("[cyan]🗺️ Contacting Cartographer's Architectural Graph...[/cyan]", spinner="earth") as status:
                graph_data = self.api.get_graph(self.repo_id)
                status.update("[green]✓ Graph retrieved.[/green]")
                time.sleep(0.5)

            if graph_data and graph_data.get("graph"):
                self.display_graph(graph_data["graph"], graph_data["repo_id"])
            elif graph_data and graph_data.get("message"):
                console.print(Panel(graph_data["message"], title="[yellow]Graph Not Available[/yellow]", border_style="yellow"))
            else:
                 console.print("[red]❌ Could not retrieve architectural graph.[/red]")
            return

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            if command in ("b", "briefing"):
                task = progress.add_task(f"[cyan]📋 Getting briefing for issue #{issue['number']}...", total=None)
                briefing_data = self.api.get_briefing(f"{self.repo_url}/issues/{issue['number']}")
                progress.remove_task(task)
                if briefing_data and briefing_data.get("briefing"):
                    console.print(Panel(
                        Markdown(briefing_data["briefing"]),
                        title=f"[bold blue]📋 Issue Briefing #{issue['number']}[/bold blue]",
                        border_style="blue"
                    ))
                else:
                    console.print("[red]❌ Could not retrieve briefing.[/red]")
            elif command in ("d", "details"):
                issue_url = f"{self.repo_url}/issues/{issue['number']}"
                console.print(Panel(
                    f"[bold]Issue #{issue['number']}[/bold]\n"
                    f"[yellow]Title:[/yellow] {issue['title']}\n"
                    f"[yellow]Priority Score:[/yellow] {issue['score']}/100\n"
                    f"[yellow]URL:[/yellow] [link={issue_url}]{issue_url}[/link]\n"
                    f"[yellow]Description:[/yellow] {issue.get('description', 'No description available')[:200]}...",
                    title="[bold green]📝 Issue Details[/bold green]",
                    border_style="green"
                ))

    def handle_rca_command(self, issue: Dict):
        """Handle the new context-aware root cause analysis command."""
        console.print(f"[cyan]🔍 Starting Root Cause Analysis for issue #{issue['number']}[/cyan]")
        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("[cyan]🕵️ Performing multi-file root cause analysis...", total=None)
            rca_data = self.api.get_rca(self.repo_url, issue_desc)
            progress.remove_task(task)

        if rca_data and rca_data.get("analysis"):
            self.last_rca_report = rca_data["analysis"]
            self.last_rca_issue_num = issue['number']
            console.print(Panel(
                Markdown(self.last_rca_report),
                title=f"[bold red]🕵️ Root Cause Analysis - Issue #{issue['number']}[/bold red]",
                border_style="red"
            ))
            console.print("\n[bold yellow]💡 Pro-tip:[/bold yellow] [dim]You can now type '[/dim][bold]f[/bold][dim]' to start fixing this issue.[/dim]")
        else:
            self.last_rca_report = None
            self.last_rca_issue_num = None
            console.print("[red]❌ Could not perform root cause analysis.[/red]")

    def _display_diff(self, original_code: str, new_code: str, filename: str):
        """Display a formatted diff of code changes for a single file."""
        diff = difflib.unified_diff(
            original_code.splitlines(keepends=True),
            new_code.splitlines(keepends=True),
            fromfile=f'🔴 {filename} (Original)',
            tofile=f'🟢 {filename} (Proposed)'
        )
        diff_panel_content = Text()
        has_changes = False
        for line in diff:
            has_changes = True
            if line.startswith('+++') or line.startswith('---'):
                diff_panel_content.append(line, style="bold blue")
            elif line.startswith('+'):
                diff_panel_content.append(line, style="green")
            elif line.startswith('-'):
                diff_panel_content.append(line, style="red")
            elif line.startswith('@'):
                diff_panel_content.append(line, style="bold yellow")
            else:
                diff_panel_content.append(line, style="dim")

        if not has_changes:
            return

        console.print(Panel(
            diff_panel_content,
            title=f"[bold yellow]📝 Proposed Changes for {filename}[/bold yellow]",
            expand=True,
            border_style="yellow"
        ))

    def _extract_filenames_from_rca(self, rca_report: str) -> List[str]:
        """Extracts filenames from markdown code fences or inline backticks."""
        pattern = r'`([\w./\\-]+)`'
        matches = re.findall(pattern, rca_report)
        filenames = sorted(list(set(matches)))
        return [f for f in filenames if '.' in f and f.lower() not in ['true', 'false']]

    def handle_fix_dialogue(self, issue: Dict):
        """Handle the complete fix dialogue, now driven by RCA and with documentation step."""
        console.print(Panel(
            f"[bold cyan]🤝 Socratic Dialogue[/bold cyan] starting for:\n"
            f"[bold green]Issue #{issue['number']}: {issue['title']}[/bold green]",
            border_style="cyan"
        ))

        if not self.last_rca_report or self.last_rca_issue_num != issue['number']:
             console.print("[yellow]⚠️  Warning: No Root Cause Analysis has been run for this issue.[/yellow]")
             try:
                 if Confirm.ask("[bold]Would you like to run RCA first to provide context for the fix?[/bold]", default=True):
                     self.handle_rca_command(issue)
                     if not self.last_rca_report:
                         console.print("[red]❌ Cannot proceed with fix without a successful RCA.[/red]")
                         return
                 else:
                     console.print("[red]❌ Fix command cancelled. Please run RCA first.[/red]")
                     return
             except KeyboardInterrupt:
                 console.print("\n[yellow]Fix command cancelled.[/yellow]")
                 return

        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        target_files = self._extract_filenames_from_rca(self.last_rca_report)
        if not target_files:
            console.print("[red]❌ Could not automatically determine target files from the RCA report.[/red]")
            return

        console.print(f"[dim]✓ Identified suspect files from RCA: {', '.join(target_files)}[/dim]")

        refinement_history = []
        iteration_count = 0
        max_iterations = 5
        modified_files = {}
        original_contents = {}
        is_documented = False

        while iteration_count < max_iterations:
            iteration_count += 1

            if not modified_files or refinement_history:
                console.print(f"\n[dim]🔄 Iteration {iteration_count}/{max_iterations}[/dim]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task(f"[cyan]⚡ Generating multi-file code fix...", total=None)
                    fix_data = self.api.generate_scaffold(self.repo_id, target_files, issue_desc, self.last_rca_report, refinement_history)
                    progress.remove_task(task)

                if not fix_data:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    return

                if "modified_files" not in fix_data or not fix_data["modified_files"]:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    if fix_data and fix_data.get("llm_response"): console.print(Panel(Text(fix_data["llm_response"], overflow="fold"), title="[yellow]🔍 LLM Raw Response (for debugging)[/yellow]", border_style="yellow"))
                    return

                modified_files = fix_data["modified_files"]
                original_contents = fix_data["original_contents"]
                is_documented = False

            console.rule("[bold]📝 Review Proposed Changes[/bold]")
            for filename, new_code in modified_files.items():
                self._display_diff(original_contents.get(filename, ""), new_code, filename)

            all_validations_passed = True

            # Create a list of files that are actually testable (i.e., not docs)
            files_to_validate = [
                f for f in modified_files.keys()
                if not f.lower().endswith(('.md', '.txt', '.json', '.toml', '.yaml', '.yml'))
            ]

            if not files_to_validate:
                 console.print(Panel("✅ [bold green]No runnable code files to validate. Skipping Crucible.[/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))
            else:
                for filename in files_to_validate:
                    new_code = modified_files[filename]
                    with Progress(SpinnerColumn(), TextColumn("[bold cyan][progress.description]{task.description}"), transient=True) as progress:
                        task = progress.add_task(f"🔥 Entering The Crucible for {filename}...", total=None)
                        validation_result = self.api.validate_in_crucible(self.repo_url, filename, new_code)
                        progress.remove_task(task)

                    if not validation_result or validation_result.get("status") != "passed":
                        all_validations_passed = False
                        console.print(Panel(f"❌ [bold red]Validation Failed for {filename}[/bold red]\n[bold]Test Results:[/bold]\n{validation_result.get('logs', 'No logs') if validation_result else 'Crucible service error'}", title=f"[red]🔥 Crucible Report: {filename}[/red]", border_style="red"))
                        # Immediately stop validation on the first failure
                        break

            if all_validations_passed and files_to_validate:
                 console.print(Panel("✅ [bold green]All tests passed for all modified files![/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))

            # Interactive loop for user actions (approve, refine, etc.)
            while True:
                try:
                    action_choices = ['r', 'c']
                    prompt_text = ""

                    if all_validations_passed:
                        action_choices.append('a')
                        prompt_text += "\n[bold]✅ All tests passed! Choose action:[/bold]\n[bold green](a)[/bold green] Approve & create PR\n"
                        if not is_documented and any(f.endswith((".py", ".js", ".ts")) for f in modified_files.keys()):
                           action_choices.append('d')
                           prompt_text += "[bold blue](d)[/bold blue] Document the changes\n"
                    else:
                        prompt_text += "\n[bold red]❌ Validation failed. Choose action:[/bold]\n"

                    prompt_text += "[bold yellow](r)[/bold yellow] Refine with feedback\n"
                    prompt_text += "[bold red](c)[/bold red] Cancel"

                    default_choice = 'a' if all_validations_passed else 'r'
                    choice = Prompt.ask(prompt_text, choices=action_choices, default=default_choice).lower()

                except KeyboardInterrupt:
                    choice = 'c'

                if choice == 'd':
                    if is_documented:
                        console.print("[yellow]Code is already documented.[/yellow]")
                        continue
                    if not all_validations_passed:
                        console.print("[red]Cannot document code that has failed validation.[/red]")
                        continue

                    documented_files = {}
                    with Status("[bold blue]✒️  Calling The Chronicler agent to document changes...[/bold blue]") as status:
                        for filename, code in modified_files.items():
                            if not any(filename.endswith(ext) for ext in [".py", ".js", ".ts"]): continue
                            status.update(f"[bold blue]✒️  Documenting {filename}...[/bold blue]")
                            doc_result = self.api.generate_docstring(self.repo_id, code, issue_desc)
                            if doc_result and doc_result.get("docstring"):
                                documented_code = _insert_docstring_into_code(code, doc_result["docstring"])
                                documented_files[filename] = documented_code
                            else:
                                documented_files[filename] = code
                    modified_files.update(documented_files)
                    is_documented = True
                    console.print("[green]✓ Documentation complete.[/green]")
                    console.rule("[bold]📝 Review Updated Changes with Documentation[/bold]")
                    for filename, new_code in modified_files.items():
                        self._display_diff(original_contents.get(filename, ""), new_code, filename)
                    continue

                if choice == 'c': break
                if choice == 'a' and all_validations_passed: break
                if choice == 'r': break

            if choice == 'c':
                console.print("[yellow]🛑 Operation cancelled.[/yellow]")
                break

            if choice == 'r':
                if iteration_count >= max_iterations:
                    console.print(f"[yellow]⚠️ Maximum iterations ({max_iterations}) reached.[/yellow]")
                    break
                try:
                    feedback = Prompt.ask("\n[bold]💭 Your feedback for improvement[/bold]")
                    if not feedback.strip():
                        console.print("[yellow]⚠️ Empty feedback, skipping refinement.[/yellow]")
                        continue
                    refinement_history.append({"feedback": feedback, "code_generated": modified_files})
                    modified_files.clear()
                    continue
                except KeyboardInterrupt:
                    break

            if choice == 'a' and all_validations_passed:
                with Progress(SpinnerColumn(),TextColumn("[progress.description]{task.description}"),transient=True) as progress:
                    task = progress.add_task("[cyan]🚀 Dispatching Ambassador for multi-file PR...", total=None)
                    pr_data = self.api.create_pr(f"{self.repo_url}/issues/{issue['number']}", modified_files)
                    progress.remove_task(task)
                if pr_data and pr_data.get("pull_request_url"):
                    console.print(Panel(f"✅ [bold green]Success![/bold green]\nPull request created: [link={pr_data['pull_request_url']}]{pr_data['pull_request_url']}[/link]", title="[green]🚀 Mission Complete[/green]", border_style="green"))
                else:
                    console.print("[red]❌ Failed to create pull request.[/red]")
                break

# --- Utility Function for Help Display ---
def display_interactive_help(context: str = 'main'):
    """Display help instructions based on the current CLI context."""
    title = f"🆘 Lumière Help — {context.capitalize()} Context"
    help_table = Table(title=f"[bold magenta]{title}[/bold magenta]", border_style="magenta")
    help_table.add_column("Command", style="bold cyan")
    help_table.add_column("Description", style="white")

    if context == 'main':
        help_table.add_row("analyze / a", "Start analysis on a GitHub repo")
        help_table.add_row("profile / p", "Get GitHub user profile analysis")
        # --- DYNAMIC HELP TEXT ---
        if cli_state.get("model"):
            help_table.add_row("config / c", "Change LLM model or view settings")
        else:
            help_table.add_row("config / c", "Choose LLM provider & model")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("exit / quit", "Exit the application")
    elif context == 'analyze':
        help_table.add_row("list / l", "Show prioritized issues")
        help_table.add_row("graph / g", "Display the repository's architectural graph")
        help_table.add_row("briefing / b", "Show issue briefing")
        help_table.add_row("details / d", "Show issue metadata")
        help_table.add_row("rca / r", "Root cause analysis")
        help_table.add_row("fix / f", "Launch fix dialogue")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")

    console.print(help_table)

# --- Main Entry Point ---
app = typer.Typer()

@app.command()
def run():
    """Launch Lumière interactive shell."""
    api_client = LumiereAPIClient()
    if not api_client.health_check():
        sys.exit(1) # Error already printed by client

    load_config()

    # --- ENHANCED WELCOME PANEL ---
    welcome_text = (
        f"[bold cyan]✨ Welcome to Lumière ✨[/bold cyan]\n"
        f"AI Dev Assistant for Open Source Projects\n\n"
        f"[dim]Backend Status: [green]Online[/green] at [underline]{API_BASE_URL}[/underline]\n"
        f"Date: {datetime.now().strftime('%B %d, %Y %H:%M:%S')}[/dim]"
    )
    console.print(Panel(welcome_text, border_style="cyan"))

    display_interactive_help('main')

    while True:
        try:
            # --- CORRECTED PROMPT HANDLING ---
            prompt_text = get_prompt_text()
            command = prompt_session.prompt(prompt_text).strip()

            if not command:
                continue

            if command.lower() in ("exit", "quit", "x"):
                console.print("[dim]👋 Goodbye![/dim]")
                break

            if command.lower() in ("help", "h"):
                display_interactive_help('main')
                continue

            if command.lower() in ("config", "c"):
                console.print(Panel(
                    f"[bold]Current Settings[/bold]\n"
                    f"  [cyan]LLM Model:[/cyan] [yellow]{cli_state.get('model', 'Not set')}[/yellow]\n"
                    f"  [cyan]Last Repo:[/cyan] {cli_state.get('last_repo_url', 'Not set')}\n"
                    f"  [cyan]Debug Mode:[/cyan] {'On' if cli_state.get('debug_mode') else 'Off'}",
                    title="⚙️ Configuration", border_style="magenta"
                ))
                handle_model_selection(api_client)
                continue

            if command.lower() in ("profile", "p"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                username = Prompt.ask("Enter GitHub username")
                if not username.strip():
                    continue
                with Status("[cyan]Generating profile analysis...[/cyan]"):
                    profile = api_client.get_profile(username)
                if profile and profile.get("profile_summary"):
                    console.print(Panel(Markdown(profile["profile_summary"]), title=f"👤 Profile Analysis for {username}"))
                else:
                    console.print("[red]❌ Could not retrieve profile.[/red]")
                continue

            if command.lower() in ("analyze", "a"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                repo_url = Prompt.ask("Enter GitHub repository URL").strip()
                if not repo_url: # Gracefully handle empty input
                    continue
                try:
                    session = AnalysisSession(repo_url)
                    if session.start():
                        session.loop()
                except ValueError as e:
                    console.print(f"[red]{e}[/red]")
                continue

            console.print("[red]❌ Unknown command. Type 'help' for options.[/red]")

        except KeyboardInterrupt:
            console.print("\n[dim]💤 Interrupted. Type 'exit' to quit.[/dim]")
            continue
        except EOFError:
            break

if __name__ == "__main__":
    app()

--- FILE_END: lumiere.py ---


--- FILE_START: backend/ingestion/migrations/__init__.py ---

--- FILE_END: backend/ingestion/migrations/__init__.py ---


--- FILE_START: backend/ingestion/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/ingestion/models.py ---


--- FILE_START: backend/ingestion/management/__init__.py ---

--- FILE_END: backend/ingestion/management/__init__.py ---


--- FILE_START: backend/ingestion/management/commands/__init__.py ---

--- FILE_END: backend/ingestion/management/commands/__init__.py ---


--- FILE_START: backend/ingestion/management/commands/generate_briefing.py ---
# In ingestion/management/commands/generate_briefing.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index
from backend.lumiere_core.services.ollama_service import generate_text

class Command(BaseCommand):
    help = 'Generates a "Pre-flight Briefing" for a given query using a RAG pipeline.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The user query or GitHub issue description.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')
        # --- CHANGE 1: Add an argument for the generation model ---
        parser.add_argument('--generation_model', type=str, default='qwen3:4b', help='The Ollama model to use for text generation.')
        parser.add_argument('--k', type=int, default=7, help='Number of context chunks to retrieve.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        embedding_model = options['embedding_model']
        generation_model = options['generation_model'] # <-- Get the new option
        k = options['k']

        self.stdout.write(self.style.NOTICE(f"Step 1: Retrieving context for query: '{query}'..."))

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        try:
            context_chunks = search_index(
                query_text=query,
                model_name=embedding_model, # Use the embedding model here
                index_path=index_path,
                map_path=map_path,
                k=k
            )
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Failed to retrieve context: {e}"))
            return

        self.stdout.write(self.style.SUCCESS(f"✓ Retrieved {len(context_chunks)} context chunks."))

        context_string = ""
        for i, chunk in enumerate(context_chunks):
            context_string += f"--- Context Chunk {i+1} from file '{chunk['file_path']}' ---\n"
            context_string += chunk['text']
            context_string += "\n\n"

        prompt = f"""
        You are Lumière Sémantique, an expert AI programming assistant acting as a Principal Engineer.
        Your mission is to provide a "Pre-flight Briefing" for a developer about to work on a task.
        Analyze the user's query and the provided context from the codebase to generate your report.

        The report must be clear, concise, and structured in Markdown. It must include the following sections:
        1.  **Task Summary:** Briefly rephrase the user's request.
        2.  **Core Analysis:** Based on the provided context, explain how the system currently works in relation to the query. Synthesize information from the different context chunks.
        3.  **Key Files & Code:** Point out the most important files or functions from the context that the developer should focus on.
        4.  **Suggested Approach or Potential Challenges:** Offer a high-level plan or mention any potential issues you foresee.

        --- PROVIDED CONTEXT FROM THE CODEBASE ---
        {context_string}
        --- END OF CONTEXT ---

        USER'S QUERY: "{query}"

        Now, generate the Pre-flight Briefing.
        """

        self.stdout.write(self.style.NOTICE(f"\nStep 2: Sending context and query to the LLM ('{generation_model}') for generation..."))

        # --- CHANGE 2: Pass the generation model name to the function ---
        final_report = generate_text(prompt, model_name=generation_model)

        self.stdout.write(self.style.SUCCESS("\n--- LUMIÈRE SÉMANTIQUE: PRE-FLIGHT BRIEFING ---"))
        self.stdout.write(final_report)

--- FILE_END: backend/ingestion/management/commands/generate_briefing.py ---


--- FILE_START: backend/ingestion/management/commands/run_indexer.py ---
# In ingestion/management/commands/run_indexer.py

from django.core.management.base import BaseCommand
from ingestion.indexing import EmbeddingIndexer
import os

class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and creates a Faiss index from its text chunks using Ollama.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')
        parser.add_argument(
            '--model',
            type=str,
            default='snowflake-arctic-embed2:latest', # <-- Defaults to your preferred model
            help='The name of the Ollama embedding model to use.'
        )

    def handle(self, *args, **options):
        cortex_file_path = options['cortex_file']
        model_name = options['model']

        if not os.path.exists(cortex_file_path):
            self.stdout.write(self.style.ERROR(f"Error: File not found at '{cortex_file_path}'"))
            return

        self.stdout.write(self.style.NOTICE(f"Starting Ollama indexing for {cortex_file_path} using model '{model_name}'..."))

        try:
            # Pass the model name to the indexer
            indexer = EmbeddingIndexer(model_name=model_name)
            indexer.process_cortex(cortex_file_path)
            self.stdout.write(self.style.SUCCESS('✓ Ollama indexing process completed successfully.'))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'An unexpected error occurred during indexing: {e}'))

--- FILE_END: backend/ingestion/management/commands/run_indexer.py ---


--- FILE_START: backend/ingestion/management/commands/search.py ---
# In ingestion/management/commands/search.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index # <-- Import our new function

class Command(BaseCommand):
    help = 'Searches a Faiss index for a given query string.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The search query string.')
        parser.add_argument('--model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use.')
        parser.add_argument('--k', type=int, default=5, help='The number of results to return.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        model = options['model']
        k = options['k']

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        self.stdout.write(self.style.NOTICE(f"Searching for '{query}'..."))

        try:
            results = search_index(
                query_text=query,
                model_name=model,
                index_path=index_path,
                map_path=map_path,
                k=k
            )

            self.stdout.write(self.style.SUCCESS(f"\n--- Top {len(results)} search results ---"))
            for i, res in enumerate(results):
                self.stdout.write(self.style.HTTP_INFO(f"\n{i+1}. File: {res['file_path']} (Distance: {res['distance']:.4f})"))
                self.stdout.write(f"Chunk ID: {res['chunk_id']}")
                self.stdout.write("---")
                # Print the first few lines of the text chunk
                content_preview = "\n".join(res['text'].splitlines()[:5])
                self.stdout.write(content_preview)
                self.stdout.write("...")

        except FileNotFoundError:
            self.stdout.write(self.style.ERROR(f"Could not find index files for '{repo_id}'. Please run the indexer first."))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred: {e}"))

--- FILE_END: backend/ingestion/management/commands/search.py ---


--- FILE_START: backend/ingestion/management/commands/inspect_graph.py ---
# backend/ingestion/management/commands/inspect_graph.py

import json
from pathlib import Path
from collections import defaultdict
from django.core.management.base import BaseCommand
from rich.console import Console
from rich.tree import Tree
from rich.panel import Panel


class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and displays its architectural graph in a human-readable format.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')

    def handle(self, *args, **options):
        console = Console()
        cortex_file_path = Path(options['cortex_file'])

        if not cortex_file_path.exists():
            console.print(f"[bold red]Error: File not found at '{cortex_file_path}'[/bold red]")
            return

        data = self._load_json(console, cortex_file_path)
        if data is None:
            return

        graph_data = data.get('architectural_graph')
        if not graph_data:
            self._print_graph_not_found(console)
            return

        self._display_graph(console, data['repo_id'], graph_data)

    def _load_json(self, console: Console, path: Path):
        try:
            console.print(f"🔎 Reading Cortex file: [cyan]{path}[/cyan]")
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            console.print(f"[bold red]Error: Invalid JSON in file '{path}'[/bold red]")
            return None

    def _print_graph_not_found(self, console: Console):
        console.print(
            Panel(
                "This Project Cortex file was generated before 'The Cartographer' was implemented or the project contains no Python files.\nNo architectural graph is available to display.",
                title="[yellow]Architectural Graph Not Found[/yellow]",
                border_style="yellow",
                expand=False
            )
        )

    def _display_graph(self, console: Console, repo_id: str, graph_data: dict):
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])
        edges_by_source = defaultdict(list)
        for edge in edges:
            edges_by_source[edge['source']].append(edge)

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]")
        file_tree_nodes = {}

        # First pass: Build file, class, and function structure
        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                file_branch = tree.add(f"📄 [bold green]{node_id}[/bold green]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")
                    for method_name in sorted(nodes.get(class_id, {}).get('methods', [])):
                        class_branch.add(f"  - 🐍 [dim]def[/dim] {method_name}()")

                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"🐍 [dim]def[/dim] {func_name}()")

        # Second pass: Add import/call edges
        for source_id, edge_list in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]
                for edge in edge_list:
                    target = edge.get('target', 'Unknown')
                    edge_type = edge.get('type', 'RELATES_TO')
                    if edge_type == 'IMPORTS':
                        parent_branch.add(f"📥 [dim]imports[/dim] [yellow]{target}[/yellow]")
                    elif edge_type == 'CALLS':
                        parent_branch.add(f"📞 [dim]calls[/dim] [magenta]{target}[/magenta]")

        console.print(tree)
        console.print("\n[bold magenta]------------------------------------[/bold magenta]")

--- FILE_END: backend/ingestion/management/commands/inspect_graph.py ---


--- FILE_START: backend/ingestion/management/commands/ingest_repo.py ---
# backend/ingestion/management/commands/ingest_repo.py

from django.core.management.base import BaseCommand
from rich.console import Console
from rich.panel import Panel
import traceback

from lumiere_core.services import ingestion_service

class Command(BaseCommand):
    help = 'Runs the full ingestion pipeline (clone, embed, index) for a single repository URL.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The full URL of the GitHub repository to ingest.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')

    def handle(self, *args, **options):
        console = Console()
        repo_url = options['repo_url']
        embedding_model = options['embedding_model']

        console.print(
            Panel(
                f"[bold]Starting full ingestion for:[/] [cyan]{repo_url}[/cyan]\n"
                f"[bold]Using embedding model:[/] [yellow]{embedding_model}[/yellow]",
                title="🚀 Lumière Ingestion Service",
                border_style="blue"
            )
        )

        try:
            result = ingestion_service.clone_and_embed_repository(
                repo_url=repo_url,
                embedding_model=embedding_model
            )

            if result.get("status") == "success":
                console.print(
                    Panel(
                        f"[bold green]✓ Success![/bold green]\n{result.get('message', 'Ingestion complete.')}",
                        title="✅ Mission Complete",
                        border_style="green"
                    )
                )
                repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

                # --- THE FIX: Update the output path to match the new structure ---
                cortex_file_path = f"backend/cloned_repositories/{repo_id}/{repo_id}_cortex.json"
                console.print(f"\n[dim]To inspect the graph, run:[/dim]\n[bold cyan]python backend/manage.py inspect_graph {cortex_file_path}[/bold cyan]")

            else:
                error_details = result.get('details', result.get('error', 'An unknown error occurred.'))
                console.print(
                    Panel(
                        f"[bold red]✗ Ingestion Failed[/bold red]\n\n[yellow]Reason:[/yellow] {error_details}",
                        title="🚨 Error",
                        border_style="red"
                    )
                )

        except Exception as e:
            console.print(
                Panel(
                    f"[bold red]An unexpected critical error occurred:[/bold red]\n\n{traceback.format_exc()}",
                    title="💥 Critical Failure",
                    border_style="red"
                )
            )

--- FILE_END: backend/ingestion/management/commands/ingest_repo.py ---


--- FILE_START: backend/ingestion/management/commands/run_crawler.py ---
# In backend/ingestion/management/commands/run_crawler.py

import json
import traceback
from django.core.management.base import BaseCommand
from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier

class Command(BaseCommand):
    help = 'Clones a Git repository, creates the Project Cortex JSON, and saves it.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The URL of the Git repository to clone.')

    def handle(self, *args, **options):
        repo_url = options['repo_url']
        # Generate the repo_id just like the API does.
        repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

        self.stdout.write(self.style.NOTICE(f'Starting process for {repo_id} ({repo_url})...'))

        try:
            # --- FIX: Use the IntelligentCrawler as a context manager ---
            # The `with` statement correctly handles the setup (cloning) and
            # teardown (cleanup) of the temporary repository directory.
            with IntelligentCrawler(repo_url=repo_url) as crawler:
                # The cloning is now handled automatically when the 'with' block is entered.
                # We simply need to get the list of files to process.
                files_to_process = crawler.get_file_paths()

                if files_to_process:
                    self.stdout.write(self.style.SUCCESS(f'\nFound {len(files_to_process)} files. Starting JSON-ification...'))

                    # We now correctly pass the crawler's repo_path attribute.
                    jsonifier = Jsonifier(
                        file_paths=files_to_process,
                        repo_root=crawler.repo_path,
                        repo_id=repo_id
                    )
                    project_cortex = jsonifier.generate_cortex()

                    output_filename = f"{repo_id}_cortex.json"
                    with open(output_filename, 'w', encoding='utf-8') as f:
                        json.dump(project_cortex, f, indent=2)

                    self.stdout.write(self.style.SUCCESS(f'✓ Project Cortex created successfully: {output_filename}'))
                    self.stdout.write(self.style.NOTICE(f"\nNext Step: Run the indexer command:"))
                    self.stdout.write(self.style.SUCCESS(f"python manage.py run_indexer {output_filename}"))


                else:
                    self.stdout.write(self.style.WARNING('No files found to process or an error occurred.'))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f'\nAn unexpected error occurred: {e}'))
            self.stdout.write(self.style.ERROR('--- Full Traceback ---'))
            traceback.print_exc()
            self.stdout.write(self.style.ERROR('--- End Traceback ---'))
        # NOTE: No explicit crawler.cleanup() is needed here because the
        # `with` statement guarantees cleanup even if errors occur.

--- FILE_END: backend/ingestion/management/commands/run_crawler.py ---


--- FILE_START: backend/ingestion/__init__.py ---

--- FILE_END: backend/ingestion/__init__.py ---


--- FILE_START: backend/ingestion/apps.py ---
from django.apps import AppConfig


class IngestionConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "ingestion"

--- FILE_END: backend/ingestion/apps.py ---


--- FILE_START: backend/ingestion/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/ingestion/admin.py ---


--- FILE_START: backend/ingestion/jsonifier.py ---
import json
import pathlib
import datetime
import ast
from typing import List, Dict, TypedDict, Optional, Any

from lumiere_core.services import cartographer


class TextChunk(TypedDict):
    chunk_id: str
    chunk_text: str
    token_count: int
    chunk_type: str


class FileCortex(TypedDict):
    file_path: str
    file_size_kb: float
    raw_content: str
    code_smells: List[str]
    ast_summary: str
    text_chunks: List[TextChunk]


class ProjectCortex(TypedDict):
    repo_id: str
    last_crawled_utc: str
    project_health_score: float
    project_structure_tree: str
    github_metadata: Dict
    files: List[FileCortex]
    architectural_graph: Optional[Dict[str, Any]]


class CodeChunker(ast.NodeVisitor):
    """
    AST visitor that extracts function and class definitions as code chunks.
    """
    def __init__(self, source_code: str):
        self.source_code = source_code
        self.chunks = []

    def visit_FunctionDef(self, node: ast.FunctionDef):
        chunk_text = ast.get_source_segment(self.source_code, node)
        if chunk_text:
            self.chunks.append({"text": chunk_text, "type": "function_definition"})

    def visit_ClassDef(self, node: ast.ClassDef):
        chunk_text = ast.get_source_segment(self.source_code, node)
        if chunk_text:
            self.chunks.append({"text": chunk_text, "type": "class_definition"})


class Jsonifier:
    """
    Reads a list of files, chunks their content, runs the Cartographer,
    and builds the Project Cortex JSON object.
    """
    def __init__(self, file_paths: List[pathlib.Path], repo_root: pathlib.Path, repo_id: str):
        self.file_paths = file_paths
        self.repo_root = repo_root
        self.repo_id = repo_id

    def _read_file_content(self, file_path: pathlib.Path) -> str:
        try:
            return file_path.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            return file_path.read_text(encoding='latin-1', errors='replace')

    def _chunk_python_file(self, content: str) -> List[Dict[str, str]]:
        try:
            tree = ast.parse(content)
            chunker = CodeChunker(content)
            chunker.visit(tree)
            return chunker.chunks
        except SyntaxError:
            return [{"text": line, "type": "line"} for line in content.splitlines() if line.strip()]


    def generate_cortex(self) -> ProjectCortex:
        """
        Processes all repository files to generate the Project Cortex JSON.

        This method now acts as a pre-processor for the Polyglot Cartographer:
        - For Python files (.py), it generates a standard Python AST.
        - For JavaScript files (.js, .gs), it notes them for JS parsing.
        - For all files, it generates text chunks for vector indexing (RAG).
        - It then calls the Cartographer with the collected ASTs and file content.
        """
        all_files_cortex: List[FileCortex] = []

        # This dictionary will hold analysis data (ASTs, content) for ALL supported languages,
        # ready to be passed to the Polyglot Cartographer.
        files_for_cartographer: Dict[str, Dict[str, Any]] = {}

        for file_path in self.file_paths:
            content = self._read_file_content(file_path)
            relative_path_str = str(file_path.relative_to(self.repo_root))
            file_ext = file_path.suffix

            raw_chunks = []
            if file_ext == '.py':
                try:
                    tree = ast.parse(content)
                    # 1. Prepare data for the Cartographer (Python AST)
                    files_for_cartographer[relative_path_str] = {"language": "python", "ast": tree}
                    # 2. Prepare data for RAG/Indexing (detailed chunks)
                    chunker = CodeChunker(content)
                    chunker.visit(tree)
                    raw_chunks = chunker.chunks
                except SyntaxError:
                    # Fallback for RAG if Python parsing fails
                    raw_chunks = [{"text": line, "type": "line"} for line in content.splitlines() if line.strip()]

            # NEW: Handle JavaScript files for the Polyglot Cartographer
            elif file_ext in ['.js', '.gs', '.mjs']:
                # 1. Prepare data for the Cartographer (raw JS content)
                files_for_cartographer[relative_path_str] = {"language": "javascript", "content": content}
                # 2. Prepare data for RAG/Indexing (simple paragraph chunks)
                raw_chunks = [{"text": chunk, "type": "paragraph"} for chunk in content.split('\n\n') if chunk.strip()]

            else:
                # For all other file types, just do simple paragraph chunking for RAG.
                # The Cartographer will ignore these.
                raw_chunks = [{"text": chunk, "type": "paragraph"} for chunk in content.split('\n\n') if chunk.strip()]

            # This part remains the same: process whatever chunks were generated above.
            text_chunks: List[TextChunk] = []
            for i, chunk_data in enumerate(raw_chunks):
                chunk_id = f"{self.repo_id}_{relative_path_str}_{i}"
                chunk_text = chunk_data["text"]
                text_chunks.append({
                    "chunk_id": chunk_id,
                    "chunk_text": chunk_text,
                    "token_count": len(chunk_text.split()),
                    "chunk_type": chunk_data["type"],
                })

            file_cortex: FileCortex = {
                "file_path": relative_path_str,
                "file_size_kb": round(file_path.stat().st_size / 1024, 2),
                "raw_content": content,
                "code_smells": [],
                "ast_summary": json.dumps({}),
                "text_chunks": text_chunks,
            }

            all_files_cortex.append(file_cortex)

        # Call the Polyglot Cartographer only if there are supported files to analyze.
        architectural_graph = None
        if files_for_cartographer:
            architectural_graph = cartographer.generate_graph(files_for_cartographer)

        # Assemble the final Project Cortex object.
        return {
            "repo_id": self.repo_id,
            "last_crawled_utc": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "project_health_score": 0.0,
            "project_structure_tree": "...",
            "github_metadata": {},
            "files": all_files_cortex,
            "architectural_graph": architectural_graph,
        }

--- FILE_END: backend/ingestion/jsonifier.py ---


--- FILE_START: backend/ingestion/indexing.py ---
# backend/ingestion/indexing.py

import json
import numpy as np
import faiss
from pathlib import Path
from lumiere_core.services.ollama import get_ollama_embeddings
from tqdm import tqdm

class EmbeddingIndexer:
    """
    Loads a Project Cortex JSON, generates embeddings via Ollama,
    and saves the Faiss index and the ID-to-chunk mapping into the SAME
    directory as the source cortex file.
    """
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.dimension = None

    def _extract_repo_id_from_cortex(self, project_cortex: dict, cortex_path_obj: Path) -> str:
        """
        Extract repo_id from cortex with fallback strategies for backward compatibility.

        Args:
            project_cortex: The loaded cortex JSON data
            cortex_path_obj: Path object of the cortex file

        Returns:
            A valid repo_id string
        """
        # Primary: Try to get repo_id from the cortex data
        if 'repo_id' in project_cortex and project_cortex['repo_id']:
            return project_cortex['repo_id']

        # Fallback 1: Extract from cortex filename (assumes format: {repo_id}_cortex.json)
        cortex_filename = cortex_path_obj.stem  # Gets filename without extension
        if cortex_filename.endswith('_cortex'):
            potential_repo_id = cortex_filename[:-7]  # Remove '_cortex' suffix
            if potential_repo_id:
                print(f"Warning: repo_id not found in cortex data, using filename-derived ID: {potential_repo_id}")
                return potential_repo_id

        # Fallback 2: Use the parent directory name
        parent_dir_name = cortex_path_obj.parent.name
        if parent_dir_name and parent_dir_name != 'cloned_repositories':
            print(f"Warning: repo_id not found, using parent directory name: {parent_dir_name}")
            return parent_dir_name

        # Fallback 3: Generate from cortex filename
        fallback_id = cortex_filename.replace('_cortex', '') if '_cortex' in cortex_filename else cortex_filename
        print(f"Warning: Using fallback repo_id derived from filename: {fallback_id}")
        return fallback_id or 'unknown_repo'

    def process_cortex(self, cortex_file_path: str):
        """
        Main method to load cortex, create embeddings, and build the index.
        """
        cortex_path_obj = Path(cortex_file_path)

        # Validate that the cortex file exists
        if not cortex_path_obj.exists():
            raise FileNotFoundError(f"Cortex file not found: {cortex_path_obj}")

        print(f"Loading Project Cortex from: {cortex_path_obj}")

        # --- THIS IS THE KEY FIX ---
        # Derive the output directory from the location of the cortex file.
        output_dir = cortex_path_obj.parent

        try:
            with open(cortex_path_obj, 'r', encoding='utf-8') as f:
                project_cortex = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in cortex file {cortex_path_obj}: {e}")
        except Exception as e:
            raise IOError(f"Error reading cortex file {cortex_path_obj}: {e}")

        # 1. Collect all text chunks and their IDs
        all_chunks_text = []
        all_chunk_ids = []
        id_to_chunk_map = {}

        files_data = project_cortex.get('files', [])
        if not files_data:
            print("Warning: No 'files' array found in cortex data.")
            return

        for file_data in files_data:
            if not isinstance(file_data, dict):
                print(f"Warning: Skipping invalid file data entry: {file_data}")
                continue

            file_path = file_data.get('file_path', 'unknown_file')
            text_chunks = file_data.get('text_chunks', [])

            for chunk in text_chunks:
                if not isinstance(chunk, dict):
                    print(f"Warning: Skipping invalid chunk in {file_path}: {chunk}")
                    continue

                chunk_text = chunk.get('chunk_text', '')
                chunk_id = chunk.get('chunk_id')

                if not chunk_text or not chunk_id:
                    print(f"Warning: Skipping chunk with missing text or ID in {file_path}")
                    continue

                all_chunks_text.append(chunk_text)
                all_chunk_ids.append(chunk_id)
                id_to_chunk_map[chunk_id] = {
                    "text": chunk_text,
                    "file_path": file_path
                }

        if not all_chunks_text:
            print("No valid text chunks found. Exiting.")
            return

        print(f"Found {len(all_chunks_text)} text chunks to embed using Ollama model '{self.model_name}'.")

        # 2. Generate embeddings using our Ollama service
        try:
            embeddings_list = get_ollama_embeddings(all_chunks_text, model_name=self.model_name)
        except Exception as e:
            print(f"Error generating embeddings: {e}")
            return

        if not embeddings_list:
            print("Embedding generation failed. Exiting.")
            return

        # Validate embeddings
        if len(embeddings_list) != len(all_chunks_text):
            print(f"Error: Mismatch between chunks ({len(all_chunks_text)}) and embeddings ({len(embeddings_list)})")
            return

        self.dimension = len(embeddings_list[0])
        print(f"Ollama model '{self.model_name}' produced embeddings with dimension: {self.dimension}")

        embeddings = np.array(embeddings_list).astype('float32')

        # 3. Create and populate the Faiss index
        print("Creating Faiss index...")
        try:
            index = faiss.IndexFlatL2(self.dimension)
            index.add(embeddings)
            print(f"Faiss index created. Total vectors in index: {index.ntotal}")
        except Exception as e:
            print(f"Error creating Faiss index: {e}")
            return

        # 4. Save the artifacts
        # Use the enhanced repo_id extraction method
        repo_id = self._extract_repo_id_from_cortex(project_cortex, cortex_path_obj)

        # Use the 'output_dir' to save files in the correct location
        index_filename = output_dir / f"{repo_id}_faiss.index"
        map_filename = output_dir / f"{repo_id}_id_map.json"

        print(f"Saving Faiss index to: {index_filename}")
        try:
            faiss.write_index(index, str(index_filename))
        except Exception as e:
            print(f"Error saving Faiss index: {e}")
            return

        print(f"Saving ID-to-Chunk mapping to: {map_filename}")
        save_data = {
            "faiss_id_to_chunk_id": all_chunk_ids,
            "chunk_id_to_data": id_to_chunk_map
        }

        try:
            with open(map_filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2)
        except Exception as e:
            print(f"Error saving ID mapping: {e}")
            return

        print("Indexing complete.")

--- FILE_END: backend/ingestion/indexing.py ---


--- FILE_START: backend/ingestion/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/ingestion/tests.py ---


--- FILE_START: backend/ingestion/views.py ---
from django.shortcuts import render

# Create your views here.

--- FILE_END: backend/ingestion/views.py ---


--- FILE_START: backend/ingestion/crawler.py ---
# In ingestion/crawler.py
import subprocess
import tempfile
import pathlib
from typing import List, Optional, Union, Dict

class IntelligentCrawler:
    """
    Clones a Git repository and performs file operations safely.
    Includes path-finding, git blame, and git diff capabilities.
    """

    def __init__(self, repo_url: str):
        """
        Initializes the crawler with the repository URL.
        """
        self.repo_url = repo_url
        self.temp_dir_handle = tempfile.TemporaryDirectory()
        self.repo_path = pathlib.Path(self.temp_dir_handle.name)
        self._file_paths_cache: Optional[List[pathlib.Path]] = None

    def __enter__(self):
        """
        Enters the context manager, cloning the repository.
        """
        print(f"Entering context: Cloning {self.repo_url} into {self.repo_path}")
        self._clone_repo()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Exits the context manager, cleaning up resources.
        """
        print("Exiting context: Cleaning up resources.")
        self.cleanup()

    def _clone_repo(self):
        """
        Clones the full git repository, including all branches and tags.
        """
        try:
            # '--mirror' is too aggressive, '--bare' isn't a working tree.
            # We will clone normally and then fetch all tags and branches.
            subprocess.run(
                ['git', 'clone', self.repo_url, str(self.repo_path)],
                check=True, capture_output=True, text=True
            )
            # After cloning, fetch all tags and remote branches explicitly.
            # 'git fetch origin --tags' and 'git fetch origin' ensures everything is available.
            subprocess.run(['git', 'fetch', 'origin', '--tags'], cwd=self.repo_path, check=True, capture_output=True, text=True)
            subprocess.run(['git', 'remote', 'update'], cwd=self.repo_path, check=True, capture_output=True, text=True)

            print(f"Repository cloned successfully and all refs fetched from {self.repo_path}")
        except subprocess.CalledProcessError as e:
            print(f"Error cloning repository: {e.stderr.strip()}")
            raise

    def get_blame_for_file(self, target_file: str) -> str:
        """
        Runs `git blame` on a specific file in the repo.
        """
        file_full_path = self.repo_path / target_file
        if not file_full_path.exists():
            return f"Error from crawler: File '{target_file}' does not exist in the repository."

        try:
            print(f"Running 'git blame' on {file_full_path}...")
            result = subprocess.run(
                ['git', 'blame', '--show-email', str(file_full_path)],
                cwd=self.repo_path, check=True, capture_output=True, text=True
            )
            return result.stdout
        except subprocess.CalledProcessError as e:
            error_message = f"Error running 'git blame' on '{target_file}': {e.stderr.strip()}"
            print(error_message)
            return error_message

    def get_diff_for_branch(self, ref_name: str, base_ref: str = 'main') -> str:
        """
        [Final Version] Gets the `git diff` between two refs (branch, tag, or commit).
        This version is robust and relies on the three-dot diff syntax.
        """
        try:
            print(f"Attempting to calculate diff for '{ref_name}' against base '{base_ref}'...")

            # The '...' syntax finds the diff from the common ancestor, which is what a
            # code review for a PR/feature branch usually wants. We prepend 'origin/'
            # to ensure we're comparing against the fetched remote state.
            # For tags, they don't need 'origin/'. Git is smart enough.
            # Let's check if the ref is a tag.

            # To make this truly robust, we'll try to resolve the refs first.
            # Let's stick to the simplest, most powerful git syntax.

            # The refs are specified as 'origin/<branch_name>' for remote branches.
            # Tags are just referred to by their name. Git resolves this automatically
            # if we have fetched all data. The logic here simplifies to trying 'main'
            # then 'master' as a base.

            diff_command = ['git', 'diff', f'origin/{base_ref}...{ref_name}']

            print(f"   -> Running command: {' '.join(diff_command)}")
            result = subprocess.run(
                diff_command,
                cwd=self.repo_path,
                check=True,
                capture_output=True,
                text=True
            )
            return result.stdout
        except subprocess.CalledProcessError as e:
            # If the command failed, it might be because the base is 'master' not 'main'.
            if base_ref == 'main':
                print(f"   -> Diff against 'origin/main' failed. Trying 'origin/master' as base...")
                return self.get_diff_for_branch(ref_name, 'master')

            error_message = f"Error running 'git diff' between '{base_ref}' and '{ref_name}': {e.stderr.strip()}"
            print(error_message)
            return f"Error from crawler: {error_message}"


    def find_file_path(self, target_filename: str) -> Union[str, Dict, None]:
        """
        Searches the repository for a file by its name.
        """
        print(f"Searching for file matching '{target_filename}'...")
        all_files = self.get_file_paths()

        possible_matches = []
        for file_path in all_files:
            relative_path = file_path.relative_to(self.repo_path)
            if relative_path.name == target_filename or str(relative_path).endswith('/' + target_filename):
                possible_matches.append(relative_path)

        if not possible_matches:
            print(f"   -> No match found for '{target_filename}'.")
            return None

        if len(possible_matches) == 1:
            match = str(possible_matches[0])
            print(f"   -> Found unique match: {match}")
            return match

        print(f"   -> Found multiple matches: {[str(p) for p in possible_matches]}. Checking for a definitive root-level file.")
        root_matches = [p for p in possible_matches if len(p.parts) == 1]

        if len(root_matches) == 1:
            match = str(root_matches[0])
            print(f"   -> Prioritized unique root match: {match}")
            return match

        print(f"   -> Ambiguity detected. Multiple candidates found. Reporting conflict.")
        return {
            "error": "ambiguous_path",
            "message": f"Multiple files found matching '{target_filename}'. Please specify one.",
            "options": [str(p) for p in possible_matches]
        }


    def get_file_paths(self) -> List[pathlib.Path]:
        """
        Scans the cloned repo and returns a list of relevant files.
        Caches the result for performance.
        """
        if self._file_paths_cache is not None:
            return self._file_paths_cache

        print("Scanning for relevant files...")
        files_to_process = []
        included_extensions = [
            # Python ecosystem
            '*.py', '*.pyx', '*.pyi', '*.pyw',

            # Documentation
            '*.md', '*.txt', '*.rst', '*.adoc', '*.asciidoc',

            # Configuration files
            '*.json', '*.toml', '*.yaml', '*.yml', '*.ini', '*.cfg', '*.conf',
            '*.xml', '*.properties', '*.env', '*.dotenv',

            # JavaScript/TypeScript ecosystem
            '*.js', '*.mjs', '*.cjs', '*.ts', '*.tsx', '*.jsx',
            '*.vue', '*.svelte', '*.astro',

            # Web technologies
            '*.html', '*.htm', '*.xhtml', '*.css', '*.scss', '*.sass', '*.less',
            '*.styl', '*.stylus',

            # Compiled languages
            '*.java', '*.kt', '*.kts', '*.scala', '*.groovy',  # JVM languages
            '*.c', '*.cpp', '*.cxx', '*.cc', '*.h', '*.hpp', '*.hxx',  # C/C++
            '*.cs', '*.vb', '*.fs', '*.fsx',  # .NET languages
            '*.go', '*.mod', '*.sum',  # Go
            '*.rs', '*.rlib',  # Rust
            '*.swift',  # Swift
            '*.m', '*.mm',  # Objective-C

            # Scripting languages
            '*.rb', '*.rake', '*.gemspec',  # Ruby
            '*.php', '*.phtml',  # PHP
            '*.pl', '*.pm', '*.t',  # Perl
            '*.lua',  # Lua
            '*.r', '*.R',  # R
            '*.jl',  # Julia
            '*.dart',  # Dart

            # Shell scripts
            '*.sh', '*.bash', '*.zsh', '*.fish', '*.csh', '*.tcsh',
            '*.bat', '*.cmd', '*.ps1', '*.psm1',

            # Database
            '*.sql', '*.sqlite', '*.db',

            # Data formats
            '*.csv', '*.tsv', '*.jsonl', '*.ndjson',
            '*.parquet', '*.avro', '*.orc',

            # Infrastructure as Code
            '*.tf', '*.tfvars', '*.hcl',  # Terraform
            '*.k8s.yaml', '*.k8s.yml',  # Kubernetes (if you want to be specific)

            # CI/CD
            '*.gitlab-ci.yml', '*.github/workflows/*.yml', '*.github/workflows/*.yaml',
            '*.jenkinsfile', 'Jenkinsfile',

            # Mobile development
            '*.gradle', '*.gradle.kts',  # Android

            # Game development
            '*.cs', '*.shader', '*.hlsl', '*.glsl',  # Unity/Graphics
            '*.gd', '*.tres', '*.tscn',  # Godot

            # Functional languages
            '*.hs', '*.lhs',  # Haskell
            '*.ml', '*.mli',  # OCaml
            '*.elm',  # Elm
            '*.clj', '*.cljs', '*.cljc', '*.edn',  # Clojure
            '*.ex', '*.exs',  # Elixir
            '*.erl', '*.hrl',  # Erlang

            # Markup and templating
            '*.tex', '*.latex',  # LaTeX
            '*.handlebars', '*.hbs', '*.mustache',  # Templates
            '*.twig', '*.blade.php', '*.erb',  # Server-side templates

            # Special Google/Cloud files
            '*.gs',  # Google Apps Script
            '*.clasp.json',  # Google Apps Script

            # Build files and configs (without extensions)
            'Dockerfile', 'Containerfile', 'docker-compose.yml', 'docker-compose.yaml',
            'Makefile', 'makefile', 'GNUmakefile',
            'CMakeLists.txt', 'meson.build',
            'package.json', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
            'requirements.txt', 'requirements-dev.txt', 'Pipfile', 'Pipfile.lock',
            'pyproject.toml', 'setup.py', 'setup.cfg', 'poetry.lock',
            'Cargo.toml', 'Cargo.lock',
            'go.mod', 'go.sum',
            'build.gradle', 'build.gradle.kts', 'settings.gradle',
            'pom.xml', 'build.xml',
            'LICENSE', 'LICENSE.txt', 'LICENSE.md',
            'README', 'README.txt', 'README.md', 'README.rst',
            'CHANGELOG', 'CHANGELOG.md', 'CHANGELOG.txt',
            'CONTRIBUTING.md', 'CODE_OF_CONDUCT.md',
            '.gitignore', '.gitattributes', '.editorconfig',
            '.eslintrc.js', '.eslintrc.json', '.prettierrc',
            'tsconfig.json', 'jsconfig.json',
            'webpack.config.js', 'vite.config.js', 'rollup.config.js',
            'tailwind.config.js', 'postcss.config.js',
            'babel.config.js', '.babelrc',
            'jest.config.js', 'vitest.config.js',
            'tox.ini', 'pytest.ini', '.coveragerc',
            'mypy.ini', '.mypy.ini', 'flake8.cfg', '.flake8',
            'black.toml', 'isort.cfg', '.isort.cfg'
        ]
        excluded_dirs = {'.git', '__pycache__', 'venv', 'node_modules', '.vscode', '.idea', 'dist', 'build'}

        for file_path in self.repo_path.rglob('*'):
            if any(part in excluded_dirs for part in file_path.relative_to(self.repo_path).parts):
                continue
            if file_path.is_file() and any(file_path.match(ext) for ext in included_extensions):
                files_to_process.append(file_path)

        self._file_paths_cache = files_to_process
        print(f"Found and cached {len(files_to_process)} files to process.")
        return self._file_paths_cache

    def cleanup(self):
        """
        Removes the temporary directory and all its contents.
        """
        self.temp_dir_handle.cleanup()
        print(f"Cleaned up temporary directory: {self.repo_path}")

--- FILE_END: backend/ingestion/crawler.py ---


--- FILE_START: backend/requirements.txt ---
# In backend/requirements.txt

# Django Core
Django
djangorestframework

# LLM & Vector Database
ollama
faiss-cpu
numpy
tqdm

# Web Scraping (Legacy, but keep for now)
requests
beautifulsoup4

# GitHub API Client
PyGithub

# --- loading .env files ---
python-dotenv

# --- The Conductor CLI ---
typer[all]
rich
prompt-toolkit

# --- The Crucible service ---
docker

# --- Google Gemini API ---
google-generativeai

#  --- The Polyglot Cartographer ---
tree_sitter==0.20.1

--- FILE_END: backend/requirements.txt ---


--- FILE_START: backend/lumiere_core/asgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/asgi.py
"""
ASGI config for backend project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_asgi_application()

--- FILE_END: backend/lumiere_core/asgi.py ---


--- FILE_START: backend/lumiere_core/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/__init__.py

--- FILE_END: backend/lumiere_core/__init__.py ---


--- FILE_START: backend/lumiere_core/.gitignore ---
# In ~/lumiere_semantique/backend/lumiere_core/.gitignore
# Python
__pycache__/
*.pyc

# Virtual Environment
venv/

# Django
db.sqlite3
*.log

# Environment variables
.env

--- FILE_END: backend/lumiere_core/.gitignore ---


--- FILE_START: backend/lumiere_core/.env ---
ANTHROPIC_API_KEY="sk-ant-your-api-key-here"

GITHUB_ACCESS_TOKEN="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"

# The GitHub username the agent will use to fork repos and create PRs.
GITHUB_FORK_USERNAME="LatchyCat"

--- FILE_END: backend/lumiere_core/.env ---


--- FILE_START: backend/lumiere_core/settings.py ---
# In ~/lumiere_semantique/backend/lumiere_core/settings.py
"""
Django settings for lumiere_core project.

Generated by 'django-admin startproject' using Django 5.2.3.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.2/ref/settings/
"""

from pathlib import Path
from dotenv import load_dotenv

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent

load_dotenv(BASE_DIR / '.env')

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = "django-insecure-7f#&l-vg3lb9%s5lkx!352hf2^&!w%ro6wa97*kqm@8+d94*67"

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",

    # --- Third-party apps ---
    'rest_framework',

    # --- Our local apps ---
    'ingestion',
    'api',
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
]

# This should already be correct from our previous fixes.
ROOT_URLCONF = 'lumiere_core.urls'

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

# This should also be correct, but ensure it points to 'lumiere_core'.
WSGI_APPLICATION = "lumiere_core.wsgi.application"


# Database
# https://docs.djangoproject.com/en/5.2/ref/settings/#databases

DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.sqlite3",
        "NAME": BASE_DIR / "db.sqlite3",
    }
}


# Password validation
# https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        "NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",
    },
]


# Internationalization
# https://docs.djangoproject.com/en/5.2/topics/i18n/

LANGUAGE_CODE = "en-us"

TIME_ZONE = "UTC"

USE_I18N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/5.2/howto/static-files/

STATIC_URL = "static/"

# Default primary key field type
# https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"

# --- Add this section for Django REST Framework ---
# This allows DRF to have sensible default settings.
REST_FRAMEWORK = {
    'DEFAULT_RENDERER_CLASSES': [
        'rest_framework.renderers.JSONRenderer',
    ],
    # Use BrowsableAPIRenderer only during development for easier debugging
    'DEFAULT_PARSER_CLASSES': [
        'rest_framework.parsers.JSONParser',
    ]
}

--- FILE_END: backend/lumiere_core/settings.py ---


--- FILE_START: backend/lumiere_core/urls.py ---
# In ~/lumiere_semantique/backend/lumiere_core/urls.py
# In lumiere_core/urls.py

from django.contrib import admin
from django.urls import path, include # <-- Make sure 'include' is imported

urlpatterns = [
    path('admin/', admin.site.urls),

    # This line tells Django that any URL starting with 'api/v1/'
    # should be handled by the URL patterns defined in our 'api.urls' file.
    path('api/v1/', include('api.urls')),
]

--- FILE_END: backend/lumiere_core/urls.py ---


--- FILE_START: backend/lumiere_core/services/documentation.py ---
# backend/lumiere_core/services/documentation.py
from typing import Dict
# --- All services should import the master llm_service ---
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output


def generate_docstring_for_code(repo_id: str, new_code: str, instruction: str, model_identifier: str) -> Dict[str, str]:
    """
    The core logic for the Chronicler Agent (Documentation).
    It finds existing docstring patterns in the repo and uses them as a style guide
    to generate a new docstring for the provided code.

    Args:
        repo_id: Identifier for the repository
        new_code: The code that needs documentation
        instruction: Instructions for docstring generation
        model_identifier: The model to use for generation

    Returns:
        Dict containing the generated docstring
    """
    print(f"✒️  Initiating Chronicler Agent for repo '{repo_id}'")

    # Step 1: Find Existing Documentation Patterns with RAG
    print("   -> Step 1: Finding existing docstring patterns with RAG...")
    search_query = f"Example docstrings in Python code for a function about: {instruction}"

    try:
        # --- CORRECTED CALL: Pass repo_id directly ---
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=5
        )
    except Exception as e:
        print(f"   -> Warning: RAG search failed for Chronicler: {e}. Proceeding without context examples.")
        context_chunks = []

    doc_context_string = ""
    found_files = set()

    for chunk in context_chunks:
        text = chunk.get('text', '').strip()
        file_path = chunk.get('file_path', '')
        if text.startswith(('def ', 'class ')) and file_path not in found_files:
            doc_context_string += f"--- Example from file \"{file_path}\" ---\n{text}\n\n"
            found_files.add(file_path)

    if not doc_context_string:
        doc_context_string = "No specific docstring styles found. Please generate a standard Google-style docstring."
        print("   -> Warning: No existing docstring examples found via RAG.")
    else:
        print(f"   -> Found docstring patterns from files: {list(found_files)}")

    # Step 2: Construct the Docstring Generation Prompt
    print("   -> Step 2: Constructing docstring generation prompt...")
    prompt = f"""You are an expert technical writer specializing in Python documentation.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING DOCSTRING EXAMPLES"** to learn the project's documentation style (e.g., Google, reStructuredText, numpy). Pay attention to sections like `Args:`, `Returns:`, `Raises:`.
2. **Analyze the "CODE TO BE DOCUMENTED"** to understand its parameters, logic, and what it returns.
3. **Write a complete and professional docstring** for the provided code. It is CRITICAL that you exactly match the style of the examples.
4. **Output ONLY the docstring itself.** Do not include the function definition or any other text, just the \"\"\"...\"\"\" block.

---
### EXISTING DOCSTRING EXAMPLES
{doc_context_string}

---
### CODE TO BE DOCUMENTED
```python
{new_code}
```

Now, generate ONLY the docstring for the code above."""

    # Step 3: Generate and Clean the Docstring
    print(f"   -> Step 3: Sending request to model '{model_identifier}'...")
    raw_docstring = llm_service.generate_text(prompt, model_identifier=model_identifier)

    # Step 4: Clean the docstring output
    print("   -> Step 4: Cleaning and finalizing the docstring...")
    final_docstring = clean_llm_code_output(raw_docstring)

    # Remove surrounding triple quotes if present
    if final_docstring.startswith('"""') and final_docstring.endswith('"""'):
        final_docstring = final_docstring[3:-3].strip()

    return {"docstring": final_docstring}

--- FILE_END: backend/lumiere_core/services/documentation.py ---


--- FILE_START: backend/lumiere_core/services/review_service.py ---
# In lumiere_core/services/review_service.py
import uuid
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Optional, List

from .ollama_service import generate_text

# This is a simple in-memory cache for our development server.
REVIEW_ENVIRONMENTS = {}

def _resolve_git_ref(repo_path: Path, ref_name: str) -> Optional[str]:
    """
    [TRUE FINAL VERSION] Verifies a git reference by trying a list of candidates
    to handle common naming variations (e.g., tags with/without 'v' prefix).
    """
    # Build a list of potential candidates to check.
    candidates: List[str] = []

    # 1. Add the ref_name as a potential remote branch.
    candidates.append(f"origin/{ref_name}")

    # 2. Add the ref_name as a direct reference (exact match for a tag, commit, etc.).
    candidates.append(ref_name)

    # 3. Handle the 'v' prefix for version tags, which is the source of the issue.
    if ref_name.startswith('v') and len(ref_name) > 1:
        # If the user provided 'v1.26.15', try '1.26.15' as a direct ref and as a branch.
        ref_without_v = ref_name[1:]
        candidates.append(f"origin/{ref_without_v}")
        candidates.append(ref_without_v)

    # We will now iterate through our candidates and return the first one that is valid.
    print(f"   -> Attempting to resolve '{ref_name}' with candidates: {candidates}")
    for candidate in candidates:
        try:
            # `rev-parse --verify` is the correct, simple tool for this.
            # It exits 0 if the ref is valid and can be resolved, 1 otherwise.
            subprocess.run(
                ['git', 'rev-parse', '--verify', '--quiet', candidate],
                cwd=repo_path, check=True, capture_output=True
            )
            # SUCCESS: The candidate is a valid git object.
            print(f"   -> SUCCESS: Resolved '{ref_name}' as valid git object '{candidate}'")
            return candidate
        except subprocess.CalledProcessError:
            # This candidate failed, continue to the next one.
            continue

    # If the loop completes without finding a valid candidate, the ref does not exist.
    print(f"   -> FAILED: Could not resolve '{ref_name}' with any of the candidates.")
    return None

def prepare_review_environment(repo_url: str, ref_name: str) -> Dict[str, str]:
    """
    [TRUE FINAL VERSION] Clones a repo, fetches all refs, then uses the truly robust
    _resolve_git_ref function to validate and store them for the review.
    """
    review_id = str(uuid.uuid4())
    temp_dir_handle = tempfile.TemporaryDirectory()
    repo_path = Path(temp_dir_handle.name)
    print(f"Preparing review environment {review_id} at {repo_path}")

    try:
        print(f"   -> Cloning {repo_url}...")
        subprocess.run(
            ['git', 'clone', repo_url, str(repo_path)],
            check=True, capture_output=True, text=True
        )

        print("   -> Fetching all data from remote 'origin'...")
        subprocess.run(
            ['git', 'fetch', 'origin', '--prune', '--tags', '--force'],
             cwd=repo_path, check=True, capture_output=True, text=True
        )

        print(f"   -> Resolving feature ref '{ref_name}'...")
        resolved_feature_ref = _resolve_git_ref(repo_path, ref_name)
        if not resolved_feature_ref:
            raise Exception(f"The branch or tag '{ref_name}' could not be found.")

        print("   -> Resolving base branch...")
        resolved_base_ref = _resolve_git_ref(repo_path, 'main')
        if not resolved_base_ref:
            resolved_base_ref = _resolve_git_ref(repo_path, 'master')
        if not resolved_base_ref:
            raise Exception("Could not find a valid base branch ('main' or 'master').")

        REVIEW_ENVIRONMENTS[review_id] = {
            "path": repo_path, "temp_dir_handle": temp_dir_handle,
            "base_ref": resolved_base_ref, "feature_ref": resolved_feature_ref
        }
        print(f"✓ Review environment '{review_id}' is ready.")
        return {"review_id": review_id}

    except Exception as e:
        temp_dir_handle.cleanup()
        error_details = str(e)
        if hasattr(e, 'stderr') and e.stderr and isinstance(e.stderr, str):
            error_details = e.stderr.strip()
        print(f"Error preparing environment: {error_details}")
        raise Exception(error_details)

def get_diff_for_review(review_id: str) -> str:
    env = REVIEW_ENVIRONMENTS.get(review_id)
    if not env: raise FileNotFoundError(f"Review ID '{review_id}' not found or has expired.")
    repo_path, base_ref, feature_ref = env['path'], env['base_ref'], env['feature_ref']
    print(f"   -> Calculating diff for review '{review_id}' between '{base_ref}' and '{feature_ref}'...")
    try:
        diff_command = ['git', 'diff', f'{base_ref}...{feature_ref}']
        result = subprocess.run(diff_command, cwd=repo_path, check=True, capture_output=True, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        raise Exception(f"Failed to generate diff: {e.stderr.strip() if e.stderr else str(e)}")

def cleanup_review_environment(review_id: str) -> bool:
    env = REVIEW_ENVIRONMENTS.pop(review_id, None)
    if env:
        try:
            env['temp_dir_handle'].cleanup()
            print(f"✓ Cleaned up review environment '{review_id}'.")
            return True
        except Exception as e:
            print(f"Warning: Error during cleanup of review environment '{review_id}': {e}")
            return False
    else:
        print(f"Warning: Review environment '{review_id}' not found for cleanup.")
        return False

def review_code_diff(diff_text: str) -> Dict[str, str]:
    if not diff_text.strip(): return {"review": "No changes detected between the references. The review is complete."}
    prompt = f"You are an expert Senior Software Engineer...\n---\nGIT DIFF\n```diff\n{diff_text}\n```\nNow, provide your review."
    try:
        return {"review": generate_text(prompt, model_name='qwen3:4b')}
    except Exception as e:
        return {"review": f"Error occurred during code review analysis: {str(e)}"}

def get_active_review_count() -> int: return len(REVIEW_ENVIRONMENTS)
def list_active_reviews() -> Dict[str, Dict[str, str]]:
    summary = {}
    for r_id, env in REVIEW_ENVIRONMENTS.items():
        summary[r_id] = {"base": env["base_ref"], "feat": env["feature_ref"], "path": str(env["path"])}
    return summary

--- FILE_END: backend/lumiere_core/services/review_service.py ---


--- FILE_START: backend/lumiere_core/services/rca_service.py ---
# backend/lumiere_core/services/rca_service.py

import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum

from . import llm_service, github
from .ollama import search_index

# Configure logging
logger = logging.getLogger(__name__)

class ComplexityLevel(Enum):
    """Enumeration for complexity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"

class FileCategory(Enum):
    """Enumeration for file categories."""
    BACKEND = "backend"
    FRONTEND = "frontend"
    STYLING = "styling"
    CONFIG = "config"
    DOCS = "docs"
    DATABASE = "database"
    INFRASTRUCTURE = "infrastructure"
    BUILD = "build"
    TESTING = "testing"
    OTHER = "other"

@dataclass
class FileRelationships:
    """Data class for file relationship analysis results."""
    total_files: int
    file_types: Dict[str, List[str]]
    type_distribution: Dict[str, int]
    cross_layer_issue: bool
    complexity_indicator: ComplexityLevel
    primary_category: Optional[str] = None
    secondary_categories: List[str] = None

@dataclass
class AnalysisMetadata:
    """Enhanced metadata for analysis results."""
    total_context_chunks: int
    search_time_ms: Optional[float] = None
    analysis_time_ms: Optional[float] = None
    confidence_score: Optional[float] = None
    architectural_context_available: bool = False

def _get_repo_id_from_url(repo_url: str) -> str:
    """
    Helper to derive a filesystem-safe repo_id from a URL.

    Args:
        repo_url: The GitHub repository URL

    Returns:
        Filesystem-safe repository identifier

    Raises:
        ValueError: If the URL format is invalid
    """
    if not repo_url or not isinstance(repo_url, str):
        raise ValueError("Repository URL must be a non-empty string")

    if not repo_url.startswith("https://github.com/"):
        raise ValueError("Repository URL must be a valid GitHub URL")

    try:
        return repo_url.replace("https://github.com/", "").replace("/", "_")
    except Exception as e:
        raise ValueError(f"Failed to parse repository URL: {e}")

def _classify_file_by_extension(file_path: str) -> FileCategory:
    """
    Classify files into broad categories for better analysis context.

    Args:
        file_path: Path to the file

    Returns:
        FileCategory enum value
    """
    if not file_path:
        return FileCategory.OTHER

    # Enhanced extension mapping with more comprehensive coverage
    extension_map = {
        # Backend languages
        '.py': FileCategory.BACKEND, '.rb': FileCategory.BACKEND,
        '.java': FileCategory.BACKEND, '.go': FileCategory.BACKEND,
        '.php': FileCategory.BACKEND, '.cs': FileCategory.BACKEND,
        '.cpp': FileCategory.BACKEND, '.c': FileCategory.BACKEND,
        '.rs': FileCategory.BACKEND, '.kt': FileCategory.BACKEND,

        # Frontend
        '.js': FileCategory.FRONTEND, '.ts': FileCategory.FRONTEND,
        '.jsx': FileCategory.FRONTEND, '.tsx': FileCategory.FRONTEND,
        '.html': FileCategory.FRONTEND, '.htm': FileCategory.FRONTEND,
        '.vue': FileCategory.FRONTEND, '.svelte': FileCategory.FRONTEND,

        # Styling
        '.css': FileCategory.STYLING, '.scss': FileCategory.STYLING,
        '.sass': FileCategory.STYLING, '.less': FileCategory.STYLING,
        '.styl': FileCategory.STYLING,

        # Configuration
        '.json': FileCategory.CONFIG, '.yaml': FileCategory.CONFIG,
        '.yml': FileCategory.CONFIG, '.toml': FileCategory.CONFIG,
        '.ini': FileCategory.CONFIG, '.conf': FileCategory.CONFIG,
        '.env': FileCategory.CONFIG, '.properties': FileCategory.CONFIG,

        # Documentation
        '.md': FileCategory.DOCS, '.rst': FileCategory.DOCS,
        '.txt': FileCategory.DOCS, '.adoc': FileCategory.DOCS,

        # Database
        '.sql': FileCategory.DATABASE, '.graphql': FileCategory.DATABASE,
        '.gql': FileCategory.DATABASE,

        # Infrastructure
        '.tf': FileCategory.INFRASTRUCTURE, '.hcl': FileCategory.INFRASTRUCTURE,
        '.sh': FileCategory.INFRASTRUCTURE, '.bash': FileCategory.INFRASTRUCTURE,
        '.ps1': FileCategory.INFRASTRUCTURE, '.bat': FileCategory.INFRASTRUCTURE,
    }

    # Special filename mappings
    filename_map = {
        'dockerfile': FileCategory.INFRASTRUCTURE,
        'dockerfile.dev': FileCategory.INFRASTRUCTURE,
        'dockerfile.prod': FileCategory.INFRASTRUCTURE,
        'requirements.txt': FileCategory.BUILD,
        'package.json': FileCategory.BUILD,
        'package-lock.json': FileCategory.BUILD,
        'yarn.lock': FileCategory.BUILD,
        'pipfile': FileCategory.BUILD,
        'pipfile.lock': FileCategory.BUILD,
        'gemfile': FileCategory.BUILD,
        'gemfile.lock': FileCategory.BUILD,
        'composer.json': FileCategory.BUILD,
        'composer.lock': FileCategory.BUILD,
        'pom.xml': FileCategory.BUILD,
        'build.gradle': FileCategory.BUILD,
        'cargo.toml': FileCategory.BUILD,
        'cargo.lock': FileCategory.BUILD,
        'makefile': FileCategory.BUILD,
        'cmake.txt': FileCategory.BUILD,
    }

    file_path_lower = file_path.lower()

    # Check for test files first (highest priority)
    test_patterns = ['.test.', '.spec.', '_test.', '/test/', '/tests/', '__test__', '__tests__']
    if any(pattern in file_path_lower for pattern in test_patterns):
        return FileCategory.TESTING

    # Check filename mappings
    filename = Path(file_path_lower).name
    if filename in filename_map:
        return filename_map[filename]

    # Check extension mappings
    suffix = Path(file_path_lower).suffix
    return extension_map.get(suffix, FileCategory.OTHER)

def _determine_complexity_level(file_count: int, type_count: int, cross_layer: bool) -> ComplexityLevel:
    """
    Determine complexity level based on multiple factors.

    Args:
        file_count: Number of files involved
        type_count: Number of different file types
        cross_layer: Whether the issue crosses multiple layers

    Returns:
        ComplexityLevel enum value
    """
    if cross_layer and file_count > 15:
        return ComplexityLevel.VERY_HIGH
    elif file_count > 12 or (cross_layer and type_count > 4):
        return ComplexityLevel.HIGH
    elif file_count > 6 or type_count > 3:
        return ComplexityLevel.MEDIUM
    else:
        return ComplexityLevel.LOW

def _analyze_file_relationships(context_chunks: List[Dict]) -> FileRelationships:
    """
    Analyze relationships between files found in RAG results to provide better context.

    Args:
        context_chunks: List of context chunks from RAG search

    Returns:
        FileRelationships object with comprehensive analysis
    """
    if not context_chunks:
        return FileRelationships(
            total_files=0,
            file_types={},
            type_distribution={},
            cross_layer_issue=False,
            complexity_indicator=ComplexityLevel.LOW,
            primary_category=None,
            secondary_categories=[]
        )

    file_types = defaultdict(list)
    file_paths = set()
    category_counts = defaultdict(int)

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        if file_path:
            file_paths.add(file_path)
            file_category = _classify_file_by_extension(file_path)
            category_str = file_category.value
            file_types[category_str].append(file_path)
            category_counts[category_str] += 1

    # Determine primary and secondary categories
    sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
    primary_category = sorted_categories[0][0] if sorted_categories else None
    secondary_categories = [cat for cat, count in sorted_categories[1:4] if count > 1]

    total_files = len(file_paths)
    type_count = len(file_types)
    cross_layer_issue = type_count > 2
    complexity = _determine_complexity_level(total_files, type_count, cross_layer_issue)

    return FileRelationships(
        total_files=total_files,
        file_types=dict(file_types),
        type_distribution={k: len(set(v)) for k, v in file_types.items()},
        cross_layer_issue=cross_layer_issue,
        complexity_indicator=complexity,
        primary_category=primary_category,
        secondary_categories=secondary_categories
    )

def _load_architectural_context(repo_id: str) -> Tuple[str, bool]:
    """
    Load architectural context from cortex file.

    Args:
        repo_id: Repository identifier

    Returns:
        Tuple of (architectural_context_string, context_available_bool)
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_path.exists():
            logger.debug(f"Cortex file not found at {cortex_path}")
            return "No architectural context available for this analysis.", False

        with open(cortex_path, 'r', encoding='utf-8') as f:
            cortex_data = json.load(f)

        graph_data = cortex_data.get('architectural_graph')
        if not graph_data:
            return "Architectural context file found but contains no graph data.", False

        nodes = graph_data.get('nodes', {})  # Default to empty dict
        edges = graph_data.get('edges', [])

        # Correctly get the first 5 nodes from the dictionary's values
        nodes_list = list(nodes.values())

        context = f"""Architectural graph available with {len(nodes)} components and {len(edges)} connections.
Key components identified: {', '.join([node.get('name', 'Unknown') for node in nodes_list[:5]])}
This provides insights into system architecture and component relationships."""

        logger.info(f"Loaded architectural context for {repo_id}: {len(nodes)} nodes, {len(edges)} edges")
        return context, True

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse cortex JSON for {repo_id}: {e}")
        return "Architectural context file found but contains invalid JSON.", False
    except Exception as e:
        logger.error(f"Failed to load architectural context for {repo_id}: {e}")
        return "Error loading architectural context.", False

def _format_context_for_prompt(context_chunks: List[Dict], relationships: FileRelationships) -> str:
    """
    Format context chunks for LLM prompt with improved organization.

    Args:
        context_chunks: List of context chunks
        relationships: File relationship analysis

    Returns:
        Formatted context string
    """
    if not context_chunks:
        return "No relevant code context found."

    # Group chunks by file category for better organization
    categorized_chunks = defaultdict(list)
    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        category = _classify_file_by_extension(file_path).value
        categorized_chunks[category].append(chunk)

    context_parts = []

    # Present primary category first
    if relationships.primary_category and relationships.primary_category in categorized_chunks:
        chunks = categorized_chunks[relationships.primary_category]
        context_parts.append(f"=== PRIMARY CATEGORY: {relationships.primary_category.upper()} FILES ===")
        for chunk in chunks:
            context_parts.append(
                f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
            )

    # Then secondary categories
    for category in relationships.secondary_categories:
        if category in categorized_chunks:
            chunks = categorized_chunks[category]
            context_parts.append(f"=== SECONDARY CATEGORY: {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    # Finally, remaining categories
    for category, chunks in categorized_chunks.items():
        if category != relationships.primary_category and category not in relationships.secondary_categories:
            context_parts.append(f"=== {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    return "\n\n".join(context_parts)

def generate_briefing(issue_url: str, model_identifier: str) -> Dict[str, Any]:
    """
    Generates a 'Pre-flight Briefing' for a GitHub issue using RAG.

    Args:
        issue_url: GitHub issue URL
        model_identifier: LLM model identifier

    Returns:
        Dictionary containing briefing and metadata
    """
    logger.info(f"Generating briefing for issue: {issue_url}")

    try:
        # Validate inputs
        if not issue_url or not isinstance(issue_url, str):
            return {"error": "Invalid issue URL provided"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        # Scrape GitHub issue
        issue_data = github.scrape_github_issue(issue_url)
        if not issue_data:
            return {"error": "Could not retrieve issue details from GitHub. Please check the URL and try again."}

        repo_id = _get_repo_id_from_url(issue_data['repo_url'])
        query = issue_data['full_text_query']

        if not query:
            return {"error": "No query text found in issue data"}

        # Perform RAG search
        try:
            context_chunks = search_index(
                query_text=query,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=7
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed for {repo_id}: {e}")
            return {"error": f"Failed to retrieve context from vector index: {str(e)}"}

        # Analyze relationships and format context
        relationships = _analyze_file_relationships(context_chunks)
        context_string = _format_context_for_prompt(context_chunks, relationships)

        # Enhanced prompt with better structure
        prompt = f"""You are Lumière Sémantique, an expert AI programming assistant.
Your mission is to provide a comprehensive "Pre-flight Briefing" for a developer about to work on this GitHub issue.

**CODEBASE ANALYSIS SUMMARY:**
- Files involved: {relationships.total_files} files across {len(relationships.file_types)} different categories
- Primary category: {relationships.primary_category or 'Unknown'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

**INSTRUCTIONS:**
Analyze the GitHub issue and the provided codebase context to generate a comprehensive briefing report.

The report must be clear, well-structured, and formatted in Markdown. Include these sections:

1. **🎯 Task Summary**
   - Concise rephrasing of the issue request
   - Key objectives and expected outcomes

2. **🏗️ Codebase Architecture**
   - Relevant system architecture and component relationships
   - How the affected components interact

3. **🔍 Current System Analysis**
   - How the system currently handles the functionality in question
   - Existing patterns and conventions

4. **📁 Key Files & Components**
   - Most important files and functions from the context
   - Their roles and relationships

5. **🚀 Suggested Approach**
   - High-level implementation strategy
   - Recommended order of operations
   - Potential challenges and considerations

6. **⚠️ Important Notes**
   - Dependencies and side effects to consider
   - Testing recommendations

--- CODEBASE CONTEXT ---
{context_string}
--- END CONTEXT ---

**GITHUB ISSUE DETAILS:**
{query}

Generate the comprehensive Pre-flight Briefing now:
"""

        # Generate briefing
        briefing_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare metadata
        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        metadata = {
            "file_relationships": relationships_dict,
            "issue_url": issue_url,
            "repo_id": repo_id,
            "context_chunks_count": len(context_chunks),
            "model_used": model_identifier
        }

        logger.info(f"Successfully generated briefing for {issue_url}")
        return {"briefing": briefing_report, "metadata": metadata}

    except ValueError as e:
        logger.error(f"Validation error in generate_briefing: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in generate_briefing: {e}")
        return {"error": f"An unexpected error occurred: {str(e)}"}

def perform_rca(
    repo_url: str,
    bug_description: str,
    model_identifier: str,
    advanced_analysis: bool = False,
    confidence_threshold: float = 0.7
) -> Dict[str, Any]:
    """
    Performs a multi-file, context-aware Root Cause Analysis using RAG.

    Args:
        repo_url: GitHub repository URL
        bug_description: Description of the bug to analyze
        model_identifier: LLM model identifier
        advanced_analysis: Whether to perform advanced analysis with more context
        confidence_threshold: Minimum confidence threshold for results

    Returns:
        Dictionary containing analysis results and metadata
    """
    logger.info(f"Performing Multi-file RCA for bug: '{bug_description[:100]}...'")

    try:
        # Validate inputs
        if not repo_url or not isinstance(repo_url, str):
            return {"error": "Invalid repository URL provided"}

        if not bug_description or not isinstance(bug_description, str):
            return {"error": "Bug description is required"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        if not 0.0 <= confidence_threshold <= 1.0:
            return {"error": "Confidence threshold must be between 0.0 and 1.0"}

        repo_id = _get_repo_id_from_url(repo_url)

        # Load architectural context
        architectural_context, arch_available = _load_architectural_context(repo_id)

        # Perform RAG search with enhanced parameters
        try:
            logger.debug("Searching for relevant code chunks...")
            initial_k = 25 if advanced_analysis else 15

            context_chunks = search_index(
                query_text=bug_description,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=initial_k
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed during RCA for {repo_id}: {e}")
            return {"error": f"RAG search failed during RCA: {str(e)}"}

        if not context_chunks:
            return {
                "analysis": "Could not find any relevant code context for the bug description. "
                           "Unable to perform RCA. Please try rephrasing the bug description or "
                           "ensure the repository has been properly indexed."
            }

        logger.debug("Analyzing file relationships and filtering context...")
        relationships = _analyze_file_relationships(context_chunks)

        # Format context with improved organization
        logger.debug("Synthesizing context from suspect files...")
        formatted_context = _format_context_for_prompt(context_chunks, relationships)

        # Generate complexity guidance
        complexity_guidance = ""
        if relationships.cross_layer_issue:
            complexity_guidance = """
**COMPLEXITY ALERT:** This issue spans multiple system layers. Pay special attention to:
- Interface boundaries and data contracts
- State management across components
- Error propagation paths
- Dependency chains and side effects"""

        if relationships.complexity_indicator in [ComplexityLevel.HIGH, ComplexityLevel.VERY_HIGH]:
            complexity_guidance += """
- Consider breaking down the analysis into smaller, focused areas
- Look for common patterns or shared dependencies
- Pay attention to configuration and environment differences"""

        # Enhanced RCA prompt
        prompt = f"""You are a world-class debugging expert performing a comprehensive Root Cause Analysis (RCA).

**BUG DESCRIPTION:**
{bug_description}

**SYSTEM CONTEXT:**
{architectural_context}

**CODEBASE ANALYSIS:**
- Relevant files: {relationships.total_files} files analyzed
- Primary category: {relationships.primary_category or 'Mixed'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

{complexity_guidance}

**ANALYSIS INSTRUCTIONS:**
You must analyze ALL provided context systematically. Use the code evidence to build a comprehensive understanding of the bug's root cause.

Your analysis must follow this structure:

## 🎯 Executive Summary
A clear, single-sentence explanation of the root cause.

## 🏗️ System Overview
Brief explanation of how the affected components are designed to interact and what the expected behavior should be.

## 🔍 Root Cause Analysis
Detailed breakdown of:
- What is happening vs. what should happen
- The specific mechanism causing the failure
- Why this particular scenario triggers the bug

## 📋 Evidence & Reasoning
Cite specific files, functions, and code snippets that support your analysis:
- Direct evidence from the code
- Logical connections between components
- Data flow analysis

## 💥 Impact Assessment
Explain the cascading effects:
- What breaks when this bug occurs
- Which users/systems are affected
- Performance or security implications

## 🛠️ Recommended Fix Strategy
High-level approach including:
- Which specific files need modification
- Order of operations for the fix
- Testing strategy to verify the fix
- Potential risks and mitigation strategies

## ⚠️ Prevention Recommendations
Suggestions to prevent similar issues in the future.

**RELEVANT CODE CONTEXT:**
{formatted_context}

Now generate your comprehensive Root Cause Analysis:
"""

        logger.debug("Generating comprehensive RCA report...")
        analysis_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare enhanced metadata
        metadata = AnalysisMetadata(
            total_context_chunks=len(context_chunks),
            architectural_context_available=arch_available,
            confidence_score=None  # Could be implemented based on context quality
        )

        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        result = {
            "analysis": analysis_report,
            "metadata": {
                **relationships_dict,
                **metadata.__dict__,
                "repo_id": repo_id,
                "advanced_analysis": advanced_analysis,
                "confidence_threshold": confidence_threshold
            }
        }

        logger.info(f"Successfully completed RCA for {repo_id}")
        return result

    except ValueError as e:
        logger.error(f"Validation error in perform_rca: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in perform_rca: {e}")
        return {"error": f"An unexpected error occurred during RCA: {str(e)}"}

def get_analysis_health_check(repo_id: str) -> Dict[str, Any]:
    """
    Perform a health check for RCA analysis capabilities.

    Args:
        repo_id: Repository identifier

    Returns:
        Dictionary containing health check results
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        health_status = {
            "repo_id": repo_id,
            "vector_index_available": False,
            "architectural_context_available": False,
            "cortex_file_exists": cortex_path.exists(),
            "recommendations": []
        }

        # Check vector index (this would need to be implemented based on your vector storage)
        try:
            # Placeholder for vector index check
            test_chunks = search_index(
                query_text="test query",
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=1
            )
            health_status["vector_index_available"] = len(test_chunks) > 0
        except FileNotFoundError:
            health_status["recommendations"].append(
                "Vector index not found. Please re-ingest the repository with embedding enabled."
            )
        except Exception as e:
            health_status["recommendations"].append(f"Vector index check failed: {str(e)}")

        # Check architectural context
        if cortex_path.exists():
            try:
                with open(cortex_path, 'r', encoding='utf-8') as f:
                    cortex_data = json.load(f)
                graph_data = cortex_data.get('architectural_graph')
                health_status["architectural_context_available"] = bool(graph_data)

                if not graph_data:
                    health_status["recommendations"].append(
                        "Cortex file exists but contains no architectural graph data."
                    )
            except Exception as e:
                health_status["recommendations"].append(f"Failed to parse cortex file: {str(e)}")
        else:
            health_status["recommendations"].append(
                "No architectural context available. Consider running architectural analysis."
            )

        # Overall health assessment
        if health_status["vector_index_available"] and health_status["architectural_context_available"]:
            health_status["overall_status"] = "excellent"
        elif health_status["vector_index_available"]:
            health_status["overall_status"] = "good"
        else:
            health_status["overall_status"] = "poor"

        return health_status

    except Exception as e:
        logger.error(f"Health check failed for {repo_id}: {e}")
        return {
            "repo_id": repo_id,
            "overall_status": "error",
            "error": str(e)
        }

--- FILE_END: backend/lumiere_core/services/rca_service.py ---


--- FILE_START: backend/lumiere_core/services/cartographer.py ---
# backend/lumiere_core/services/cartographer.py

import ast
import logging
from collections import defaultdict
from typing import Dict, Any, List, Optional, Set, Tuple
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# --- Tree-sitter imports ---
try:
    from tree_sitter import Language, Parser
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False
    logging.warning("Tree-sitter not available. JavaScript analysis will be limited.")

# --- Setup Tree-sitter for JavaScript ---
if TREE_SITTER_AVAILABLE:
    try:
        LANGUAGE_LIBRARY_PATH = 'build/my-languages.so'
        JS_LANGUAGE = Language(LANGUAGE_LIBRARY_PATH, 'javascript')
        js_parser = Parser()
        js_parser.set_language(JS_LANGUAGE)
        JAVASCRIPT_PARSER_READY = True
    except Exception as e:
        JAVASCRIPT_PARSER_READY = False
        logging.warning(f"Failed to initialize JavaScript parser: {e}")
else:
    JAVASCRIPT_PARSER_READY = False

# Configure logging
logger = logging.getLogger(__name__)


# ==============================================================================
# SECTION 0: ENHANCED DATA STRUCTURES AND ENUMS
# ==============================================================================

class NodeType(Enum):
    """Enumeration of node types in the architecture graph."""
    FILE = "file"
    CLASS = "class"
    FUNCTION = "function"
    METHOD = "method"
    MODULE = "module"
    INTERFACE = "interface"
    NAMESPACE = "namespace"


class EdgeType(Enum):
    """Enumeration of relationship types between nodes."""
    IMPORTS = "IMPORTS"
    INHERITS_FROM = "INHERITS_FROM"
    IMPLEMENTS = "IMPLEMENTS"
    CALLS = "CALLS"
    USES = "USES"
    EXTENDS = "EXTENDS"
    COMPOSES = "COMPOSES"
    DEPENDS_ON = "DEPENDS_ON"


class Language(Enum):
    """Supported programming languages."""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript"
    JAVA = "java"
    CSHARP = "csharp"
    UNKNOWN = "unknown"


@dataclass
class AnalysisMetadata:
    """Metadata for analysis results."""
    file_path: str
    language: Language
    lines_of_code: int = 0
    complexity_score: float = 0.0
    last_modified: Optional[str] = None
    analysis_timestamp: Optional[str] = None
    errors: List[str] = None
    warnings: List[str] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []


@dataclass
class GraphNode:
    """Enhanced node representation with metadata."""
    id: str
    type: NodeType
    name: str
    file_path: str
    language: Language
    metadata: Dict[str, Any] = None
    children: List[str] = None
    complexity: float = 0.0

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.children is None:
            self.children = []


@dataclass
class GraphEdge:
    """Enhanced edge representation with metadata."""
    source: str
    target: str
    type: EdgeType
    weight: float = 1.0
    metadata: Dict[str, Any] = None
    confidence: float = 1.0

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


# ==============================================================================
# SECTION 1: ENHANCED LANGUAGE-SPECIFIC MAPPERS
# ==============================================================================

def _detect_language(file_path: str, content: str = "") -> Language:
    """Enhanced language detection based on file extension and content."""
    path = Path(file_path)
    extension = path.suffix.lower()

    extension_map = {
        '.py': Language.PYTHON,
        '.js': Language.JAVASCRIPT,
        '.jsx': Language.JAVASCRIPT,
        '.ts': Language.TYPESCRIPT,
        '.tsx': Language.TYPESCRIPT,
        '.java': Language.JAVA,
        '.cs': Language.CSHARP,
    }

    detected = extension_map.get(extension, Language.UNKNOWN)

    # Content-based detection for ambiguous cases
    if detected == Language.UNKNOWN and content:
        if content.strip().startswith('#!/usr/bin/env python') or 'import ' in content[:200]:
            detected = Language.PYTHON
        elif 'function ' in content[:200] or 'const ' in content[:200] or 'let ' in content[:200]:
            detected = Language.JAVASCRIPT

    return detected


def _calculate_complexity(tree: ast.AST) -> float:
    """Calculate cyclomatic complexity for Python AST."""
    complexity = 1  # Base complexity

    class ComplexityVisitor(ast.NodeVisitor):
        def __init__(self):
            self.complexity = 1

        def visit_If(self, node):
            self.complexity += 1
            self.generic_visit(node)

        def visit_While(self, node):
            self.complexity += 1
            self.generic_visit(node)

        def visit_For(self, node):
            self.complexity += 1
            self.generic_visit(node)

        def visit_Try(self, node):
            self.complexity += len(node.handlers)
            self.generic_visit(node)

        def visit_With(self, node):
            self.complexity += 1
            self.generic_visit(node)

    visitor = ComplexityVisitor()
    visitor.visit(tree)
    return visitor.complexity


def _map_python_ast(file_path: str, tree: ast.AST, nodes: defaultdict, edges: list) -> AnalysisMetadata:
    """Enhanced Python AST mapping with improved error handling and metadata."""
    metadata = AnalysisMetadata(file_path=file_path, language=Language.PYTHON)

    try:
        visitor = _PythonCartographerVisitor()
        visitor.visit(tree)

        # Calculate complexity
        complexity = _calculate_complexity(tree)
        metadata.complexity_score = complexity

        # Enhanced node creation with metadata
        nodes[file_path].update({
            "type": NodeType.FILE.value,
            "language": Language.PYTHON.value,
            "classes": [],
            "functions": [],
            "complexity": complexity,
            "imports_count": len(visitor.imports),
            "metadata": {
                "total_classes": len(visitor.class_defs),
                "total_functions": len(visitor.function_defs),
                "total_imports": len(visitor.imports)
            }
        })

        # Process imports with enhanced metadata
        for imp in visitor.imports:
            edge_metadata = {"import_type": imp['type']}
            if imp['type'] == 'from_import':
                edge_metadata["module"] = imp.get('module', '')

            edges.append({
                "source": file_path,
                "target": imp['name'],
                "type": EdgeType.IMPORTS.value,
                "metadata": edge_metadata,
                "confidence": 0.9
            })

        # Process classes with inheritance tracking
        for c_def in visitor.class_defs:
            class_node_id = f"{file_path}::{c_def['name']}"
            nodes[class_node_id].update({
                "type": NodeType.CLASS.value,
                "name": c_def['name'],
                "file": file_path,
                "methods": [],
                "language": Language.PYTHON.value,
                "metadata": {
                    "base_classes": c_def['inherits_from'],
                    "method_count": len([f for f in visitor.function_defs if f['class_context'] == c_def['name']])
                }
            })
            nodes[file_path]['classes'].append(c_def['name'])

            # Enhanced inheritance relationships
            for base in c_def['inherits_from']:
                edges.append({
                    "source": class_node_id,
                    "target": base,
                    "type": EdgeType.INHERITS_FROM.value,
                    "confidence": 0.95
                })

        # Process functions with context awareness
        for f_def in visitor.function_defs:
            func_metadata = {
                "is_method": bool(f_def['class_context']),
                "is_private": f_def['name'].startswith('_'),
                "is_dunder": f_def['name'].startswith('__') and f_def['name'].endswith('__')
            }

            if f_def['class_context']:
                parent_node_id = f"{file_path}::{f_def['class_context']}"
                if parent_node_id in nodes:
                    nodes[parent_node_id]['methods'].append(f_def['name'])

                    # Create method node
                    method_node_id = f"{parent_node_id}::{f_def['name']}"
                    nodes[method_node_id].update({
                        "type": NodeType.METHOD.value,
                        "name": f_def['name'],
                        "parent_class": f_def['class_context'],
                        "file": file_path,
                        "metadata": func_metadata
                    })
            else:
                nodes[file_path]['functions'].append(f_def['name'])

                # Create function node
                func_node_id = f"{file_path}::{f_def['name']}"
                nodes[func_node_id].update({
                    "type": NodeType.FUNCTION.value,
                    "name": f_def['name'],
                    "file": file_path,
                    "metadata": func_metadata
                })

        # Process function calls with improved context tracking
        for call in visitor.function_calls:
            source_context = f"::{call['class_context']}" if call['class_context'] else ""
            source_id = f"{file_path}{source_context}"
            target_name = call['name'].split('.')[-1]

            edges.append({
                "source": source_id,
                "target": target_name,
                "type": EdgeType.CALLS.value,
                "metadata": {
                    "full_call": call['name'],
                    "context": call['class_context']
                },
                "confidence": 0.8
            })

    except Exception as e:
        error_msg = f"Error processing Python AST for {file_path}: {str(e)}"
        logger.error(error_msg)
        metadata.errors.append(error_msg)

    return metadata


def _map_javascript_ast(file_path: str, content: str, nodes: defaultdict, edges: list) -> AnalysisMetadata:
    """Enhanced JavaScript AST mapping with fallback parsing."""
    metadata = AnalysisMetadata(file_path=file_path, language=Language.JAVASCRIPT)
    metadata.lines_of_code = len(content.splitlines())

    try:
        if JAVASCRIPT_PARSER_READY:
            _map_javascript_with_treesitter(file_path, content, nodes, edges, metadata)
        else:
            _map_javascript_with_regex(file_path, content, nodes, edges, metadata)
            metadata.warnings.append("Using regex-based fallback for JavaScript parsing")

    except Exception as e:
        error_msg = f"Error processing JavaScript for {file_path}: {str(e)}"
        logger.error(error_msg)
        metadata.errors.append(error_msg)

        # Fallback to regex parsing
        try:
            _map_javascript_with_regex(file_path, content, nodes, edges, metadata)
            metadata.warnings.append("Fell back to regex parsing due to Tree-sitter error")
        except Exception as fallback_error:
            metadata.errors.append(f"Fallback parsing also failed: {str(fallback_error)}")

    return metadata


def _map_javascript_with_treesitter(file_path: str, content: str, nodes: defaultdict, edges: list, metadata: AnalysisMetadata):
    """Tree-sitter based JavaScript parsing (original logic preserved)."""
    tree = js_parser.parse(bytes(content, "utf8"))
    root_node = tree.root_node

    nodes[file_path].update({
        "type": NodeType.FILE.value,
        "language": Language.JAVASCRIPT.value,
        "classes": [],
        "functions": [],
        "lines_of_code": metadata.lines_of_code
    })

    # Enhanced query patterns with better error handling
    query_patterns = {
        "requires": '(call_expression function: (identifier) @func (#eq? @func "require") arguments: (arguments (string (string_fragment) @module)))',
        "imports": '(import_statement source: (string (string_fragment) @module))',
        "functions": '(function_declaration name: (identifier) @name)',
        "arrow_functions": '(variable_declarator id: (identifier) @name value: (arrow_function))',
        "classes": '(class_declaration name: (identifier) @name)',
        "calls": '(call_expression function: [ (identifier) @name (member_expression property: (property_identifier) @name) ] )',
        "exports": '(export_statement)',
        "methods": '(method_definition name: (property_identifier) @name)'
    }

    function_count = 0
    class_count = 0
    import_count = 0

    for query_name, pattern in query_patterns.items():
        try:
            query = JS_LANGUAGE.query(pattern)
            captures = query.captures(root_node)

            for node, capture_name in captures:
                text = node.text.decode('utf8')

                if capture_name == 'module':
                    import_count += 1
                    edges.append({
                        "source": file_path,
                        "target": text,
                        "type": EdgeType.IMPORTS.value,
                        "metadata": {"import_style": query_name},
                        "confidence": 0.9
                    })
                elif capture_name == 'name':
                    if query_name in ['functions', 'arrow_functions']:
                        function_count += 1
                        nodes[file_path]['functions'].append(text)

                        # Create function node
                        func_node_id = f"{file_path}::{text}"
                        nodes[func_node_id].update({
                            "type": NodeType.FUNCTION.value,
                            "name": text,
                            "file": file_path,
                            "language": Language.JAVASCRIPT.value,
                            "metadata": {"function_type": "arrow" if "arrow" in query_name else "declaration"}
                        })

                    elif query_name == 'classes':
                        class_count += 1
                        nodes[file_path]['classes'].append(text)

                        # Create class node
                        class_node_id = f"{file_path}::{text}"
                        nodes[class_node_id].update({
                            "type": NodeType.CLASS.value,
                            "name": text,
                            "file": file_path,
                            "language": Language.JAVASCRIPT.value,
                            "methods": []
                        })

                    elif query_name == 'calls':
                        edges.append({
                            "source": file_path,
                            "target": text,
                            "type": EdgeType.CALLS.value,
                            "confidence": 0.8
                        })

                    elif query_name == 'methods':
                        # Find parent class for method
                        parent = node.parent
                        while parent and parent.type != 'class_declaration':
                            parent = parent.parent

                        if parent:
                            class_name_node = parent.child_by_field_name('name')
                            if class_name_node:
                                class_name = class_name_node.text.decode('utf8')
                                class_node_id = f"{file_path}::{class_name}"
                                if class_node_id in nodes:
                                    nodes[class_node_id]['methods'].append(text)

                                    # Create method node
                                    method_node_id = f"{class_node_id}::{text}"
                                    nodes[method_node_id].update({
                                        "type": NodeType.METHOD.value,
                                        "name": text,
                                        "parent_class": class_name,
                                        "file": file_path,
                                        "language": Language.JAVASCRIPT.value
                                    })

        except Exception as query_error:
            metadata.warnings.append(f"Query '{query_name}' failed: {str(query_error)}")
            continue

    # Update metadata
    nodes[file_path]['metadata'] = {
        "total_functions": function_count,
        "total_classes": class_count,
        "total_imports": import_count
    }

    metadata.complexity_score = function_count + class_count * 2  # Simple heuristic


def _map_javascript_with_regex(file_path: str, content: str, nodes: defaultdict, edges: list, metadata: AnalysisMetadata):
    """Regex-based fallback JavaScript parsing."""
    import re

    nodes[file_path].update({
        "type": NodeType.FILE.value,
        "language": Language.JAVASCRIPT.value,
        "classes": [],
        "functions": []
    })

    # Basic regex patterns for fallback parsing
    patterns = {
        'imports': [
            r"import\s+.*?from\s+['\"]([^'\"]+)['\"]",
            r"require\s*\(\s*['\"]([^'\"]+)['\"]\s*\)"
        ],
        'functions': [
            r"function\s+(\w+)\s*\(",
            r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>"
        ],
        'classes': [r"class\s+(\w+)"],
        'calls': [r"(\w+)\s*\("]
    }

    function_count = 0
    class_count = 0
    import_count = 0

    for pattern_type, regex_list in patterns.items():
        for regex_pattern in regex_list:
            matches = re.findall(regex_pattern, content, re.MULTILINE)

            for match in matches:
                if pattern_type == 'imports':
                    import_count += 1
                    edges.append({
                        "source": file_path,
                        "target": match,
                        "type": EdgeType.IMPORTS.value,
                        "confidence": 0.7  # Lower confidence for regex
                    })
                elif pattern_type == 'functions':
                    function_count += 1
                    nodes[file_path]['functions'].append(match)
                elif pattern_type == 'classes':
                    class_count += 1
                    nodes[file_path]['classes'].append(match)
                elif pattern_type == 'calls':
                    edges.append({
                        "source": file_path,
                        "target": match,
                        "type": EdgeType.CALLS.value,
                        "confidence": 0.6  # Lower confidence for regex calls
                    })

    metadata.complexity_score = function_count + class_count * 2


# ==============================================================================
# SECTION 2: ENHANCED MAIN `generate_graph` FUNCTION
# ==============================================================================

def generate_graph(analyzed_files: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Enhanced orchestration of multi-language architectural analysis.

    Args:
        analyzed_files: Dictionary of file paths to analysis data
        config: Optional configuration for analysis behavior

    Returns:
        Enhanced graph structure with nodes, edges, and metadata
    """
    if config is None:
        config = {}

    # Configuration options
    include_private = config.get('include_private', True)
    min_confidence = config.get('min_confidence', 0.0)
    enable_metrics = config.get('enable_metrics', True)

    logger.info("--- ENHANCED POLYGLOT CARTOGRAPHER AGENT ACTIVATED ---")
    logger.info(f"   -> Mapping architecture for {len(analyzed_files)} files...")

    nodes = defaultdict(dict)
    edges = []
    analysis_metadata = []
    supported_languages = set()
    total_complexity = 0.0

    # Language-specific mappers registry
    mappers = {
        Language.PYTHON: _map_python_ast,
        Language.JAVASCRIPT: _map_javascript_ast,
        # Future languages can be registered here
    }

    for file_path, analysis_data in analyzed_files.items():
        # Enhanced language detection
        content = analysis_data.get('content', '')
        detected_language = _detect_language(file_path, content)

        # Override with provided language if available
        if 'language' in analysis_data:
            try:
                detected_language = Language(analysis_data['language'])
            except ValueError:
                logger.warning(f"Unknown language '{analysis_data['language']}' for {file_path}")

        supported_languages.add(detected_language)
        logger.info(f"      - Analyzing: {file_path} ({detected_language.value})")

        try:
            # Dispatch to appropriate mapper
            if detected_language in mappers:
                if detected_language == Language.PYTHON and analysis_data.get('ast'):
                    metadata = mappers[detected_language](file_path, analysis_data['ast'], nodes, edges)
                elif detected_language == Language.JAVASCRIPT and content:
                    metadata = mappers[detected_language](file_path, content, nodes, edges)
                else:
                    # Create basic node for unsupported analysis
                    metadata = AnalysisMetadata(file_path=file_path, language=detected_language)
                    metadata.warnings.append("Limited analysis - missing required data")
                    nodes[file_path].update({
                        "type": NodeType.FILE.value,
                        "language": detected_language.value,
                        "status": "limited_analysis"
                    })
            else:
                # Unsupported language - create basic node
                metadata = AnalysisMetadata(file_path=file_path, language=detected_language)
                metadata.warnings.append(f"Language {detected_language.value} not fully supported")
                nodes[file_path].update({
                    "type": NodeType.FILE.value,
                    "language": detected_language.value,
                    "status": "unsupported_language"
                })

            analysis_metadata.append(metadata)
            total_complexity += metadata.complexity_score

        except Exception as e:
            error_msg = f"Failed to process {file_path}: {str(e)}"
            logger.error(error_msg)

            # Create error node
            error_metadata = AnalysisMetadata(file_path=file_path, language=detected_language)
            error_metadata.errors.append(error_msg)
            analysis_metadata.append(error_metadata)

            nodes[file_path].update({
                "type": NodeType.FILE.value,
                "language": detected_language.value,
                "status": "analysis_failed",
                "error": str(e)
            })

    # Filter edges based on confidence threshold
    if min_confidence > 0.0:
        edges = [edge for edge in edges if edge.get('confidence', 1.0) >= min_confidence]
        logger.info(f"   -> Filtered edges with confidence >= {min_confidence}")

    # Filter private elements if requested
    if not include_private:
        filtered_nodes = {}
        for node_id, node_data in nodes.items():
            if not node_data.get('name', '').startswith('_'):
                filtered_nodes[node_id] = node_data
        nodes = filtered_nodes
        logger.info("   -> Filtered private elements")

    # Calculate project-wide metrics
    project_metrics = {}
    if enable_metrics:
        project_metrics = {
            "total_files": len(analyzed_files),
            "supported_languages": [lang.value for lang in supported_languages],
            "total_nodes": len(nodes),
            "total_edges": len(edges),
            "average_complexity": total_complexity / len(analyzed_files) if analyzed_files else 0.0,
            "total_complexity": total_complexity,
            "analysis_errors": sum(len(m.errors) for m in analysis_metadata),
            "analysis_warnings": sum(len(m.warnings) for m in analysis_metadata)
        }

    logger.info("✓ Enhanced Polyglot Cartographer mapping complete.")
    logger.info(f"   -> Generated {len(nodes)} nodes and {len(edges)} edges")
    logger.info(f"   -> Analyzed languages: {', '.join(lang.value for lang in supported_languages)}")

    # Enhanced return structure
    result = {
        "nodes": dict(nodes),
        "edges": edges,
        "metadata": {
            "analysis_metadata": [
                {
                    "file_path": m.file_path,
                    "language": m.language.value,
                    "complexity_score": m.complexity_score,
                    "lines_of_code": m.lines_of_code,
                    "errors": m.errors,
                    "warnings": m.warnings
                }
                for m in analysis_metadata
            ],
            "project_metrics": project_metrics,
            "config": config,
            "version": "2.0.0"
        }
    }

    return result


# ==============================================================================
# SECTION 3: ENHANCED HELPER CLASSES AND FUNCTIONS
# ==============================================================================

def _safe_unparse(node: ast.AST) -> str:
    """Enhanced robust version of ast.unparse with better type handling."""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        return f"{_safe_unparse(node.value)}.{node.attr}"
    elif isinstance(node, ast.Constant):
        return str(node.value)
    elif isinstance(node, ast.Subscript):
        return f"{_safe_unparse(node.value)}[{_safe_unparse(node.slice)}]"
    elif isinstance(node, ast.Call):
        func_name = _safe_unparse(node.func)
        return f"{func_name}()"

    try:
        return ast.unparse(node)
    except Exception:
        return f"ComplexType_{type(node).__name__}"


class _PythonCartographerVisitor(ast.NodeVisitor):
    """Enhanced AST visitor for Python with improved analysis capabilities."""

    def __init__(self):
        self.imports = []
        self.function_calls = []
        self.class_defs = []
        self.function_defs = []
        self.current_class = None
        self.current_function = None
        self.decorators = []
        self.variables = []
        self.call_stack = []  # Track nested contexts

    def visit_Import(self, node: ast.Import):
        for alias in node.names:
            import_info = {
                "type": "direct_import",
                "name": alias.name,
                "alias": alias.asname,
                "lineno": node.lineno
            }
            self.imports.append(import_info)
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom):
        module = node.module or '.'
        for alias in node.names:
            import_info = {
                "type": "from_import",
                "module": module,
                "name": alias.name,
                "alias": alias.asname,
                "level": node.level,
                "lineno": node.lineno
            }
            self.imports.append(import_info)
        self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef):
        original_class = self.current_class
        self.current_class = node.name

        # Extract base classes and decorators
        base_classes = [_safe_unparse(b) for b in node.bases]
        decorators = [_safe_unparse(d) for d in node.decorator_list]

        class_info = {
            "name": node.name,
            "inherits_from": base_classes,
            "decorators": decorators,
            "lineno": node.lineno,
            "methods": [],
            "is_abstract": any("abstract" in dec.lower() for dec in decorators)
        }
        self.class_defs.append(class_info)

        self.generic_visit(node)
        self.current_class = original_class

    def visit_FunctionDef(self, node: ast.FunctionDef):
        self._visit_function_def(node, is_async=False)

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        self._visit_function_def(node, is_async=True)

    def _visit_function_def(self, node, is_async=False):
        original_function = self.current_function
        self.current_function = node.name

        # Extract decorators and arguments
        decorators = [_safe_unparse(d) for d in node.decorator_list]
        args = [arg.arg for arg in node.args.args]

        function_info = {
            "name": node.name,
            "class_context": self.current_class,
            "decorators": decorators,
            "args": args,
            "is_async": is_async,
            "lineno": node.lineno,
            "is_property": any("property" in dec for dec in decorators),
            "is_staticmethod": any("staticmethod" in dec for dec in decorators),
            "is_classmethod": any("classmethod" in dec for dec in decorators)
        }
        self.function_defs.append(function_info)

        self.generic_visit(node)
        self.current_function = original_function

    def visit_Call(self, node: ast.Call):
        call_name = _safe_unparse(node.func)

        # Extract more call context
        call_info = {
            "name": call_name,
            "class_context": self.current_class,
            "function_context": self.current_function,
            "lineno": node.lineno,
            "args_count": len(node.args),
            "kwargs_count": len(node.keywords)
        }
        self.function_calls.append(call_info)

        self.generic_visit(node)

    def visit_Assign(self, node: ast.Assign):
        """Track variable assignments for better context."""
        for target in node.targets:
            if isinstance(target, ast.Name):
                var_info = {
                    "name": target.id,
                    "class_context": self.current_class,
                    "function_context": self.current_function,
                    "lineno": node.lineno,
                    "value_type": type(node.value).__name__
                }
                self.variables.append(var_info)
        self.generic_visit(node)


# ==============================================================================
# SECTION 4: ADDITIONAL UTILITY FUNCTIONS
# ==============================================================================

def analyze_project_structure(graph_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyze the project structure and provide architectural insights.

    Args:
        graph_data: The graph data returned by generate_graph()

    Returns:
        Dictionary containing structural analysis and insights
    """
    nodes = graph_data.get('nodes', {})
    edges = graph_data.get('edges', [])

    # Calculate various metrics
    file_nodes = {k: v for k, v in nodes.items() if v.get('type') == NodeType.FILE.value}
    class_nodes = {k: v for k, v in nodes.items() if v.get('type') == NodeType.CLASS.value}
    function_nodes = {k: v for k, v in nodes.items() if v.get('type') == NodeType.FUNCTION.value}

    # Language distribution
    language_dist = {}
    for node in file_nodes.values():
        lang = node.get('language', 'unknown')
        language_dist[lang] = language_dist.get(lang, 0) + 1

    # Dependency analysis
    import_edges = [e for e in edges if e.get('type') == EdgeType.IMPORTS.value]
    call_edges = [e for e in edges if e.get('type') == EdgeType.CALLS.value]
    inheritance_edges = [e for e in edges if e.get('type') == EdgeType.INHERITS_FROM.value]

    # Find highly connected nodes (potential architectural hotspots)
    node_connections = {}
    for edge in edges:
        source = edge.get('source', '')
        target = edge.get('target', '')
        node_connections[source] = node_connections.get(source, 0) + 1
        node_connections[target] = node_connections.get(target, 0) + 1

    hotspots = sorted(node_connections.items(), key=lambda x: x[1], reverse=True)[:10]

    # Calculate modularity metrics
    total_files = len(file_nodes)
    total_imports = len(import_edges)
    coupling_ratio = total_imports / total_files if total_files > 0 else 0

    # Identify potential issues
    issues = []
    if coupling_ratio > 5:
        issues.append("High coupling detected - consider reducing dependencies")

    circular_deps = _detect_circular_dependencies(edges)
    if circular_deps:
        issues.extend([f"Circular dependency detected: {' -> '.join(cycle)}" for cycle in circular_deps])

    return {
        "summary": {
            "total_files": total_files,
            "total_classes": len(class_nodes),
            "total_functions": len(function_nodes),
            "language_distribution": language_dist,
            "coupling_ratio": coupling_ratio
        },
        "dependencies": {
            "total_imports": total_imports,
            "total_calls": len(call_edges),
            "inheritance_relationships": len(inheritance_edges)
        },
        "hotspots": hotspots,
        "issues": issues,
        "recommendations": _generate_recommendations(graph_data)
    }


def _detect_circular_dependencies(edges: List[Dict[str, Any]]) -> List[List[str]]:
    """
    Detect circular dependencies in the graph using DFS.

    Args:
        edges: List of edge dictionaries

    Returns:
        List of circular dependency paths
    """
    # Build adjacency list for import relationships
    graph = defaultdict(list)
    for edge in edges:
        if edge.get('type') == EdgeType.IMPORTS.value:
            graph[edge['source']].append(edge['target'])

    def dfs(node, path, visited, rec_stack):
        visited.add(node)
        rec_stack.add(node)
        path.append(node)

        cycles = []
        for neighbor in graph[node]:
            if neighbor in rec_stack:
                # Found a cycle
                cycle_start = path.index(neighbor)
                cycles.append(path[cycle_start:] + [neighbor])
            elif neighbor not in visited:
                cycles.extend(dfs(neighbor, path[:], visited, rec_stack))

        rec_stack.remove(node)
        return cycles

    visited = set()
    all_cycles = []

    for node in graph:
        if node not in visited:
            all_cycles.extend(dfs(node, [], visited, set()))

    return all_cycles


def _generate_recommendations(graph_data: Dict[str, Any]) -> List[str]:
    """
    Generate architectural recommendations based on graph analysis.

    Args:
        graph_data: The complete graph data

    Returns:
        List of recommendation strings
    """
    recommendations = []

    nodes = graph_data.get('nodes', {})
    edges = graph_data.get('edges', [])
    metadata = graph_data.get('metadata', {})

    # Check for high complexity files
    project_metrics = metadata.get('project_metrics', {})
    avg_complexity = project_metrics.get('average_complexity', 0)

    if avg_complexity > 10:
        recommendations.append("Consider refactoring high-complexity files to improve maintainability")

    # Check for files with many dependencies
    file_import_counts = {}
    for edge in edges:
        if edge.get('type') == EdgeType.IMPORTS.value:
            source = edge['source']
            file_import_counts[source] = file_import_counts.get(source, 0) + 1

    max_imports = max(file_import_counts.values()) if file_import_counts else 0
    if max_imports > 15:
        recommendations.append("Some files have excessive imports - consider dependency injection or facade patterns")

    # Check for large classes
    large_classes = []
    for node_id, node_data in nodes.items():
        if node_data.get('type') == NodeType.CLASS.value:
            method_count = len(node_data.get('methods', []))
            if method_count > 20:
                large_classes.append(node_data.get('name', 'Unknown'))

    if large_classes:
        recommendations.append(f"Large classes detected ({', '.join(large_classes[:3])}) - consider splitting responsibilities")

    # Check language diversity
    languages = project_metrics.get('supported_languages', [])
    if len(languages) > 3:
        recommendations.append("Multiple languages detected - ensure proper integration patterns and documentation")

    # Check for missing documentation patterns
    analysis_warnings = project_metrics.get('analysis_warnings', 0)
    if analysis_warnings > len(nodes) * 0.1:  # More than 10% warning rate
        recommendations.append("High analysis warning rate - consider improving code documentation and structure")

    return recommendations


def export_graph_data(graph_data: Dict[str, Any], format_type: str = 'json', output_path: Optional[str] = None) -> str:
    """
    Export graph data in various formats.

    Args:
        graph_data: The graph data to export
        format_type: Export format ('json', 'dot', 'cytoscape')
        output_path: Optional file path to save the export

    Returns:
        Exported data as string
    """
    if format_type == 'json':
        return _export_as_json(graph_data, output_path)
    elif format_type == 'dot':
        return _export_as_dot(graph_data, output_path)
    elif format_type == 'cytoscape':
        return _export_as_cytoscape(graph_data, output_path)
    else:
        raise ValueError(f"Unsupported export format: {format_type}")


def _export_as_json(graph_data: Dict[str, Any], output_path: Optional[str]) -> str:
    """Export as JSON format."""
    import json

    json_str = json.dumps(graph_data, indent=2, ensure_ascii=False)

    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(json_str)

    return json_str


def _export_as_dot(graph_data: Dict[str, Any], output_path: Optional[str]) -> str:
    """Export as Graphviz DOT format."""
    nodes = graph_data.get('nodes', {})
    edges = graph_data.get('edges', [])

    dot_lines = ['digraph ArchitectureGraph {']
    dot_lines.append('  rankdir=TB;')
    dot_lines.append('  node [shape=box];')

    # Add nodes
    for node_id, node_data in nodes.items():
        node_type = node_data.get('type', 'unknown')
        node_name = node_data.get('name', node_id.split('::')[-1])
        language = node_data.get('language', 'unknown')

        # Color by type
        colors = {
            NodeType.FILE.value: 'lightblue',
            NodeType.CLASS.value: 'lightgreen',
            NodeType.FUNCTION.value: 'lightyellow',
            NodeType.METHOD.value: 'lightcoral'
        }
        color = colors.get(node_type, 'lightgray')

        safe_id = node_id.replace(':', '_').replace('/', '_').replace('.', '_')
        label = f"{node_name}\\n({node_type}, {language})"

        dot_lines.append(f'  {safe_id} [label="{label}", fillcolor="{color}", style=filled];')

    # Add edges
    for edge in edges:
        source = edge['source'].replace(':', '_').replace('/', '_').replace('.', '_')
        target = edge['target'].replace(':', '_').replace('/', '_').replace('.', '_')
        edge_type = edge.get('type', 'unknown')

        # Edge styles by type
        styles = {
            EdgeType.IMPORTS.value: 'solid',
            EdgeType.INHERITS_FROM.value: 'dashed',
            EdgeType.CALLS.value: 'dotted'
        }
        style = styles.get(edge_type, 'solid')

        dot_lines.append(f'  {source} -> {target} [label="{edge_type}", style="{style}"];')

    dot_lines.append('}')
    dot_content = '\n'.join(dot_lines)

    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(dot_content)

    return dot_content


def _export_as_cytoscape(graph_data: Dict[str, Any], output_path: Optional[str]) -> str:
    """Export as Cytoscape.js JSON format."""
    import json

    nodes = graph_data.get('nodes', {})
    edges = graph_data.get('edges', [])

    cytoscape_data = {
        "elements": {
            "nodes": [],
            "edges": []
        }
    }

    # Convert nodes
    for node_id, node_data in nodes.items():
        cyto_node = {
            "data": {
                "id": node_id,
                "label": node_data.get('name', node_id.split('::')[-1]),
                "type": node_data.get('type', 'unknown'),
                "language": node_data.get('language', 'unknown')
            }
        }
        cytoscape_data["elements"]["nodes"].append(cyto_node)

    # Convert edges
    for i, edge in enumerate(edges):
        cyto_edge = {
            "data": {
                "id": f"edge_{i}",
                "source": edge['source'],
                "target": edge['target'],
                "type": edge.get('type', 'unknown'),
                "weight": edge.get('weight', 1.0)
            }
        }
        cytoscape_data["elements"]["edges"].append(cyto_edge)

    json_str = json.dumps(cytoscape_data, indent=2)

    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(json_str)

    return json_str


# ==============================================================================
# SECTION 5: CONFIGURATION AND VALIDATION
# ==============================================================================

def validate_graph_data(graph_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate the integrity of graph data and return validation results.

    Args:
        graph_data: The graph data to validate

    Returns:
        Dictionary containing validation results and any issues found
    """
    validation_results = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "statistics": {}
    }

    nodes = graph_data.get('nodes', {})
    edges = graph_data.get('edges', [])

    # Check for orphaned edges
    node_ids = set(nodes.keys())
    orphaned_edges = []

    for i, edge in enumerate(edges):
        source = edge.get('source')
        target = edge.get('target')

        if source and source not in node_ids:
            orphaned_edges.append(f"Edge {i}: source '{source}' not found in nodes")

        # Note: targets might be external dependencies, so we don't validate them as strictly

    if orphaned_edges:
        validation_results["warnings"].extend(orphaned_edges)

    # Check for required node fields
    invalid_nodes = []
    for node_id, node_data in nodes.items():
        if not isinstance(node_data, dict):
            invalid_nodes.append(f"Node '{node_id}': data is not a dictionary")
            continue

        required_fields = ['type']
        missing_fields = [field for field in required_fields if field not in node_data]

        if missing_fields:
            invalid_nodes.append(f"Node '{node_id}': missing fields {missing_fields}")

    if invalid_nodes:
        validation_results["errors"].extend(invalid_nodes)
        validation_results["is_valid"] = False

    # Gather statistics
    validation_results["statistics"] = {
        "total_nodes": len(nodes),
        "total_edges": len(edges),
        "node_types": {},
        "edge_types": {},
        "languages": set()
    }

    # Count node types and languages
    for node_data in nodes.values():
        node_type = node_data.get('type', 'unknown')
        language = node_data.get('language', 'unknown')

        validation_results["statistics"]["node_types"][node_type] = \
            validation_results["statistics"]["node_types"].get(node_type, 0) + 1
        validation_results["statistics"]["languages"].add(language)

    # Count edge types
    for edge in edges:
        edge_type = edge.get('type', 'unknown')
        validation_results["statistics"]["edge_types"][edge_type] = \
            validation_results["statistics"]["edge_types"].get(edge_type, 0) + 1

    # Convert set to list for JSON serialization
    validation_results["statistics"]["languages"] = list(validation_results["statistics"]["languages"])

    return validation_results


def create_analysis_config(**kwargs) -> Dict[str, Any]:
    """
    Create a configuration dictionary for graph analysis.

    Keyword Args:
        include_private: Whether to include private methods/functions
        min_confidence: Minimum confidence threshold for edges
        enable_metrics: Whether to calculate project metrics
        max_depth: Maximum analysis depth for nested structures
        excluded_patterns: List of file patterns to exclude
        language_specific: Dictionary of language-specific options

    Returns:
        Configuration dictionary
    """
    default_config = {
        'include_private': True,
        'min_confidence': 0.0,
        'enable_metrics': True,
        'max_depth': 10,
        'excluded_patterns': ['*.pyc', '*.pyo', '__pycache__/*', 'node_modules/*'],
        'language_specific': {
            'python': {
                'include_decorators': True,
                'track_async_functions': True
            },
            'javascript': {
                'include_arrow_functions': True,
                'track_async_functions': True,
                'parse_jsx': False
            }
        }
    }

    # Update with provided kwargs
    config = default_config.copy()
    config.update(kwargs)

    return config

--- FILE_END: backend/lumiere_core/services/cartographer.py ---


--- FILE_START: backend/lumiere_core/services/cortex_service.py ---
# backend/lumiere_core/services/cortex_service.py

import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

# Constants
CORTEX_FILENAME_TEMPLATE = "{repo_id}_cortex.json"
CLONED_REPOS_DIR = "cloned_repositories"


class CortexFileNotFound(Exception):
    """Raised when the Cortex file is missing for a given repository."""


class CortexFileMalformed(Exception):
    """Raised when the Cortex file is unreadable or not valid JSON."""


def _get_cortex_path(repo_id: str) -> Path:
    """
    Constructs the full path to a repository's Cortex file.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Path object pointing to the expected Cortex file location.
    """
    base_dir = Path(__file__).resolve().parent.parent.parent
    return base_dir / CLONED_REPOS_DIR / repo_id / CORTEX_FILENAME_TEMPLATE.format(repo_id=repo_id)


def load_cortex_data(repo_id: str) -> Dict[str, Any]:
    """
    Loads and parses the cortex JSON file for a given repository.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Parsed JSON data as a dictionary.

    Raises:
        CortexFileNotFound: If the cortex file does not exist.
        CortexFileMalformed: If the file is not valid JSON.
    """
    cortex_path = _get_cortex_path(repo_id)

    if not cortex_path.exists():
        logger.error(f"Cortex file not found at: {cortex_path}")
        raise CortexFileNotFound(f"Cortex file not found for repo: {repo_id}")

    try:
        with cortex_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.exception(f"Failed to parse cortex file: {cortex_path}")
        raise CortexFileMalformed(f"Failed to load or parse cortex file: {e}") from e


def get_file_content(repo_id: str, file_path: str) -> Optional[str]:
    """
    Retrieves the raw content of a specific file from the repository's Cortex data.

    Args:
        repo_id: The unique ID of the repository.
        file_path: The relative path of the file within the repo.

    Returns:
        The raw file content as a string, or None if the file isn't found.
    """
    try:
        cortex_data = load_cortex_data(repo_id)
        for file_entry in cortex_data.get("files", []):
            if file_entry.get("file_path") == file_path:
                return file_entry.get("raw_content")
    except (CortexFileNotFound, CortexFileMalformed):
        return None

    return None

--- FILE_END: backend/lumiere_core/services/cortex_service.py ---


--- FILE_START: backend/lumiere_core/services/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/__init__.py

--- FILE_END: backend/lumiere_core/services/__init__.py ---


--- FILE_START: backend/lumiere_core/services/code_surgery.py ---
# backend/lumiere_core/services/code_surgery.py

import re
import json
from typing import Dict, Tuple, Optional, List

def _get_function_signature(code_block: str) -> str:
    """Extracts the function signature (e.g., 'function setupWelcomeSheet(ss)') from a block of code."""
    # Handle both regular functions and arrow functions
    match = re.search(r"(?:function\s+\w+\s*\(.*?\)|\w+\s*[:=]\s*\(.*?\)\s*=>)", code_block)
    if match:
        return match.group(0)
    return ""

def _validate_function_code(func_code: str, expected_func_name: str) -> bool:
    """Validates that the function code contains a proper function definition."""
    if not func_code or not func_code.strip():
        return False

    # Check if it contains the expected function name
    if expected_func_name not in func_code:
        return False

    # Check if it has function keyword or arrow function syntax
    has_function_keyword = "function" in func_code
    has_arrow_function = "=>" in func_code

    if not (has_function_keyword or has_arrow_function):
        return False

    # Basic brace balance check
    open_braces = func_code.count('{')
    close_braces = func_code.count('}')

    return open_braces > 0 and open_braces == close_braces

def _extract_function_parameters(signature: str) -> List[str]:
    """Extract parameter names from a function signature."""
    match = re.search(r'\((.*?)\)', signature)
    if not match:
        return []

    params_str = match.group(1).strip()
    if not params_str:
        return []

    # Split by comma and clean up parameter names
    params = [param.strip().split('=')[0].strip() for param in params_str.split(',')]
    return [p for p in params if p]

def _signatures_compatible(old_signature: str, new_signature: str) -> bool:
    """Check if two function signatures are compatible."""
    old_params = _extract_function_parameters(old_signature)
    new_params = _extract_function_parameters(new_signature)

    # Allow new function to have same or fewer required parameters
    # This is a basic check - could be more sophisticated
    return len(new_params) <= len(old_params) + 1  # Allow one extra parameter

def _find_function_boundaries(content: str, func_name: str) -> Optional[Tuple[int, int]]:
    """
    Find the start and end positions of a function in the content.
    Returns (start_pos, end_pos) or None if not found.
    """
    # Escape function name for regex
    escaped_name = re.escape(func_name)

    # Pattern to match function declaration
    pattern = rf"function\s+{escaped_name}\s*\(.*?\)\s*\{{"

    match = re.search(pattern, content, re.MULTILINE)
    if not match:
        return None

    start_pos = match.start()

    # Find the matching closing brace
    brace_count = 0
    pos = match.end() - 1  # Start from the opening brace

    while pos < len(content):
        char = content[pos]
        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0:
                return (start_pos, pos + 1)
        pos += 1

    return None

def _validate_snippets_response(data: Dict) -> Tuple[bool, str]:
    """
    Validate that the LLM response has the expected structure.
    Returns (is_valid, error_message)
    """
    if not isinstance(data, dict):
        return False, "Response is not a dictionary"

    for file_path, snippets in data.items():
        if not isinstance(file_path, str):
            return False, f"File path is not a string: {file_path}"

        if not isinstance(snippets, dict):
            return False, f"Snippets for {file_path} is not a dictionary"

        for func_name, func_code in snippets.items():
            if not isinstance(func_name, str):
                return False, f"Function name is not a string: {func_name}"

            if not isinstance(func_code, str):
                return False, f"Function code for {func_name} is not a string"

            if not _validate_function_code(func_code, func_name):
                return False, f"Invalid function code for {func_name}"

    return True, ""

def replace_functions_in_file(original_content: str, changed_snippets: Dict[str, str]) -> str:
    """
    Surgically replaces functions in an original file with new versions.

    Args:
        original_content: The full original content of the file.
        changed_snippets: A dictionary where keys are function names and values are the new function code.

    Returns:
        The full file content with the functions replaced.
    """
    if not original_content or not original_content.strip():
        print("⚠️ Warning: Original content is empty")
        return original_content

    if not changed_snippets:
        print("⚠️ Warning: No snippets to replace")
        return original_content

    modified_content = original_content
    replacement_count = 0

    for func_name, new_func_code in changed_snippets.items():
        # Validate the new function code
        if not _validate_function_code(new_func_code, func_name):
            print(f"⚠️ Skipping snippet for '{func_name}' - invalid function code")
            continue

        # Find function boundaries
        boundaries = _find_function_boundaries(modified_content, func_name)
        if not boundaries:
            print(f"⚠️ Warning: Could not find function '{func_name}' in the original file")
            continue

        start_pos, end_pos = boundaries

        # Extract original function for comparison
        original_func = modified_content[start_pos:end_pos]
        original_signature = _get_function_signature(original_func)
        new_signature = _get_function_signature(new_func_code)

        # Check signature compatibility
        if original_signature and new_signature:
            if not _signatures_compatible(original_signature, new_signature):
                print(f"⚠️ Warning: Function '{func_name}' has incompatible signature")
                print(f"   Original: {original_signature}")
                print(f"   New: {new_signature}")
                # Still proceed but warn the user

        # Perform the replacement
        modified_content = (
            modified_content[:start_pos] +
            new_func_code +
            modified_content[end_pos:]
        )

        replacement_count += 1
        print(f"✓ Replaced function '{func_name}'")

    if replacement_count == 0:
        print("⚠️ Warning: No functions were replaced")
    else:
        print(f"✓ Successfully replaced {replacement_count} function(s)")

    return modified_content

def validate_and_parse_snippets(llm_response: str) -> Tuple[Optional[Dict], str]:
    """
    Validate and parse the LLM response containing function snippets.
    Returns (parsed_data, error_message)
    """
    if not llm_response or not llm_response.strip():
        return None, "LLM response is empty"

    # Try to extract JSON from the response
    json_str = extract_json_from_llm(llm_response)
    if not json_str:
        return None, "Could not extract JSON from LLM response"

    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        return None, f"Failed to parse JSON: {str(e)}"

    # Validate structure
    is_valid, error_msg = _validate_snippets_response(data)
    if not is_valid:
        return None, f"Invalid response structure: {error_msg}"

    return data, ""

def extract_json_from_llm(llm_response: str) -> Optional[str]:
    """
    Extract JSON content from LLM response that might contain additional text.
    """
    # Look for content between ```json and ``` or between { and }
    json_patterns = [
        r'```json\s*(.*?)\s*```',
        r'```\s*(.*?)\s*```',
        r'(\{.*\})',
    ]

    for pattern in json_patterns:
        match = re.search(pattern, llm_response, re.DOTALL)
        if match:
            json_candidate = match.group(1).strip()
            # Try to parse to validate it's JSON
            try:
                json.loads(json_candidate)
                return json_candidate
            except json.JSONDecodeError:
                continue

    return None

def get_relevant_code_from_cortex(content: str, rca_report: str) -> str:
    """
    Extract relevant code sections based on the RCA report.
    This is a simplified version - you might want to make this more sophisticated.
    """
    if not content or not rca_report:
        return content

    # For now, return the full content
    # In a more sophisticated version, you could:
    # 1. Extract function names mentioned in the RCA report
    # 2. Find and return only those functions
    # 3. Include imports and dependencies

    return content

--- FILE_END: backend/lumiere_core/services/code_surgery.py ---


--- FILE_START: backend/lumiere_core/services/ambassador.py ---
# In backend/lumiere_core/services/ambassador.py

import os
import re
import time
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dotenv import load_dotenv

from github import Github, GithubException

from .github import scrape_github_issue, _parse_github_issue_url
from . import llm_service
from ingestion.crawler import IntelligentCrawler

# --- Configuration ---
load_dotenv(dotenv_path=Path(__file__).resolve().parent.parent / '.env')
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
GITHUB_USERNAME = os.getenv("GITHUB_FORK_USERNAME")

if not GITHUB_TOKEN or not GITHUB_USERNAME:
    raise ValueError("GITHUB_ACCESS_TOKEN and GITHUB_FORK_USERNAME must be set in the .env file.")

g = Github(GITHUB_TOKEN)
user = g.get_user(GITHUB_USERNAME)

def _sanitize_branch_name(text: str) -> str:
    """Creates a URL- and git-safe branch name from a string."""
    text = text.lower()
    text = re.sub(r'[\s/]+', '-', text)
    text = re.sub(r'[^a-z0-9-]', '', text)
    text = text.strip('-')
    return text[:60]

def _validate_file_changes(modified_files: Dict[str, str]) -> List[str]:
    """Validates the file changes dictionary and returns any validation errors."""
    errors = []

    if not modified_files:
        errors.append("No files provided for modification")
        return errors

    for file_path, content in modified_files.items():
        if not isinstance(file_path, str) or not file_path.strip():
            errors.append(f"Invalid file path: {repr(file_path)}")

        if not isinstance(content, str):
            errors.append(f"Invalid content type for {file_path}: expected string, got {type(content)}")

        # Check for potentially dangerous paths
        normalized_path = Path(file_path).resolve()
        if str(normalized_path).startswith('/') or '..' in file_path:
            errors.append(f"Potentially unsafe file path: {file_path}")

    return errors

def _write_files_safely(repo_path: Path, modified_files: Dict[str, str]) -> List[str]:
    """
    Safely writes files to the repository with proper error handling.
    Returns a list of successfully written files.
    """
    successfully_written = []

    for file_path, new_content in modified_files.items():
        try:
            full_target_path = repo_path / file_path

            # Ensure parent directories exist
            full_target_path.parent.mkdir(parents=True, exist_ok=True)

            # Create backup if file exists
            backup_path = None
            if full_target_path.exists():
                backup_path = full_target_path.with_suffix(full_target_path.suffix + '.bak')
                full_target_path.rename(backup_path)

            try:
                # Write new content with explicit encoding
                full_target_path.write_text(new_content, encoding='utf-8')

                # Stage the file
                result = subprocess.run(
                    ['git', 'add', file_path],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    check=True
                )

                successfully_written.append(file_path)
                print(f"✓ Staged changes for: {file_path}")

                # Remove backup if successful
                if backup_path and backup_path.exists():
                    backup_path.unlink()

            except Exception as e:
                # Restore backup if write failed
                if backup_path and backup_path.exists():
                    if full_target_path.exists():
                        full_target_path.unlink()
                    backup_path.rename(full_target_path)
                raise e

        except Exception as e:
            print(f"⚠ Failed to write {file_path}: {e}")
            # Continue with other files rather than failing completely
            continue

    return successfully_written

def _run_git_command(command: List[str], repo_path: Path, description: str) -> bool:
    """
    Runs a git command with proper error handling and logging.
    Returns True if successful, False otherwise.
    """
    try:
        result = subprocess.run(
            command,
            cwd=repo_path,
            capture_output=True,
            text=True,
            check=True
        )
        print(f"✓ {description}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"✗ {description} failed: {e.stderr}")
        return False

def _generate_pr_body(issue_data: Dict[str, Any], issue_number: int, modified_files: Dict[str, str]) -> str:
    """Generates a comprehensive PR body with file change summary."""
    pr_body = f"""
This pull request was automatically generated and approved by the user via the Lumière Sémantique 'Socratic Dialogue' interface to address Issue #{issue_number}.

## Issue Summary
{issue_data.get('body', 'No description provided.')[:500]}{'...' if len(issue_data.get('body', '')) > 500 else ''}

## Changes in this PR
This PR modifies **{len(modified_files)}** file(s) to resolve the issue:

"""

    # Group files by type for better organization
    file_groups = {}
    for file_path in modified_files.keys():
        ext = Path(file_path).suffix or 'no-extension'
        if ext not in file_groups:
            file_groups[ext] = []
        file_groups[ext].append(file_path)

    for ext, files in sorted(file_groups.items()):
        if ext != 'no-extension':
            pr_body += f"\n**{ext.upper()} files:**\n"
        else:
            pr_body += f"\n**Other files:**\n"

        for file_path in sorted(files):
            pr_body += f"- `{file_path}`\n"

    pr_body += "\n---\n*This fix was validated by The Crucible against the project's existing test suite.*"

    return pr_body

# --- ENHANCED: Multi-file dispatch with better error handling and validation ---
def dispatch_pr(
    issue_url: str,
    modified_files: Union[Dict[str, str], str],
    model_identifier: str,
    custom_commit_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Orchestrates the git operations and PR creation for a multi-file change set.

    Args:
        issue_url: GitHub issue URL
        modified_files: Dictionary of file paths to content, or single file path for backward compatibility
        model_identifier: LLM model to use for generating commit messages
        custom_commit_message: Optional custom commit message to override LLM generation

    Returns:
        Dictionary with status and either success data or error information
    """
    print("--- AMBASSADOR AGENT ACTIVATED (MULTI-FILE MODE) ---")
    print(f"Target Issue: {issue_url}")

    # Handle backward compatibility - convert single file to dict format
    if isinstance(modified_files, str):
        print("⚠ Legacy single-file mode detected - consider upgrading to dictionary format")
        # This would need additional parameters for content in legacy mode
        # For now, we'll assume it's already in the new format
        pass

    if isinstance(modified_files, dict):
        print(f"Files to modify: {list(modified_files.keys())}")

        # Validate input
        validation_errors = _validate_file_changes(modified_files)
        if validation_errors:
            return {
                "error": f"Validation failed: {'; '.join(validation_errors)}",
                "validation_errors": validation_errors
            }
    else:
        return {"error": "modified_files must be a dictionary of file paths to content"}

    print("\n[Step 1/4] Gathering Intel...")
    try:
        issue_data = scrape_github_issue(issue_url)
        if not issue_data:
            raise ValueError("Failed to scrape issue data from GitHub.")

        parsed_url = _parse_github_issue_url(issue_url)
        if not parsed_url:
            raise ValueError("Could not parse the provided issue URL.")
        owner, repo_name, issue_number = parsed_url
        repo_full_name = f"{owner}/{repo_name}"
        print(f"✓ Intel gathered for issue #{issue_number}.")
    except Exception as e:
        return {"error": f"Failed during intel gathering: {e}", "step": "intel_gathering"}

    print("\n[Step 2/4] Preparing Repository...")
    try:
        upstream_repo = g.get_repo(repo_full_name)
        working_repo = None

        if upstream_repo.owner.login == GITHUB_USERNAME:
            print(f"✓ Target repo is owned by '{GITHUB_USERNAME}'. Skipping fork.")
            working_repo = upstream_repo
        else:
            print(f"Target repo owned by '{upstream_repo.owner.login}'. Finding or creating fork.")
            try:
                working_repo = g.get_repo(f"{GITHUB_USERNAME}/{repo_name}")
                print(f"✓ Fork '{working_repo.full_name}' already exists.")
            except GithubException:
                print("Fork not found. Creating fork...")
                working_repo = upstream_repo.create_fork()
                print(f"✓ Fork created at '{working_repo.full_name}'. Waiting for it to be ready...")
                time.sleep(15)

        with IntelligentCrawler(repo_url=working_repo.clone_url) as crawler:
            repo_path = crawler.repo_path
            sanitized_title = _sanitize_branch_name(issue_data['title'])
            branch_name = f"lumiere-fix/{issue_number}-{sanitized_title}"
            print(f"✓ Cloned {working_repo.full_name} to '{repo_path}'")

            print(f"\n[Step 3/4] Applying Fix on new branch '{branch_name}'...")
            default_branch = upstream_repo.default_branch

            # Use the new git command helper
            if not _run_git_command(['git', 'checkout', default_branch], repo_path, f"Switched to {default_branch}"):
                return {"error": "Failed to checkout default branch", "step": "git_operations"}

            if not _run_git_command(['git', 'pull', 'origin', default_branch], repo_path, f"Pulled latest {default_branch}"):
                return {"error": "Failed to pull latest changes", "step": "git_operations"}

            if not _run_git_command(['git', 'checkout', '-b', branch_name], repo_path, f"Created branch {branch_name}"):
                return {"error": "Failed to create new branch", "step": "git_operations"}

            # Write files with improved error handling
            successfully_written = _write_files_safely(repo_path, modified_files)

            if not successfully_written:
                return {
                    "error": "No files were successfully written",
                    "step": "file_operations",
                    "attempted_files": list(modified_files.keys())
                }

            if len(successfully_written) < len(modified_files):
                print(f"⚠ Warning: Only {len(successfully_written)}/{len(modified_files)} files were successfully written")

            # Generate or use custom commit message
            if custom_commit_message:
                commit_message = custom_commit_message.strip()
                print(f"✓ Using custom commit message: \"{commit_message}\"")
            else:
                commit_prompt = f"""Based on the issue title '{issue_data['title']}' and the following files being modified: {', '.join(successfully_written)}, write a concise, one-line commit message following the Conventional Commits specification (e.g., 'fix: ...', 'feat: ...', 'refactor: ...'). Output ONLY the commit message line."""

                commit_message = llm_service.generate_text(commit_prompt, model_identifier=model_identifier).strip()
                print(f"✓ Generated commit message: \"{commit_message}\"")

            if not _run_git_command(['git', 'commit', '-m', commit_message], repo_path, "Committed changes"):
                return {"error": "Failed to commit changes", "step": "git_operations"}

            if not _run_git_command(['git', 'push', '--set-upstream', 'origin', branch_name], repo_path, "Pushed branch"):
                return {"error": "Failed to push branch", "step": "git_operations"}

            print("\n[Step 4/4] Creating Pull Request...")
            pr_title = f"Fix: {issue_data['title']} (Resolves #{issue_number})"
            pr_body = _generate_pr_body(issue_data, issue_number, modified_files)

            head_branch = f"{working_repo.owner.login}:{branch_name}"
            time.sleep(5)  # Brief pause to ensure branch is available

            try:
                pull_request = upstream_repo.create_pull(
                    title=pr_title, body=pr_body, head=head_branch, base=default_branch
                )
                print(f"✓ Pull Request created successfully: {pull_request.html_url}")

                return {
                    "status": "success",
                    "pull_request_url": pull_request.html_url,
                    "files_modified": successfully_written,
                    "files_attempted": list(modified_files.keys()),
                    "commit_message": commit_message,
                    "branch_name": branch_name
                }
            except GithubException as e:
                return {
                    "error": f"Failed to create pull request: {e.data}",
                    "step": "pr_creation",
                    "files_modified": successfully_written
                }

    except Exception as e:
        error_details = str(e)
        if isinstance(e, subprocess.CalledProcessError):
            error_details = f"Git Command Failed: {e.stderr if e.stderr else e.stdout}"
        elif isinstance(e, GithubException):
            error_details = f"GitHub API Error ({e.status}): {e.data}"

        print(f"Error during Git Operations or PR Creation: {error_details}")
        return {
            "error": f"Failed during repository operations: {error_details}",
            "step": "repository_operations"
        }

--- FILE_END: backend/lumiere_core/services/ambassador.py ---


--- FILE_START: backend/lumiere_core/services/scaffolding.py ---
# backend/lumiere_core/services/scaffolding.py

import json
import re
from pathlib import Path
from typing import Dict, Optional, List, Any

from . import llm_service
from .utils import clean_llm_code_output
from . import code_surgery 

def _is_valid_json(json_str: str) -> bool:
    """Helper function to validate JSON strings."""
    try:
        parsed = json.loads(json_str)
        return isinstance(parsed, dict) and len(parsed) > 0
    except (json.JSONDecodeError, TypeError, ValueError):
        return False


def _extract_json_from_llm(raw_text: str) -> Optional[str]:
    """
    Ultra-robust JSON extraction from LLM responses.
    Tries multiple strategies with extensive pattern matching.
    """

    # Clean the input text
    raw_text = raw_text.strip()

    # Strategy 1: Try Markdown JSON code block with comprehensive patterns
    markdown_patterns = [
        r"```(?:json|JSON)?\s*(\{.*?\})\s*```",      # Standard markdown
        r"```(?:json|JSON)?\n(\{.*?\})\n```",        # With newlines
        r"```(\{.*?\})```",                          # Simple fences
        r"`(\{.*?\})`",                              # Single backticks
        r"```(?:json|JSON)?\s*(\{.*?)\s*```",        # Incomplete end brace
        r"(?i)```json\s*(\{.*?\})\s*```",            # Case insensitive
    ]

    for pattern in markdown_patterns:
        matches = re.finditer(pattern, raw_text, re.DOTALL | re.IGNORECASE)
        for match in matches:
            json_candidate = match.group(1).strip()
            if _is_valid_json(json_candidate):
                return json_candidate

    # Strategy 2: Enhanced bracket counting with multiple attempts
    json_candidates = []
    stack = []
    start_indices = []

    for i, char in enumerate(raw_text):
        if char == '{':
            if not stack:
                start_indices.append(i)
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
                if not stack and start_indices:
                    start_index = start_indices.pop()
                    json_candidate = raw_text[start_index:i+1]
                    json_candidates.append(json_candidate)

    # Test all candidates from bracket counting
    for candidate in json_candidates:
        if _is_valid_json(candidate):
            return candidate

    # Strategy 3: Look for file path patterns (more flexible)
    file_patterns = [
        r'\{[^{}]*?"[^"]*\.(?:py|js|ts|java|cpp|c|h|html|css|json|yaml|yml|xml|md|txt|go|rb|php|swift|kt|dart|rs|scala|sql)"[^{}]*?:.*?\}',
        r'\{.*?"[^"]*/"[^"]*\.(?:py|js|ts|java|cpp|c|h|html|css|json|yaml|yml|xml|md|txt|go|rb|php|swift|kt|dart|rs|scala|sql)".*?:.*?\}',
        r'\{.*?["\'][^"\']*\.(?:py|js|ts|java|cpp|c|h|html|css|json|yaml|yml|xml|md|txt|go|rb|php|swift|kt|dart|rs|scala|sql)["\'].*?:.*?\}',
    ]

    for pattern in file_patterns:
        matches = re.finditer(pattern, raw_text, re.DOTALL)
        for match in matches:
            json_candidate = match.group(0)
            if _is_valid_json(json_candidate):
                return json_candidate

    # Strategy 4: Try to extract from common LLM response patterns
    response_patterns = [
        r'(?:Here(?:\'s|s| is)? (?:the )?(?:JSON|json|response|fix|solution|code)?:?\s*)(\{.*?\})',
        r'(?:Response|Answer|Solution|Fix):\s*(\{.*?\})',
        r'(\{[^{}]*?["\'][^"\']*\.(?:py|js|ts|java|cpp)["\'][^{}]*?:.*?\})',
    ]

    for pattern in response_patterns:
        matches = re.finditer(pattern, raw_text, re.DOTALL | re.IGNORECASE)
        for match in matches:
            json_candidate = match.group(1).strip()
            if _is_valid_json(json_candidate):
                return json_candidate

    # Strategy 5: Last resort - try to find ANY valid JSON structure
    # Look for balanced braces and try to extract
    brace_positions = []
    for i, char in enumerate(raw_text):
        if char in '{}':
            brace_positions.append((i, char))

    # Try different combinations of brace positions
    for start_pos in range(len(brace_positions)):
        if brace_positions[start_pos][1] == '{':
            brace_count = 0
            for end_pos in range(start_pos, len(brace_positions)):
                if brace_positions[end_pos][1] == '{':
                    brace_count += 1
                elif brace_positions[end_pos][1] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        start_idx = brace_positions[start_pos][0]
                        end_idx = brace_positions[end_pos][0] + 1
                        json_candidate = raw_text[start_idx:end_idx]
                        if _is_valid_json(json_candidate):
                            return json_candidate
                        break

    return None


def _sanitize_json_response(json_str: str) -> str:
    """
    Comprehensive JSON sanitization to fix common LLM formatting issues.
    """
    # Remove common prefixes/suffixes that LLMs add
    prefixes_to_remove = [
        "Here's the JSON response:",
        "Here is the JSON:",
        "Here's the JSON:",
        "The JSON object is:",
        "Response:",
        "Here's the fix:",
        "Solution:",
        "The fix is:",
        "Here's your fix:",
        "JSON:",
        "```json",
        "```",
        "Answer:",
        "Result:",
    ]

    suffixes_to_remove = [
        "Let me know if you need any clarification!",
        "This should fix the issue.",
        "Hope this helps!",
        "```",
        "Let me know if you have any questions.",
        "Please let me know if you need any modifications.",
        "This addresses the issue mentioned in the RCA report.",
    ]

    cleaned = json_str.strip()

    # Remove prefixes (case insensitive)
    for prefix in prefixes_to_remove:
        if cleaned.lower().startswith(prefix.lower()):
            cleaned = cleaned[len(prefix):].strip()
            break  # Only remove one prefix

    # Remove suffixes (case insensitive)
    for suffix in suffixes_to_remove:
        if cleaned.lower().endswith(suffix.lower()):
            cleaned = cleaned[:-len(suffix)].strip()
            break  # Only remove one suffix

    # Fix common JSON syntax issues
    # Replace smart quotes with regular quotes
    cleaned = cleaned.replace('"', '"').replace('"', '"')
    cleaned = cleaned.replace(''', "'").replace(''', "'")

    # Replace backticks that might be used as quotes
    cleaned = cleaned.replace('`', '"')

    # Fix trailing commas (common LLM mistake)
    cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)

    # Fix missing commas between objects/arrays
    cleaned = re.sub(r'}\s*{', '},{', cleaned)
    cleaned = re.sub(r']\s*\[', '],[', cleaned)

    # Fix unescaped quotes in strings (basic attempt)
    # This is tricky, so we'll just try to fix obvious cases
    cleaned = re.sub(r'([^\\])"([^",:}\]]*)"([^",:}\]]*)"', r'\1"\2\"\3"', cleaned)

    # Fix common spacing issues
    cleaned = re.sub(r'\s*:\s*', ':', cleaned)
    cleaned = re.sub(r'\s*,\s*', ',', cleaned)

    # Remove any remaining markdown code block indicators
    cleaned = re.sub(r'^```[a-zA-Z]*\s*', '', cleaned)
    cleaned = re.sub(r'\s*```$', '', cleaned)

    return cleaned


def _attempt_json_repair(raw_text: str) -> Optional[str]:
    """
    Last resort JSON repair for severely malformed responses.
    Attempts to construct valid JSON from recognizable patterns.
    """
    try:
        # Look for file path and content patterns
        file_pattern = r'["\']([^"\']*\.(?:py|js|ts|java|cpp|c|h|html|css|json|yaml|yml|xml|md|txt))["\']'
        files_found = re.findall(file_pattern, raw_text)

        if not files_found:
            return None

        # Try to build a JSON structure
        result = {}

        # Simple pattern matching for file content
        for file_path in files_found:
            # Look for content after the file path
            content_patterns = [
                rf'["\']({re.escape(file_path)})["\']:\s*["\']([^"\']*?)["\']',
                rf'["\']({re.escape(file_path)})["\']:\s*"""([^"]*?)"""',
                rf'["\']({re.escape(file_path)})["\']:\s*```([^`]*?)```',
            ]

            for pattern in content_patterns:
                match = re.search(pattern, raw_text, re.DOTALL)
                if match:
                    result[file_path] = match.group(2).strip()
                    break

        if result:
            return json.dumps(result)

    except Exception:
        pass

    return None


def generate_scaffold(
    repo_id: str,
    target_files: List[str],
    instruction: str,
    model_identifier: str,
    rca_report: str,
    refinement_history: Optional[List[Dict[str, str]]] = None
) -> Dict[str, Any]:
    """
    Core logic for Code Scaffolding using the new "Surgical" Two-Step method.

    Returns:
        Dict containing either success data or error information
    """
    print(f"🔧 Initiating Surgical Scaffolding for {target_files} in repo '{repo_id}'")

    try:
        # --- SETUP AND VALIDATION ---
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_path.exists():
            return {
                "error": "Cortex file not found",
                "details": f"Expected path: {cortex_path}"
            }

        # Load cortex data
        try:
            with open(cortex_path, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            return {
                "error": "Failed to load cortex file",
                "details": str(e)
            }

        # Build file map and validate target files
        file_map = {file['file_path']: file['raw_content'] for file in cortex_data.get('files', [])}
        original_contents = {}
        missing_files = []

        for fp in target_files:
            if fp in file_map and file_map[fp]:
                original_contents[fp] = file_map[fp]
            else:
                missing_files.append(fp)

        if missing_files:
            return {
                "error": "Some target files not found in cortex",
                "missing_files": missing_files,
                "available_files": list(file_map.keys())
            }

        if not original_contents:
            return {
                "error": "No valid target files found",
                "target_files": target_files
            }

        print(f"✓ Loaded {len(original_contents)} target files")

        # --- STEP 1: GENERATE THE SNIPPETS ---
        print("📝 Step 1: Generating function snippets...")

        # Build relevant code section
        file_content_prompt_section = "\n\n### RELEVANT EXISTING CODE\n"
        for path, content in original_contents.items():
            if path.endswith(('.js', '.gs', '.ts')):
                # Use the improved function from code_surgery
                compressed_content = code_surgery.get_relevant_code_from_cortex(content, rca_report)
                file_content_prompt_section += f"<file path=\"{path}\">\n{compressed_content}\n</file>\n\n"

        # Build refinement context
        refinement_context = ""
        if refinement_history:
            refinement_context = "\n\n### PREVIOUS REFINEMENT ATTEMPTS\n"
            for i, refinement in enumerate(refinement_history[-3:]):  # Last 3 attempts
                refinement_context += f"Attempt {i+1}: {refinement.get('issue', 'Unknown issue')}\n"
            refinement_context += "\nPlease avoid these previous issues in your response.\n"

        snippet_prompt = f"""You are a code generation expert specializing in Google Apps Script and JavaScript.

<Instructions>
1. Read the <Goal> and <RCA_Report> carefully.
2. Based on the <RELEVANT_EXISTING_CODE>, identify and rewrite ONLY the functions that need to be changed.
3. Your response MUST be a single, valid JSON object with no additional text.
4. The keys of the JSON object MUST be the full file paths (e.g., "UtilityFunctions.gs").
5. The values MUST be JSON objects where keys are function names and values are the complete, new code for JUST THAT FUNCTION.
6. Preserve existing function signatures unless specifically required to change them.
7. Ensure all functions are syntactically correct and complete.
8. Include proper error handling and logging where appropriate.
</Instructions>

<Example_Response_Format>
{{
  "UtilityFunctions.gs": {{
    "setupWelcomeSheet": "function setupWelcomeSheet(ss) {{\\n  // New implementation\\n  try {{\\n    // ... code ...\\n  }} catch (error) {{\\n    console.error('Error in setupWelcomeSheet:', error);\\n    throw error;\\n  }}\\n}}",
    "getConfigMap": "function getConfigMap() {{\\n  // New implementation\\n  return {{}};\\n}}"
  }},
  "Code.gs": {{
    "onOpen": "function onOpen(e) {{\\n  // New implementation\\n}}"
  }}
}}
</Example_Response_Format>

<Goal>{instruction}</Goal>

<RCA_Report>{rca_report}</RCA_Report>
{refinement_context}
{file_content_prompt_section}

Generate the JSON object containing only the modified function snippets now:"""

        # Generate snippets with retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"  🤖 Attempt {attempt + 1}: Calling LLM...")
                llm_response = llm_service.generate_text(snippet_prompt, model_identifier)

                if not llm_response or not llm_response.strip():
                    if attempt < max_retries - 1:
                        print(f"  ⚠️ Empty response, retrying...")
                        continue
                    return {
                        "error": "LLM returned empty response after all retries",
                        "attempts": max_retries
                    }

                # Use improved validation from code_surgery
                changed_snippets_data, error_msg = code_surgery.validate_and_parse_snippets(llm_response)

                if changed_snippets_data:
                    print(f"  ✓ Successfully parsed {len(changed_snippets_data)} file(s) with snippets")
                    break
                else:
                    print(f"  ⚠️ Attempt {attempt + 1} failed: {error_msg}")
                    if attempt < max_retries - 1:
                        continue

                    return {
                        "error": f"Failed to generate valid snippets after {max_retries} attempts",
                        "last_error": error_msg,
                        "llm_response": llm_response[:500] + "..." if len(llm_response) > 500 else llm_response
                    }

            except Exception as e:
                print(f"  ❌ Attempt {attempt + 1} error: {str(e)}")
                if attempt < max_retries - 1:
                    continue

                return {
                    "error": f"Exception during snippet generation: {str(e)}",
                    "attempts": max_retries
                }

        # --- STEP 2: PERFORM THE CODE SURGERY ---
        print("🔬 Step 2: Performing code surgery...")

        final_modified_files = {}
        surgery_errors = []

        for file_path, snippets in changed_snippets_data.items():
            if file_path in original_contents:
                print(f"  🔧 Operating on {file_path}...")
                try:
                    modified_content = code_surgery.replace_functions_in_file(
                        original_contents[file_path],
                        snippets
                    )
                    final_modified_files[file_path] = modified_content
                    print(f"  ✓ Successfully modified {file_path}")

                except Exception as e:
                    error_msg = f"Surgery failed for {file_path}: {str(e)}"
                    surgery_errors.append(error_msg)
                    print(f"  ❌ {error_msg}")
                    # Keep the original content as fallback
                    final_modified_files[file_path] = original_contents[file_path]
            else:
                warning_msg = f"LLM generated snippets for untracked file: {file_path}"
                surgery_errors.append(warning_msg)
                print(f"  ⚠️ {warning_msg}")

        # Ensure we have results for all target files
        for file_path in target_files:
            if file_path not in final_modified_files:
                final_modified_files[file_path] = original_contents[file_path]

        result = {
            "modified_files": final_modified_files,
            "original_contents": original_contents,
            "snippets_generated": changed_snippets_data,
            "files_modified": len(final_modified_files),
            "functions_targeted": sum(len(snippets) for snippets in changed_snippets_data.values())
        }

        if surgery_errors:
            result["warnings"] = surgery_errors

        print(f"🎉 Surgical scaffolding completed successfully!")
        print(f"   Files processed: {len(final_modified_files)}")
        print(f"   Functions targeted: {result['functions_targeted']}")

        return result

    except Exception as e:
        print(f"❌ Critical error in generate_scaffold: {str(e)}")
        return {
            "error": "Critical error in generate_scaffold",
            "details": str(e),
            "traceback": str(e.__traceback__) if hasattr(e, '__traceback__') else None
        }

--- FILE_END: backend/lumiere_core/services/scaffolding.py ---


--- FILE_START: backend/lumiere_core/services/diplomat.py ---
# In backend/lumiere_core/services/diplomat.py

import os
import re
import requests
from typing import Dict, Any, List
from github import Github, GithubException, Issue

from . import llm_service
from .utils import clean_llm_code_output

# --- Configuration ---
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    raise ValueError("GITHUB_ACCESS_TOKEN must be set in the .env file.")

g = Github(GITHUB_TOKEN)

def _get_pr_for_issue(issue: Issue) -> Dict[str, Any] | None:
    """Finds the Pull Request that closed a given issue."""
    try:
        for event in issue.get_timeline():
            if event.event == "closed" and event.source and event.source.issue:
                pr = event.source.issue
                return {
                    "url": pr.html_url,
                    "title": pr.title,
                    "diff_url": pr.diff_url,
                }
    except GithubException as e:
        print(f"   -> API error while fetching timeline for {issue.html_url}: {e}")
    return None

def find_similar_solved_issues(issue_title: str, issue_body: str, model_identifier: str) -> Dict[str, Any]:
    """
    The main logic for The Diplomat agent.
    Searches GitHub for similar, solved issues and synthesizes the findings.
    """
    print("--- DIPLOMAT AGENT ACTIVATED ---")
    print(f"Using model: {model_identifier}")

    print("\n[Step 1/3] Generating a targeted search query from issue details...")
    query_generation_prompt = f"""
You are an expert GitHub search querycrafter. Based on the following issue title and body, generate a concise, powerful search query for finding similar issues.
Focus on extracting key library names, error messages, and critical function names.
For example, for a "TypeError" in "requests", the query might be: `requests "TypeError: timeout value must be a float"`.

ISSUE TITLE: {issue_title}
ISSUE BODY:
{issue_body}

Now, provide ONLY the search query string. Do not include any of your own commentary or XML tags.
"""
    raw_query = llm_service.generate_text(query_generation_prompt, model_identifier)
    search_query = clean_llm_code_output(raw_query).replace('"', '')
    print(f"✓ Generated Search Query: '{search_query}'")

    print("\n[Step 2/3] Searching GitHub for similar, solved issues...")
    qualified_query = f'{search_query} is:issue is:closed stars:>100 in:body'

    try:
        issues = g.search_issues(query=qualified_query, order="desc")
        print(f"✓ Found {issues.totalCount} potential matches. Analyzing the top 5...")

        # --- FIX: Check if there are any results before trying to iterate ---
        if issues.totalCount == 0:
            return {
                "summary": "The Diplomat was unable to find relevant, solved issues on GitHub for this specific problem.",
                "evidence": []
            }

        evidence = []
        # We can now safely iterate over the slice
        for issue in issues[:5]:
            print(f"   -> Analyzing: {issue.html_url}")
            closing_pr = _get_pr_for_issue(issue)
            if closing_pr:
                evidence.append({
                    "issue_title": issue.title,
                    "issue_url": issue.html_url,
                    "repo_name": issue.repository.full_name,
                    "solution_url": closing_pr['url'],
                    "diff_url": closing_pr['diff_url'],
                })

        if not evidence:
            return {
                "summary": "The Diplomat found some potentially related issues, but none had a clear linked Pull Request to analyze for a solution.",
                "evidence": []
            }

    except GithubException as e:
        return {"error": f"An error occurred while searching GitHub: {e.data.get('message', str(e))}"}

    print("\n[Step 3/3] Synthesizing findings into an intelligence briefing...")
    evidence_str = ""
    for item in evidence:
        evidence_str += f"- Issue in **{item['repo_name']}**: \"{item['issue_title']}\"\n"
        evidence_str += f"  - Issue Link: {item['issue_url']}\n"
        evidence_str += f"  - Solved by PR: {item['solution_url']}\n"

    synthesis_prompt = f"""
You are "The Diplomat," an AI agent for Lumière Sémantique.
You have found several solved issues on GitHub that are similar to the user's current problem.
Your mission is to write a concise intelligence briefing summarizing your findings. Do NOT tell the user how to fix their code. Instead, highlight the PATTERNS you found in the solutions.

Example summary format:
"This appears to be a known configuration issue. I found similar reports in `psf/requests` and `org/project` that were solved by changing a specific parameter. This strengthens the case for a configuration-based fix."

Here is the evidence you collected:
{evidence_str}

Now, generate the "Diplomat Intelligence Briefing" in Markdown.
"""
    summary = llm_service.generate_text(synthesis_prompt, model_identifier)

    print("--- DIPLOMAT AGENT MISSION COMPLETE ---")
    return {"summary": summary, "evidence": evidence}

--- FILE_END: backend/lumiere_core/services/diplomat.py ---


--- FILE_START: backend/lumiere_core/services/llm_service.py ---
# In backend/lumiere_core/services/llm_service.py

from typing import List, Dict, Any

from . import ollama_service
from . import gemini_service

# --- Public API for the LLM Service ---

def generate_text(prompt: str, model_identifier: str) -> str:
    """
    Generates text using the specified model provider.
    This is the single entry point for all other services.

    Args:
        prompt: The text prompt for the model.
        model_identifier: A string like "provider/model-name"
                          (e.g., "ollama/qwen3:4b" or "gemini/gemini-1.5-pro-latest").

    Returns:
        The generated text from the model.
    """
    parts = model_identifier.split('/', 1)
    if len(parts) != 2:
        return f"Error: Invalid model identifier format '{model_identifier}'. Expected 'provider/model-name'."

    provider, model_name = parts

    if provider == "ollama":
        return ollama_service.generate_text(prompt, model_name)
    elif provider == "gemini":
        return gemini_service.generate_text(prompt, model_name)
    else:
        return f"Error: Unknown LLM provider '{provider}'."


# === THIS IS THE FUNCTION TO MODIFY ===
def list_available_models() -> List[Dict[str, Any]]:
    """
    Aggregates available models from all configured providers.
    """
    all_models = []
    # This will gracefully return [] if the Ollama server is down.
    all_models.extend(ollama_service.list_models())
    # This will now work because the .env is loaded, returning Gemini models.
    all_models.extend(gemini_service.list_models())
    return all_models

--- FILE_END: backend/lumiere_core/services/llm_service.py ---


--- FILE_START: backend/lumiere_core/services/gemini_service.py ---
# In backend/lumiere_core/services/gemini_service.py

import os
import google.generativeai as genai
from typing import List, Dict

# Configure the Gemini client from environment variables
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

def is_configured() -> bool:
    """Check if the Gemini service is ready to be used."""
    return GEMINI_API_KEY is not None

def generate_text(prompt: str, model_name: str) -> str:
    """
    Sends a prompt to the Google Gemini API.

    Args:
        prompt: The full prompt to send.
        model_name: The specific Gemini model to use (e.g., 'gemini-1.5-pro-latest').
    """
    if not is_configured():
        return "Error: GEMINI_API_KEY is not configured in the environment."

    print(f"Sending prompt to Google Gemini model: '{model_name}'...")
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"An error occurred while communicating with the Gemini API: {e}")
        return f"Error from Gemini API: {e}"

def list_models() -> List[Dict[str, str]]:
    """Lists available Gemini models that support text generation."""
    if not is_configured():
        return []

    print("Fetching available Google Gemini models...")
    available = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                model_id = f"gemini/{m.name.replace('models/', '')}"
                available.append({
                    "id": model_id,
                    "provider": "gemini",
                    "name": m.display_name
                })
        return available
    except Exception as e:
        print(f"Could not fetch Gemini models: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/gemini_service.py ---


--- FILE_START: backend/lumiere_core/services/utils.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/utils.py
# In lumiere_core/services/utils.py
import re

def clean_llm_code_output(raw_code: str) -> str:
    """
    [Robustness] Removes Markdown code fences and extraneous whitespace from LLM output.

    This function uses a regular expression to find and remove
    common Markdown code block fences (like ```python or ```) from the start
    and end of the string. It also strips any leading or trailing whitespace.
    This is a shared utility to ensure all code-generating agents produce
    clean, machine-readable output.
    """
    # This regex matches an optional language specifier (like 'python', 'toml')
    # and the code fences themselves at the start and end of the string.
    code_fence_pattern = r"^\s*```[a-zA-Z]*\n?|```\s*$"
    cleaned_code = re.sub(code_fence_pattern, '', raw_code)
    return cleaned_code.strip()

--- FILE_END: backend/lumiere_core/services/utils.py ---


--- FILE_START: backend/lumiere_core/services/ollama_service.py ---
# In backend/lumiere_core/services/ollama_service.py

import ollama
import requests 
from typing import Dict, List

def generate_text(prompt: str, model_name: str = 'qwen3:4b') -> str:
    """
    Sends a prompt to a local Ollama model and returns the response.

    Args:
        prompt: The full prompt to send to the model.
        model_name: The name of the Ollama model to use for generation.

    Returns:
        The generated text content from the model.
    """
    print(f"Sending prompt to local Ollama model: '{model_name}'...")
    try:
        client = ollama.Client()
        response = client.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return (f"An error occurred while communicating with the Ollama server: {e}\n"
                f"Please ensure the Ollama server is running and the model '{model_name}' is available.")

# === REPLACE THE ENTIRE list_models FUNCTION WITH THIS NEW VERSION ===
def list_models() -> List[Dict[str, str]]:
    """
    Fetches the list of locally available Ollama models by calling the API directly.
    This is more robust than relying on the library's internal list() parsing.
    """
    print("Fetching available local Ollama models via direct API call...")
    try:
        # Use requests to call the /api/tags endpoint directly
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        data = response.json()

        available = []
        # The structure from the API is {"models": [...]}, so we access the 'models' key
        for model_data in data.get('models', []):
            # The model's full name (e.g., "qwen3:4b") is in the 'name' key
            full_name = model_data.get('name')
            if not full_name:
                continue  # Skip if a model entry is malformed

            model_id = f"ollama/{full_name}"
            available.append({
                "id": model_id,
                "provider": "ollama",
                "name": full_name,  # Use the full name for display as well
            })

        if not available:
            print("Ollama API responded, but no local models were found.")
        else:
            print(f"✓ Found {len(available)} local Ollama models.")

        return available

    except requests.exceptions.ConnectionError:
        print("Could not fetch Ollama models. Is the Ollama server running?")
        return []
    except requests.exceptions.RequestException as e:
        print(f"An error occurred while calling the Ollama API: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/ollama_service.py ---


--- FILE_START: backend/lumiere_core/services/crucible.py ---
# In backend/lumiere_core/services/crucible.py

import os
import uuid
from pathlib import Path
from typing import Dict, Tuple, Optional

import docker
from docker.errors import BuildError, ContainerError, APIError, DockerException

from .utils import clean_llm_code_output
from ingestion.crawler import IntelligentCrawler

# --- Environment Detection Helpers ---
def _detect_dependencies(repo_path: Path) -> Tuple[str, str]:
    print("   -> Detecting dependency file...")
    if (repo_path / "requirements.txt").exists():
        print("      - Found requirements.txt")
        return "pip install -r requirements.txt", "requirements.txt"
    if (repo_path / "pyproject.toml").exists():
        print("      - Found pyproject.toml (assuming standard PEP 517 build)")
        return "pip install .", "pyproject.toml"
    print("      - No dependency file found. Assuming no special dependencies.")
    return "echo 'No dependencies to install'", ""

def _detect_test_runner(repo_path: Path) -> str:
    print("   -> Detecting test runner...")
    if (repo_path / "pytest.ini").exists() or (repo_path / "tox.ini").exists():
        print("      - Found pytest.ini or tox.ini. Using 'pytest'.")
        return "pytest"
    if (repo_path / "tests").is_dir() or (repo_path / "test").is_dir():
        print("      - Found 'tests' directory. Assuming 'pytest'.")
        return "pytest"
    print("      - No specific test runner found. Falling back to 'python -m unittest discover'.")
    return "python -m unittest discover"

def _generate_dockerfile(install_command: str, dependency_file_path: str, test_command: str) -> str:
    print("   -> Generating dynamic Dockerfile...")
    dockerfile_content = f"""
FROM python:3.10-slim
WORKDIR /app
COPY . /app
"""
    if dependency_file_path:
        dockerfile_content += f"RUN {install_command}\n"
    dockerfile_content += f"CMD {test_command}\n"
    print("      - Dockerfile generated successfully.")
    return dockerfile_content

# --- Main Service Orchestrator ---
def validate_fix(repo_url: str, target_file: str, modified_code: str) -> Dict[str, str]:
    """
    The main logic for The Crucible agent.
    """
    print("\n--- CRUCIBLE AGENT ACTIVATED ---")
    print(f"Validating fix for '{target_file}' in '{repo_url}'")

    # ** THE FIX **: Add a top-level try/except to catch the Docker connection error.
    try:
        client = docker.from_env()
        # Ping the docker daemon to ensure it's running before we do anything else
        client.ping()
        print("✓ Successfully connected to Docker daemon.")
    except DockerException:
        error_msg = "Crucible Error: Could not connect to the Docker daemon. Please ensure Docker Desktop is running."
        print(f"✗ {error_msg}")
        return {"status": "error", "logs": error_msg}

    with IntelligentCrawler(repo_url=repo_url) as crawler:
        repo_path = crawler.repo_path
        print("[Step 1/5] Patching file in local clone...")
        full_target_path = repo_path / target_file
        if not full_target_path.exists():
            return {"status": "error", "logs": f"Crucible Error: Target file '{target_file}' not found."}
        full_target_path.write_text(modified_code, encoding='utf-8')
        print("✓ File patched.")

        print("[Step 2/5] Analyzing project environment...")
        install_cmd, dep_file = _detect_dependencies(repo_path)
        test_cmd = _detect_test_runner(repo_path)
        dockerfile_str = _generate_dockerfile(install_cmd, dep_file, test_cmd)
        (repo_path / "Dockerfile.lumiere").write_text(dockerfile_str, encoding='utf-8')
        print("✓ Environment analysis complete.")

        print("[Step 3/5] Building validation image...")
        image_tag = f"lumiere-crucible/{uuid.uuid4()}"
        image = None
        try:
            image, build_logs = client.images.build(path=str(repo_path), dockerfile="Dockerfile.lumiere", tag=image_tag, rm=True, forcerm=True)
            print(f"✓ Image '{image_tag}' built.")
        except BuildError as e:
            print(f"✗ Build Failed: {e}")
            logs = "\n".join([log.get('stream', '').strip() for log in e.build_log])
            return {"status": "failed", "logs": f"Docker image build failed:\n{logs}"}

        try:
            print(f"[Step 4/5] Running tests in container from image '{image_tag}'...")
            container_output = client.containers.run(image_tag, remove=True)
            print("✓ Tests PASSED.")
            return {"status": "passed", "logs": container_output.decode('utf-8')}
        except ContainerError as e:
            print(f"✗ Tests FAILED. Exit code: {e.exit_status}")
            logs = e.stderr.decode('utf-8') if e.stderr else e.stdout.decode('utf-8')
            return {"status": "failed", "logs": logs}
        finally:
            print("[Step 5/5] Cleaning up validation image...")
            if image:
                try: client.images.remove(image.id, force=True); print(f"✓ Image '{image_tag}' removed.")
                except APIError as e: print(f"Warning: Could not remove image '{image_tag}'. Error: {e}")

    print("--- CRUCIBLE AGENT MISSION COMPLETE ---")
    return {"status": "error", "logs": "Crucible process completed without a definitive pass/fail result."}

--- FILE_END: backend/lumiere_core/services/crucible.py ---


--- FILE_START: backend/lumiere_core/services/strategist.py ---
# In backend/lumiere_core/services/strategist.py

import json
import re
from typing import Dict, List, Any

from . import github
# The import for 'ambassador' is no longer needed as auto-dispatch is removed.
# from . import ambassador
# --- CORRECTED IMPORT: Only import the master llm_service ---
from . import llm_service

# --- CORRECTED FUNCTION SIGNATURE: Added model_identifier ---
def analyze_and_prioritize(repo_url: str, model_identifier: str) -> Dict[str, Any]:
    """
    The core logic for The Strategist agent.
    Fetches all open issues and uses a specified LLM to prioritize them.
    NOTE: Auto-dispatch has been removed to align with the interactive Socratic/Crucible workflow.
    """
    print("--- STRATEGIST AGENT ACTIVATED ---")
    print(f"Analyzing repository: {repo_url}")
    print(f"Using model: {model_identifier}")

    # Step 1: Fetch and Enrich All Open Issues
    print("\n[Step 1/3] Fetching and enriching all open issues...")

    match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if not match:
        return {"error": f"Could not parse repository name from URL: {repo_url}"}
    repo_full_name = f"{match.group(1)}/{match.group(2)}"

    raw_issues = github.list_open_issues(repo_full_name)
    if not raw_issues:
        return {"analysis_summary": "No open issues found for this repository.", "prioritized_issues": []}

    enriched_issues = []
    issues_for_prompt = ""
    for issue_stub in raw_issues:
        issue_details = github.scrape_github_issue(issue_stub['url'])
        if issue_details:
            enriched_issue_data = {**issue_stub, **issue_details}
            enriched_issues.append(enriched_issue_data)
            # Ensure description is not None before concatenation
            description = issue_details.get('description') or ""
            issues_for_prompt += f"### Issue #{issue_stub['number']}: {issue_stub['title']}\n{description}\n\n---\n\n"

    print(f"✓ Found and enriched {len(enriched_issues)} open issues.")

    # Step 2: Use LLM to score and justify prioritization
    print("\n[Step 2/3] Submitting issues to LLM for prioritization analysis...")

    prompt = f"""You are "The Strategist", an expert engineering manager for the Lumière Sémantique project. Your mission is to analyze a list of open GitHub issues and prioritize them.

You MUST produce a valid JSON array as your output. For each issue, create a JSON object with these exact fields:
- "issue_number": The integer issue number.
- "score": An integer from 0 to 100, where 100 is most critical.
- "justification": A concise, one-sentence explanation for your score.

SCORING CRITERIA:
- **Critical (90-100):** Crashes, data corruption, security vulnerabilities, broken core features.
- **High (70-89):** Major feature bugs, performance problems, incorrect calculations.
- **Medium (40-69):** Minor bugs, UI/UX issues, dependency updates.
- **Low (0-39):** Feature requests, documentation, questions, refactoring.

Analyze the following issues and provide ONLY the JSON array as your response. Do not include any other text or XML tags like <think>.

--- START OF ISSUES ---
{issues_for_prompt}
--- END OF ISSUES ---
"""

    # --- CORRECTED LLM CALL: Use the llm_service and pass the identifier ---
    llm_response_str = llm_service.generate_text(prompt, model_identifier=model_identifier)

    try:
        cleaned_str = re.sub(r'<think>.*?</think>', '', llm_response_str, flags=re.DOTALL)
        json_str_match = re.search(r'\[.*\]', cleaned_str, re.DOTALL)
        if not json_str_match:
            raise json.JSONDecodeError("No JSON array found in the LLM's cleaned response.", llm_response_str, 0)

        prioritization_data = json.loads(json_str_match.group(0))
        priority_map = {item['issue_number']: item for item in prioritization_data}

    except (json.JSONDecodeError, KeyError) as e:
        print(f"Error parsing LLM response: {e}\nLLM Response was:\n{llm_response_str}")
        return {"error": "Failed to parse prioritization data from LLM.", "llm_response": llm_response_str}

    print("✓ LLM analysis complete.")

    # Step 3: Merge data and sort
    print("\n[Step 3/3] Finalizing report...")

    final_ranked_list = []
    for issue in enriched_issues:
        issue_number = issue['number']
        if issue_number in priority_map:
            issue.update(priority_map[issue_number])
            final_ranked_list.append(issue)

    final_ranked_list.sort(key=lambda x: x.get('score', 0), reverse=True)

    summary = f"Analyzed {len(final_ranked_list)} open issues using the '{model_identifier}' model."

    for i, issue in enumerate(final_ranked_list):
        issue['rank'] = i + 1

    return {
        "repository": repo_full_name,
        "analysis_summary": summary,
        "prioritized_issues": final_ranked_list
    }

--- FILE_END: backend/lumiere_core/services/strategist.py ---


--- FILE_START: backend/lumiere_core/services/github.py ---
# In backend/lumiere_core/services/github.py

import os
import re
from datetime import datetime, timezone
from typing import Dict, Optional, Tuple, List, Any
from github import Github, GithubException, PaginatedList
from dotenv import load_dotenv

load_dotenv()

GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    print("WARNING: GITHUB_ACCESS_TOKEN not found. API calls will be heavily rate-limited.")
    g = Github()
else:
    g = Github(GITHUB_TOKEN)


def _paginated_to_list(paginated_list: PaginatedList, max_items: int = 10) -> List[Dict[str, Any]]:
    items = []
    for i, item in enumerate(paginated_list):
        if i >= max_items:
            break
        item_data = {
            "name": item.name,
            "full_name": item.full_name,
            "description": item.description,
            "html_url": item.html_url,
            "language": item.language,
            "stargazers_count": item.stargazers_count
        }
        items.append(item_data)
    return items


def get_user_profile(username: str) -> Optional[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return {
            "login": user.login, "name": user.name, "bio": user.bio,
            "html_url": user.html_url, "public_repos": user.public_repos,
            "followers": user.followers, "following": user.following,
        }
    except GithubException:
        return None

def get_user_repos(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_repos(sort='updated'), max_items=10)
    except GithubException:
        return []

def get_user_starred(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_starred(), max_items=10)
    except GithubException:
        return []

def get_user_comment_threads(username: str) -> List[Dict[str, Any]]:
    threads = []
    try:
        user = g.get_user(username)
        events = user.get_events()
        # Increase check limit to ensure we find comment events
        max_events_to_check = 50
        comment_events_found = 0
        max_comments_to_process = 5

        for i, event in enumerate(events):
            if i >= max_events_to_check or comment_events_found >= max_comments_to_process:
                break

            if event.type in ['IssueCommentEvent', 'PullRequestReviewCommentEvent']:
                payload = event.payload
                comment_data = payload.get('comment')
                issue_data = payload.get('issue', payload.get('pull_request'))

                if not comment_data or not issue_data or comment_data['user']['login'] != username:
                    continue

                comment_events_found += 1
                repo_name, issue_number = event.repo.name, issue_data['number']

                try:
                    repo_obj = g.get_repo(repo_name)
                    issue_obj = repo_obj.get_issue(number=issue_number)

                    created_at_str = comment_data.get('created_at')
                    if not created_at_str: continue

                    # Correctly parse the ISO 8601 string into a timezone-aware datetime object
                    created_at_dt = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))

                    user_comment = {"id": comment_data['id'], "body": comment_data['body'], "html_url": comment_data['html_url']}

                    replies = []
                    # Fetch comments created *after* the user's comment
                    for reply_comment in issue_obj.get_comments(since=created_at_dt):
                        if reply_comment.user.login != username and reply_comment.id != user_comment['id']:
                            replies.append({"user": reply_comment.user.login, "body": reply_comment.body, "html_url": reply_comment.html_url})

                    threads.append({
                        "repo_name": repo_name, "issue_number": issue_number, "issue_title": issue_data['title'],
                        "issue_url": issue_data['html_url'], "user_comment": user_comment, "replies": replies
                    })
                except GithubException as ge:
                    print(f"Warning: Could not fully process event for {repo_name}#{issue_number}. Skipping. Reason: {ge}")
                    continue
        return threads
    except GithubException as e:
        print(f"GitHub API Error while fetching comment threads: {e}")
        return []

def _parse_github_issue_url(issue_url: str) -> Optional[Tuple[str, str, int]]:
    match = re.match(r"https://github\.com/([^/]+)/([^/]+)/(?:issues|pull)/(\d+)", issue_url)
    if match:
        owner, repo_name, issue_number_str = match.groups()
        return owner, repo_name, int(issue_number_str)
    return None

def scrape_github_issue(issue_url: str) -> Optional[Dict[str, str]]:
    print(f"Fetching GitHub issue via API: {issue_url}")
    parsed_url = _parse_github_issue_url(issue_url)
    if not parsed_url:
        print(f"Error: Could not parse GitHub issue URL: {issue_url}")
        return None
    owner, repo_name, issue_number = parsed_url
    repo_full_name = f"{owner}/{repo_name}"
    try:
        repo = g.get_repo(repo_full_name)
        issue = repo.get_issue(number=issue_number)
        title, description = issue.title, issue.body if issue.body else ""
        full_text_query, repo_url = f"Issue Title: {title}\n\nDescription:\n{description}", f"https://github.com/{owner}/{repo_name}"
        return {"title": title, "description": description, "full_text_query": full_text_query, "repo_url": repo_url}
    except GithubException as e:
        print(f"GitHub API Error: {e.status}, {e.data}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during GitHub API call: {e}")
        return None

def list_open_issues(repo_full_name: str) -> List[Dict[str, Any]]:
    """
    Fetches a list of all open issues for a given repository.
    """
    print(f"Fetching open issues for repository: {repo_full_name}")
    try:
        repo = g.get_repo(repo_full_name)
        open_issues = repo.get_issues(state='open')
        issues_list = []
        for issue in open_issues:
            if not issue.pull_request:
                issues_list.append({
                    "number": issue.number,
                    "title": issue.title,
                    "url": issue.html_url,
                    "author": issue.user.login,
                })
        return issues_list
    except GithubException as e:
        print(f"GitHub API Error while listing issues: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/github.py ---


--- FILE_START: backend/lumiere_core/services/testing.py ---
# backend/lumiere_core/services/testing.py
import logging
from typing import Dict, List, Optional, Any, Set
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output

# Set up logging for better debugging
logger = logging.getLogger(__name__)

def generate_tests_for_code(repo_id: str, new_code: str, instruction: str) -> Dict[str, str]:
    """
    The core logic for the Test Generation Agent.
    It finds existing test files in the repository to learn the project's testing
    style and then generates a new test function consistent with that style.

    Args:
        repo_id: The unique identifier for the repository.
        new_code: The new code for which tests need to be generated.
        instruction: A natural language description of the code's purpose.

    Returns:
        A dictionary containing the generated test code or an error message.
        Format: {"generated_tests": str, "error": str (optional)}
    """
    logger.info(f"Initiating Test Generation Agent for repo '{repo_id}'")

    # Input validation
    if not all([repo_id, new_code]):
        error_msg = "Missing required parameters: repo_id and new_code."
        logger.error(error_msg)
        return {"generated_tests": "", "error": error_msg}

    if not instruction:
        logger.warning("No instruction provided, using default description")
        instruction = "Generate appropriate tests for the given code"

    try:
        # --- Step 1: Find existing test patterns with RAG ---
        logger.info("Step 1: Finding existing test patterns with RAG...")
        test_context_string = _find_existing_test_patterns(repo_id, instruction)

        # --- Step 2: Construct the Reinforced Prompt ---
        logger.info("Step 2: Constructing reinforced test generation prompt...")
        prompt = _build_test_generation_prompt(test_context_string, new_code)

        # --- Step 3: Generate the Test Code ---
        logger.info("Step 3: Generating test code...")
        raw_generated_tests = _generate_test_code(prompt)

        # --- Step 4: Clean and validate the output ---
        logger.info("Step 4: Cleaning and validating the generated test code...")
        final_tests = clean_llm_code_output(raw_generated_tests)

        # Validate the generated tests
        validation_result = validate_test_code(final_tests)
        if not validation_result["is_valid"]:
            logger.warning(f"Generated test code has validation issues: {validation_result['syntax_errors']}")
            # Still return the code but include warning in logs

        logger.info("✓ Test generation completed successfully.")
        return {"generated_tests": final_tests}

    except Exception as e:
        error_msg = f"An unexpected error occurred during test generation: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {"generated_tests": "", "error": error_msg}


def _find_existing_test_patterns(repo_id: str, instruction: str) -> str:
    """
    Find existing test patterns using RAG search.

    Args:
        repo_id: Repository identifier
        instruction: Code description for search context

    Returns:
        String containing formatted test examples or default message
    """
    search_query = f"Example test cases for python code like this: {instruction}"

    try:
        # CORRECTED CALL: Pass repo_id directly to the centralized search function.
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=7  # Look for up to 7 relevant chunks
        )
    except Exception as e:
        logger.warning(f"RAG search failed for test generation in repo '{repo_id}': {e}")
        context_chunks = []

    test_context_string = ""
    found_test_files: Set[str] = set()

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        chunk_text = chunk.get('text', '')

        # Heuristic to identify test files and avoid duplicates
        if (_is_test_file(file_path) and
            file_path not in found_test_files and
            chunk_text and
            len(chunk_text.strip()) > 10):  # Ensure meaningful content

            test_context_string += f"--- Example test from file `{file_path}` ---\n```python\n{chunk_text}\n```\n\n"
            found_test_files.add(file_path)

    if not test_context_string:
        test_context_string = "No specific test patterns found. Please generate a standard `pytest` function using `assert`."
        logger.info("Warning: No existing test files found via RAG. Will generate a generic test.")
    else:
        logger.info(f"Found test patterns from files: {list(found_test_files)}")

    return test_context_string


def _is_test_file(file_path: str) -> bool:
    """Check if a file path indicates a test file."""
    if not file_path:
        return False

    file_path_lower = file_path.lower()
    return (
        'test' in file_path_lower or
        file_path_lower.endswith('_test.py') or
        file_path_lower.startswith('test_') or
        '/tests/' in file_path_lower
    )


def _build_test_generation_prompt(test_context_string: str, new_code: str) -> str:
    """
    Build the prompt for test generation.

    Args:
        test_context_string: Existing test examples
        new_code: Code to generate tests for

    Returns:
        Formatted prompt string
    """
    return f"""You are an expert QA Engineer and Python programmer. Your task is to write a new unit test for a piece of code.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING TEST EXAMPLES"** to understand the project's testing style. Pay close attention to:
   * Imports (e.g., `unittest`, `pytest`).
   * Structure: Are tests inside a `class`?
   * Assertions: Do they use `self.assertEqual` or plain `assert`?
   * Setup: Are there fixtures or `setUp` methods?

2. **Analyze the "NEW CODE TO BE TESTED"** to understand its functionality.

3. **Write a complete test function.** It is CRITICAL that you exactly match the style of the examples.
   If the examples are standalone functions (e.g., `def test_...():`), your test MUST also be a standalone function.
   DO NOT invent a class if the examples do not use one.

4. **Output ONLY raw Python code.** Do not include any explanations, commentary, or Markdown fences like ```python.

---
### EXISTING TEST EXAMPLES
{test_context_string}

---
### NEW CODE TO BE TESTED
```python
{new_code}
```

Now, generate ONLY the new, stylistically-consistent test function."""


def _generate_test_code(prompt: str) -> str:
    """
    Generate test code using the LLM service.

    Args:
        prompt: The formatted prompt for test generation

    Returns:
        Generated test code

    Raises:
        Exception: If LLM generation fails
    """
    model_to_use = "ollama/qwen2.5-coder:3b"
    logger.info(f"Sending request to code generation model '{model_to_use}'...")

    try:
        # Use the master LLM service and pass the full model identifier
        raw_generated_tests = llm_service.generate_text(prompt, model_identifier=model_to_use)

        if not raw_generated_tests or not raw_generated_tests.strip():
            raise ValueError("LLM returned empty response")

        return raw_generated_tests

    except Exception as e:
        logger.error(f"LLM generation failed for tests: {e}")
        raise Exception(f"LLM generation failed: {str(e)}")


def validate_test_code(test_code: str) -> Dict[str, Any]:
    """
    Validates the generated test code for basic syntax and structure.

    Args:
        test_code: The generated test code to validate

    Returns:
        Dictionary with validation results containing:
        - is_valid: bool
        - has_test_function: bool
        - syntax_errors: List[str]
    """
    validation_result = {
        "is_valid": False,
        "has_test_function": False,
        "syntax_errors": [],
    }

    if not test_code or not test_code.strip():
        validation_result["syntax_errors"].append("Test code is empty")
        return validation_result

    # Check for test function presence
    if "def test_" in test_code:
        validation_result["has_test_function"] = True
    else:
        validation_result["syntax_errors"].append("No test function found (should start with 'def test_')")

    # Validate syntax
    try:
        compile(test_code, '<string>', 'exec')
        validation_result["is_valid"] = True
    except SyntaxError as e:
        error_msg = f"Syntax error: {e.msg}"
        if e.lineno:
            error_msg += f" on line {e.lineno}"
        validation_result["syntax_errors"].append(error_msg)
    except Exception as e:
        validation_result["syntax_errors"].append(f"Unexpected compilation error: {str(e)}")

    return validation_result


def generate_test_suggestions(code_snippet: str) -> List[str]:
    """
    Generates high-level suggestions for what types of tests should be written.

    Args:
        code_snippet: The code to analyze for test suggestions

    Returns:
        List of test case suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return ["No code provided for analysis"]

    suggestions = []
    code_lower = code_snippet.lower()

    # Function-based suggestions
    if "def " in code_lower:
        suggestions.extend([
            "Test the happy path with valid inputs",
            "Test edge cases (e.g., empty strings, zero, None)",
            "Test with invalid inputs to verify error handling"
        ])

    # Control flow suggestions
    if "if " in code_lower or "elif " in code_lower:
        suggestions.append("Ensure all conditional branches are tested")

    # Loop suggestions
    if "for " in code_lower or "while " in code_lower:
        suggestions.append("Test loop behavior (e.g., zero, one, and multiple iterations)")

    # Exception handling suggestions
    if "try:" in code_lower and "except" in code_lower:
        suggestions.extend([
            "Verify that expected exceptions are raised correctly",
            "Verify behavior when no exception occurs"
        ])

    # Class-based suggestions
    if "class " in code_lower:
        suggestions.extend([
            "Test object initialization",
            "Test all public methods",
            "Test method interactions and state changes"
        ])

    # Data structure suggestions
    if any(keyword in code_lower for keyword in ["list", "dict", "set", "tuple"]):
        suggestions.append("Test with different data structure sizes and types")

    # Return default suggestions if none found
    if not suggestions:
        suggestions = [
            "Test basic functionality",
            "Test edge cases",
            "Test error conditions"
        ]

    return suggestions


def get_test_coverage_suggestions(code_snippet: str) -> Dict[str, List[str]]:
    """
    Analyze code and provide comprehensive test coverage suggestions.

    Args:
        code_snippet: Code to analyze

    Returns:
        Dictionary categorizing different types of test suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return {"error": ["No code provided for analysis"]}

    suggestions = {
        "unit_tests": generate_test_suggestions(code_snippet),
        "integration_tests": [],
        "edge_cases": [],
        "performance_tests": []
    }

    code_lower = code_snippet.lower()

    # Integration test suggestions
    if any(keyword in code_lower for keyword in ["import", "from", "api", "database", "http"]):
        suggestions["integration_tests"].extend([
            "Test external API interactions",
            "Test database operations if applicable",
            "Test module integration"
        ])

    # Edge case suggestions
    suggestions["edge_cases"].extend([
        "Test with None values",
        "Test with empty collections",
        "Test with maximum/minimum values",
        "Test with malformed input"
    ])

    # Performance test suggestions
    if any(keyword in code_lower for keyword in ["for", "while", "sort", "search"]):
        suggestions["performance_tests"].extend([
            "Test with large datasets",
            "Test execution time constraints",
            "Test memory usage"
        ])

    return suggestions

--- FILE_END: backend/lumiere_core/services/testing.py ---


--- FILE_START: backend/lumiere_core/services/profile_service.py ---
# In backend/lumiere_core/services/profile_service.py
from typing import Dict, Any
from . import github
from . import llm_service

def _format_data_for_llm(profile_data: Dict[str, Any]) -> str:
    """Formats the aggregated GitHub data into a text block for the LLM."""

    user = profile_data['user_profile']
    text = f"""
# GitHub User Profile Analysis for: {user['login']} ({user.get('name', 'N/A')})
Bio: {user.get('bio', 'N/A')}
Followers: {user.get('followers', 0)} | Following: {user.get('following', 0)}
Public Repos: {user.get('public_repos', 0)}

---
## Owned Repositories (Sample)
"""
    if profile_data['repositories']:
        for repo in profile_data['repositories'][:5]:
            text += f"- **{repo['name']}**: {repo.get('language', 'N/A')} | ☆{repo['stargazers_count']} | {repo.get('description', 'No description')}\n"
    else:
        text += "No public repositories found.\n"

    text += "\n---"
    text += "\n## Starred Repositories (Sample)\n"
    if profile_data['starred_repositories']:
        for repo in profile_data['starred_repositories'][:5]:
             text += f"- **{repo['full_name']}**: {repo.get('description', 'No description')}\n"
    else:
        text += "No starred repositories found.\n"

    text += "\n---"
    text += f"\n## Recent Issue/PR Comments & Replies by {user['login']}\n"
    if profile_data['comment_threads']:
        for thread in profile_data['comment_threads']:
            text += f"\nOn repo `{thread['repo_name']}` (Issue/PR #{thread['issue_number']}):\n"
            text += f"  - **Their Comment**: \"{thread['user_comment']['body']}\"\n"
            if thread['replies']:
                for reply in thread['replies']:
                    text += f"    - **Reply from {reply['user']}**: \"{reply['body']}\"\n"
            else:
                text += "    - No replies to this comment found.\n"
    else:
        text += "No recent comments found.\n"

    return text

def generate_profile_review(username: str, model_identifier: str) -> Dict[str, Any]:
    """
    The core logic for the Chronicler Agent.
    Fetches a user's GitHub activity and generates a narrative summary.
    """
    print(f"Initiating Chronicler Agent for user '{username}'")
    print(f"Using model: {model_identifier}")

    print("   -> Step 1: Fetching profile data from GitHub API...")
    user_profile = github.get_user_profile(username)
    if not user_profile:
        raise FileNotFoundError(f"User '{username}' not found on GitHub.")

    repositories = github.get_user_repos(username)
    starred = github.get_user_starred(username)
    comment_threads = github.get_user_comment_threads(username)

    raw_data = {
        "user_profile": user_profile, "repositories": repositories,
        "starred_repositories": starred, "comment_threads": comment_threads,
    }

    print("   -> Step 2: Formatting data and constructing FINAL prompt for LLM...")
    context_string = _format_data_for_llm(raw_data)

    prompt = f"""You are an expert GitHub profile analyst. Your task is to analyze the user '{username}' based ONLY on the provided data.

**CRITICAL INSTRUCTION: Your entire analysis MUST be about the user '{username}'. Do NOT summarize the technical problems in the comments. Instead, use the comments to understand the USER'S BEHAVIOR.**

Generate a "Developer Profile Briefing" in Markdown with these exact sections:

### 1. Identity & Technical Focus
*   Based on their bio, owned repos, and starred repos, what are '{username}'s primary technical interests?
*   What are their main programming languages? (e.g., JavaScript, C++, Python)

### 2. Community Engagement Style
*   Based on their comments, what is '{username}'s role in the community? Are they reporting bugs, asking for help, or providing solutions?
*   Analyze the tone and content of THEIR comments. For example: `The user provides detailed debugging reports ("Debugging Report: itzzzme/anime-api Integration Issues") suggesting a methodical approach to problem-solving.`

### 3. Community Reception
*   Look at the replies to '{username}'s comments. Are others engaging with them? Are they receiving help and feedback?
*   Briefly summarize the nature of the replies they receive (e.g., "The user receives helpful replies from other developers, who offer suggestions and updated decryption keys.").

---
### RAW GITHUB DATA FOR {username}
{context_string}
---

Now, generate the Developer Profile Briefing about the user '{username}'.
"""

    print("   -> Step 3: Sending request to LLM for narrative generation...")
    summary = llm_service.generate_text(prompt, model_identifier=model_identifier)

    final_response = { "profile_summary": summary, "raw_data": raw_data }

    return final_response

--- FILE_END: backend/lumiere_core/services/profile_service.py ---


--- FILE_START: backend/lumiere_core/services/ollama.py ---
# In lumiere_core/services/ollama.py

import ollama
from tqdm import tqdm
from typing import List
import faiss
import numpy as np
import json
from pathlib import Path # <--- ADD THIS

def get_ollama_embeddings(chunks: List[str], model_name: str) -> List[List[float]]:
    """
    Generates embeddings for a list of text chunks using a local Ollama model.

    Args:
        chunks: A list of strings to be embedded.
        model_name: The name of the Ollama model to use (e.g., 'snowflake-arctic-embed').

    Returns:
        A list of embeddings, where each embedding is a list of floats.
    """
    embeddings = []
    # The ollama client automatically connects to http://localhost:11434
    client = ollama.Client()

    # Show a progress bar because this can take time
    for text in tqdm(chunks, desc="Generating Ollama Embeddings"):
        response = client.embeddings(model=model_name, prompt=text)
        embeddings.append(response['embedding'])

    return embeddings

def search_index(
    query_text: str,
    model_name: str,
    repo_id: str, # <--- MODIFIED: Take repo_id directly
    k: int = 10,
    **kwargs # <--- MODIFIED: Accept and ignore old path args for compatibility
) -> List[dict]:
    """
    Searches the Faiss index for the top k most similar chunks to a query for a given repo_id.

    Args:
        query_text: The user's search query.
        model_name: The name of the Ollama model used to create the index.
        repo_id: The unique ID of the repository whose index should be searched.
        k: The number of results to return.

    Returns:
        A list of dictionaries, where each dictionary contains the chunk_id,
        file_path, and the original text of the matching chunk.
    """
    # --- THIS IS THE FIX ---
    # Centralize path construction based on repo_id.
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_dir = backend_dir / "cloned_repositories" / repo_id
    index_path = artifacts_dir / f"{repo_id}_faiss.index"
    map_path = artifacts_dir / f"{repo_id}_id_map.json"

    print(f"Loading index '{index_path}' and map '{map_path}'...")
    # Load the Faiss index
    index = faiss.read_index(str(index_path))

    # Load the ID mapping files
    with open(map_path, 'r', encoding='utf-8') as f:
        id_maps = json.load(f)
    faiss_id_to_chunk_id = id_maps['faiss_id_to_chunk_id']
    chunk_id_to_data = id_maps['chunk_id_to_data']

    print(f"Generating embedding for query: '{query_text}'...")
    # 1. Embed the query using the same Ollama model
    client = ollama.Client()
    response = client.embeddings(model=model_name, prompt=query_text)
    query_vector = np.array([response['embedding']]).astype('float32')

    print(f"Searching index for top {k} results...")
    # 2. Search the Faiss index
    distances, indices = index.search(query_vector, k)

    # 3. Retrieve the results
    results = []
    for i in range(min(k, len(indices[0]))): # Ensure we don't go out of bounds
        faiss_id = indices[0][i]
        chunk_id = faiss_id_to_chunk_id[faiss_id]
        chunk_data = chunk_id_to_data[chunk_id]

        results.append({
            "chunk_id": chunk_id,
            "file_path": chunk_data['file_path'],
            "text": chunk_data['text'],
            "distance": float(distances[0][i])
        })

    return results

--- FILE_END: backend/lumiere_core/services/ollama.py ---


--- FILE_START: backend/lumiere_core/services/ingestion_service.py ---
# backend/lumiere_core/services/ingestion_service.py

import json
import traceback
import os
import re
from pathlib import Path
from typing import Dict, Any

from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier
from ingestion.indexing import EmbeddingIndexer

def generate_repo_id(repo_url: str, max_length: int = 100) -> str:
    """
    Generate a safe repository ID from a GitHub URL with intelligent truncation.

    Args:
        repo_url: The GitHub repository URL
        max_length: Maximum length for the generated repo_id (default: 100)

    Returns:
        A safe, filesystem-friendly repository identifier
    """
    # Extract the repo path (owner/repo-name)
    repo_path = repo_url.replace("https://github.com/", "").replace("/", "_")

    # If it's already within limits, return as-is (backward compatibility)
    if len(repo_path) <= max_length:
        return repo_path

    # Split into owner and repo name for intelligent truncation
    original_path = repo_url.replace("https://github.com/", "")
    parts = original_path.split("/", 1)

    if len(parts) != 2:
        # Fallback: just truncate the whole thing
        return repo_path[:max_length]

    owner, repo_name = parts

    # Calculate available space for repo name after owner and underscore
    available_space = max_length - len(owner) - 1  # -1 for the underscore

    if available_space <= 10:  # If owner name is too long, truncate both
        # Use first 40 chars for owner, rest for repo (with underscore)
        truncated_owner = owner[:40]
        available_for_repo = max_length - len(truncated_owner) - 1
        truncated_repo = repo_name[:available_for_repo] if available_for_repo > 0 else ""
        return f"{truncated_owner}_{truncated_repo}".rstrip("_")

    # Truncate repo name intelligently
    if len(repo_name) > available_space:
        # Try to keep meaningful parts of the repo name
        # Remove common words and separators, keep important parts
        cleaned_repo = re.sub(r'[-_\s]+', '-', repo_name)
        words = cleaned_repo.split('-')

        if len(words) > 1:
            # Keep first and last words, add middle words until we hit the limit
            truncated_parts = [words[0]]
            remaining_space = available_space - len(words[0])

            # Add words from the end working backwards
            for word in reversed(words[1:]):
                if remaining_space >= len(word) + 1:  # +1 for separator
                    truncated_parts.insert(-1, word)
                    remaining_space -= len(word) + 1
                else:
                    break

            truncated_repo = '-'.join(truncated_parts)
        else:
            # Single word, just truncate
            truncated_repo = repo_name[:available_space]

        return f"{owner}_{truncated_repo}"

    return repo_path


def clone_and_embed_repository(repo_url: str, embedding_model: str = 'snowflake-arctic-embed2:latest') -> Dict[str, Any]:
    """
    Orchestrates the entire ingestion pipeline, saving all artifacts into a
    repository-specific subdirectory. This is the single source of truth for ingestion.
    """
    # Use the enhanced repo_id generation
    repo_id = generate_repo_id(repo_url)

    # Define the output directory structure inside the backend
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_base_dir = backend_dir / "cloned_repositories"
    repo_output_dir = artifacts_base_dir / repo_id

    # Ensure the final destination directory exists
    repo_output_dir.mkdir(parents=True, exist_ok=True)

    # Define the full path for the cortex file within the new directory
    output_cortex_path = repo_output_dir / f"{repo_id}_cortex.json"

    print(f"--- INGESTION SERVICE: Starting for {repo_id} ---")
    print(f"   -> Original URL: {repo_url}")
    print(f"   -> Generated repo_id: {repo_id}")
    print(f"   -> Artifacts will be saved to: {repo_output_dir}")

    try:
        # --- Step 1: Crawl & Jsonify ---
        print(f"[1/3] Cloning repository and generating Project Cortex file...")
        with IntelligentCrawler(repo_url=repo_url) as crawler:
            files_to_process = crawler.get_file_paths()
            if not files_to_process:
                return {"status": "failed", "error": "No files found to process in the repository."}

            jsonifier = Jsonifier(
                file_paths=files_to_process,
                repo_root=crawler.repo_path,
                repo_id=repo_id
            )
            project_cortex = jsonifier.generate_cortex()

            with open(output_cortex_path, 'w', encoding='utf-8') as f:
                json.dump(project_cortex, f, indent=2)
            print(f"✓ Project Cortex created successfully: {output_cortex_path}")

        # --- Step 2: Index ---
        print(f"[2/3] Starting vector indexing with model '{embedding_model}'...")
        # The indexer will now automatically save its files alongside the cortex file.
        indexer = EmbeddingIndexer(model_name=embedding_model)
        indexer.process_cortex(str(output_cortex_path))
        print(f"✓ Vector indexing complete.")

        print(f"[3/3] Ingestion complete. Artifacts preserved in '{repo_output_dir}'.")

        return {
            "status": "success",
            "message": f"Repository '{repo_id}' was successfully cloned, embedded, and indexed.",
            "repo_id": repo_id,
            "original_url": repo_url
        }

    except Exception as e:
        print(f"--- INGESTION FAILED for {repo_id} ---")
        traceback.print_exc()
        if output_cortex_path.exists():
            os.remove(output_cortex_path)
        return {"status": "failed", "error": str(e), "details": traceback.format_exc(), "repo_id": repo_id}

--- FILE_END: backend/lumiere_core/services/ingestion_service.py ---


--- FILE_START: backend/lumiere_core/wsgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/wsgi.py
"""
WSGI config for backend project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_wsgi_application()

--- FILE_END: backend/lumiere_core/wsgi.py ---


--- FILE_START: backend/run_server.sh ---
# In ~/lumiere_semantique/backend/run_server.sh
#!/bin/bash
#
# This script is the standard way to run the Lumière Sémantique development server.
# It ensures the server always starts on the correct port (8002).
#
# To run:
# 1. Make it executable: chmod +x run_server.sh
# 2. Execute it: ./run_server.sh

echo "Starting Lumière Sémantique development server on http://127.0.0.1:8002/"
python manage.py runserver 8002

--- FILE_END: backend/run_server.sh ---


--- FILE_START: backend/.env ---

GEMINI_API_KEY='AIzaSyAVvfR4hTMO15HP0Skk0y3A3Bmsvg9ivmI'

--- FILE_END: backend/.env ---


--- FILE_START: backend/api/migrations/__init__.py ---

--- FILE_END: backend/api/migrations/__init__.py ---


--- FILE_START: backend/api/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/api/models.py ---


--- FILE_START: backend/api/__init__.py ---

--- FILE_END: backend/api/__init__.py ---


--- FILE_START: backend/api/apps.py ---
from django.apps import AppConfig


class ApiConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "api"

--- FILE_END: backend/api/apps.py ---


--- FILE_START: backend/api/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/api/admin.py ---


--- FILE_START: backend/api/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/api/tests.py ---


--- FILE_START: backend/api/urls.py ---
# In backend/api/urls.py

from django.urls import path
from .views import (
    BriefingView, ScaffoldView, TestGenerationView, RcaView,
    DocstringGenerationView, PrepareReviewView, ExecuteReviewView,
    ProfileReviewView, AmbassadorDispatchView, IssueListView,
    StrategistPrioritizeView, FileContentView, DiplomatView,
    CrucibleValidateView, ListModelsView, HealthCheckView,
    IngestRepositoryView, GraphDataView,
)

urlpatterns = [
    # --- HEALTH CHECK ENDPOINT ---
    path('health/', HealthCheckView.as_view(), name='health_check'),

    # --- MODEL MANAGEMENT ENDPOINT ---
    path('models/list/', ListModelsView.as_view(), name='list_models'),

     # --- INGESTION ENDPOINT ---
    path('ingest/', IngestRepositoryView.as_view(), name='ingest_repository'),

    # --- Graph ENDPOINT ---
    path('graph/', GraphDataView.as_view(), name='graph_data'),

    # --- "Triage & Strategy" ENDPOINTS ---
    path('issues/list/', IssueListView.as_view(), name='list_issues'),
    path('strategist/prioritize/', StrategistPrioritizeView.as_view(), name='strategist_prioritize'),
    path('diplomat/find-similar-issues/', DiplomatView.as_view(), name='diplomat_find_similar'),

    # --- "Execution & Validation" ENDPOINTS ---
    path('briefing/', BriefingView.as_view(), name='briefing'),
    path('scaffold/', ScaffoldView.as_view(), name='scaffold'),
    path('crucible/validate/', CrucibleValidateView.as_view(), name='crucible_validate'),
    path('file-content/', FileContentView.as_view(), name='file_content'),
    path('generate-tests/', TestGenerationView.as_view(), name='generate_tests'),
    path('generate-docstring/', DocstringGenerationView.as_view(), name='generate_docstring'),
    path('rca/', RcaView.as_view(), name='rca'),
    path('ambassador/dispatch/', AmbassadorDispatchView.as_view(), name='ambassador_dispatch'),

    # --- "Review" ENDPOINTS ---
    path('review/prepare', PrepareReviewView.as_view(), name='prepare_review'),
    path('review/execute', ExecuteReviewView.as_view(), name='execute_review'),
    path('profile/review/', ProfileReviewView.as_view(), name='profile_review'),
]

--- FILE_END: backend/api/urls.py ---


--- FILE_START: backend/api/views.py ---
# In backend/api/views.py

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
import os
import re
import traceback
import json
from pathlib import Path

# --- Correctly import services from lumiere_core ---
from lumiere_core.services import (
    llm_service, github, ambassador, crucible, diplomat,
    documentation, profile_service, rca_service, scaffolding, strategist,
    testing, review_service, ingestion_service, cortex_service,
)

# A sensible default model, preferably a fast one.
# It will be used if the client doesn't specify a model.
DEFAULT_MODEL = "ollama/qwen3:4b"

# --- NEW VIEW: Health Check ---
class HealthCheckView(APIView):
    """A simple view to confirm the server is running."""
    def get(self, request, *args, **kwargs):
        return Response({"status": "ok"}, status=status.HTTP_200_OK)

class ListModelsView(APIView):
    """Returns a list of all available LLM models from configured providers."""
    def get(self, request, *args, **kwargs):
        try:
            models = llm_service.list_available_models()
            return Response(models, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({"error": "Failed to list models.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class StrategistPrioritizeView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        model = request.data.get('model', DEFAULT_MODEL)
        if not repo_url:
            return Response({"error": "'repo_url' is required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = strategist.analyze_and_prioritize(repo_url, model_identifier=model)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class DiplomatView(APIView):
    def post(self, request, *args, **kwargs):
        issue_title = request.data.get('issue_title')
        issue_body = request.data.get('issue_body')
        model = request.data.get('model', DEFAULT_MODEL)
        if not all([issue_title, issue_body]):
            return Response({"error": "'issue_title' and 'issue_body' are required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = diplomat.find_similar_solved_issues(issue_title, issue_body, model_identifier=model)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class BriefingView(APIView):
    def post(self, request, *args, **kwargs):
        issue_url = request.data.get('issue_url')
        model = request.data.get('model', DEFAULT_MODEL)
        if not issue_url:
            return Response({"error": "'issue_url' is required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = rca_service.generate_briefing(issue_url, model_identifier=model)
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class RcaView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        bug_description = request.data.get('bug_description')
        model = request.data.get('model', DEFAULT_MODEL)

        # New optional parameters
        advanced_analysis = request.data.get('advanced_analysis', False)
        confidence_threshold = request.data.get('confidence_threshold', 0.7)

        if not all([repo_url, bug_description]):
            return Response(
                {"error": "'repo_url' and 'bug_description' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            result = rca_service.perform_rca(
                repo_url=repo_url,
                bug_description=bug_description,
                model_identifier=model,
                advanced_analysis=advanced_analysis,
                confidence_threshold=confidence_threshold
            )

            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)

            return Response(result, status=status.HTTP_200_OK)

        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class ScaffoldView(APIView):
    def post(self, request, *args, **kwargs):
        # --- UPDATED TO MATCH NEW SERVICE ---
        repo_id = request.data.get('repo_id')
        target_files = request.data.get('target_files') # Expects a list
        instruction = request.data.get('instruction')
        rca_report = request.data.get('rca_report')
        model = request.data.get('model', DEFAULT_MODEL)
        refinement_history = request.data.get('refinement_history')

        if not all([repo_id, target_files, instruction, rca_report]):
            return Response({"error": "'repo_id', 'target_files' (list), 'instruction', and 'rca_report' are required."}, status=status.HTTP_400_BAD_REQUEST)
        if not isinstance(target_files, list):
            return Response({"error": "'target_files' must be a list of strings."}, status=status.HTTP_400_BAD_REQUEST)

        try:
            result = scaffolding.generate_scaffold(
                repo_id, target_files, instruction, model, rca_report, refinement_history
            )
            if "error" in result:
                # Use 500 for server-side issues like missing cortex file
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class CrucibleValidateView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        target_file = request.data.get('target_file')
        modified_code = request.data.get('modified_code')
        if not all([repo_url, target_file, modified_code]):
            return Response({"error": "'repo_url', 'target_file', 'modified_code' are required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = crucible.validate_fix(repo_url, target_file, modified_code)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            error_details = traceback.format_exc()
            return Response({"error": "An unexpected internal server error occurred in The Crucible.", "details": str(e), "traceback": error_details}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AmbassadorDispatchView(APIView):
    def post(self, request, *args, **kwargs):
        # --- MODIFIED: Expect a dictionary of files ---
        issue_url = request.data.get('issue_url')
        modified_files = request.data.get('modified_files') # Expects dict
        model = request.data.get('model', DEFAULT_MODEL)

        if not all([issue_url, modified_files]):
            return Response({"error": "'issue_url' and 'modified_files' (dict) are required."}, status=status.HTTP_400_BAD_REQUEST)
        if not isinstance(modified_files, dict):
            return Response({"error": "'modified_files' must be a dictionary."}, status=status.HTTP_400_BAD_REQUEST)

        try:
            result = ambassador.dispatch_pr(issue_url, modified_files, model)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ProfileReviewView(APIView):
    def post(self, request, *args, **kwargs):
        username = request.data.get('username')
        model = request.data.get('model', DEFAULT_MODEL)
        if not username:
            return Response({"error": "'username' is required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = profile_service.generate_profile_review(username, model_identifier=model)
            if "error" in result:
                return Response(result, status=status.HTTP_404_NOT_FOUND)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class IssueListView(APIView):
    def get(self, request, *args, **kwargs):
        repo_url = request.query_params.get('repo_url')
        if not repo_url:
            return Response({"error": "'repo_url' query parameter is required."}, status=status.HTTP_400_BAD_REQUEST)
        match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            return Response({"error": "Invalid 'repo_url' format."}, status=status.HTTP_400_BAD_REQUEST)
        repo_full_name = f"{match.group(1)}/{match.group(2)}"
        try:
            issues = github.list_open_issues(repo_full_name)
            return Response(issues, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class FileContentView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        file_path = request.data.get('file_path')
        if not all([repo_id, file_path]):
            return Response({"error": "'repo_id' and 'file_path' are required."}, status=status.HTTP_400_BAD_REQUEST)

        # --- THE FIX: Call the new, correct service ---
        content = cortex_service.get_file_content(repo_id, file_path)

        if content is None:
            return Response({"error": f"File '{file_path}' not found for repo '{repo_id}'."}, status=status.HTTP_404_NOT_FOUND)

        return Response({"content": content}, status=status.HTTP_200_OK)


class TestGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        if not all([repo_id, new_code, instruction]):
            return Response({"error": "'repo_id', 'new_code', and 'instruction' are required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = testing.generate_tests_for_code(repo_id, new_code, instruction)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class DocstringGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        # --- ADDED: Get the model from the request ---
        model = request.data.get('model', DEFAULT_MODEL)

        if not all([repo_id, new_code, instruction]):
            return Response({"error": "'repo_id', 'new_code', and 'instruction' are required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            # --- MODIFIED: Pass the model to the service ---
            result = documentation.generate_docstring_for_code(repo_id, new_code, instruction, model)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class PrepareReviewView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        ref_name = request.data.get('ref_name') # e.g., a branch or tag name
        if not all([repo_url, ref_name]):
            return Response({"error": "'repo_url' and 'ref_name' are required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            result = review_service.prepare_review_environment(repo_url, ref_name)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "Failed to prepare review environment.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class ExecuteReviewView(APIView):
    def post(self, request, *args, **kwargs):
        review_id = request.data.get('review_id')
        model = request.data.get('model', DEFAULT_MODEL)
        if not review_id:
            return Response({"error": "'review_id' is required."}, status=status.HTTP_400_BAD_REQUEST)
        try:
            diff_text = review_service.get_diff_for_review(review_id)
            result = review_service.review_code_diff(diff_text) # Assumes review_code_diff uses default model
            review_service.cleanup_review_environment(review_id)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "Failed to execute review.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# --- Ingestion ---
class IngestRepositoryView(APIView):
    """
    Triggers the full ingestion pipeline for a given repository URL.
    This is a long-running, synchronous operation.
    """
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        if not repo_url:
            return Response({"error": "'repo_url' is required."}, status=status.HTTP_400_BAD_REQUEST)

        try:
            # We can also get the embedding model from the request in the future
            result = ingestion_service.clone_and_embed_repository(repo_url)
            if result.get("status") == "failed":
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response({"error": "An unexpected internal server error occurred during ingestion.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class GraphDataView(APIView):
    """
    Retrieves the architectural graph data from a repository's Cortex file.
    """
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        if not repo_id:
            return Response({"error": "'repo_id' query parameter is required."}, status=status.HTTP_400_BAD_REQUEST)

        # Construct the path to the cortex file
        backend_dir = Path(__file__).resolve().parent.parent
        cortex_file = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_file.exists():
            return Response({"error": f"Cortex file for repo '{repo_id}' not found."}, status=status.HTTP_404_NOT_FOUND)

        try:
            with open(cortex_file, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)

            graph_data = cortex_data.get('architectural_graph')
            if not graph_data:
                new_message = "Architectural graph is not available. The Cartographer feature currently only supports Python projects (.py files). This repository does not appear to contain Python code suitable for graphing."
                return Response({
                    "message": new_message,
                    "graph": None
                }, status=status.HTTP_200_OK)

            return Response({"graph": graph_data, "repo_id": repo_id}, status=status.HTTP_200_OK)

        except Exception as e:
            traceback.print_exc()
            return Response({"error": "Failed to read or parse graph data.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

--- FILE_END: backend/api/views.py ---


--- FILE_START: backend/manage.py ---
# In ~/lumiere_semantique/backend/manage.py
#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys


def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lumiere_core.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == "__main__":
    main()

--- FILE_END: backend/manage.py ---


--- FILE_START: README.md ---
# Lumière Sémantique

--- FILE_END: README.md ---


--- FILE_START: .gitignore ---
# In ~/lumiere_semantique/.gitignore

# --- Python / Django ---
__pycache__/
*.pyc

# --- Virtual Environments ---
# This will ignore venv, venv_broken, etc.
venv/
venv_broken/
*.env
.env

# --- OS / IDE Files ---
.DS_Store
.idea/
.vscode/

# --- Database Files ---
db.sqlite3
*.sqlite3-journal

# --- Lumière Sémantique Generated Artifacts ---
# We don't want to commit the large index and JSON files.
# These should be generated by anyone who clones the repo.
*.json
*.index

# --- Django Media/Static Files ---
media/
static/

# --- Build Artifacts ---
build/
dist/
*.egg-info/

--- FILE_END: .gitignore ---



--- PROJECT STRUCTURE ---
/Users/latchy/lumiere_semantique
├── .claude
│   └── settings.local.json
├── .gitignore
├── backend
│   ├── .env
│   ├── api
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   ├── urls.py
│   │   └── views.py
│   ├── build_parsers.py
│   ├── db.sqlite3
│   ├── ingestion
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── crawler.py
│   │   ├── indexing.py
│   │   ├── jsonifier.py
│   │   ├── management
│   │   │   ├── __init__.py
│   │   │   └── commands
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   └── views.py
│   ├── lumiere_core
│   │   ├── __init__.py
│   │   ├── .env
│   │   ├── .gitignore
│   │   ├── asgi.py
│   │   ├── services
│   │   │   ├── __init__.py
│   │   │   ├── ambassador.py
│   │   │   ├── bom_parser.py
│   │   │   ├── cartographer.py
│   │   │   ├── code_surgery.py
│   │   │   ├── cortex_service.py
│   │   │   ├── crucible.py
│   │   │   ├── dance_service.py
│   │   │   ├── diff_parser.py
│   │   │   ├── diplomat.py
│   │   │   ├── documentation.py
│   │   │   ├── expertise_service.py
│   │   │   ├── gemini_service.py
│   │   │   ├── github.py
│   │   │   ├── graph_differ.py
│   │   │   ├── impact_analyzer.py
│   │   │   ├── ingestion_service.py
│   │   │   ├── llm_service.py
│   │   │   ├── mage_service.py
│   │   │   ├── ollama_service.py
│   │   │   ├── ollama.py
│   │   │   ├── onboarding_service.py
│   │   │   ├── oracle_service.py
│   │   │   ├── profile_service.py
│   │   │   ├── rca_service.py
│   │   │   ├── review_service.py
│   │   │   ├── scaffolding.py
│   │   │   ├── sentinel_service.py
│   │   │   ├── strategist.py
│   │   │   ├── suggester_service.py
│   │   │   ├── summoner_service.py
│   │   │   ├── testing.py
│   │   │   └── utils.py
│   │   ├── settings.py
│   │   ├── urls.py
│   │   └── wsgi.py
│   ├── manage.py
│   └── requirements.txt
├── crawler.sh
├── llm_project_context.txt
├── lumiere.py
├── manage.py
├── README.md
└── start_server.sh

11 directories, 72 files

--- END PROJECT STRUCTURE ---

--- FILE_START: lumiere.py ---
# In /Users/latchy/lumiere_semantique/lumiere.py

import typer
import sys
sys.path.append('backend')
import requests
import sys
import re
import shlex
import difflib
import traceback
from typing import Optional, List, Dict, Tuple
from collections import defaultdict
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.markdown import Markdown
from rich.text import Text
from rich.status import Status
from rich.live import Live
from rich.align import Align
from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.shortcuts import confirm
from prompt_toolkit.styles import Style
from pathlib import Path
import json
import time
from datetime import datetime
import textwrap
from rich.tree import Tree

# --- Global Objects & Configuration ---
console = Console()
history_path = Path.home() / ".lumiere" / "history.txt"
config_path = Path.home() / ".lumiere" / "config.json"
history_path.parent.mkdir(parents=True, exist_ok=True)

# --- Centralized API URL ---
API_BASE_URL = "http://127.0.0.1:8002/api/v1"

# Create command completers for better UX
main_commands = ['analyze', 'a', 'ask', 'oracle', 'dance', 'da', 'summon', 'su', 'review', 'dashboard', 'd', 'profile', 'p', 'bom', 'b', 'onboard', 'o', 'repo-mgmt', 'rm', 'config', 'c', 'help', 'h', 'exit', 'x', 'quit', 'list-repos', 'lr']
analysis_commands = ['list', 'l', 'fix', 'f', 'briefing', 'b', 'rca', 'r', 'details', 'd', 'graph', 'g', 'mage', 'm', 'help', 'h', 'back', 'exit', 'quit']
oracle_commands = ['help', 'h', 'back', 'exit', 'quit']
bom_commands = ['overview', 'o', 'dependencies', 'deps', 'services', 's', 'security', 'sec', 'compare', 'c', 'regenerate', 'r', 'help', 'h', 'back', 'exit', 'quit']
onboard_commands = ['scout', 's', 'expert', 'e', 'guide', 'g', 'help', 'h', 'back', 'exit', 'quit']
repo_commands = ['list', 'l', 'status', 's', 'delete', 'd', 'help', 'h', 'back', 'exit', 'quit']

main_completer = WordCompleter(main_commands, ignore_case=True)
analysis_completer = WordCompleter(analysis_commands, ignore_case=True)
oracle_completer = WordCompleter(oracle_commands, ignore_case=True)
bom_completer = WordCompleter(bom_commands, ignore_case=True)
onboard_completer = WordCompleter(onboard_commands, ignore_case=True)
repo_completer = WordCompleter(repo_commands, ignore_case=True)

# --- Style for prompt_toolkit prompt to match rich colors ---
prompt_style = Style.from_dict({
    'lumiere': 'bold #00ffff',  # bold cyan
    'provider': 'yellow',
    'separator': 'white'
})

prompt_session = PromptSession(
    history=FileHistory(str(history_path)),
    completer=main_completer,
    style=prompt_style
)

# --- Global CLI State ---
cli_state = {
    "model": None,  # Will be populated from config
    "available_models": [],
    "last_repo_url": None,
    "debug_mode": False,
}

# --- NEW UTILITY FUNCTIONS for managing analyzed repos ---

def check_if_repo_is_analyzed(repo_id: str) -> bool:
    """Checks repository analysis status via API instead of filesystem."""
    api_client = LumiereAPIClient()
    try:
        response = api_client.get_repository_status(repo_id)
        return response and response.get("status") == "complete"
    except Exception:
        # Fallback to filesystem check if API fails
        cloned_repos_dir = Path("backend/cloned_repositories")
        repo_dir = cloned_repos_dir / repo_id
        if not repo_dir.is_dir():
            return False

        cortex_file = repo_dir / f"{repo_id}_cortex.json"
        faiss_file = repo_dir / f"{repo_id}_faiss.index"
        map_file = repo_dir / f"{repo_id}_id_map.json"

        return all([cortex_file.exists(), faiss_file.exists(), map_file.exists()])

def find_analyzed_repos() -> List[Dict[str, str]]:
    """Fetches analyzed repositories from the backend API instead of scanning filesystem."""
    api_client = LumiereAPIClient()
    try:
        response = api_client.list_repositories()
        return response if response else []
    except Exception:
        # Fallback to filesystem scan if API fails
        analyzed_repos = []
        cloned_repos_dir = Path("backend/cloned_repositories")
        if not cloned_repos_dir.is_dir():
            return []

        for repo_dir in cloned_repos_dir.iterdir():
            if repo_dir.is_dir():
                repo_id = repo_dir.name
                if check_if_repo_is_analyzed(repo_id):
                    # Attempt to reconstruct a user-friendly name and the full URL
                    display_name = repo_id.replace("_", "/", 1)
                    full_url = f"https://github.com/{display_name}"
                    analyzed_repos.append({"repo_id": repo_id, "display_name": display_name, "url": full_url})

        return sorted(analyzed_repos, key=lambda x: x['repo_id'])


def load_config():
    """Load configuration from file if it exists."""
    try:
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                cli_state.update(config)
                console.print(f"[dim]✓ Configuration loaded from {config_path}[/dim]")
    except Exception as e:
        console.print(f"[yellow]Warning: Could not load config: {e}[/yellow]")

def save_config():
    """Save current configuration to file."""
    try:
        with open(config_path, 'w') as f:
            json.dump({
                "model": cli_state["model"],
                "last_repo_url": cli_state["last_repo_url"],
                "debug_mode": cli_state["debug_mode"]
            }, f, indent=2)
    except Exception as e:
        console.print(f"[yellow]Warning: Could not save config: {e}[/yellow]")

def validate_github_url(url: str) -> bool:
    """Validate that the URL is a proper GitHub repository URL."""
    github_pattern = r'^https://github\.com/[\w\-\.]+/[\w\-\.]+/?$'
    return bool(re.match(github_pattern, url))

def format_url(url: str) -> str:
    """Normalize GitHub URL format."""
    url = url.strip().rstrip('/')
    if not url.startswith('https://'):
        if url.startswith('github.com/'):
            url = 'https://' + url
        elif '/' in url and not url.startswith('http'):
            url = 'https://github.com/' + url
    return url

def _insert_docstring_into_code(code: str, docstring: str) -> str:
    """Intelligently inserts a docstring into a Python code snippet."""
    # Find the first function or class definition
    match = re.search(r"^(?P<indent>\s*)(def|class)\s+\w+", code, re.MULTILINE)
    if not match:
        return code # Cannot find where to insert, return original

    indentation = match.group('indent')
    insertion_point = match.end()

    # Prepare the docstring with the correct indentation
    indented_docstring = textwrap.indent(f'"""{docstring}"""', indentation + '    ')

    # Insert the docstring
    return f"{code[:insertion_point]}\n{indented_docstring}{code[insertion_point:]}"

# --- Enhanced API Client ---
class LumiereAPIClient:
    def __init__(self, base_url: str = API_BASE_URL, timeout: int = 600):
        self.base_url = base_url
        self.timeout = timeout
        self.session = requests.Session()

    def _request(self, method: str, endpoint: str, **kwargs):
        try:
            if method.upper() in ["POST"]:
                data = kwargs.get("json", {})
                # The Task Router now handles model selection, so we don't add it here.
                # Only check for existence if it's a legacy endpoint that needs it.
                kwargs["json"] = data
            url = f"{self.base_url}/{endpoint}"
            if cli_state["debug_mode"]:
                console.print(f"[dim]DEBUG: {method} {url}[/dim]")
                if "json" in kwargs:
                    console.print(f"[dim]DEBUG: Payload: {kwargs['json']}[/dim]")
            response = self.session.request(method, url, timeout=self.timeout, **kwargs)
            response.raise_for_status()
            # Handle potential empty responses from server
            if response.status_code == 204 or not response.content:
                return {}
            return response.json()
        except requests.exceptions.ConnectionError as e:
            console.print(Panel(f"[bold red]Cannot connect to Lumière backend[/bold red]\n[yellow]Expected URL:[/yellow] {self.base_url}\n[yellow]Error:[/yellow] {str(e)}\n\n[dim]💡 Make sure the backend server is running.[/dim]", title="[red]Connection Error[/red]", border_style="red"))
            return None
        except requests.exceptions.HTTPError as e:
            try:
                error_json = e.response.json()
                error_details = error_json.get('error', str(error_json))
                console.print(Panel(f"[bold red]API Request Failed[/bold red]\n[yellow]URL:[/yellow] {e.request.url}\n[yellow]Status:[/yellow] {e.response.status_code}\n[yellow]Error:[/yellow] {error_details}", title="[red]API Error[/red]", border_style="red"))
                llm_response_for_debug = error_json.get('llm_response')
                if llm_response_for_debug:
                    console.print(Panel(Text(llm_response_for_debug, overflow="fold"), title="[bold yellow]🔍 LLM Raw Response (for debugging)[/bold yellow]", border_style="yellow", expand=False))
            except json.JSONDecodeError:
                console.print(Panel(f"[bold red]HTTP Error {e.response.status_code}[/bold red]\n[yellow]URL:[/yellow] {e.request.url}\n\n[bold]Response Text:[/bold]\n{e.response.text[:500]}...", title="[red]Non-JSON API Error[/red]", border_style="red"))
            return None
        except requests.exceptions.Timeout:
            console.print(Panel(f"The request took longer than {self.timeout} seconds.", title="[red]Timeout Error[/red]"))
            return None
        except requests.exceptions.RequestException as e:
            console.print(Panel(f"[bold red]Request Error[/bold red]\n[yellow]Error:[/yellow] {str(e)}", title="[red]Request Error[/red]"))
            return None

    def health_check(self) -> bool:
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False

    def list_models(self): return self._request("GET", "models/list/")
    def get_analysis(self, repo_url: str): return self._request("POST", "strategist/prioritize/", json={"repo_url": repo_url})
    def get_briefing(self, issue_url: str): return self._request("POST", "briefing/", json={"issue_url": issue_url})
    def get_rca(self, repo_url: str, bug_description: str): return self._request("POST", "rca/", json={"repo_url": repo_url, "bug_description": bug_description})
    def get_profile(self, username: str): return self._request("POST", "profile/review/", json={"username": username})
    def get_graph(self, repo_id: str): return self._request("GET", f"graph/{repo_id}/")
    def generate_docstring(self, repo_id: str, code: str, instruction: str): return self._request("POST", "generate-docstring/", json={"repo_id": repo_id, "new_code": code, "instruction": instruction})
    def generate_tests(self, repo_id: str, code_to_test: str, instruction: str): return self._request("POST", "generate-tests/", json={"repo_id": repo_id, "new_code": code_to_test, "instruction": instruction})
    def generate_scaffold(self, repo_id: str, target_files: List[str], instruction: str, rca_report: str, refinement_history: Optional[List[Dict]] = None):
        payload = {"repo_id": repo_id, "target_files": target_files, "instruction": instruction, "rca_report": rca_report, "refinement_history": refinement_history or []}
        return self._request("POST", "scaffold/", json=payload)
    def create_pr(self, issue_url: str, modified_files: Dict[str, str]): return self._request("POST", "ambassador/dispatch/", json={"issue_url": issue_url, "modified_files": modified_files})
    def get_diplomat_report(self, issue_title: str, issue_body: str): return self._request("POST", "diplomat/find-similar-issues/", json={"issue_title": issue_title, "issue_body": issue_body})
    def validate_in_crucible(self, repo_url: str, target_file: str, modified_code: str): return self._request("POST", "crucible/validate/", json={"repo_url": repo_url, "target_file": target_file, "modified_code": modified_code})
    def ingest_repository(self, repo_url: str): return self._request("POST", "ingest/", json={"repo_url": repo_url})
    def ask_oracle(self, repo_id: str, question: str): return self._request("POST", "oracle/ask/", json={"repo_id": repo_id, "question": question})
    def adjudicate_pr(self, pr_url: str): return self._request("POST", "review/adjudicate/", json={"pr_url": pr_url})
    def harmonize_fix(self, pr_url: str, review_text: str): return self._request("POST", "review/harmonize/", json={"pr_url": pr_url, "review_text": review_text})
    def get_sentinel_briefing(self, repo_id: str): return self._request("GET", f"sentinel/briefing/{repo_id}/")
    # --- NEW: Method for the Mission Controller ---
    def get_next_actions(self, last_action: str, result_data: dict): return self._request("POST", "suggest-actions/", json={"last_action": last_action, "result_data": result_data})
    
    # --- Repository Management Methods ---
    def list_repositories(self): return self._request("GET", "repositories/")
    def get_repository_detail(self, repo_id: str): return self._request("GET", f"repositories/{repo_id}/")
    def delete_repository(self, repo_id: str): return self._request("DELETE", f"repositories/{repo_id}/")
    def get_repository_status(self, repo_id: str): return self._request("GET", f"repositories/{repo_id}/status/")
    
    # --- BOM (Bill of Materials) Methods ---
    def get_bom_data(self, repo_id: str, format_type: str = "json"): return self._request("GET", f"bom/?repo_id={repo_id}&format={format_type}")
    def get_bom_dependencies(self, repo_id: str, **filters): 
        params = "&".join([f"{k}={v}" for k, v in filters.items() if v is not None])
        url = f"bom/dependencies/?repo_id={repo_id}"
        if params: url += "&" + params
        return self._request("GET", url)
    def get_bom_services(self, repo_id: str, service_type: str = None): 
        url = f"bom/services/?repo_id={repo_id}"
        if service_type: url += f"&service_type={service_type}"
        return self._request("GET", url)
    def get_bom_security(self, repo_id: str, severity: str = None): 
        url = f"bom/security/?repo_id={repo_id}"
        if severity: url += f"&severity={severity}"
        return self._request("GET", url)
    def regenerate_bom(self, repo_id: str, force: bool = False): return self._request("POST", "bom/regenerate/", json={"repo_id": repo_id, "force": force})
    def compare_bom(self, repo_id_1: str, repo_id_2: str, comparison_type: str = "dependencies"): return self._request("POST", "bom/compare/", json={"repo_id_1": repo_id_1, "repo_id_2": repo_id_2, "comparison_type": comparison_type})
    
    # --- Metrics History Method ---
    def get_sentinel_metrics_history(self, repo_id: str): return self._request("GET", f"sentinel/metrics/{repo_id}/")


def _present_next_actions(api_client, last_action: str, context: dict) -> Tuple[Optional[str], dict]:
    """
    Gets suggestions from the backend and presents a dynamic menu to the user.
    This is the core of the Conversational Mission Controller.

    Args:
        api_client: The instance of LumiereAPIClient.
        last_action: The command that was just executed.
        context: A dictionary containing data from the last action's result.

    Returns:
        A tuple of (command_string, context_dict) for the next action, or (None, {})
    """
    response = api_client.get_next_actions(last_action, context)
    if not response or "suggestions" not in response:
        return None, {}  # No suggestions, fall back to main loop

    suggestions = response["suggestions"]
    recommended_choice = response["recommended_choice"]

    if not suggestions:
        return None, {}

    # Build the rich prompt
    table = Table(title="[bold yellow]🚀 What's Next?[/bold yellow]", show_header=False, box=None, padding=(0, 2))
    choices = []
    choice_map = {}
    for item in suggestions:
        key = item["key"]
        text = item["text"]
        command = item["command"]
        table.add_row(f"([bold cyan]{key}[/bold cyan])", text)
        choices.append(key)
        choice_map[key] = command

    console.print(table)

    try:
        user_choice_key = Prompt.ask("Select an action", choices=choices, default=recommended_choice)
        selected_command = choice_map[user_choice_key]

        if selected_command == "back":
            return "back", {}

        # The context dictionary is passed through to the next command handler.
        return selected_command, context

    except (KeyboardInterrupt, EOFError):
        console.print("\n[yellow]Action cancelled.[/yellow]")
        return "back", {} # Treat cancel as going back to main menu
    except (ValueError, KeyError):
        console.print("[red]Invalid selection.[/red]")
        return "back", {}

# --- NEW: Oracle Session Manager ---
class OracleSession:
    def __init__(self, repo_url: str):
        self.repo_url = repo_url
        self.repo_id = self.repo_url.replace("https://github.com/", "").replace("/", "_")
        self.api = LumiereAPIClient()
        console.print(Panel(
            f"[bold magenta]🔮 Oracle Session Activated[/bold magenta]\n"
            f"[yellow]Repository:[/yellow] {self.repo_id}\n"
            f"[yellow]Model:[/yellow] {cli_state['model']}\n\n"
            "[dim]Ask any architectural question about the codebase. Type 'back' or 'exit' to finish.[/dim]",
            border_style="magenta"
        ))

    def loop(self):
        """Main interactive Q&A loop for The Oracle."""
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=oracle_completer,
            style=prompt_style
        )

        while True:
            try:
                # Custom prompt for the Oracle
                oracle_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', f' (Oracle/{self.repo_id})'),
                    ('class:separator', ' > '),
                ]

                question = prompt_session.prompt(oracle_prompt_text).strip()

                if not question:
                    continue
                if question.lower() in ("q", "quit", "exit", "back"):
                    break
                if question.lower() in ("h", "help"):
                     console.print("\n[dim]Enter your question or type 'back' to exit The Oracle.[/dim]")
                     continue

                with Status("[cyan]The Oracle is consulting the archives...[/cyan]", spinner="dots"):
                    response = self.api.ask_oracle(self.repo_id, question)

                if response and response.get("answer"):
                    console.print(Panel(
                        Markdown(response["answer"]),
                        title="[bold magenta]🔮 The Oracle's Answer[/bold magenta]",
                        border_style="magenta"
                    ))
                elif response and response.get("error"):
                    console.print(Panel(response["error"], title="[yellow]Oracle Warning[/yellow]", border_style="yellow"))
                else:
                    console.print("[red]❌ The Oracle did not provide an answer.[/red]")

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to the main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[magenta]🔮 Oracle session ended.[/magenta]")
        # Restore main completer
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )


# --- NEW: BOM Session Manager ---
class BOMSession:
    def __init__(self, repo_id: str, repo_url: str):
        self.repo_id = repo_id
        self.repo_url = repo_url
        self.api = LumiereAPIClient()
        console.print(Panel(
            f"[bold blue]📦 Bill of Materials Analysis[/bold blue]\n"
            f"[yellow]Repository:[/yellow] {self.repo_id}\n"
            f"[yellow]URL:[/yellow] {self.repo_url}\n\n"
            "[dim]Analyze dependencies, services, security, and more. Type 'help' for commands.[/dim]",
            border_style="blue"
        ))

    def loop(self):
        """Main interactive loop for BOM analysis."""
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=bom_completer,
            style=prompt_style
        )

        while True:
            try:
                bom_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', f' (BOM/{self.repo_id})'),
                    ('class:separator', ' > '),
                ]

                command = prompt_session.prompt(bom_prompt_text).strip()

                if not command:
                    continue
                if command.lower() in ("q", "quit", "exit", "back"):
                    break
                if command.lower() in ("h", "help"):
                    self.display_help()
                    continue

                self.handle_bom_command(command)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to the main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[blue]📦 BOM session ended.[/blue]")
        # Restore main completer
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def display_help(self):
        """Display BOM help commands."""
        help_table = Table(title="[bold blue]📦 BOM Analysis Commands[/bold blue]", border_style="blue")
        help_table.add_column("Command", style="bold cyan")
        help_table.add_column("Description", style="white")
        
        help_table.add_row("overview / o", "Complete BOM overview")
        help_table.add_row("dependencies / deps", "Analyze dependencies")
        help_table.add_row("services / s", "View services and infrastructure")
        help_table.add_row("security / sec", "Security analysis")
        help_table.add_row("compare / c", "Compare with another repository")
        help_table.add_row("regenerate / r", "Regenerate BOM data")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")
        
        console.print(help_table)

    def handle_bom_command(self, command: str):
        """Handle BOM commands."""
        cmd = command.lower().strip()
        
        if cmd in ("overview", "o"):
            self.show_overview()
        elif cmd in ("dependencies", "deps"):
            self.show_dependencies()
        elif cmd in ("services", "s"):
            self.show_services()
        elif cmd in ("security", "sec"):
            self.show_security()
        elif cmd in ("compare", "c"):
            self.show_compare()
        elif cmd in ("regenerate", "r"):
            self.regenerate_bom()
        else:
            console.print("[red]❌ Unknown command. Type 'help' for available commands.[/red]")

    def show_overview(self):
        """Show complete BOM overview."""
        with Status("[cyan]📦 Retrieving BOM overview...[/cyan]"):
            bom_data = self.api.get_bom_data(self.repo_id, "summary")
        
        if not bom_data:
            console.print("[red]❌ Could not retrieve BOM data. Repository may not be analyzed.[/red]")
            return
        
        summary = bom_data.get('summary', {})
        
        overview_content = f"""[bold]📊 Repository Summary[/bold]
• Primary Language: {summary.get('primary_language', 'Unknown')}
• Total Dependencies: {summary.get('total_dependencies', 0)}
• Total Services: {summary.get('total_services', 0)}
• Build Tools: {summary.get('total_build_tools', 0)}
• Languages Detected: {summary.get('languages_detected', 0)}
• Ecosystems: {', '.join(summary.get('ecosystems', []))}"""
        
        console.print(Panel(overview_content, title="[bold blue]📦 BOM Overview[/bold blue]", border_style="blue"))

    def show_dependencies(self):
        """Show dependency analysis with filtering options."""
        try:
            ecosystem = Prompt.ask("Filter by ecosystem (python/javascript/docker or press Enter for all)", default="")
            dependency_type = Prompt.ask("Filter by type (application/development/testing or press Enter for all)", default="")
            
            filters = {}
            if ecosystem: filters['ecosystem'] = ecosystem
            if dependency_type: filters['dependency_type'] = dependency_type
            
            with Status("[cyan]📦 Analyzing dependencies...[/cyan]"):
                deps_data = self.api.get_bom_dependencies(self.repo_id, **filters)
            
            if not deps_data:
                console.print("[red]❌ Could not retrieve dependency data.[/red]")
                return
            
            dependencies = deps_data.get('dependencies', [])
            stats = deps_data.get('statistics', {})
            
            if not dependencies:
                console.print("[yellow]📭 No dependencies found with the specified filters.[/yellow]")
                return
            
            table = Table(title=f"[bold cyan]📦 Dependencies ({len(dependencies)} found)[/bold cyan]", border_style="cyan")
            table.add_column("Name", style="white")
            table.add_column("Version", style="green")
            table.add_column("Type", style="blue")
            table.add_column("Ecosystem", style="yellow")
            
            for dep in dependencies[:20]:  # Show first 20
                table.add_row(
                    dep.get('name', 'Unknown'),
                    dep.get('version', 'Unknown'),
                    dep.get('category', 'Unknown'),
                    dep.get('ecosystem', 'Unknown')
                )
            
            if len(dependencies) > 20:
                table.add_row("...", f"({len(dependencies) - 20} more)", "...", "...")
            
            console.print(table)
            
            if stats:
                stats_content = f"📊 **Statistics:**\n"
                for ecosystem, count in stats.get('ecosystems', {}).items():
                    stats_content += f"• {ecosystem.capitalize()}: {count} dependencies\n"
                console.print(Panel(stats_content, title="[cyan]📊 Dependency Statistics[/cyan]", border_style="cyan"))
                
        except KeyboardInterrupt:
            console.print("\n[yellow]Dependency analysis cancelled.[/yellow]")

    def show_services(self):
        """Show services and infrastructure analysis."""
        with Status("[cyan]🏗️ Analyzing services and infrastructure...[/cyan]"):
            services_data = self.api.get_bom_services(self.repo_id)
        
        if not services_data:
            console.print("[red]❌ Could not retrieve services data.[/red]")
            return
        
        services = services_data.get('services', [])
        infrastructure = services_data.get('infrastructure_analysis', {})
        
        if not services:
            console.print("[yellow]🏗️ No services detected in this repository.[/yellow]")
        else:
            table = Table(title="[bold green]🏗️ Detected Services[/bold green]", border_style="green")
            table.add_column("Service", style="white")
            table.add_column("Version", style="green")
            table.add_column("Type", style="blue")
            table.add_column("Source", style="yellow")
            
            for service in services:
                table.add_row(
                    service.get('name', 'Unknown'),
                    service.get('version', 'Unknown'),
                    service.get('service_type', 'Unknown'),
                    service.get('source', 'Unknown')
                )
            
            console.print(table)
        
        if infrastructure:
            infra_content = "🏗️ **Infrastructure Analysis:**\n"
            if infrastructure.get('containerized'):
                infra_content += "• ✅ Containerized deployment detected\n"
            
            for infra_type, items in infrastructure.items():
                if isinstance(items, list) and items:
                    infra_content += f"• {infra_type.replace('_', ' ').title()}: {len(items)} found\n"
            
            console.print(Panel(infra_content, title="[green]🏗️ Infrastructure[/green]", border_style="green"))

    def show_security(self):
        """Show security analysis."""
        with Status("[cyan]🔒 Performing security analysis...[/cyan]"):
            security_data = self.api.get_bom_security(self.repo_id)
        
        if not security_data:
            console.print("[red]❌ Could not retrieve security data.[/red]")
            return
        
        summary = security_data.get('summary', {})
        recommendations = security_data.get('security_recommendations', [])
        compliance = security_data.get('compliance_status', {})
        
        security_content = f"""🔒 **Security Summary:**
• Total Dependencies Scanned: {summary.get('total_dependencies', 0)}
• High Risk Dependencies: {summary.get('high_risk_dependencies', 0)}
• Last Scan: {summary.get('last_scan', 'Never')}

🛡️ **Compliance Status:**
• Overall Compliant: {'✅ Yes' if compliance.get('compliant') else '❌ No'}"""
        
        if compliance.get('checks'):
            for check, status in compliance.get('checks', {}).items():
                emoji = "✅" if status == "pass" else "❌"
                security_content += f"\n• {check.replace('_', ' ').title()}: {emoji} {status}"
        
        console.print(Panel(security_content, title="[bold red]🔒 Security Analysis[/bold red]", border_style="red"))
        
        if recommendations:
            rec_content = "\n".join([f"• {rec}" for rec in recommendations])
            console.print(Panel(rec_content, title="[yellow]💡 Security Recommendations[/yellow]", border_style="yellow"))

    def show_compare(self):
        """Compare BOM with another repository."""
        try:
            analyzed_repos = find_analyzed_repos()
            other_repos = [repo for repo in analyzed_repos if repo['repo_id'] != self.repo_id]
            
            if not other_repos:
                console.print("[yellow]No other analyzed repositories found for comparison.[/yellow]")
                return
            
            console.print("Select a repository to compare with:")
            table = Table(show_header=False, box=None, padding=(0, 2))
            for i, repo in enumerate(other_repos, 1):
                table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
            console.print(table)
            
            choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(other_repos) + 1)], show_choices=False, default='1')
            other_repo_id = other_repos[int(choice) - 1]['repo_id']
            
            comparison_type = Prompt.ask("Comparison type", choices=["dependencies", "services", "languages", "comprehensive"], default="dependencies")
            
            with Status(f"[cyan]📊 Comparing {self.repo_id} with {other_repo_id}...[/cyan]"):
                comparison_data = self.api.compare_bom(self.repo_id, other_repo_id, comparison_type)
            
            if not comparison_data:
                console.print("[red]❌ Could not perform comparison.[/red]")
                return
            
            comparison = comparison_data.get('comparison', {})
            
            comp_content = f"""📊 **BOM Comparison Results:**
**Repositories:** {self.repo_id} vs {other_repo_id}
**Comparison Type:** {comparison_type.title()}

🔍 **Common Items:** {len(comparison.get('common', []))}
🔹 **Unique to {self.repo_id}:** {len(comparison.get('unique_to_repo_1', []))}
🔸 **Unique to {other_repo_id}:** {len(comparison.get('unique_to_repo_2', []))}"""
            
            console.print(Panel(comp_content, title="[bold magenta]📊 BOM Comparison[/bold magenta]", border_style="magenta"))
            
        except (KeyboardInterrupt, ValueError, IndexError):
            console.print("\n[yellow]Comparison cancelled.[/yellow]")

    def regenerate_bom(self):
        """Regenerate BOM data."""
        try:
            force = Confirm.ask("Force regeneration even if BOM is recent?", default=False)
            
            with Status("[cyan]🔄 Regenerating BOM data...[/cyan]"):
                result = self.api.regenerate_bom(self.repo_id, force)
            
            if result and result.get('regenerated'):
                console.print("[green]✅ BOM data regenerated successfully.[/green]")
            elif result and result.get('message'):
                console.print(Panel(result['message'], title="[yellow]BOM Regeneration[/yellow]", border_style="yellow"))
            else:
                console.print("[red]❌ Failed to regenerate BOM data.[/red]")
                
        except KeyboardInterrupt:
            console.print("\n[yellow]BOM regeneration cancelled.[/yellow]")


# --- NEW: Repository Management Session ---
class RepositoryManagementSession:
    def __init__(self):
        self.api = LumiereAPIClient()
        console.print(Panel(
            f"[bold green]🗃️ Repository Management[/bold green]\n\n"
            "[dim]Manage analyzed repositories. Type 'help' for commands.[/dim]",
            border_style="green"
        ))

    def loop(self):
        """Main interactive loop for repository management."""
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=repo_completer,
            style=prompt_style
        )

        while True:
            try:
                repo_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', ' (Repo-Mgmt)'),
                    ('class:separator', ' > '),
                ]

                command = prompt_session.prompt(repo_prompt_text).strip()

                if not command:
                    continue
                if command.lower() in ("q", "quit", "exit", "back"):
                    break
                if command.lower() in ("h", "help"):
                    self.display_help()
                    continue

                self.handle_repo_command(command)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to the main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[green]🗃️ Repository management session ended.[/green]")
        # Restore main completer
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def display_help(self):
        """Display repository management help commands."""
        help_table = Table(title="[bold green]🗃️ Repository Management Commands[/bold green]", border_style="green")
        help_table.add_column("Command", style="bold cyan")
        help_table.add_column("Description", style="white")
        
        help_table.add_row("list / l", "List all analyzed repositories")
        help_table.add_row("status / s", "Check repository analysis status")
        help_table.add_row("delete / d", "Delete repository data")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")
        
        console.print(help_table)

    def handle_repo_command(self, command: str):
        """Handle repository management commands."""
        cmd = command.lower().strip()
        
        if cmd in ("list", "l"):
            self.list_repositories()
        elif cmd in ("status", "s"):
            self.check_status()
        elif cmd in ("delete", "d"):
            self.delete_repository()
        else:
            console.print("[red]❌ Unknown command. Type 'help' for available commands.[/red]")

    def list_repositories(self):
        """List all analyzed repositories."""
        with Status("[cyan]🗃️ Fetching repository list...[/cyan]"):
            repos = self.api.list_repositories()
        
        if not repos:
            console.print("[yellow]📭 No analyzed repositories found.[/yellow]")
            return
        
        table = Table(title=f"[bold green]🗃️ Analyzed Repositories ({len(repos)} found)[/bold green]", border_style="green")
        table.add_column("Repository", style="white")
        table.add_column("Display Name", style="cyan")
        table.add_column("URL", style="blue")
        
        for repo in repos:
            table.add_row(
                repo.get('repo_id', 'Unknown'),
                repo.get('display_name', 'Unknown'),
                repo.get('url', 'Unknown')
            )
        
        console.print(table)

    def check_status(self):
        """Check repository analysis status."""
        try:
            repo_id = Prompt.ask("Enter repository ID to check")
            
            with Status(f"[cyan]🔍 Checking status of {repo_id}...[/cyan]"):
                status_data = self.api.get_repository_status(repo_id)
                detail_data = self.api.get_repository_detail(repo_id)
            
            if not status_data:
                console.print(f"[red]❌ Repository '{repo_id}' not found.[/red]")
                return
            
            status = status_data.get('status', 'unknown')
            status_emoji = "✅" if status == "complete" else "❌"
            
            status_content = f"""🔍 **Repository Status**
• Repository ID: {repo_id}
• Analysis Status: {status_emoji} {status.title()}"""
            
            if detail_data:
                metadata = detail_data.get('metadata', {})
                status_content += f"""
• Files Analyzed: {metadata.get('total_files', 'Unknown')}
• Analysis Date: {metadata.get('analysis_date', 'Unknown')}
• Primary Language: {metadata.get('primary_language', 'Unknown')}"""
            
            console.print(Panel(status_content, title=f"[bold cyan]🔍 Status: {repo_id}[/bold cyan]", border_style="cyan"))
            
        except KeyboardInterrupt:
            console.print("\n[yellow]Status check cancelled.[/yellow]")

    def delete_repository(self):
        """Delete repository data."""
        try:
            analyzed_repos = find_analyzed_repos()
            if not analyzed_repos:
                console.print("[yellow]📭 No repositories available to delete.[/yellow]")
                return
            
            console.print("Select a repository to delete:")
            table = Table(show_header=False, box=None, padding=(0, 2))
            for i, repo in enumerate(analyzed_repos, 1):
                table.add_row(f"([bold red]{i}[/bold red])", repo['display_name'])
            console.print(table)
            
            choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
            selected_repo = analyzed_repos[int(choice) - 1]
            
            if not Confirm.ask(f"[bold red]Are you sure you want to delete '{selected_repo['display_name']}'?[/bold red]", default=False):
                console.print("[yellow]Deletion cancelled.[/yellow]")
                return
            
            with Status(f"[red]🗑️ Deleting {selected_repo['repo_id']}...[/red]"):
                result = self.api.delete_repository(selected_repo['repo_id'])
            
            if result is not None:  # 204 No Content returns None but is success
                console.print(f"[green]✅ Repository '{selected_repo['display_name']}' deleted successfully.[/green]")
            else:
                console.print(f"[red]❌ Failed to delete repository.[/red]")
                
        except (KeyboardInterrupt, ValueError, IndexError):
            console.print("\n[yellow]Repository deletion cancelled.[/yellow]")


# --- MODIFIED: Implemented a two-step provider/model selection process. ---
def handle_model_selection(api: "LumiereAPIClient"):
    """
    Guides the user through a two-step process: first selecting an LLM provider,
    then selecting a model from that provider.
    """
    console.print("\n[bold cyan]🤖 LLM Provider & Model Selection[/bold cyan]")
    with Status("[cyan]Fetching available models from backend...[/cyan]"):
        available_models_data = api.list_models()

    if not available_models_data:
        # Error is printed by the API client
        return

    cli_state["available_models"] = available_models_data

    # Step 1: Group models by provider
    providers = defaultdict(list)
    for model in available_models_data:
        provider = model.get('provider', 'unknown').capitalize()
        providers[provider].append(model)

    if not providers:
        console.print("[red]No providers with available models found.[/red]")
        return

    provider_names = list(providers.keys())

    try:
        # Step 2: Prompt for the provider
        console.print("\n[bold]First, select a provider:[/bold]")
        provider_table = Table(show_header=False, box=None, padding=(0, 2))
        for i, name in enumerate(provider_names, 1):
            provider_table.add_row(f"[cyan]({i})[/cyan]", name)
        console.print(provider_table)

        # Determine default provider choice
        current_provider = ""
        if cli_state.get("model"):
            current_provider = cli_state.get("model").split('/')[0].capitalize()

        default_provider_idx = "1"
        if current_provider in provider_names:
            default_provider_idx = str(provider_names.index(current_provider) + 1)

        provider_choice_str = Prompt.ask(
            "Enter your provider choice",
            choices=[str(i) for i in range(1, len(provider_names) + 1)],
            show_choices=False,
            default=default_provider_idx
        )
        selected_provider_name = provider_names[int(provider_choice_str) - 1]

        # Step 3: Prompt for a model from the selected provider
        models_for_provider = providers[selected_provider_name]

        model_table = Table(title=f"Available Models from [yellow]{selected_provider_name}[/yellow]", border_style="blue")
        model_table.add_column("Choice #", style="dim", justify="center")
        model_table.add_column("Model Name", style="white")
        model_table.add_column("Model ID", style="cyan")

        model_choices_for_prompt = []
        for i, model in enumerate(models_for_provider, 1):
            model_choices_for_prompt.append(str(i))
            model_table.add_row(
                str(i),
                model.get('name', 'N/A'),
                model.get('id', 'N/A')
            )
        console.print(model_table)

        # Find default selection if a model from this provider is already selected
        current_model_index_str = "1"
        current_model_id = cli_state.get("model")
        if current_model_id and current_model_id.startswith(selected_provider_name.lower()):
            for i, model in enumerate(models_for_provider):
                if model['id'] == current_model_id:
                    current_model_index_str = str(i + 1)
                    break

        model_choice_str = Prompt.ask(
            "[bold]Select a model number to use[/bold]",
            choices=model_choices_for_prompt,
            show_choices=False,
            default=current_model_index_str
        )
        selected_model_index = int(model_choice_str) - 1
        selected_model_id = models_for_provider[selected_model_index]['id']

        cli_state["model"] = selected_model_id
        save_config()
        console.print(f"✅ Model set to [bold green]{cli_state['model']}[/bold green]. This will be saved for future sessions.")

    except (ValueError, IndexError):
        console.print("[red]❌ Invalid selection.[/red]")
    except KeyboardInterrupt:
        console.print("\n[yellow]Model selection cancelled.[/yellow]")

# --- MODIFIED: Dynamic prompt text based on selected model/provider ---
def get_prompt_text() -> List[Tuple[str, str]]:
    """
    Builds a prompt_toolkit-compatible formatted text list for the prompt.
    Dynamically displays the provider or model name.
    """
    display_name = "Choose Provider"  # Default text when no model is selected
    model_id = cli_state.get("model")

    if model_id:
        parts = model_id.split('/', 1)
        if len(parts) == 2:
            provider, model_name = parts
            if provider == "ollama":
                display_name = model_name  # Show specific model name for ollama
            else:
                display_name = provider.capitalize()  # Show provider name for others (e.g., Gemini)
        else:
            display_name = model_id  # Fallback for malformed ID

    return [
        ('class:lumiere', 'Lumière'),
        ('class:provider', f' ({display_name})'),
        ('class:separator', ' > '),
    ]

# --- Onboarding Session Manager ---
class OnboardingSession:
    def __init__(self, repo_id: str, repo_url: str):
        self.repo_id = repo_id
        self.repo_url = repo_url
        self.api = LumiereAPIClient()
        console.print(Panel(
            f"[bold green]🎓 Onboarding Concierge[/bold green]\n"
            f"[yellow]Repository:[/yellow] {self.repo_id}\n"
            f"[yellow]URL:[/yellow] {self.repo_url}\n\n"
            "[dim]Helping new developers get started. Type 'help' for commands.[/dim]",
            border_style="green"
        ))

    def loop(self):
        """Main interactive loop for onboarding assistance."""
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=onboard_completer,
            style=prompt_style
        )

        while True:
            try:
                onboard_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', f' (Onboard/{self.repo_id})'),
                    ('class:separator', ' > '),
                ]

                command = prompt_session.prompt(onboard_prompt_text).strip()

                if not command:
                    continue
                if command.lower() in ("q", "quit", "exit", "back"):
                    break
                if command.lower() in ("h", "help"):
                    display_interactive_help('onboard')
                    continue

                self.handle_onboard_command(command)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to the main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[green]🎓 Onboarding session ended.[/green]")
        # Restore main completer
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def handle_onboard_command(self, command: str):
        """Handle onboarding-specific commands."""
        parts = command.split()
        if not parts:
            return

        cmd = parts[0].lower()

        if cmd in ("scout", "s"):
            self.scout_good_first_issues()
        elif cmd in ("expert", "e"):
            if len(parts) < 2:
                console.print("[red]Usage: expert <file_path>[/red]")
                return
            file_path = " ".join(parts[1:])
            self.find_expert_for_file(file_path)
        elif cmd in ("guide", "g"):
            if len(parts) < 2:
                console.print("[red]Usage: guide <issue_number>[/red]")
                return
            try:
                issue_number = int(parts[1])
                self.generate_onboarding_guide(issue_number)
            except ValueError:
                console.print("[red]Issue number must be a valid integer.[/red]")
        else:
            console.print(f"[red]Unknown command: {cmd}. Type 'help' for available commands.[/red]")

    def scout_good_first_issues(self):
        """Find and display issues suitable for newcomers."""
        console.print("\n[bold cyan]🔍 Scouting for good first issues...[/bold cyan]")
        
        with Status("[bold green]Analyzing issues with onboarding suitability scoring...", console=console):
            try:
                # Call the strategist to get prioritized issues with onboarding scores
                response = self.api._request("POST", "strategist/prioritize/", {
                    "repo_url": self.repo_url
                })
                
                if not response or 'prioritized_issues' not in response:
                    console.print("[red]❌ Failed to fetch issues for analysis.[/red]")
                    return
                
                issues = response['prioritized_issues']
                # Filter for issues with high onboarding suitability
                suitable_issues = [
                    issue for issue in issues 
                    if issue.get('onboarding_suitability_score', 0) > 70
                ]
                
                if not suitable_issues:
                    console.print("[yellow]No issues found with high onboarding suitability scores.[/yellow]")
                    return
                
                # Display results in a nice table
                table = Table(title="🌟 Good First Issues", border_style="green")
                table.add_column("Issue #", style="bold cyan", width=8)
                table.add_column("Title", style="white", width=40)
                table.add_column("Onboarding Score", style="bold green", width=15)
                table.add_column("Blast Radius", style="yellow", width=12)
                
                for issue in suitable_issues[:10]:  # Show top 10
                    table.add_row(
                        f"#{issue['number']}",
                        issue['title'][:35] + "..." if len(issue['title']) > 35 else issue['title'],
                        f"{issue.get('onboarding_suitability_score', 0):.1f}/100",
                        str(issue.get('blast_radius', 'N/A'))
                    )
                
                console.print(table)
                
                if len(suitable_issues) > 10:
                    console.print(f"[dim]... and {len(suitable_issues) - 10} more suitable issues[/dim]")
                
                console.print(f"\n[green]✅ Found {len(suitable_issues)} issues suitable for newcomers![/green]")
                
            except Exception as e:
                console.print(f"[red]❌ Error during issue analysis: {str(e)}[/red]")

    def find_expert_for_file(self, file_path: str):
        """Find experts for a specific file."""
        console.print(f"\n[bold cyan]👨‍💻 Finding experts for: {file_path}[/bold cyan]")
        
        with Status("[bold green]Analyzing file expertise...", console=console):
            try:
                response = self.api._request("POST", "expertise/find/", {
                    "repo_id": self.repo_id,
                    "file_path": file_path,
                    "type": "file"
                })
                
                if not response or 'experts' not in response:
                    console.print("[red]❌ Failed to find experts for this file.[/red]")
                    return
                
                experts = response['experts']
                
                if not experts:
                    console.print(f"[yellow]No experts found for {file_path}.[/yellow]")
                    return
                
                # Display experts in a table
                table = Table(title=f"🎯 Experts for {file_path}", border_style="blue")
                table.add_column("Rank", style="bold cyan", width=6)
                table.add_column("Expert", style="white", width=25)
                table.add_column("Email", style="yellow", width=30)
                table.add_column("Score", style="bold green", width=8)
                table.add_column("Lines", style="magenta", width=8)
                
                for i, expert in enumerate(experts[:5], 1):  # Show top 5
                    table.add_row(
                        str(i),
                        expert['author'],
                        expert['email'],
                        f"{expert['score']:.1f}",
                        str(expert['details']['blame_lines'])
                    )
                
                console.print(table)
                console.print(f"\n[green]✅ Found {len(experts)} experts for this file![/green]")
                console.print("[dim]💡 Tip: Reach out to the top expert for guidance on this file.[/dim]")
                
            except Exception as e:
                console.print(f"[red]❌ Error finding experts: {str(e)}[/red]")

    def generate_onboarding_guide(self, issue_number: int):
        """Generate a personalized onboarding guide for an issue."""
        console.print(f"\n[bold cyan]📚 Generating onboarding guide for issue #{issue_number}[/bold cyan]")
        
        with Status("[bold green]Creating personalized learning path...", console=console):
            try:
                response = self.api._request("POST", "onboarding/guide/", {
                    "repo_id": self.repo_id,
                    "issue_number": issue_number
                })
                
                if not response or not response.get('onboarding_guide'):
                    console.print("[red]❌ Failed to generate onboarding guide.[/red]")
                    if response and response.get('error'):
                        console.print(f"[red]Error: {response['error']}[/red]")
                    return
                
                # Display the guide
                guide_content = response['onboarding_guide']
                
                # Show summary first
                console.print(Panel(
                    f"[bold green]📖 Onboarding Guide Generated![/bold green]\n"
                    f"[yellow]Issue:[/yellow] #{issue_number} - {response.get('issue_title', 'Unknown')}\n"
                    f"[yellow]Learning Steps:[/yellow] {response.get('learning_path_steps', 0)}\n"
                    f"[yellow]Core Files:[/yellow] {len(response.get('locus_files', []))}\n",
                    border_style="green"
                ))
                
                # Display the full guide using Rich Markdown
                console.print(Markdown(guide_content))
                
                console.print(f"\n[green]✅ Your personalized onboarding guide is ready![/green]")
                console.print("[dim]💡 Follow the steps above to understand this issue. Good luck! 🍀[/dim]")
                
            except Exception as e:
                console.print(f"[red]❌ Error generating guide: {str(e)}[/red]")


# --- Enhanced Analysis Session Manager ---
class AnalysisSession:
    def __init__(self, repo_url: str):
        self.repo_url = format_url(repo_url)
        if not validate_github_url(self.repo_url):
            raise ValueError(f"Invalid GitHub URL: {self.repo_url}")

        self.repo_id = self.repo_url.replace("https://github.com/", "").replace("/", "_")
        self.issues = []
        # --- State for RCA-to-Fix Pipeline ---
        self.last_rca_report = None
        self.last_rca_issue_num = None
        # ---
        self.api = LumiereAPIClient()
        cli_state["last_repo_url"] = self.repo_url
        save_config()

    def start(self) -> bool:
        """Initialize the analysis session."""
        console.print(Panel(
            f"[bold cyan]🔍 Analysis Session Starting[/bold cyan]\n"
            f"[yellow]Repository:[/yellow] {self.repo_url}\n"
            f"[yellow]Model:[/yellow] {cli_state['model']}",
            border_style="cyan"
        ))

        with Status("[cyan]Checking backend connection...") as status:
            if not self.api.health_check():
                return False # Error already printed by client
            status.update("[green]✓ Backend connection established")
            time.sleep(0.5)

        is_already_analyzed = check_if_repo_is_analyzed(self.repo_id)
        do_embed = False

        if is_already_analyzed:
            console.print(f"\n[bold green]✓ Found existing analyzed repository for '{self.repo_id}'.[/bold green] [dim]Skipping ingestion.[/dim]")
        else:
            try:
                do_embed = Confirm.ask(
                    "\n[bold]Do you want to clone and embed this repo for full analysis (briefing, rca, fix)?[/bold]\n"
                    "[dim](This can take a few minutes for large repos. Choose 'N' for issue listing only.)[/dim]",
                    default=True
                )
            except KeyboardInterrupt:
                console.print("\n[yellow]Analysis cancelled.[/yellow]")
                return False

        if do_embed:
            with Status("[cyan]🚀 Beginning ingestion...[/cyan]", spinner="earth") as status:
                status.update("[cyan]Cloning repository and analyzing files...[/cyan]")
                ingest_result = self.api.ingest_repository(self.repo_url)
                if ingest_result and ingest_result.get("status") == "success":
                    status.update("[green]✓ Repository cloned and embedded successfully.[/green]")
                    time.sleep(1)
                else:
                    error_details = ingest_result.get('error', 'Unknown error during ingestion.') if ingest_result else "No response from server."
                    console.print(f"\n[bold red]❌ Ingestion failed:[/bold red] {error_details}")
                    try:
                        if not Confirm.ask("[yellow]Would you like to continue with limited (issue list only) analysis?[/yellow]", default=True):
                            return False
                    except KeyboardInterrupt:
                        return False

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("[green]🤖 Contacting The Strategist...", total=None)
            strategist_data = self.api.get_analysis(self.repo_url)
            if not strategist_data:
                return False

        self.issues = strategist_data.get("prioritized_issues", [])
        if not self.issues:
            console.print("[yellow]📭 No open issues found in this repository.[/yellow]")
            return False

        console.print(f"✨ The Strategist identified [bold green]{len(self.issues)}[/bold green] open issues for analysis.")
        self.display_issue_table()
        return True

    def display_issue_table(self):
        """Display a formatted table of prioritized issues."""
        table = Table(
            title="[bold blue]🎯 Prioritized Issue Triage[/bold blue]",
            show_header=True,
            header_style="bold magenta",
            border_style="blue"
        )
        table.add_column("Rank", style="dim", justify="center", width=6)
        table.add_column("Score", style="bold", justify="center", width=8)
        table.add_column("Issue #", style="green", justify="center", width=10)
        table.add_column("Title", style="white", no_wrap=False)

        for issue in self.issues:
            score = issue.get('score', 0)
            score_style = "red" if score >= 90 else "yellow" if score >= 70 else "white"
            score_emoji = "🔥" if score >= 90 else "⚡" if score >= 70 else "📝"

            table.add_row(
                f"#{issue['rank']}",
                f"{score_emoji} [{score_style}]{score}[/{score_style}]",
                f"#{issue['number']}",
                issue['title'][:80] + "..." if len(issue['title']) > 80 else issue['title']
            )
        console.print(table)

    def display_graph(self, graph_data: dict, repo_id: str):
        """
        [NEW & IMPROVED] Displays the architectural graph in a summarized, readable format.
        It now groups and counts calls to avoid overwhelming the user.
        """
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph (Summary) ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])

        if not nodes:
            console.print("[yellow]No architectural nodes were mapped for this project.[/yellow]")
            return

        edges_by_source = defaultdict(lambda: {'imports': [], 'calls': defaultdict(int)})
        for edge in edges:
            source_id = edge['source']
            edge_type = edge['type']
            target_id = edge['target']

            if edge_type == 'IMPORTS':
                edges_by_source[source_id]['imports'].append(target_id)
            elif edge_type == 'CALLS':
                edges_by_source[source_id]['calls'][target_id] += 1

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]", guide_style="cyan")
        file_tree_nodes = {}

        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                lang = node_data.get('language', 'unknown')
                icon = "📄"
                if lang == 'python': icon = "🐍"
                if lang == 'javascript': icon = "🟨"

                file_branch = tree.add(f"{icon} [bold green]{node_id}[/bold green] [dim]({lang})[/dim]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_node_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")

                    for method_name in sorted(nodes.get(class_node_id, {}).get('methods', [])):
                        class_branch.add(f"  -  M [dim]{method_name}()[/dim]")

                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"  - F [dim]{func_name}()[/dim]")

        for source_id, relationships in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]

                if relationships['imports']:
                    import_branch = parent_branch.add("📥 [bold]Imports[/bold]")
                    for target in sorted(list(set(relationships['imports']))):
                        import_branch.add(f"[yellow]{target}[/yellow]")

                if relationships['calls']:
                    calls_branch = parent_branch.add("📞 [bold]Calls[/bold]")
                    sorted_calls = sorted(relationships['calls'].items(), key=lambda item: item[1], reverse=True)

                    max_calls_to_show = 15
                    for i, (target, count) in enumerate(sorted_calls):
                        if i >= max_calls_to_show:
                            calls_branch.add(f"[dim]... and {len(sorted_calls) - max_calls_to_show} more.[/dim]")
                            break

                        count_str = f" [dim](x{count})[/dim]" if count > 1 else ""
                        calls_branch.add(f"[magenta]{target}[/magenta]{count_str}")


        console.print(tree)
        console.print("\n[bold magenta]--------------------------------------------------------[/bold magenta]")

    def loop(self):
        """Main interactive loop for the analysis session."""
        display_interactive_help('analyze')

        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=analysis_completer,
            style=prompt_style
        )

        while True:
            try:
                prompt_text = get_prompt_text()
                command_str = prompt_session.prompt(prompt_text).strip()

                if not command_str:
                    continue

                if command_str.lower() in ("q", "quit", "exit", "back"):
                    break

                self.handle_analysis_command(command_str)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[cyan]📊 Analysis session ended.[/cyan]")

        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def handle_analysis_command(self, command_str: str):
        """Handle commands within the analysis session."""
        try:
            parts = shlex.split(command_str.lower())
        except ValueError:
            console.print("[red]❌ Invalid command syntax.[/red]")
            return

        command = parts[0] if parts else ""
        args = parts[1:] if len(parts) > 1 else []

        if command in ("h", "help"):
            display_interactive_help('analyze')
            return

        if command in ("l", "list"):
            self.display_issue_table()
            return

        if command in ('g', 'graph'):
            self.execute_action(command, {}) # Pass empty dict, issue not needed
            console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
            return

        if command in ('m', 'mage'):
            self.execute_action(command, {}) # Pass empty dict, issue not needed
            console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
            return

        if command in ('f', 'fix') and self.last_rca_report:
             issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
             if issue:
                 self.execute_action(command, issue)
                 console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
                 return

        if command not in ('f', 'fix', 'b', 'briefing', 'r', 'rca', 'd', 'details', 'm', 'mage'):
            console.print("[red]❌ Unknown command. Type 'help' for available commands.[/red]")
            return

        issue_num_str = None
        if args and args[0].isdigit():
            issue_num_str = args[0]
        else:
            try:
                prompt_ask_text = f"Which issue # for '[cyan]{command}[/cyan]'?"
                if command in ('f', 'fix') and self.last_rca_report:
                     prompt_ask_text += f"\n[dim](Press Enter to fix issue #{self.last_rca_issue_num} from the last RCA)[/dim]"

                issue_num_str = Prompt.ask(prompt_ask_text).strip()
            except KeyboardInterrupt:
                console.print("\n[yellow]Command cancelled.[/yellow]")
                return

        if not issue_num_str:
            if command in ('f', 'fix') and self.last_rca_report:
                issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
                if issue:
                    self.execute_action(command, issue)
                else:
                    console.print("[red]❌ Could not find issue from last RCA. Please specify an issue number.[/red]")
            else:
                console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        if not issue_num_str.isdigit():
            console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        target_issue = next((iss for iss in self.issues if iss.get('number') == int(issue_num_str)), None)
        if not target_issue:
            console.print(f"[red]❌ Issue #{issue_num_str} not found in the prioritized list.[/red]")
            console.print("[dim]💡 Use 'list' to see available issues.[/dim]")
            return

        self.execute_action(command, target_issue)
        console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")

    def execute_action(self, command: str, issue: Dict):
        """Execute the specified action on an issue."""
        if command in ("f", "fix"):
            self.handle_fix_dialogue(issue)
            return

        if command in ("r", "rca"):
            self.handle_rca_command(issue)
            return

        if command in ('g', 'graph'):
            with Status("[cyan]🗺️ Contacting Cartographer's Architectural Graph...[/cyan]", spinner="earth") as status:
                graph_data = self.api.get_graph(self.repo_id)
                status.update("[green]✓ Graph retrieved.[/green]")
                time.sleep(0.5)

            if graph_data and graph_data.get("graph"):
                self.display_graph(graph_data["graph"], graph_data["repo_id"])
            elif graph_data and graph_data.get("message"):
                console.print(Panel(graph_data["message"], title="[yellow]Graph Not Available[/yellow]", border_style="yellow"))
            else:
                 console.print("[red]❌ Could not retrieve architectural graph.[/red]")
            return

        if command in ('m', 'mage'):
            self.handle_mage_session()
            return

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            if command in ("b", "briefing"):
                task = progress.add_task(f"[cyan]📋 Getting briefing for issue #{issue['number']}...", total=None)
                briefing_data = self.api.get_briefing(f"{self.repo_url}/issues/{issue['number']}")
                progress.remove_task(task)
                if briefing_data and briefing_data.get("briefing"):
                    console.print(Panel(
                        Markdown(briefing_data["briefing"]),
                        title=f"[bold blue]📋 Issue Briefing #{issue['number']}[/bold blue]",
                        border_style="blue"
                    ))
                else:
                    console.print("[red]❌ Could not retrieve briefing.[/red]")
            elif command in ("d", "details"):
                issue_url = f"{self.repo_url}/issues/{issue['number']}"
                console.print(Panel(
                    f"[bold]Issue #{issue['number']}[/bold]\n"
                    f"[yellow]Title:[/yellow] {issue['title']}\n"
                    f"[yellow]Priority Score:[/yellow] {issue['score']}/100\n"
                    f"[yellow]URL:[/yellow] [link={issue_url}]{issue_url}[/link]\n"
                    f"[yellow]Description:[/yellow] {issue.get('description', 'No description available')[:200]}...",
                    title="[bold green]📝 Issue Details[/bold green]",
                    border_style="green"
                ))

    def handle_rca_command(self, issue: Dict):
        """Handle the new context-aware root cause analysis command."""
        console.print(f"[cyan]🔍 Starting Root Cause Analysis for issue #{issue['number']}[/cyan]")
        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("[cyan]🕵️ Performing multi-file root cause analysis...", total=None)
            rca_data = self.api.get_rca(self.repo_url, issue_desc)
            progress.remove_task(task)

        if rca_data and rca_data.get("analysis"):
            analysis_text = rca_data["analysis"]

            is_error = "Error from Gemini API" in analysis_text or "API Request Failed" in analysis_text

            if is_error:
                self.last_rca_report = None
                self.last_rca_issue_num = None
                console.print(Panel(
                    Markdown(analysis_text),
                    title=f"[bold red]🕵️ Root Cause Analysis - Issue #{issue['number']} (Failed)[/bold red]",
                    border_style="red"
                ))
            else:
                self.last_rca_report = analysis_text
                self.last_rca_issue_num = issue['number']
                console.print(Panel(
                    Markdown(self.last_rca_report),
                    title=f"[bold red]🕵️ Root Cause Analysis - Issue #{issue['number']}[/bold red]",
                    border_style="red"
                ))
                console.print("\n[bold yellow]💡 Pro-tip:[/bold yellow] [dim]You can now type '[/dim][bold]f[/bold][dim]' to start fixing this issue.[/dim]")
        else:
            self.last_rca_report = None
            self.last_rca_issue_num = None
            console.print("[red]❌ Could not perform root cause analysis.[/red]")

    def _display_diff(self, original_code: str, new_code: str, filename: str):
        """Display a formatted diff of code changes for a single file."""
        diff = difflib.unified_diff(
            original_code.splitlines(keepends=True),
            new_code.splitlines(keepends=True),
            fromfile=f'🔴 {filename} (Original)',
            tofile=f'🟢 {filename} (Proposed)'
        )
        diff_panel_content = Text()
        has_changes = False
        for line in diff:
            has_changes = True
            if line.startswith('+++') or line.startswith('---'):
                diff_panel_content.append(line, style="bold blue")
            elif line.startswith('+'):
                diff_panel_content.append(line, style="green")
            elif line.startswith('-'):
                diff_panel_content.append(line, style="red")
            elif line.startswith('@'):
                diff_panel_content.append(line, style="bold yellow")
            else:
                diff_panel_content.append(line, style="dim")

        if not has_changes:
            return

        console.print(Panel(
            diff_panel_content,
            title=f"[bold yellow]📝 Proposed Changes for {filename}[/bold yellow]",
            expand=True,
            border_style="yellow"
        ))

    def _extract_filenames_from_rca(self, rca_report: str) -> List[str]:
        """Extracts filenames from markdown code fences, inline backticks, and lists."""
        pattern = r'(?:\s|-|\*|`)([\w./-]+\.(?:py|js|ts|gs|json|md|html|css|yaml|yml|toml|txt))\b'
        matches = re.findall(pattern, rca_report)

        backtick_pattern = r'`([\w./\\-]+)`'
        matches.extend(re.findall(backtick_pattern, rca_report))

        filenames = sorted(list(set(matches)))
        return [f for f in filenames if '.' in f and f.lower() not in ['true', 'false']]

    def handle_fix_dialogue(self, issue: Dict):
        """Handle the complete fix dialogue, now driven by RCA and with documentation and automated test generation."""
        console.print(Panel(
            f"[bold cyan]🤝 Socratic Dialogue[/bold cyan] starting for:\n"
            f"[bold green]Issue #{issue['number']}: {issue['title']}[/bold green]",
            border_style="cyan"
        ))

        if not self.last_rca_report or self.last_rca_issue_num != issue['number']:
             console.print("[yellow]⚠️  Warning: No Root Cause Analysis has been run for this issue.[/yellow]")
             try:
                 if Confirm.ask("[bold]Would you like to run RCA first to provide context for the fix?[/bold]", default=True):
                     self.handle_rca_command(issue)
                     if not self.last_rca_report:
                         console.print("[red]❌ Cannot proceed with fix without a successful RCA.[/red]")
                         return
                 else:
                     console.print("[red]❌ Fix command cancelled. Please run RCA first.[/red]")
                     return
             except KeyboardInterrupt:
                 console.print("\n[yellow]Fix command cancelled.[/yellow]")
                 return

        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        target_files = self._extract_filenames_from_rca(self.last_rca_report)
        if not target_files:
            console.print("[red]❌ Could not automatically determine target files from the RCA report.[/red]")
            return

        console.print(f"[dim]✓ Identified suspect files from RCA: {', '.join(target_files)}[/dim]")

        refinement_history = []
        iteration_count = 0
        max_iterations = 5
        modified_files = {}
        original_contents = {}
        is_documented = False
        are_tests_generated = False

        while iteration_count < max_iterations:
            iteration_count += 1

            if not modified_files or refinement_history:
                console.print(f"\n[dim]🔄 Iteration {iteration_count}/{max_iterations}[/dim]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task(f"[cyan]⚡ Generating multi-file code fix...", total=None)
                    fix_data = self.api.generate_scaffold(self.repo_id, target_files, issue_desc, self.last_rca_report, refinement_history)
                    progress.remove_task(task)

                if not fix_data:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    return

                if "modified_files" not in fix_data or not fix_data["modified_files"]:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    if fix_data and fix_data.get("llm_response"): console.print(Panel(Text(fix_data["llm_response"], overflow="fold"), title="[yellow]🔍 LLM Raw Response (for debugging)[/yellow]", border_style="yellow"))
                    return

                modified_files = fix_data["modified_files"]
                original_contents = fix_data["original_contents"]
                is_documented = False
                are_tests_generated = False

            console.rule("[bold]📝 Review Proposed Changes[/bold]")
            for filename, new_code in modified_files.items():
                if original_contents.get(filename, "") != new_code:
                    self._display_diff(original_contents.get(filename, ""), new_code, filename)

            all_validations_passed = True
            files_to_validate = [
                f for f in modified_files.keys()
                if not f.lower().endswith(('.md', '.txt', '.json', '.toml', '.yaml', '.yml', '.ron'))
            ]

            if not files_to_validate:
                 console.print(Panel("✅ [bold green]No runnable code files to validate. Skipping Crucible.[/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))
            else:
                for filename in files_to_validate:
                    new_code = modified_files[filename]
                    with Progress(SpinnerColumn(), TextColumn("[bold cyan][progress.description]{task.description}"), transient=True) as progress:
                        task = progress.add_task(f"🔥 Entering The Crucible for {filename}...", total=None)
                        validation_result = self.api.validate_in_crucible(self.repo_url, filename, new_code)
                        progress.remove_task(task)

                    if not validation_result or validation_result.get("status") != "passed":
                        all_validations_passed = False
                        console.print(Panel(f"❌ [bold red]Validation Failed for {filename}[/bold red]\n[bold]Test Results:[/bold]\n{validation_result.get('logs', 'No logs') if validation_result else 'Crucible service error'}", title=f"[red]🔥 Crucible Report: {filename}[/red]", border_style="red"))
                        break

            if all_validations_passed and files_to_validate:
                 console.print(Panel("✅ [bold green]All tests passed for all modified files![/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))

            while True:
                try:
                    action_choices = ['r', 'c']
                    prompt_text = ""

                    if all_validations_passed:
                        action_choices.append('a')
                        prompt_text += "\n[bold]✅ All tests passed! Choose action:[/bold]\n[bold green](a)[/bold green] Approve & create PR\n"
                        if not is_documented and any(f.endswith((".py", ".js", ".ts")) for f in modified_files.keys()):
                           action_choices.append('d')
                           prompt_text += "[bold blue](d)[/bold blue] Document the changes\n"
                        if not are_tests_generated:
                            action_choices.append('t')
                            prompt_text += "[bold yellow](t)[/bold yellow] Generate tests for the fix\n"
                    else:
                        prompt_text += "\n[bold red]❌ Validation failed. Choose action:[/bold red]\n"

                    prompt_text += "[bold yellow](r)[/bold yellow] Refine with feedback\n"
                    prompt_text += "[bold red](c)[/bold red] Cancel"

                    default_choice = 'a' if all_validations_passed else 'r'
                    choice = Prompt.ask(prompt_text, choices=action_choices, default=default_choice).lower()

                except KeyboardInterrupt:
                    choice = 'c'

                if choice == 'd':
                    if is_documented:
                        console.print("[yellow]Code is already documented.[/yellow]")
                        continue
                    if not all_validations_passed:
                        console.print("[red]Cannot document code that has failed validation.[/red]")
                        continue

                    documented_files = {}
                    with Status("[bold blue]✒️  Calling The Chronicler agent to document changes...[/bold blue]") as status:
                        for filename, code in modified_files.items():
                            if not any(filename.endswith(ext) for ext in [".py", ".js", ".ts"]): continue
                            status.update(f"[bold blue]✒️  Documenting {filename}...[/bold blue]")
                            doc_result = self.api.generate_docstring(self.repo_id, code, issue_desc)
                            if doc_result and doc_result.get("docstring"):
                                documented_code = _insert_docstring_into_code(code, doc_result["docstring"])
                                documented_files[filename] = documented_code
                            else:
                                documented_files[filename] = code
                    modified_files.update(documented_files)
                    is_documented = True
                    console.print("[green]✓ Documentation complete.[/green]")
                    console.rule("[bold]📝 Review Updated Changes with Documentation[/bold]")
                    for filename, new_code in modified_files.items():
                         if original_contents.get(filename, "") != new_code:
                            self._display_diff(original_contents.get(filename, ""), new_code, filename)
                    continue

                if choice == 't':
                    if are_tests_generated:
                        console.print("[yellow]Tests have already been generated for this fix.[/yellow]")
                        continue
                    if not all_validations_passed:
                        console.print("[red]Cannot generate tests for code that has failed validation.[/red]")
                        continue

                    generated_test_files = {}
                    with Status("[bold yellow]🔬 Calling Test Generation Agent...[/bold yellow]") as status:
                        for filename, code in modified_files.items():
                            if any(filename.endswith(ext) for ext in [".py", ".js", ".ts"]) and 'test' not in filename:
                                status.update(f"[bold yellow]🔬 Generating tests for {filename}...[/bold yellow]")
                                test_result = self.api.generate_tests(self.repo_id, code, issue_desc)

                                if test_result and test_result.get("generated_tests"):
                                    test_file_path = f"tests/test_{Path(filename).stem}.py"
                                    generated_test_files[test_file_path] = test_result["generated_tests"]
                                    console.print(f"  [green]✓[/green] Generated test file: [cyan]{test_file_path}[/cyan]")
                                else:
                                    console.print(f"  [red]✗[/red] Failed to generate tests for [cyan]{filename}[/cyan].")

                    if generated_test_files:
                        modified_files.update(generated_test_files)
                        are_tests_generated = True
                        console.print("\n[green]✓ Test generation complete.[/green]")
                        console.rule("[bold]📝 Review New Test Files[/bold]")
                        for filename, new_code in generated_test_files.items():
                            self._display_diff("", new_code, filename)
                    else:
                        console.print("[yellow]No new tests were generated.[/yellow]")

                    continue

                if choice == 'c': break
                if choice == 'a' and all_validations_passed: break
                if choice == 'r': break

            if choice == 'c':
                console.print("[yellow]🛑 Operation cancelled.[/yellow]")
                break

            if choice == 'r':
                if iteration_count >= max_iterations:
                    console.print(f"[yellow]⚠️ Maximum iterations ({max_iterations}) reached.[/yellow]")
                    break
                try:
                    feedback = Prompt.ask("\n[bold]💭 Your feedback for improvement[/bold]")
                    if not feedback.strip():
                        console.print("[yellow]⚠️ Empty feedback, skipping refinement.[/yellow]")
                        continue
                    refinement_history.append({"feedback": feedback, "code_generated": modified_files})
                    modified_files.clear()
                    continue
                except KeyboardInterrupt:
                    break

            if choice == 'a' and all_validations_passed:
                with Progress(SpinnerColumn(),TextColumn("[progress.description]{task.description}"),transient=True) as progress:
                    task = progress.add_task("[cyan]🚀 Dispatching Ambassador for multi-file PR...", total=None)
                    pr_data = self.api.create_pr(f"{self.repo_url}/issues/{issue['number']}", modified_files)
                    progress.remove_task(task)
                if pr_data and pr_data.get("pull_request_url"):
                    console.print(Panel(f"✅ [bold green]Success![/bold green]\nPull request created: [link={pr_data['pull_request_url']}]{pr_data['pull_request_url']}[/link]", title="[green]🚀 Mission Complete[/green]", border_style="green"))
                else:
                    console.print("[red]❌ Failed to create pull request.[/red]")
                break

        self.last_rca_report = None
        self.last_rca_issue_num = None

    def handle_mage_session(self):
        """Handle The Mage interactive session for code transformation."""
        # Import mage service
        sys.path.append('backend')
        from backend.lumiere_core.services import mage_service
        
        console.print(Panel(
            f"[bold magenta]🔮 The Mage's Sanctum[/bold magenta]\n"
            f"[yellow]Repository:[/yellow] {self.repo_id}\n"
            f"[yellow]URL:[/yellow] {self.repo_url}\n\n"
            "[dim]Master of code transmutation. Cast spells to transform your code.[/dim]",
            border_style="magenta"
        ))
        
        # Show available spells
        spells = mage_service.list_available_spells()
        console.print("\n[bold magenta]📜 Available Spells:[/bold magenta]")
        
        spells_table = Table(show_header=True, header_style="bold cyan", border_style="magenta")
        spells_table.add_column("Spell", style="cyan")
        spells_table.add_column("Description", style="white")
        
        for spell_name, description in spells.items():
            spells_table.add_row(spell_name, description)
        
        console.print(spells_table)
        
        # Interactive spell casting loop
        mage_completer = WordCompleter(list(spells.keys()) + ['help', 'h', 'list', 'l', 'back', 'exit', 'quit'], ignore_case=True)
        mage_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=mage_completer,
            style=prompt_style
        )
        
        console.print("\n[dim]💡 Usage: cast <spell_name> <file_path> <target_identifier> [options][/dim]")
        console.print("[dim]Example: cast translate_contract src/models.py UserProfile --target=typescript[/dim]")
        
        while True:
            try:
                mage_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', f' (Mage/{self.repo_id})'),
                    ('class:separator', ' > '),
                ]
                
                command = mage_session.prompt(mage_prompt_text).strip()
                
                if not command:
                    continue
                if command.lower() in ("q", "quit", "exit", "back"):
                    break
                if command.lower() in ("h", "help"):
                    console.print(spells_table)
                    continue
                if command.lower() in ("l", "list"):
                    console.print(spells_table)
                    continue
                
                # Parse cast command
                if command.startswith("cast "):
                    self._handle_cast_command(command[5:].strip())
                else:
                    console.print("[yellow]💡 Start your command with 'cast' followed by spell name, file path, and target.[/yellow]")
                    console.print("[dim]Type 'help' to see available spells, or 'back' to return.[/dim]")
                    
            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'back' or 'exit' to return to analysis session.[/yellow]")
                continue
            except EOFError:
                break
        
        console.print("[magenta]🔮 The Mage session ended.[/magenta]")
    
    def _handle_cast_command(self, command_args: str):
        """Handle casting a specific spell."""
        try:
            # Import mage service
            from backend.lumiere_core.services import mage_service
            
            # Parse arguments
            parts = shlex.split(command_args)
            if len(parts) < 3:
                console.print("[red]❌ Usage: cast <spell_name> <file_path> <target_identifier> [--option=value][/red]")
                return
            
            spell_name = parts[0]
            file_path = parts[1]
            target_identifier = parts[2]
            
            # Parse additional options
            kwargs = {}
            for part in parts[3:]:
                if part.startswith("--"):
                    if "=" in part:
                        key, value = part[2:].split("=", 1)
                        kwargs[key] = value
                    else:
                        kwargs[part[2:]] = True
            
            console.print(f"\n[magenta]🔮 Casting {spell_name} on {target_identifier} in {file_path}...[/magenta]")
            
            # Cast the spell
            with Status("[magenta]✨ The Mage is weaving magic...[/magenta]"):
                result = mage_service.cast_transformation_spell(
                    self.repo_id, spell_name, file_path, target_identifier, **kwargs
                )
            
            if "error" in result:
                console.print(f"[red]❌ Spell failed: {result['error']}[/red]")
                return
            
            # Display the transformation
            console.print(Panel(
                f"[bold green]✨ Spell Cast Successfully![/bold green]\n"
                f"[yellow]Spell:[/yellow] {result['spell_name']}\n"
                f"[yellow]Target:[/yellow] {result['target_identifier']}\n"
                f"[yellow]File:[/yellow] {result['file_path']}\n"
                f"[yellow]Type:[/yellow] {result.get('transformation_type', 'unknown')}\n"
                f"[yellow]Description:[/yellow] {result.get('description', 'No description')}",
                title="[green]🔮 Magic Complete[/green]",
                border_style="green"
            ))
            
            # Show before and after
            console.print("\n[bold]📜 Original Code:[/bold]")
            console.print(Panel(result['original_code'], border_style="red", title="[red]Before[/red]"))
            
            console.print("\n[bold]✨ Transformed Code:[/bold]")
            console.print(Panel(result['transformed_code'], border_style="green", title="[green]After[/green]"))
            
            # Ask if user wants to apply the transformation
            try:
                if Confirm.ask("\n[bold]Apply this transformation to the file?[/bold]", default=False):
                    with Status("[magenta]📝 Applying transformation...[/magenta]"):
                        apply_result = mage_service.apply_code_transformation(self.repo_id, result)
                    
                    if apply_result.get("success"):
                        console.print(Panel(
                            f"[bold green]✅ Transformation Applied![/bold green]\n"
                            f"{apply_result.get('message', 'Code has been updated successfully.')}",
                            border_style="green"
                        ))
                    else:
                        console.print(f"[red]❌ Failed to apply transformation: {apply_result.get('error', 'Unknown error')}[/red]")
                else:
                    console.print("[yellow]Transformation not applied. The spell result is shown above for your reference.[/yellow]")
            except KeyboardInterrupt:
                console.print("\n[yellow]Transformation not applied.[/yellow]")
                
        except Exception as e:
            console.print(f"[red]❌ Error casting spell: {str(e)}[/red]")


# --- Utility Function for Help Display ---
def display_interactive_help(context: str = 'main'):
    """Display help instructions based on the current CLI context."""
    title = f"🆘 Lumière Help — {context.capitalize()} Context"
    help_table = Table(title=f"[bold magenta]{title}[/bold magenta]", border_style="magenta")
    help_table.add_column("Command", style="bold cyan")
    help_table.add_column("Description", style="white")

    if context == 'main':
        help_table.add_row("analyze / a", "Ingest or re-ingest a repo for analysis")
        help_table.add_row("ask / oracle", "Ask architectural questions about a repo")
        help_table.add_row("dance / da", "🕺 The Masked Dancer - visualize execution flow")
        help_table.add_row("summon / su", "🧙 The Summoner - generate code patterns")
        help_table.add_row("review", "Perform an AI-powered review of a Pull Request")
        help_table.add_row("dashboard / d", "View the project health dashboard")
        help_table.add_row("profile / p", "Get GitHub user profile analysis")
        help_table.add_row("bom / b", "Bill of Materials analysis for a repository")
        help_table.add_row("onboard / o", "Onboarding Concierge - help new developers")
        help_table.add_row("repo-mgmt / rm", "Repository management (list, delete, status)")

        # --- DYNAMIC HELP TEXT ---
        if cli_state.get("model"):
            help_table.add_row("config / c", "Change LLM model or view settings")
        else:
            help_table.add_row("config / c", "Choose LLM provider & model")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("exit / quit", "Exit the application")
    elif context == 'analyze':
        help_table.add_row("list / l", "Show prioritized issues")
        help_table.add_row("graph / g", "Display the repository's architectural graph")
        help_table.add_row("briefing / b", "Show issue briefing")
        help_table.add_row("details / d", "Show issue metadata")
        help_table.add_row("rca / r", "Root cause analysis")
        help_table.add_row("fix / f", "Launch fix dialogue")
        help_table.add_row("mage / m", "🔮 The Mage - intelligent code transformation")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")
    elif context == 'onboard':
        help_table.add_row("scout / s", "Find 'good first issues' with onboarding scores")
        help_table.add_row("expert <file_path>", "Find experts for a specific file")
        help_table.add_row("guide <issue_number>", "Generate personalized onboarding guide")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")

    console.print(help_table)

# --- Main Entry Point ---
app = typer.Typer()

@app.command()
def run():
    """Launch Lumière interactive shell."""
    api_client = LumiereAPIClient()

    health_status = api_client._request("GET", "health/")
    if health_status is None:
        console.print("[bold red]Lumière CLI cannot start without a backend connection.[/bold red]")
        sys.exit(1)

    load_config()

    welcome_text = (f"[bold cyan]✨ Welcome to Lumière Sémantique ✨[/bold cyan]\n"
                    f"The Conversational Mission Controller is active.\n\n"
                    f"[dim]Backend Status: [green]Online[/green] at [underline]{API_BASE_URL}[/underline][/dim]")
    console.print(Panel(welcome_text, border_style="cyan"))
    display_interactive_help('main')

    next_command = None
    context = {}

    while True:
        try:
            if next_command is None:
                prompt_text = get_prompt_text()
                command = prompt_session.prompt(prompt_text).strip()
            else:
                command = next_command
                console.print(f"\n[dim]Executing suggested action: [bold]{command}[/bold]...[/dim]")

            next_command = None

            if not command:
                continue

            if command.lower() in ("exit", "quit", "x"):
                console.print("[dim]👋 Goodbye![/dim]")
                break

            elif command.lower() == "back":
                console.print()
                continue

            elif command.lower() in ("help", "h"):
                display_interactive_help('main')
                continue

            elif command.lower() in ("config", "c"):
                console.print(Panel(
                    f"[bold]Current Settings[/bold]\n"
                    f"  [cyan]LLM Model:[/cyan] [yellow]{cli_state.get('model', 'Not set')}[/yellow]\n"
                    f"  [cyan]Last Repo:[/cyan] {cli_state.get('last_repo_url', 'Not set')}\n"
                    f"  [cyan]Debug Mode:[/cyan] {'On' if cli_state.get('debug_mode') else 'Off'}",
                    title="⚙️ Configuration", border_style="magenta"
                ))
                handle_model_selection(api_client)
                continue

            elif command.lower() in ("profile", "p"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                username = Prompt.ask("Enter GitHub username")
                if not username.strip(): continue
                with Status("[cyan]Generating profile analysis...[/cyan]"):
                    profile = api_client.get_profile(username)
                if profile and profile.get("profile_summary"):
                    console.print(Panel(Markdown(profile["profile_summary"]), title=f"👤 Profile Analysis for {username}"))
                else: console.print("[red]❌ Could not retrieve profile.[/red]")
                continue

            elif command.lower() in ("ask", "oracle"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_url = context.get("repo_url") or context.get("pr_url", "").split("/pull/")[0]
                if not repo_url:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to ask questions about.", title="[magenta]🔮 The Oracle[/magenta]", border_style="magenta"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_url = analyzed_repos[int(choice) - 1]['url']
                    except (ValueError, IndexError, KeyboardInterrupt): continue

                oracle_session = OracleSession(repo_url)
                oracle_session.loop()
                context = {}
                continue

            elif command.lower() in ("dance", "da"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                
                # Import dance service
                sys.path.append('backend')
                from backend.lumiere_core.services import dance_service
                
                # Get repository to analyze
                repo_url = context.get("repo_url")
                if not repo_url:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to dance with.", title="[magenta]💃 The Masked Dancer[/magenta]", border_style="magenta"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): 
                        table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_url = analyzed_repos[int(choice) - 1]['url']
                    except (ValueError, IndexError, KeyboardInterrupt): 
                        continue
                
                repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")
                
                # Get user query for what to trace
                console.print("\n[bold magenta]💃 The Masked Dancer[/bold magenta] reveals the secret choreography of your application.")
                console.print("[dim]Describe what execution flow you'd like to trace (e.g., 'user login API call', 'data processing pipeline').[/dim]")
                
                try:
                    query = Prompt.ask("\n[bold]What dance would you like to see?[/bold]").strip()
                    if not query:
                        console.print("[yellow]No query provided. Dance cancelled.[/yellow]")
                        continue
                except KeyboardInterrupt:
                    continue
                
                # Ask for output format
                try:
                    format_choice = Prompt.ask(
                        "\n[bold]Choose visualization format[/bold]",
                        choices=["cli", "svg"],
                        default="cli"
                    )
                except KeyboardInterrupt:
                    continue
                
                with Status("[magenta]🔍 The Oracle is finding the best starting point...[/magenta]"):
                    entry_point_result = dance_service.find_entry_point(repo_id, query)
                
                if "error" in entry_point_result:
                    console.print(f"[red]❌ Could not find starting point: {entry_point_result['error']}[/red]")
                    continue
                
                suggested_node = entry_point_result.get("suggested_node")
                confidence = entry_point_result.get("confidence", 0)
                
                console.print(Panel(
                    f"[bold green]🎯 Starting Point Found[/bold green]\n"
                    f"[yellow]Function/Method:[/yellow] {suggested_node}\n"
                    f"[yellow]File:[/yellow] {entry_point_result.get('file_path', 'Unknown')}\n"
                    f"[yellow]Confidence:[/yellow] {confidence:.2f}\n\n"
                    f"[dim]Context: {entry_point_result.get('context', '')[:100]}...[/dim]",
                    border_style="green"
                ))
                
                try:
                    if not Confirm.ask(f"[bold]Begin dance with '{suggested_node}'?[/bold]", default=True):
                        # Show alternatives
                        alternatives = entry_point_result.get("alternatives", [])
                        if alternatives:
                            console.print("\n[bold]Alternative starting points:[/bold]")
                            for i, alt in enumerate(alternatives[:5], 1):
                                console.print(f"  {i}. {alt['node_id']} (confidence: {alt['confidence']:.2f})")
                            
                            try:
                                alt_choice = Prompt.ask("Choose alternative (1-5) or press Enter to cancel", default="")
                                if alt_choice and alt_choice.isdigit() and 1 <= int(alt_choice) <= len(alternatives):
                                    suggested_node = alternatives[int(alt_choice) - 1]['node_id']
                                else:
                                    console.print("[yellow]Dance cancelled.[/yellow]")
                                    continue
                            except KeyboardInterrupt:
                                continue
                        else:
                            console.print("[yellow]Dance cancelled.[/yellow]")
                            continue
                except KeyboardInterrupt:
                    continue
                
                # Perform the dance!
                with Status("[magenta]💃 The Masked Dancer is tracing the execution flow...[/magenta]"):
                    if format_choice == "svg":
                        output_file = f"dance_of_{suggested_node.replace('.', '_')}.svg"
                        dance_result = dance_service.visualize_dance(repo_id, suggested_node, "svg", output_file)
                        console.print(Panel(
                            f"[bold green]✨ Dance visualization complete![/bold green]\n"
                            f"[yellow]SVG file saved:[/yellow] {output_file}\n\n"
                            "[dim]Open the SVG file in a browser to see the animated execution flow![/dim]",
                            title="[green]🎭 Dance Complete[/green]",
                            border_style="green"
                        ))
                    else:
                        dance_result = dance_service.visualize_dance(repo_id, suggested_node, "cli")
                        console.print("\n" + dance_result)
                
                context = {"repo_url": repo_url, "repo_id": repo_id}
                continue

            elif command.lower() in ("summon", "su"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                
                # Import summoner service
                sys.path.append('backend')
                from backend.lumiere_core.services import summoner_service
                
                # Get repository to analyze
                repo_url = context.get("repo_url")
                if not repo_url:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to summon patterns for.", title="[cyan]🧙 The Summoner[/cyan]", border_style="cyan"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): 
                        table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_url = analyzed_repos[int(choice) - 1]['url']
                    except (ValueError, IndexError, KeyboardInterrupt): 
                        continue
                
                repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")
                
                console.print("\n[bold cyan]🧙 The Summoner[/bold cyan] materializes code patterns from the architectural DNA.")
                
                # Show available recipes
                recipes = summoner_service.list_summoning_recipes()
                console.print("\n[bold cyan]📜 Available Summoning Recipes:[/bold cyan]")
                
                recipes_table = Table(show_header=True, header_style="bold cyan", border_style="cyan")
                recipes_table.add_column("Recipe", style="cyan")
                recipes_table.add_column("Description", style="white")
                
                for recipe_name, recipe_info in recipes.items():
                    recipes_table.add_row(recipe_name, recipe_info["description"])
                
                console.print(recipes_table)
                
                # Get recipe choice
                try:
                    recipe_choice = Prompt.ask(
                        "\n[bold]Choose a recipe to summon[/bold]",
                        choices=list(recipes.keys()),
                        show_choices=False
                    )
                except KeyboardInterrupt:
                    continue
                
                recipe_info = recipes[recipe_choice]
                
                # Get parameters for the recipe
                console.print(f"\n[bold]Recipe: {recipe_info['name']}[/bold]")
                console.print(f"[dim]{recipe_info['description']}[/dim]")
                
                parameters = {}
                for param in recipe_info.get("parameters", []):
                    try:
                        if param == "path":
                            value = Prompt.ask(f"Enter API path (e.g., /users)", default="/items")
                        elif param == "methods":
                            value = Prompt.ask(f"Enter HTTP methods (comma-separated)", default="get,post")
                        elif param == "model_name":
                            value = Prompt.ask(f"Enter model/entity name", default="Item")
                        elif param == "component_name":
                            value = Prompt.ask(f"Enter component name", default="MyComponent")
                        else:
                            value = Prompt.ask(f"Enter {param}")
                        parameters[param] = value
                    except KeyboardInterrupt:
                        console.print("\n[yellow]Summoning cancelled.[/yellow]")
                        break
                else:
                    # All parameters collected, perform summoning
                    with Status("[cyan]🔍 Resonating with the architecture...[/cyan]"):
                        summoning_result = summoner_service.summon_code_pattern(repo_id, recipe_choice, **parameters)
                    
                    if "error" in summoning_result:
                        console.print(f"[red]❌ Summoning failed: {summoning_result['error']}[/red]")
                        continue
                    
                    # Display the summoning plan
                    console.print(Panel(
                        f"[bold green]🧙 Summoning Plan Ready[/bold green]\n"
                        f"[yellow]Recipe:[/yellow] {summoning_result['recipe_name']}\n"
                        f"[yellow]Description:[/yellow] {summoning_result['recipe_description']}\n\n"
                        f"[bold]📜 The Blueprint:[/bold]",
                        border_style="green"
                    ))
                    
                    # Show surgical plan
                    surgical_plan = summoning_result.get("surgical_plan", {})
                    operations = surgical_plan.get("operations", [])
                    
                    plan_table = Table(show_header=True, header_style="bold yellow", border_style="yellow")
                    plan_table.add_column("Operation", style="cyan")
                    plan_table.add_column("File", style="white")
                    plan_table.add_column("Description", style="dim white")
                    
                    for op in operations:
                        op_type = op["type"]
                        if op_type == "CREATE_FILE":
                            op_display = "CREATE"
                        elif op_type == "MODIFY_FILE":
                            op_display = "MODIFY"
                        else:
                            op_display = "INSERT"
                        
                        plan_table.add_row(
                            op_display,
                            op["file_path"],
                            op.get("description", "")
                        )
                    
                    console.print(plan_table)
                    
                    console.print(f"\n[bold]Summary:[/bold] {surgical_plan.get('summary', 'Unknown')}")
                    console.print(f"[yellow]Files to create:[/yellow] {surgical_plan.get('files_created', 0)}")
                    console.print(f"[yellow]Files to modify:[/yellow] {surgical_plan.get('files_modified', 0)}")
                    
                    # Ask for confirmation
                    try:
                        if Confirm.ask("\n[bold]Proceed with the summoning?[/bold]", default=False):
                            with Status("[cyan]🧙 The Summoner is weaving the ritual...[/cyan]"):
                                execution_result = summoner_service.execute_summoning_ritual(repo_id, summoning_result)
                            
                            if execution_result.get("success"):
                                console.print(Panel(
                                    f"[bold green]✨ Summoning Complete![/bold green]\n"
                                    f"[yellow]Operations completed:[/yellow] {execution_result.get('operations_completed', 0)}/{execution_result.get('total_operations', 0)}\n"
                                    f"[yellow]Files created:[/yellow] {len(execution_result.get('created_files', []))}\n"
                                    f"[yellow]Files modified:[/yellow] {len(execution_result.get('modified_files', []))}\n\n"
                                    f"[bold]Created files:[/bold]\n" + "\n".join(f"  • {f}" for f in execution_result.get('created_files', [])) + "\n\n" +
                                    f"[bold]Modified files:[/bold]\n" + "\n".join(f"  • {f}" for f in execution_result.get('modified_files', [])),
                                    title="[green]🧙 Summoning Successful[/green]",
                                    border_style="green"
                                ))
                            else:
                                console.print(Panel(
                                    f"[bold red]❌ Summoning Failed[/bold red]\n"
                                    f"Operations completed: {execution_result.get('operations_completed', 0)}/{execution_result.get('total_operations', 0)}\n"
                                    f"Some operations may have succeeded. Check the file system for partial results.",
                                    border_style="red"
                                ))
                        else:
                            console.print("[yellow]Summoning cancelled. The plan remains available for future use.[/yellow]")
                    except KeyboardInterrupt:
                        console.print("\n[yellow]Summoning cancelled.[/yellow]")
                
                context = {"repo_url": repo_url, "repo_id": repo_id}
                continue

            elif command.lower() in ("review",):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                pr_url = Prompt.ask("Enter the full GitHub Pull Request URL to review").strip()
                if not pr_url or "github.com" not in pr_url or "/pull/" not in pr_url:
                    console.print("[red]❌ Invalid Pull Request URL.[/red]")
                    continue

                result_data = None
                with Status("[cyan]The Inquisitor is reviewing the PR...[/cyan]", spinner="earth"):
                    result_data = api_client.adjudicate_pr(pr_url)

                if result_data and result_data.get("review"):
                    console.print(Panel(Markdown(result_data["review"]), title=f"⚖️ Inquisitor's Review", border_style="blue"))
                    context = {"pr_url": pr_url, "repo_url": pr_url.split("/pull/")[0], **result_data}
                    next_command, context = _present_next_actions(api_client, "review", context)
                elif result_data and result_data.get("error"):
                    console.print(Panel(f"[bold red]Review Failed:[/bold red]\n{result_data['error']}", title="[red]Inquisitor Error[/red]"))
                else: console.print("[red]❌ The Inquisitor did not provide a review.[/red]")
                continue

            elif command.lower() == "harmonize":
                 pr_url = context.get("pr_url")
                 review_text = context.get("review")
                 if not pr_url or not review_text:
                     console.print("[red]Harmonize command requires context from a review. Please run 'review' first.[/red]")
                     continue

                 with Status("[cyan]The Harmonizer is composing a fix...[/cyan]"):
                     fix_data = api_client.harmonize_fix(pr_url, review_text)

                 if fix_data and "modified_files" in fix_data:
                     console.rule("[bold]📝 Review Harmonizer's Proposed Changes[/bold]")
                     dummy_session = AnalysisSession(pr_url.split('/pull/')[0])
                     for filename, new_code in fix_data["modified_files"].items():
                         original_code = fix_data.get("original_contents", {}).get(filename, "")
                         if original_code != new_code:
                             dummy_session._display_diff(original_code, new_code, filename)
                     console.print(Panel("✅ [bold green]Harmonizer's patch generated.[/bold green]", border_style="green"))
                 else:
                     console.print(Panel(f"[red]Harmonizer failed to generate a fix: {fix_data.get('error', 'Unknown error')}[/red]", border_style="red"))

                 context = {}
                 continue

            elif command.lower() in ("dashboard", "d"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_id = context.get("repo_id")
                if not repo_id:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to view its health dashboard.", title="[cyan]🔭 The Sentinel[/cyan]", border_style="cyan"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_id = analyzed_repos[int(choice) - 1]['repo_id']
                    except (ValueError, IndexError, KeyboardInterrupt): continue

                with Status("[cyan]The Sentinel is gathering intelligence...[/cyan]"):
                    response = api_client.get_sentinel_briefing(repo_id)

                if response and response.get("briefing"):
                    console.print(Panel(Markdown(response["briefing"]), title=f"[cyan]🔭 Sentinel Health Briefing for {repo_id}[/cyan]", border_style="cyan"))
                    
                    # Offer to show historical metrics
                    try:
                        if Confirm.ask("[dim]Would you like to view historical metrics data?[/dim]", default=False):
                            with Status("[cyan]📊 Retrieving metrics history...[/cyan]"):
                                metrics_history = api_client.get_sentinel_metrics_history(repo_id)
                            
                            if metrics_history and len(metrics_history) > 1:
                                # Show trends over time
                                latest = metrics_history[-1]
                                oldest = metrics_history[0] 
                                
                                trends_content = f"""📊 **Metrics History Analysis**
**Data Points:** {len(metrics_history)} snapshots
**Time Span:** {oldest.get('timestamp', 'Unknown')} → {latest.get('timestamp', 'Now')}

**Key Trends:**"""
                                
                                # Calculate trends for numeric fields
                                for key in latest.keys():
                                    if isinstance(latest.get(key), (int, float)) and key in oldest:
                                        old_val = oldest[key]
                                        new_val = latest[key] 
                                        if old_val != 0:
                                            change = ((new_val - old_val) / abs(old_val)) * 100
                                            trend_emoji = "📈" if change > 0 else "📉" if change < 0 else "➡️"
                                            trends_content += f"\n• {key.replace('_', ' ').title()}: {old_val} → {new_val} {trend_emoji} ({change:+.1f}%)"
                                
                                console.print(Panel(trends_content, title="[bold yellow]📊 Historical Metrics[/bold yellow]", border_style="yellow"))
                            elif metrics_history:
                                console.print("[yellow]📊 Only one metrics snapshot available. Historical analysis requires multiple data points.[/yellow]")
                            else:
                                console.print("[red]❌ Could not retrieve metrics history.[/red]")
                    except KeyboardInterrupt:
                        pass
                    
                    context = {"repo_id": repo_id, "repo_url": f"https://github.com/{repo_id.replace('_', '/')}", **response}
                    next_command, context = _present_next_actions(api_client, "dashboard", context)
                elif response and response.get("error"):
                    console.print(Panel(response['error'], title="[red]Sentinel Error[/red]"))
                else: console.print("[red]❌ The Sentinel did not provide a briefing.[/red]")
                continue

            elif command.lower() in ("analyze", "a"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_url_to_analyze = context.get("repo_url")
                if not repo_url_to_analyze:
                    analyzed_repos = find_analyzed_repos()
                    if analyzed_repos:
                        console.print(Panel("Select a repository to analyze or ingest a new one.", title="[cyan]Select Repository for Analysis[/cyan]", border_style="cyan"))
                        table = Table(show_header=False, box=None, padding=(0, 2))
                        for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                        table.add_row("([bold yellow]N[/bold yellow])", "Analyze a new repository")
                        console.print(table)
                        choices = [str(i) for i in range(1, len(analyzed_repos) + 1)] + ['n', 'N']
                        try:
                            choice = Prompt.ask("Enter choice", choices=choices, show_choices=False, default='1').lower()
                            if choice == 'n': repo_url_to_analyze = Prompt.ask("Enter GitHub repository URL").strip()
                            else: repo_url_to_analyze = analyzed_repos[int(choice) - 1]['url']
                        except(ValueError, IndexError, KeyboardInterrupt): continue
                    else:
                        repo_url_to_analyze = Prompt.ask("Enter GitHub repository URL").strip()

                if not repo_url_to_analyze: continue

                try:
                    session = AnalysisSession(repo_url_to_analyze)
                    if session.start():
                        session.loop()
                except ValueError as e: console.print(f"[red]{e}[/red]")
                continue

            elif command.lower() in ("bom", "b"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                analyzed_repos = find_analyzed_repos()
                if not analyzed_repos:
                    console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                    continue
                
                console.print(Panel("Select a repository for BOM analysis.", title="[blue]📦 Bill of Materials Analysis[/blue]", border_style="blue"))
                table = Table(show_header=False, box=None, padding=(0, 2))
                for i, repo in enumerate(analyzed_repos, 1): 
                    table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                console.print(table)
                
                try:
                    choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                    selected_repo = analyzed_repos[int(choice) - 1]
                    
                    bom_session = BOMSession(selected_repo['repo_id'], selected_repo['url'])
                    bom_session.loop()
                    context = {}
                except (ValueError, IndexError, KeyboardInterrupt): 
                    continue

            elif command.lower() in ("onboard", "o"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                analyzed_repos = find_analyzed_repos()
                if not analyzed_repos:
                    console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                    continue
                
                console.print(Panel("Select a repository for onboarding assistance.", title="[green]🎓 Onboarding Concierge[/green]", border_style="green"))
                table = Table(show_header=False, box=None, padding=(0, 2))
                for i, repo in enumerate(analyzed_repos, 1): 
                    table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                console.print(table)
                
                try:
                    choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                    selected_repo = analyzed_repos[int(choice) - 1]
                    
                    onboard_session = OnboardingSession(selected_repo['repo_id'], selected_repo['url'])
                    onboard_session.loop()
                    context = {}
                except (ValueError, IndexError, KeyboardInterrupt): 
                    continue

            elif command.lower() in ("repo-mgmt", "rm"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_mgmt_session = RepositoryManagementSession()
                repo_mgmt_session.loop()
                context = {}
                continue

            else:
                console.print("[red]❌ Unknown command. Type 'help' for options.[/red]")

        except KeyboardInterrupt:
            console.print("\n[dim]💤 Interrupted. Type 'exit' to quit.[/dim]")
            next_command = None
            continue
        except EOFError:
            break

if __name__ == "__main__":
    app()

--- FILE_END: lumiere.py ---

--- FILE_START: start_server.sh ---
#!/bin/bash
echo "Starting Lumière Sémantique development server on http://127.0.0.1:8002/"
python manage.py runserver 8002

--- FILE_END: start_server.sh ---

--- FILE_START: .claude/settings.local.json ---
{
  "permissions": {
    "allow": [
      "Bash(python:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(grep:*)",
      "Bash(rg:*)"
    ],
    "deny": []
  }
}
--- FILE_END: .claude/settings.local.json ---

--- FILE_START: backend/ingestion/migrations/__init__.py ---

--- FILE_END: backend/ingestion/migrations/__init__.py ---

--- FILE_START: backend/ingestion/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/ingestion/models.py ---

--- FILE_START: backend/ingestion/management/__init__.py ---

--- FILE_END: backend/ingestion/management/__init__.py ---

--- FILE_START: backend/ingestion/management/commands/__init__.py ---

--- FILE_END: backend/ingestion/management/commands/__init__.py ---

--- FILE_START: backend/ingestion/management/commands/generate_briefing.py ---
# In ingestion/management/commands/generate_briefing.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index
from backend.lumiere_core.services.ollama_service import generate_text

class Command(BaseCommand):
    help = 'Generates a "Pre-flight Briefing" for a given query using a RAG pipeline.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The user query or GitHub issue description.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')
        # --- CHANGE 1: Add an argument for the generation model ---
        parser.add_argument('--generation_model', type=str, default='qwen3:4b', help='The Ollama model to use for text generation.')
        parser.add_argument('--k', type=int, default=7, help='Number of context chunks to retrieve.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        embedding_model = options['embedding_model']
        generation_model = options['generation_model'] # <-- Get the new option
        k = options['k']

        self.stdout.write(self.style.NOTICE(f"Step 1: Retrieving context for query: '{query}'..."))

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        try:
            context_chunks = search_index(
                query_text=query,
                model_name=embedding_model, # Use the embedding model here
                index_path=index_path,
                map_path=map_path,
                k=k
            )
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Failed to retrieve context: {e}"))
            return

        self.stdout.write(self.style.SUCCESS(f"✓ Retrieved {len(context_chunks)} context chunks."))

        context_string = ""
        for i, chunk in enumerate(context_chunks):
            context_string += f"--- Context Chunk {i+1} from file '{chunk['file_path']}' ---\n"
            context_string += chunk['text']
            context_string += "\n\n"

        prompt = f"""
        You are Lumière Sémantique, an expert AI programming assistant acting as a Principal Engineer.
        Your mission is to provide a "Pre-flight Briefing" for a developer about to work on a task.
        Analyze the user's query and the provided context from the codebase to generate your report.

        The report must be clear, concise, and structured in Markdown. It must include the following sections:
        1.  **Task Summary:** Briefly rephrase the user's request.
        2.  **Core Analysis:** Based on the provided context, explain how the system currently works in relation to the query. Synthesize information from the different context chunks.
        3.  **Key Files & Code:** Point out the most important files or functions from the context that the developer should focus on.
        4.  **Suggested Approach or Potential Challenges:** Offer a high-level plan or mention any potential issues you foresee.

        --- PROVIDED CONTEXT FROM THE CODEBASE ---
        {context_string}
        --- END OF CONTEXT ---

        USER'S QUERY: "{query}"

        Now, generate the Pre-flight Briefing.
        """

        self.stdout.write(self.style.NOTICE(f"\nStep 2: Sending context and query to the LLM ('{generation_model}') for generation..."))

        # --- CHANGE 2: Pass the generation model name to the function ---
        final_report = generate_text(prompt, model_name=generation_model)

        self.stdout.write(self.style.SUCCESS("\n--- LUMIÈRE SÉMANTIQUE: PRE-FLIGHT BRIEFING ---"))
        self.stdout.write(final_report)

--- FILE_END: backend/ingestion/management/commands/generate_briefing.py ---

--- FILE_START: backend/ingestion/management/commands/run_indexer.py ---
# In ingestion/management/commands/run_indexer.py

from django.core.management.base import BaseCommand
from ingestion.indexing import EmbeddingIndexer
import os

class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and creates a Faiss index from its text chunks using Ollama.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')
        parser.add_argument(
            '--model',
            type=str,
            default='snowflake-arctic-embed2:latest', # <-- Defaults to your preferred model
            help='The name of the Ollama embedding model to use.'
        )

    def handle(self, *args, **options):
        cortex_file_path = options['cortex_file']
        model_name = options['model']

        if not os.path.exists(cortex_file_path):
            self.stdout.write(self.style.ERROR(f"Error: File not found at '{cortex_file_path}'"))
            return

        self.stdout.write(self.style.NOTICE(f"Starting Ollama indexing for {cortex_file_path} using model '{model_name}'..."))

        try:
            # Pass the model name to the indexer
            indexer = EmbeddingIndexer(model_name=model_name)
            indexer.process_cortex(cortex_file_path)
            self.stdout.write(self.style.SUCCESS('✓ Ollama indexing process completed successfully.'))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'An unexpected error occurred during indexing: {e}'))

--- FILE_END: backend/ingestion/management/commands/run_indexer.py ---

--- FILE_START: backend/ingestion/management/commands/search.py ---
# In ingestion/management/commands/search.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index # <-- Import our new function

class Command(BaseCommand):
    help = 'Searches a Faiss index for a given query string.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The search query string.')
        parser.add_argument('--model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use.')
        parser.add_argument('--k', type=int, default=5, help='The number of results to return.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        model = options['model']
        k = options['k']

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        self.stdout.write(self.style.NOTICE(f"Searching for '{query}'..."))

        try:
            results = search_index(
                query_text=query,
                model_name=model,
                index_path=index_path,
                map_path=map_path,
                k=k
            )

            self.stdout.write(self.style.SUCCESS(f"\n--- Top {len(results)} search results ---"))
            for i, res in enumerate(results):
                self.stdout.write(self.style.HTTP_INFO(f"\n{i+1}. File: {res['file_path']} (Distance: {res['distance']:.4f})"))
                self.stdout.write(f"Chunk ID: {res['chunk_id']}")
                self.stdout.write("---")
                # Print the first few lines of the text chunk
                content_preview = "\n".join(res['text'].splitlines()[:5])
                self.stdout.write(content_preview)
                self.stdout.write("...")

        except FileNotFoundError:
            self.stdout.write(self.style.ERROR(f"Could not find index files for '{repo_id}'. Please run the indexer first."))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred: {e}"))

--- FILE_END: backend/ingestion/management/commands/search.py ---

--- FILE_START: backend/ingestion/management/commands/inspect_graph.py ---
# backend/ingestion/management/commands/inspect_graph.py

import json
from pathlib import Path
from collections import defaultdict
from django.core.management.base import BaseCommand
from rich.console import Console
from rich.tree import Tree
from rich.panel import Panel


class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and displays its architectural graph in a human-readable format.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')

    def handle(self, *args, **options):
        console = Console()
        cortex_file_path = Path(options['cortex_file'])

        if not cortex_file_path.exists():
            console.print(f"[bold red]Error: File not found at '{cortex_file_path}'[/bold red]")
            return

        data = self._load_json(console, cortex_file_path)
        if data is None:
            return

        graph_data = data.get('architectural_graph')
        if not graph_data:
            self._print_graph_not_found(console)
            return

        self._display_graph(console, data['repo_id'], graph_data)

    def _load_json(self, console: Console, path: Path):
        try:
            console.print(f"🔎 Reading Cortex file: [cyan]{path}[/cyan]")
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            console.print(f"[bold red]Error: Invalid JSON in file '{path}'[/bold red]")
            return None

    def _print_graph_not_found(self, console: Console):
        console.print(
            Panel(
                "This Project Cortex file was generated before 'The Cartographer' was implemented or the project contains no Python files.\nNo architectural graph is available to display.",
                title="[yellow]Architectural Graph Not Found[/yellow]",
                border_style="yellow",
                expand=False
            )
        )

    def _display_graph(self, console: Console, repo_id: str, graph_data: dict):
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])
        edges_by_source = defaultdict(list)
        for edge in edges:
            edges_by_source[edge['source']].append(edge)

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]")
        file_tree_nodes = {}

        # First pass: Build file, class, and function structure
        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                file_branch = tree.add(f"📄 [bold green]{node_id}[/bold green]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")
                    for method_name in sorted(nodes.get(class_id, {}).get('methods', [])):
                        class_branch.add(f"  - 🐍 [dim]def[/dim] {method_name}()")

                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"🐍 [dim]def[/dim] {func_name}()")

        # Second pass: Add import/call edges
        for source_id, edge_list in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]
                for edge in edge_list:
                    target = edge.get('target', 'Unknown')
                    edge_type = edge.get('type', 'RELATES_TO')
                    if edge_type == 'IMPORTS':
                        parent_branch.add(f"📥 [dim]imports[/dim] [yellow]{target}[/yellow]")
                    elif edge_type == 'CALLS':
                        parent_branch.add(f"📞 [dim]calls[/dim] [magenta]{target}[/magenta]")

        console.print(tree)
        console.print("\n[bold magenta]------------------------------------[/bold magenta]")

--- FILE_END: backend/ingestion/management/commands/inspect_graph.py ---

--- FILE_START: backend/ingestion/management/commands/ingest_repo.py ---
# backend/ingestion/management/commands/ingest_repo.py

from django.core.management.base import BaseCommand
from rich.console import Console
from rich.panel import Panel
import traceback

from lumiere_core.services import ingestion_service

class Command(BaseCommand):
    help = 'Runs the full ingestion pipeline (clone, embed, index) for a single repository URL.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The full URL of the GitHub repository to ingest.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')

    def handle(self, *args, **options):
        console = Console()
        repo_url = options['repo_url']
        embedding_model = options['embedding_model']

        console.print(
            Panel(
                f"[bold]Starting full ingestion for:[/] [cyan]{repo_url}[/cyan]\n"
                f"[bold]Using embedding model:[/] [yellow]{embedding_model}[/yellow]",
                title="🚀 Lumière Ingestion Service",
                border_style="blue"
            )
        )

        try:
            result = ingestion_service.clone_and_embed_repository(
                repo_url=repo_url,
                embedding_model=embedding_model
            )

            if result.get("status") == "success":
                console.print(
                    Panel(
                        f"[bold green]✓ Success![/bold green]\n{result.get('message', 'Ingestion complete.')}",
                        title="✅ Mission Complete",
                        border_style="green"
                    )
                )
                repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

                # --- THE FIX: Update the output path to match the new structure ---
                cortex_file_path = f"backend/cloned_repositories/{repo_id}/{repo_id}_cortex.json"
                console.print(f"\n[dim]To inspect the graph, run:[/dim]\n[bold cyan]python backend/manage.py inspect_graph {cortex_file_path}[/bold cyan]")

            else:
                error_details = result.get('details', result.get('error', 'An unknown error occurred.'))
                console.print(
                    Panel(
                        f"[bold red]✗ Ingestion Failed[/bold red]\n\n[yellow]Reason:[/yellow] {error_details}",
                        title="🚨 Error",
                        border_style="red"
                    )
                )

        except Exception as e:
            console.print(
                Panel(
                    f"[bold red]An unexpected critical error occurred:[/bold red]\n\n{traceback.format_exc()}",
                    title="💥 Critical Failure",
                    border_style="red"
                )
            )

--- FILE_END: backend/ingestion/management/commands/ingest_repo.py ---

--- FILE_START: backend/ingestion/management/commands/run_crawler.py ---
# In backend/ingestion/management/commands/run_crawler.py

import json
import traceback
from django.core.management.base import BaseCommand
from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier

class Command(BaseCommand):
    help = 'Clones a Git repository, creates the Project Cortex JSON, and saves it.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The URL of the Git repository to clone.')

    def handle(self, *args, **options):
        repo_url = options['repo_url']
        # Generate the repo_id just like the API does.
        repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

        self.stdout.write(self.style.NOTICE(f'Starting process for {repo_id} ({repo_url})...'))

        try:
            # --- FIX: Use the IntelligentCrawler as a context manager ---
            # The `with` statement correctly handles the setup (cloning) and
            # teardown (cleanup) of the temporary repository directory.
            with IntelligentCrawler(repo_url=repo_url) as crawler:
                # The cloning is now handled automatically when the 'with' block is entered.
                # We simply need to get the list of files to process.
                files_to_process = crawler.get_file_paths()

                if files_to_process:
                    self.stdout.write(self.style.SUCCESS(f'\nFound {len(files_to_process)} files. Starting JSON-ification...'))

                    # We now correctly pass the crawler's repo_path attribute.
                    jsonifier = Jsonifier(
                        file_paths=files_to_process,
                        repo_root=crawler.repo_path,
                        repo_id=repo_id
                    )
                    project_cortex = jsonifier.generate_cortex()

                    output_filename = f"{repo_id}_cortex.json"
                    with open(output_filename, 'w', encoding='utf-8') as f:
                        json.dump(project_cortex, f, indent=2)

                    self.stdout.write(self.style.SUCCESS(f'✓ Project Cortex created successfully: {output_filename}'))
                    self.stdout.write(self.style.NOTICE(f"\nNext Step: Run the indexer command:"))
                    self.stdout.write(self.style.SUCCESS(f"python manage.py run_indexer {output_filename}"))


                else:
                    self.stdout.write(self.style.WARNING('No files found to process or an error occurred.'))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f'\nAn unexpected error occurred: {e}'))
            self.stdout.write(self.style.ERROR('--- Full Traceback ---'))
            traceback.print_exc()
            self.stdout.write(self.style.ERROR('--- End Traceback ---'))
        # NOTE: No explicit crawler.cleanup() is needed here because the
        # `with` statement guarantees cleanup even if errors occur.

--- FILE_END: backend/ingestion/management/commands/run_crawler.py ---

--- FILE_START: backend/ingestion/__init__.py ---

--- FILE_END: backend/ingestion/__init__.py ---

--- FILE_START: backend/ingestion/apps.py ---
from django.apps import AppConfig


class IngestionConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "ingestion"

--- FILE_END: backend/ingestion/apps.py ---

--- FILE_START: backend/ingestion/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/ingestion/admin.py ---

--- FILE_START: backend/ingestion/jsonifier.py ---
import json
import pathlib
import datetime
import ast
import re
from typing import List, Dict, TypedDict, Optional, Any, Tuple
from dataclasses import asdict, dataclass
from tqdm import tqdm

from lumiere_core.services import cartographer
from lumiere_core.services import bom_parser


@dataclass
class APIEndpoint:
    """
    Standardized data structure to represent a single API endpoint.
    """
    path: str
    methods: List[str]
    handler_function_name: str
    start_line: int
    end_line: int
    framework: str


class TextChunk(TypedDict):
    chunk_id: str
    chunk_text: str
    token_count: int
    chunk_type: str
    language: Optional[str]
    start_line: Optional[int]
    end_line: Optional[int]


class FileCortex(TypedDict):
    file_path: str
    file_size_kb: float
    raw_content: str
    code_smells: List[str]
    ast_summary: str
    text_chunks: List[TextChunk]
    detected_language: str
    framework_hints: List[str]
    api_endpoints: List[Dict[str, Any]]


class ProjectCortex(TypedDict):
    repo_id: str
    last_crawled_utc: str
    project_health_score: float
    project_structure_tree: str
    github_metadata: Dict
    files: List[FileCortex]
    architectural_graph: Optional[Dict[str, Any]]
    language_statistics: Dict[str, Any]
    polyglot_summary: Dict[str, Any]
    tech_stack_bom: Optional[Dict[str, Any]]


class PolyglotChunker:
    """
    Universal code chunker that can intelligently parse and chunk code from multiple languages.
    """

    # Language-specific patterns for different constructs
    LANGUAGE_PATTERNS = {
        'python': {
            'function': r'^\s*def\s+(\w+)\s*\(',
            'class': r'^\s*class\s+(\w+)\s*[\(:]',
            'import': r'^\s*(?:from\s+\S+\s+)?import\s+',
            'comment': r'^\s*#',
            'docstring': r'^\s*["\']{{3}',
        },
        'javascript': {
            'function': r'^\s*(?:function\s+(\w+)|(?:const|let|var)\s+(\w+)\s*=\s*(?:function|\(.*\)\s*=>))',
            'class': r'^\s*class\s+(\w+)',
            'import': r'^\s*(?:import|export)',
            'comment': r'^\s*//',
            'block_comment': r'/\*.*?\*/',
        },
        'typescript': {
            'function': r'^\s*(?:function\s+(\w+)|(?:const|let|var)\s+(\w+)\s*=\s*(?:function|\(.*\)\s*=>))',
            'class': r'^\s*(?:export\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:export\s+)?interface\s+(\w+)',
            'type': r'^\s*(?:export\s+)?type\s+(\w+)',
            'import': r'^\s*(?:import|export)',
            'comment': r'^\s*//',
        },
        'java': {
            'function': r'^\s*(?:public|private|protected)?\s*(?:static\s+)?(?:\w+\s+)*(\w+)\s*\(',
            'class': r'^\s*(?:public\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:public\s+)?interface\s+(\w+)',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        },
        'csharp': {
            'function': r'^\s*(?:public|private|protected|internal)?\s*(?:static\s+)?(?:\w+\s+)*(\w+)\s*\(',
            'class': r'^\s*(?:public\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:public\s+)?interface\s+(\w+)',
            'using': r'^\s*using\s+',
            'comment': r'^\s*//',
        },
        'go': {
            'function': r'^\s*func\s+(?:\(.*\)\s+)?(\w+)\s*\(',
            'struct': r'^\s*type\s+(\w+)\s+struct',
            'interface': r'^\s*type\s+(\w+)\s+interface',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        },
        'rust': {
            'function': r'^\s*(?:pub\s+)?fn\s+(\w+)\s*\(',
            'struct': r'^\s*(?:pub\s+)?struct\s+(\w+)',
            'enum': r'^\s*(?:pub\s+)?enum\s+(\w+)',
            'trait': r'^\s*(?:pub\s+)?trait\s+(\w+)',
            'impl': r'^\s*impl\s+(?:<.*>\s+)?(\w+)',
            'use': r'^\s*use\s+',
            'comment': r'^\s*//',
        },
        'ruby': {
            'function': r'^\s*def\s+(\w+)',
            'class': r'^\s*class\s+(\w+)',
            'module': r'^\s*module\s+(\w+)',
            'require': r'^\s*require',
            'comment': r'^\s*#',
        },
        'php': {
            'function': r'^\s*(?:public|private|protected)?\s*function\s+(\w+)\s*\(',
            'class': r'^\s*class\s+(\w+)',
            'interface': r'^\s*interface\s+(\w+)',
            'namespace': r'^\s*namespace\s+',
            'use': r'^\s*use\s+',
            'comment': r'^\s*//',
        },
        'cpp': {
            'function': r'^\s*(?:\w+\s+)*(\w+)\s*\([^)]*\)\s*{',
            'class': r'^\s*class\s+(\w+)',
            'struct': r'^\s*struct\s+(\w+)',
            'namespace': r'^\s*namespace\s+(\w+)',
            'include': r'^\s*#include',
            'comment': r'^\s*//',
        },
        'swift': {
            'function': r'^\s*(?:public|private|internal)?\s*func\s+(\w+)\s*\(',
            'class': r'^\s*(?:public|private|internal)?\s*class\s+(\w+)',
            'struct': r'^\s*(?:public|private|internal)?\s*struct\s+(\w+)',
            'protocol': r'^\s*(?:public|private|internal)?\s*protocol\s+(\w+)',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        }
    }

    @staticmethod
    def detect_language_from_extension(file_path: str) -> str:
        """Detect programming language from file extension."""
        ext = pathlib.Path(file_path).suffix.lower()

        language_map = {
            '.py': 'python', '.pyx': 'python', '.pyi': 'python',
            '.js': 'javascript', '.mjs': 'javascript', '.cjs': 'javascript',
            '.jsx': 'javascript', '.gs': 'javascript',
            '.ts': 'typescript', '.tsx': 'typescript',
            '.java': 'java',
            '.cs': 'csharp',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.cpp': 'cpp', '.cxx': 'cpp', '.cc': 'cpp', '.c': 'cpp',
            '.h': 'cpp', '.hpp': 'cpp',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala',
            '.hs': 'haskell',
            '.ml': 'ocaml',
            '.ex': 'elixir',
            '.erl': 'erlang',
            '.clj': 'clojure',
            '.lua': 'lua',
            '.pl': 'perl',
            '.r': 'r',
            '.jl': 'julia',
            '.dart': 'dart',
            '.zig': 'zig',
            '.nim': 'nim',
            '.crystal': 'crystal',
        }

        return language_map.get(ext, 'unknown')

    @staticmethod
    def detect_framework_hints(content: str, language: str, file_path: str) -> List[str]:
        """Detect framework or library usage from content."""
        hints = []
        content_lower = content.lower()

        # Framework detection patterns
        framework_patterns = {
            'react': ['import react', 'from \'react\'', 'from "react"', 'usestate', 'useeffect'],
            'vue': ['vue.component', 'new vue', '@vue/', 'vue-'],
            'angular': ['@angular/', '@component', '@injectable', 'ngmodule'],
            'express': ['express()', 'app.get(', 'app.post(', 'require(\'express\')'],
            'fastapi': ['from fastapi', 'fastapi()', '@app.get', '@app.post'],
            'django': ['from django', 'django.', 'models.model', 'django.conf'],
            'flask': ['from flask', 'flask()', '@app.route'],
            'spring': ['@controller', '@service', '@repository', '@autowired'],
            'laravel': ['illuminate\\', 'artisan', 'eloquent'],
            'rails': ['activerecord', 'actioncontroller', 'rails.application'],
            'jquery': ['$(', 'jquery', '.ready('],
            'bootstrap': ['bootstrap', 'btn-', 'col-', 'row'],
            'tailwind': ['tailwind', 'tw-', 'bg-', 'text-'],
            'material-ui': ['@mui/', '@material-ui/', 'makeStyles'],
            'styled-components': ['styled-components', 'styled.'],
            'redux': ['redux', 'createstore', 'useselector', 'usedispatch'],
            'tensorflow': ['tensorflow', 'tf.', 'keras'],
            'pytorch': ['torch', 'pytorch', 'nn.module'],
            'numpy': ['import numpy', 'np.'],
            'pandas': ['import pandas', 'pd.'],
            'unittest': ['import unittest', 'testcase'],
            'pytest': ['import pytest', '@pytest.'],
            'jest': ['describe(', 'it(', 'expect('],
        }

        for framework, patterns in framework_patterns.items():
            if any(pattern in content_lower for pattern in patterns):
                hints.append(framework)

        return hints

    @classmethod
    def chunk_by_language(cls, content: str, language: str, file_path: str) -> List[Dict[str, Any]]:
        """Intelligently chunk content based on detected programming language."""
        if language == 'python':
            return cls._chunk_python(content)
        elif language in ['javascript', 'typescript']:
            return cls._chunk_javascript_typescript(content, language)
        elif language in ['java', 'csharp', 'cpp', 'swift']:
            return cls._chunk_c_style(content, language)
        elif language == 'go':
            return cls._chunk_go(content)
        elif language == 'rust':
            return cls._chunk_rust(content)
        elif language == 'ruby':
            return cls._chunk_ruby(content)
        elif language == 'php':
            return cls._chunk_php(content)
        else:
            return cls._chunk_generic(content, language)

    @classmethod
    def _chunk_python(cls, content: str) -> List[Dict[str, Any]]:
        """Enhanced Python chunking using AST when possible."""
        chunks = []

        try:
            tree = ast.parse(content)
            lines = content.splitlines()

            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "function_definition",
                            "name": node.name,
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })
                elif isinstance(node, ast.ClassDef):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "class_definition",
                            "name": node.name,
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "import_statement",
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })

            # Fill gaps with generic chunks
            if chunks:
                chunks = cls._fill_gaps_with_generic_chunks(content, chunks)

        except SyntaxError:
            # Fallback to pattern-based chunking
            chunks = cls._chunk_by_patterns(content, 'python')

        return chunks or cls._chunk_generic(content, 'python')

    @classmethod
    def _chunk_javascript_typescript(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Chunk JavaScript/TypeScript using pattern matching."""
        return cls._chunk_by_patterns(content, language)

    @classmethod
    def _chunk_c_style(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Chunk C-style languages (Java, C#, C++, Swift)."""
        return cls._chunk_by_patterns(content, language)

    @classmethod
    def _chunk_go(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Go code."""
        return cls._chunk_by_patterns(content, 'go')

    @classmethod
    def _chunk_rust(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Rust code."""
        return cls._chunk_by_patterns(content, 'rust')

    @classmethod
    def _chunk_ruby(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Ruby code."""
        return cls._chunk_by_patterns(content, 'ruby')

    @classmethod
    def _chunk_php(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk PHP code."""
        return cls._chunk_by_patterns(content, 'php')

    @classmethod
    def _chunk_by_patterns(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Generic pattern-based chunking for various languages."""
        if language not in cls.LANGUAGE_PATTERNS:
            return cls._chunk_generic(content, language)

        patterns = cls.LANGUAGE_PATTERNS[language]
        lines = content.splitlines()
        chunks = []
        current_chunk = []
        current_type = "code_block"
        current_name = None
        start_line = 1

        for i, line in enumerate(lines, 1):
            chunk_detected = False

            # Check for different construct patterns
            for construct_type, pattern in patterns.items():
                if construct_type in ['comment', 'block_comment']:
                    continue

                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    # Save previous chunk if it has meaningful content
                    if current_chunk and "\n".join(current_chunk).strip():
                        chunks.append({
                            "text": "\n".join(current_chunk),
                            "type": current_type,
                            "name": current_name,
                            "start_line": start_line,
                            "end_line": i - 1
                        })

                    # Start new chunk
                    current_chunk = [line]
                    current_type = f"{construct_type}_definition"
                    current_name = match.group(1) if match.groups() else None
                    start_line = i
                    chunk_detected = True
                    break

            if not chunk_detected:
                current_chunk.append(line)

            # For languages with braces, try to detect end of blocks
            if language in ['javascript', 'typescript', 'java', 'csharp', 'cpp', 'swift', 'go', 'rust']:
                if line.strip() == '}' and current_type.endswith('_definition'):
                    # End of current block
                    chunks.append({
                        "text": "\n".join(current_chunk),
                        "type": current_type,
                        "name": current_name,
                        "start_line": start_line,
                        "end_line": i
                    })
                    current_chunk = []
                    current_type = "code_block"
                    current_name = None
                    start_line = i + 1

        # Add remaining content as final chunk if it has meaningful content
        if current_chunk and "\n".join(current_chunk).strip():
            chunks.append({
                "text": "\n".join(current_chunk),
                "type": current_type,
                "name": current_name,
                "start_line": start_line,
                "end_line": len(lines)
            })

        return chunks

    @classmethod
    def _chunk_generic(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Fallback generic chunking for unsupported languages or when parsing fails."""
        chunks = []

        # Split by logical paragraphs (double newlines)
        paragraphs = content.split('\n\n')
        current_line = 1

        for paragraph in paragraphs:
            if paragraph.strip():
                line_count = paragraph.count('\n') + 1
                chunks.append({
                    "text": paragraph,
                    "type": "paragraph",
                    "start_line": current_line,
                    "end_line": current_line + line_count - 1
                })
                current_line += line_count + 1  # +1 for the empty line
            else:
                current_line += 1

        # If no paragraphs found, split by logical line groups
        if not chunks:
            lines = content.splitlines()
            chunk_size = min(50, max(10, len(lines) // 10))  # Adaptive chunk size

            for i in range(0, len(lines), chunk_size):
                chunk_lines = lines[i:i + chunk_size]
                chunks.append({
                    "text": "\n".join(chunk_lines),
                    "type": "line_group",
                    "start_line": i + 1,
                    "end_line": i + len(chunk_lines)
                })

        return chunks

    @classmethod
    def _fill_gaps_with_generic_chunks(cls, content: str, existing_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Fill gaps between structured chunks with generic content chunks."""
        if not existing_chunks:
            return existing_chunks

        lines = content.splitlines()
        all_chunks = []
        last_end = 0

        # Sort chunks by start line
        existing_chunks.sort(key=lambda x: x.get('start_line', 0))

        for chunk in existing_chunks:
            start_line = chunk.get('start_line', 1) - 1  # Convert to 0-based

            # Add gap content if exists
            if start_line > last_end:
                gap_content = "\n".join(lines[last_end:start_line])
                if gap_content.strip():
                    all_chunks.append({
                        "text": gap_content,
                        "type": "code_block",
                        "start_line": last_end + 1,
                        "end_line": start_line
                    })

            all_chunks.append(chunk)
            last_end = chunk.get('end_line', start_line + 1)

        # Add remaining content
        if last_end < len(lines):
            remaining_content = "\n".join(lines[last_end:])
            if remaining_content.strip():
                all_chunks.append({
                    "text": remaining_content,
                    "type": "code_block",
                    "start_line": last_end + 1,
                    "end_line": len(lines)
                })

        return all_chunks


class Jsonifier:
    """
    Enhanced Jsonifier with comprehensive polyglot support.
    Reads a list of files, intelligently chunks their content based on language,
    runs the Cartographer, and builds the Project Cortex JSON object.
    """

    # Framework-specific regex patterns for API endpoint detection
    ROUTE_PATTERNS = {
        'flask': {
            'pattern': r"@[\w\.]*\.route\(['\"](?P<path>[^'\"]+)['\"].*?(?:methods\s*=\s*\[(?P<methods>[^\]]+)\])?\)\s*\ndef\s+(?P<handler_function_name>\w+)",
            'framework': 'Flask'
        },
        'django': {
            'pattern': r"(?:url|path)\(['\"](?P<path>[^'\"]+)['\"],\s*(?P<handler_function_name>\w+)",
            'framework': 'Django'
        },
        'django_rest': {
            'pattern': r"class\s+(?P<handler_function_name>\w+)\(.*?APIView.*?\):|@api_view\(\[(?P<methods>[^\]]+)\]\)\s*\ndef\s+(?P<handler_function_name>\w+)",
            'framework': 'DjangoRestFramework'
        },
        'fastapi': {
            'pattern': r"@[\w\.]*\.(?P<method>get|post|put|delete|patch)\(['\"](?P<path>[^'\"]+)['\"].*?\)\s*\n(?:async\s+)?def\s+(?P<handler_function_name>\w+)",
            'framework': 'FastAPI'
        },
        'express': {
            'pattern': r"app\.(?P<method>get|post|put|delete|patch|use)\(['\"](?P<path>[^'\"]+)['\"],\s*(?:function\s*\(.*?\)|(?P<handler_function_name>\w+)|\(.*?\)\s*=>\s*\{)",
            'framework': 'Express'
        },
        'spring': {
            'pattern': r"@(?P<method>GetMapping|PostMapping|PutMapping|DeleteMapping|RequestMapping)\((?:value\s*=\s*)?['\"](?P<path>[^'\"]+)['\"].*?\)\s*\n\s*(?:public\s+)?[\w<>]+\s+(?P<handler_function_name>\w+)",
            'framework': 'Spring'
        },
        'gin': {
            'pattern': r"(?:router|engine)\.(?P<method>GET|POST|PUT|DELETE|PATCH)\(['\"](?P<path>[^'\"]+)['\"],\s*(?P<handler_function_name>\w+)",
            'framework': 'Gin'
        },
        'laravel': {
            'pattern': r"Route::(?P<method>get|post|put|delete|patch)\(['\"](?P<path>[^'\"]+)['\"],\s*['\"](?P<handler_function_name>[^'\"]+)['\"]",
            'framework': 'Laravel'
        },
        'rails': {
            'pattern': r"(?P<method>get|post|put|delete|patch)\s+['\"](?P<path>[^'\"]+)['\"],\s*to:\s*['\"](?P<handler_function_name>[^'\"]+)['\"]",
            'framework': 'Rails'
        }
    }

    def __init__(self, file_paths: List[pathlib.Path], repo_root: pathlib.Path, repo_id: str):
        self.file_paths = file_paths
        self.repo_root = repo_root
        self.repo_id = repo_id
        self.chunker = PolyglotChunker()

    def _read_file_content(self, file_path: pathlib.Path) -> str:
        """Read file content with multiple encoding fallbacks."""
        encodings = ['utf-8', 'utf-16', 'latin-1', 'cp1252']

        for encoding in encodings:
            try:
                return file_path.read_text(encoding=encoding)
            except UnicodeDecodeError:
                continue
            except Exception:
                break

        # Final fallback - read as binary and decode with errors='replace'
        try:
            return file_path.read_text(encoding='utf-8', errors='replace')
        except Exception:
            return ""

    def _analyze_code_quality(self, content: str, language: str, file_path: str) -> List[str]:
        """Basic code quality analysis to detect potential issues."""
        smells = []
        lines = content.splitlines()

        # Generic code smells
        if len(lines) > 1000:
            smells.append("very_large_file")

        if len(content) > 100000:  # 100KB
            smells.append("large_file_size")

        # Language-specific analysis
        if language == 'python':
            # Python-specific smells
            if 'import *' in content:
                smells.append("wildcard_import")
            if content.count('except:') > content.count('except '):
                smells.append("bare_except")
            if len([line for line in lines if len(line.strip()) > 100]) > len(lines) * 0.1:
                smells.append("long_lines")

        elif language in ['javascript', 'typescript']:
            # JS/TS-specific smells
            if 'eval(' in content:
                smells.append("eval_usage")
            if content.count('var ') > content.count('let ') + content.count('const '):
                smells.append("var_over_let_const")

        elif language == 'java':
            # Java-specific smells
            if content.count('public class') > 1:
                smells.append("multiple_public_classes")
            if 'System.out.print' in content:
                smells.append("system_out_usage")

        # Security-related patterns
        security_patterns = [
            'password', 'secret', 'api_key', 'private_key', 'token',
            'TODO', 'FIXME', 'HACK', 'XXX'
        ]

        for pattern in security_patterns:
            if pattern.lower() in content.lower():
                smells.append(f"potential_{pattern.lower()}_exposure")

        return smells

    def _create_ast_summary(self, chunks: List[Dict[str, Any]], language: str) -> str:
        """Create a summary of the AST/structure information."""
        summary = {
            "language": language,
            "total_chunks": len(chunks),
            "chunk_types": {},
            "named_constructs": []
        }

        for chunk in chunks:
            chunk_type = chunk.get("type", "unknown")
            summary["chunk_types"][chunk_type] = summary["chunk_types"].get(chunk_type, 0) + 1

            if chunk.get("name"):
                summary["named_constructs"].append({
                    "name": chunk["name"],
                    "type": chunk_type,
                    "line": chunk.get("start_line")
                })

        return json.dumps(summary, indent=2)

    def _extract_api_endpoints(self, file_content: str, language: str) -> List[APIEndpoint]:
        """
        Extract API endpoint information from file content using framework-specific regex patterns.
        
        Args:
            file_content: The full text content of the file
            language: The detected programming language
            
        Returns:
            List of APIEndpoint objects found in the file
        """
        found_endpoints = []
        
        # Skip extraction for non-web languages
        if language not in ['python', 'javascript', 'typescript', 'java', 'go', 'php', 'ruby']:
            return found_endpoints
            
        # Use re.DOTALL to allow . to match newlines for multi-line patterns
        flags = re.MULTILINE | re.DOTALL | re.IGNORECASE
        
        # Try each framework pattern
        for framework_key, pattern_info in self.ROUTE_PATTERNS.items():
            pattern = pattern_info['pattern']
            framework_name = pattern_info['framework']
            
            try:
                for match in re.finditer(pattern, file_content, flags):
                    match_data = match.groupdict()
                    
                    # Extract path - required field
                    path = match_data.get('path', '')
                    if not path:
                        continue
                        
                    # Extract handler function name - required field
                    handler_name = match_data.get('handler_function_name', '')
                    if not handler_name:
                        # For some patterns, we might have anonymous handlers
                        handler_name = f"anonymous_{framework_key}_{match.start()}"
                    
                    # Extract and process methods
                    methods = []
                    if 'methods' in match_data and match_data['methods']:
                        # Clean up methods string - remove quotes and split
                        methods_str = match_data['methods']
                        methods_str = re.sub(r'[\'"\[\]]', '', methods_str)
                        methods = [m.strip().upper() for m in methods_str.split(',') if m.strip()]
                    elif 'method' in match_data and match_data['method']:
                        # Single method from pattern
                        method = match_data['method'].upper()
                        # Map some Spring annotations to HTTP methods
                        method_mapping = {
                            'GETMAPPING': 'GET',
                            'POSTMAPPING': 'POST', 
                            'PUTMAPPING': 'PUT',
                            'DELETEMAPPING': 'DELETE',
                            'REQUESTMAPPING': 'GET'  # Default for RequestMapping
                        }
                        methods = [method_mapping.get(method, method)]
                    else:
                        # Default methods based on framework
                        if framework_key in ['django', 'django_rest']:
                            methods = ['GET', 'POST']  # Django views typically handle both
                        else:
                            methods = ['GET']  # Default assumption
                    
                    # Calculate line numbers
                    start_line = file_content[:match.start()].count('\n') + 1
                    end_line = file_content[:match.end()].count('\n') + 1
                    
                    # Create APIEndpoint object
                    endpoint = APIEndpoint(
                        path=path,
                        methods=methods,
                        handler_function_name=handler_name,
                        start_line=start_line,
                        end_line=end_line,
                        framework=framework_name
                    )
                    
                    found_endpoints.append(endpoint)
                    
            except re.error as e:
                # Log regex errors but continue processing
                print(f"  ⚠️  Regex error for {framework_key}: {e}")
                continue
            except Exception as e:
                # Log other errors but continue processing
                print(f"  ⚠️  Error processing {framework_key} pattern: {e}")
                continue
        
        return found_endpoints

    def generate_cortex(self) -> ProjectCortex:
        """
        Enhanced cortex generation with comprehensive polyglot support.

        This method now acts as a sophisticated pre-processor for the Polyglot Cartographer:
        - Detects language for each file automatically
        - Generates intelligent chunks based on language-specific patterns
        - Provides framework detection and code quality analysis
        - Supports 50+ programming languages and frameworks
        - Maintains backward compatibility with existing systems
        """
        print("🔬 Starting enhanced polyglot analysis...")

        all_files_cortex: List[FileCortex] = []
        language_stats = {}

        # Enhanced data collection for the Polyglot Cartographer
        files_for_cartographer: Dict[str, Dict[str, Any]] = {}

        for file_path in tqdm(self.file_paths, desc="Analyzing files", unit="file"):

            content = self._read_file_content(file_path)
            relative_path_str = str(file_path.relative_to(self.repo_root))

            # Enhanced language detection
            detected_language = self.chunker.detect_language_from_extension(relative_path_str)

            # Framework detection
            framework_hints = self.chunker.detect_framework_hints(content, detected_language, relative_path_str)

            # Language-aware intelligent chunking
            raw_chunks = self.chunker.chunk_by_language(content, detected_language, relative_path_str)

            # Code quality analysis
            code_smells = self._analyze_code_quality(content, detected_language, relative_path_str)

            # Extract API endpoints
            api_endpoints = self._extract_api_endpoints(content, detected_language)

            # Prepare data for Cartographer based on language
            if detected_language == 'python':
                try:
                    tree = ast.parse(content)
                    files_for_cartographer[relative_path_str] = {
                        "language": "python",
                        "ast": tree,
                        "content": content
                    }
                except SyntaxError:
                    files_for_cartographer[relative_path_str] = {
                        "language": "python",
                        "content": content,
                        "parse_error": True
                    }

            elif detected_language in ['javascript', 'typescript']:
                files_for_cartographer[relative_path_str] = {
                    "language": detected_language,
                    "content": content,
                    "framework_hints": framework_hints
                }

            elif detected_language in ['java', 'csharp', 'go', 'rust', 'swift', 'kotlin']:
                files_for_cartographer[relative_path_str] = {
                    "language": detected_language,
                    "content": content,
                    "chunks": raw_chunks
                }

            # Convert chunks to TextChunk format for RAG/Indexing
            text_chunks: List[TextChunk] = []
            for i, chunk_data in enumerate(raw_chunks):
                chunk_id = f"{self.repo_id}_{relative_path_str}_{i}"
                chunk_text = chunk_data["text"]

                text_chunks.append({
                    "chunk_id": chunk_id,
                    "chunk_text": chunk_text,
                    "token_count": len(chunk_text.split()),
                    "chunk_type": chunk_data.get("type", "unknown"),
                    "language": detected_language,
                    "start_line": chunk_data.get("start_line"),
                    "end_line": chunk_data.get("end_line")
                })

            # Update language statistics
            if detected_language not in language_stats:
                language_stats[detected_language] = {
                    "file_count": 0,
                    "total_lines": 0,
                    "total_chunks": 0,
                    "frameworks": set()
                }

            language_stats[detected_language]["file_count"] += 1
            language_stats[detected_language]["total_lines"] += len(content.splitlines())
            language_stats[detected_language]["total_chunks"] += len(text_chunks)
            language_stats[detected_language]["frameworks"].update(framework_hints)

            # Create enhanced FileCortex
            file_cortex: FileCortex = {
                "file_path": relative_path_str,
                "file_size_kb": round(file_path.stat().st_size / 1024, 2),
                "raw_content": content,
                "code_smells": code_smells,
                "ast_summary": self._create_ast_summary(raw_chunks, detected_language),
                "text_chunks": text_chunks,
                "detected_language": detected_language,
                "framework_hints": framework_hints,
                "api_endpoints": [asdict(ep) for ep in api_endpoints]
            }

            all_files_cortex.append(file_cortex)

        # Convert sets to lists for JSON serialization
        for lang_data in language_stats.values():
            lang_data["frameworks"] = list(lang_data["frameworks"])

        print("🗺️  Calling Polyglot Cartographer...")

        # Call the enhanced Polyglot Cartographer
        architectural_graph = None
        if files_for_cartographer:
            try:
                architectural_graph = cartographer.generate_graph(files_for_cartographer)
            except Exception as e:
                print(f"  ⚠️  Cartographer warning: {e}")
                architectural_graph = {"error": str(e), "supported_files": len(files_for_cartographer)}

        # Create polyglot summary
        polyglot_summary = {
            "total_languages": len(language_stats),
            "primary_language": max(language_stats.items(), key=lambda x: x[1]["file_count"])[0] if language_stats else "unknown",
            "language_distribution": {lang: data["file_count"] for lang, data in language_stats.items()},
            "detected_frameworks": list(set().union(*[data["frameworks"] for data in language_stats.values()])),
            "total_files_analyzed": len(all_files_cortex),
            "total_chunks_generated": sum(len(f["text_chunks"]) for f in all_files_cortex),
            "analysis_timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
        }

        print(f"✅ Analysis complete! Detected {len(language_stats)} languages across {len(all_files_cortex)} files")
        
        # Generate author contribution map for expertise tracking
        print("👥 Generating author contribution map...")
        author_contribution_map = self._generate_author_contribution_map()
        
        # Save blame cache to separate file
        blame_cache_path = self.repo_root / f"{self.repo_id}_blame_cache.json"
        try:
            with open(blame_cache_path, 'w', encoding='utf-8') as f:
                json.dump(author_contribution_map, f, indent=2)
            print(f"✓ Blame cache saved to {blame_cache_path}")
        except Exception as e:
            print(f"⚠️  Could not save blame cache: {e}")

        print("📋 Generating Tech Stack Bill of Materials...")
        final_bom_dict = None
        try:
            # The bom_parser now correctly returns a dataclass object.
            tech_stack_bom_obj = bom_parser.parse_all_manifests(self.repo_root)

            # --- START OF FIX ---
            if tech_stack_bom_obj:
                # Access the attributes directly from the object
                deps_count = tech_stack_bom_obj.summary.get('total_dependencies', 0)
                print(f"✓ BOM Generated. Found {deps_count} total dependencies.")
                # Convert to a dictionary for JSON serialization at the end
                final_bom_dict = asdict(tech_stack_bom_obj)
            else:
                print("✓ No dependency manifests found for BOM generation.")
                # Set a default value to ensure the key exists in the cortex
                final_bom_dict = {}
            # --- END OF FIX ---
        except Exception as e:
            print(f"⚠️  Could not generate Bill of Materials: {e}")
            final_bom_dict = {"error": str(e)}

        # Assemble the enhanced Project Cortex object
        return {
            "repo_id": self.repo_id,
            "last_crawled_utc": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "project_health_score": self._calculate_health_score(all_files_cortex, language_stats),
            "project_structure_tree": self._generate_structure_tree(),
            "github_metadata": {},
            "files": all_files_cortex,
            "architectural_graph": architectural_graph,
            "language_statistics": language_stats,
            "polyglot_summary": polyglot_summary,
            "tech_stack_bom": final_bom_dict, # Assign the final dictionary
        }

    def _generate_author_contribution_map(self) -> Dict[str, Dict[str, int]]:
        """
        Generate an author contribution map by running git blame on all processed files.
        
        Returns:
            Dictionary mapping file paths to author email -> line count mappings
            Structure: { "file_path_1": { "author_email_1": line_count, "author_email_2": line_count }, ... }
        """
        from ingestion.crawler import IntelligentCrawler
        
        contribution_map = {}
        
        # We need to create a temporary crawler instance to access git blame functionality
        # Since we're already processing files from a cloned repo, we can initialize with repo_root
        try:
            # Use a temporary crawler context to access git blame
            with IntelligentCrawler(str(self.repo_root.parent), shallow=True) as crawler:
                # Switch to the correct repo directory
                crawler.repo_path = self.repo_root
                
                for file_path in self.file_paths:
                    try:
                        relative_path_str = str(file_path.relative_to(self.repo_root))
                        
                        # Get blame data for this file
                        blame_data = crawler.get_blame_data_for_file(relative_path_str)
                        
                        if blame_data:
                            # Aggregate line counts by author email
                            author_lines = {}
                            for line_data in blame_data:
                                email = line_data.get('email', 'unknown@example.com')
                                author_lines[email] = author_lines.get(email, 0) + 1
                            
                            contribution_map[relative_path_str] = author_lines
                            
                    except Exception as e:
                        # Log but don't fail the whole process for individual files
                        print(f"  ⚠️  Could not get blame data for {relative_path_str}: {e}")
                        contribution_map[str(file_path.relative_to(self.repo_root))] = {}
                        
        except Exception as e:
            print(f"  ⚠️  Could not initialize crawler for blame analysis: {e}")
            # Return empty map if we can't get blame data
            return {}
            
        return contribution_map

    def _calculate_health_score(self, files: List[FileCortex], language_stats: Dict) -> float:
        """Calculate a basic project health score based on various metrics."""
        if not files:
            return 0.0

        total_score = 0.0
        factors = 0

        # Factor 1: Code smell density (lower is better)
        total_smells = sum(len(f["code_smells"]) for f in files)
        smell_density = total_smells / len(files)
        smell_score = max(0.0, 1.0 - (smell_density / 10))  # Normalize to 0-1
        total_score += smell_score
        factors += 1

        # Factor 2: Language diversity (moderate diversity is good)
        language_count = len(language_stats)
        if language_count == 1:
            diversity_score = 0.8  # Single language is good
        elif language_count <= 5:
            diversity_score = 1.0  # Moderate diversity is excellent
        else:
            diversity_score = max(0.5, 1.0 - (language_count - 5) * 0.1)  # Too many languages might indicate complexity
        total_score += diversity_score
        factors += 1

        # Factor 3: Documentation presence
        doc_files = [f for f in files if any(keyword in f["file_path"].lower()
                     for keyword in ["readme", "doc", "changelog", "contributing"])]
        doc_score = min(1.0, len(doc_files) / 3)  # Up to 3 doc files gives full score
        total_score += doc_score
        factors += 1

        # Factor 4: Test presence
        test_files = [f for f in files if any(keyword in f["file_path"].lower()
                      for keyword in ["test", "spec", "__test__", ".test.", ".spec."])]
        test_ratio = len(test_files) / len(files)
        test_score = min(1.0, test_ratio * 5)  # 20% test files gives full score
        total_score += test_score
        factors += 1

        return round(total_score / factors, 2)

    def _generate_structure_tree(self) -> str:
        """Generate a simple project structure tree."""
        # This is a simplified version - could be enhanced to show actual directory structure
        paths = [str(fp.relative_to(self.repo_root)) for fp in self.file_paths]
        paths.sort()

        tree_lines = []
        for path in paths[:20]:  # Limit to first 20 files for brevity
            depth = path.count('/')
            indent = "  " * depth
            filename = path.split('/')[-1]
            tree_lines.append(f"{indent}├── {filename}")

        if len(paths) > 20:
            tree_lines.append(f"  ... and {len(paths) - 20} more files")

        return "\n".join(tree_lines)

--- FILE_END: backend/ingestion/jsonifier.py ---

--- FILE_START: backend/ingestion/indexing.py ---
# backend/ingestion/indexing.py

import json
import numpy as np
import faiss
from pathlib import Path
from lumiere_core.services.ollama import get_ollama_embeddings
from tqdm import tqdm

class EmbeddingIndexer:
    """
    Loads a Project Cortex JSON, generates embeddings via Ollama,
    and saves the Faiss index and the ID-to-chunk mapping into the SAME
    directory as the source cortex file.
    """
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.dimension = None

    def _extract_repo_id_from_cortex(self, project_cortex: dict, cortex_path_obj: Path) -> str:
        """
        Extract repo_id from cortex with fallback strategies for backward compatibility.

        Args:
            project_cortex: The loaded cortex JSON data
            cortex_path_obj: Path object of the cortex file

        Returns:
            A valid repo_id string
        """
        # Primary: Try to get repo_id from the cortex data
        if 'repo_id' in project_cortex and project_cortex['repo_id']:
            return project_cortex['repo_id']

        # Fallback 1: Extract from cortex filename (assumes format: {repo_id}_cortex.json)
        cortex_filename = cortex_path_obj.stem  # Gets filename without extension
        if cortex_filename.endswith('_cortex'):
            potential_repo_id = cortex_filename[:-7]  # Remove '_cortex' suffix
            if potential_repo_id:
                print(f"Warning: repo_id not found in cortex data, using filename-derived ID: {potential_repo_id}")
                return potential_repo_id

        # Fallback 2: Use the parent directory name
        parent_dir_name = cortex_path_obj.parent.name
        if parent_dir_name and parent_dir_name != 'cloned_repositories':
            print(f"Warning: repo_id not found, using parent directory name: {parent_dir_name}")
            return parent_dir_name

        # Fallback 3: Generate from cortex filename
        fallback_id = cortex_filename.replace('_cortex', '') if '_cortex' in cortex_filename else cortex_filename
        print(f"Warning: Using fallback repo_id derived from filename: {fallback_id}")
        return fallback_id or 'unknown_repo'

    def process_cortex(self, cortex_file_path: str):
        """
        Main method to load cortex, create embeddings, and build the index.
        """
        cortex_path_obj = Path(cortex_file_path)

        # Validate that the cortex file exists
        if not cortex_path_obj.exists():
            raise FileNotFoundError(f"Cortex file not found: {cortex_path_obj}")

        print(f"Loading Project Cortex from: {cortex_path_obj}")

        # --- THIS IS THE KEY FIX ---
        # Derive the output directory from the location of the cortex file.
        output_dir = cortex_path_obj.parent

        try:
            with open(cortex_path_obj, 'r', encoding='utf-8') as f:
                project_cortex = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in cortex file {cortex_path_obj}: {e}")
        except Exception as e:
            raise IOError(f"Error reading cortex file {cortex_path_obj}: {e}")

        # 1. Collect all text chunks and their IDs
        all_chunks_text = []
        all_chunk_ids = []
        id_to_chunk_map = {}

        files_data = project_cortex.get('files', [])
        if not files_data:
            print("Warning: No 'files' array found in cortex data.")
            return

        for file_data in files_data:
            if not isinstance(file_data, dict):
                print(f"Warning: Skipping invalid file data entry: {file_data}")
                continue

            file_path = file_data.get('file_path', 'unknown_file')
            text_chunks = file_data.get('text_chunks', [])

            for chunk in text_chunks:
                if not isinstance(chunk, dict):
                    print(f"Warning: Skipping invalid chunk in {file_path}: {chunk}")
                    continue

                chunk_text = chunk.get('chunk_text', '')
                chunk_id = chunk.get('chunk_id')

                if not chunk_text or not chunk_id:
                    print(f"Warning: Skipping chunk with missing text or ID in {file_path}")
                    continue

                all_chunks_text.append(chunk_text)
                all_chunk_ids.append(chunk_id)
                id_to_chunk_map[chunk_id] = {
                    "text": chunk_text,
                    "file_path": file_path
                }

        if not all_chunks_text:
            print("No valid text chunks found. Exiting.")
            return

        print(f"Found {len(all_chunks_text)} text chunks to embed using Ollama model '{self.model_name}'.")

        # 2. Generate embeddings using our Ollama service
        try:
            embeddings_list = get_ollama_embeddings(all_chunks_text, model_name=self.model_name)
        except Exception as e:
            print(f"Error generating embeddings: {e}")
            return

        if not embeddings_list:
            print("Embedding generation failed. Exiting.")
            return

        # Validate embeddings
        if len(embeddings_list) != len(all_chunks_text):
            print(f"Error: Mismatch between chunks ({len(all_chunks_text)}) and embeddings ({len(embeddings_list)})")
            return

        self.dimension = len(embeddings_list[0])
        print(f"Ollama model '{self.model_name}' produced embeddings with dimension: {self.dimension}")

        embeddings = np.array(embeddings_list).astype('float32')

        # 3. Create and populate the Faiss index
        print("Creating Faiss index...")
        try:
            index = faiss.IndexFlatL2(self.dimension)
            index.add(embeddings)
            print(f"Faiss index created. Total vectors in index: {index.ntotal}")
        except Exception as e:
            print(f"Error creating Faiss index: {e}")
            return

        # 4. Save the artifacts
        # Use the enhanced repo_id extraction method
        repo_id = self._extract_repo_id_from_cortex(project_cortex, cortex_path_obj)

        # Use the 'output_dir' to save files in the correct location
        index_filename = output_dir / f"{repo_id}_faiss.index"
        map_filename = output_dir / f"{repo_id}_id_map.json"

        print(f"Saving Faiss index to: {index_filename}")
        try:
            faiss.write_index(index, str(index_filename))
        except Exception as e:
            print(f"Error saving Faiss index: {e}")
            return

        print(f"Saving ID-to-Chunk mapping to: {map_filename}")
        save_data = {
            "faiss_id_to_chunk_id": all_chunk_ids,
            "chunk_id_to_data": id_to_chunk_map
        }

        try:
            with open(map_filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2)
        except Exception as e:
            print(f"Error saving ID mapping: {e}")
            return

        print("Indexing complete.")

--- FILE_END: backend/ingestion/indexing.py ---

--- FILE_START: backend/ingestion/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/ingestion/tests.py ---

--- FILE_START: backend/ingestion/views.py ---
from django.shortcuts import render

# Create your views here.

--- FILE_END: backend/ingestion/views.py ---

--- FILE_START: backend/ingestion/crawler.py ---
# In ingestion/crawler.py
import subprocess
import tempfile
import pathlib
from typing import List, Optional, Union, Dict, Set
import logging
import os
from functools import lru_cache

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(name)s: %(message)s')
logger = logging.getLogger(__name__)

# --- THE FIX: Silence noisy loggers ---
# Set the log level for httpx (used by ollama) and faiss to WARNING to reduce noise.
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("faiss").setLevel(logging.WARNING)

class IntelligentCrawler:
    """
    Clones a Git repository and performs file operations safely.
    Includes path-finding, git blame, and git diff capabilities.
    Enhanced with comprehensive polyglot language support.
    """

    # Class-level constants for better maintainability
    DEFAULT_EXCLUDED_DIRS = {
        '.git', '__pycache__', 'venv', 'node_modules', '.vscode', '.idea',
        'dist', 'build', '.pytest_cache', '.mypy_cache', '.tox', 'target',
        'bin', 'obj', 'out', '.gradle', '.mvn', 'vendor', 'deps', '_build',
        '.stack-work', '.cabal-sandbox', 'elm-stuff', '.pub-cache', '.dart_tool'
    }

    # Comprehensive language support - organized by ecosystem
    DEFAULT_INCLUDED_EXTENSIONS = [
        # === WEB TECHNOLOGIES ===
        # Frontend JavaScript/TypeScript
        '*.js', '*.mjs', '*.cjs', '*.jsx', '*.ts', '*.tsx', '*.vue', '*.svelte',
        '*.astro', '*.lit', '*.stencil', '*.qwik',
        # Web markup and styling
        '*.html', '*.htm', '*.xhtml', '*.xml', '*.css', '*.scss', '*.sass',
        '*.less', '*.styl', '*.stylus', '*.postcss',
        # Web frameworks and configs
        '*.ejs', '*.pug', '*.handlebars', '*.hbs', '*.mustache', '*.twig',
        '*.blade.php', '*.erb', '*.haml', '*.slim',

        # === BACKEND LANGUAGES ===
        # Python ecosystem
        '*.py', '*.pyx', '*.pyi', '*.pyw', '*.py3',
        # JavaScript/Node.js
        '*.gs',  # Google Apps Script
        # Java ecosystem
        '*.java', '*.kt', '*.kts', '*.scala', '*.groovy', '*.clj', '*.cljs', '*.cljc',
        # .NET ecosystem
        '*.cs', '*.vb', '*.fs', '*.fsx', '*.csx',
        # Systems programming
        '*.c', '*.cpp', '*.cxx', '*.cc', '*.c++', '*.h', '*.hpp', '*.hxx', '*.h++',
        '*.rs', '*.go', '*.zig', '*.odin', '*.v', '*.nim', '*.crystal',
        # Apple ecosystem
        '*.swift', '*.m', '*.mm',
        # Other compiled languages
        '*.d', '*.ada', '*.adb', '*.ads',

        # === SCRIPTING LANGUAGES ===
        '*.rb', '*.rake', '*.gemspec', '*.rbw',  # Ruby
        '*.php', '*.phtml', '*.php3', '*.php4', '*.php5', '*.php7', '*.php8',  # PHP
        '*.pl', '*.pm', '*.t', '*.pod',  # Perl
        '*.lua', '*.luac',  # Lua
        '*.tcl', '*.tk',  # Tcl/Tk
        '*.ps1', '*.psm1', '*.psd1',  # PowerShell

        # === SHELL SCRIPTING ===
        '*.sh', '*.bash', '*.zsh', '*.fish', '*.csh', '*.tcsh', '*.ksh',
        '*.bat', '*.cmd', '*.command',

        # === FUNCTIONAL LANGUAGES ===
        '*.hs', '*.lhs',  # Haskell
        '*.ml', '*.mli', '*.mll', '*.mly',  # OCaml
        '*.elm',  # Elm
        '*.ex', '*.exs',  # Elixir
        '*.erl', '*.hrl',  # Erlang
        '*.lisp', '*.lsp', '*.cl', '*.el',  # Lisp family
        '*.scm', '*.ss', '*.rkt',  # Scheme/Racket
        '*.f', '*.for', '*.f90', '*.f95', '*.f03', '*.f08',  # Fortran

        # === DATA SCIENCE & MATH ===
        '*.r', '*.R', '*.rmd', '*.rnw',  # R
        '*.jl',  # Julia
        '*.m', '*.mat',  # MATLAB/Octave
        '*.nb', '*.wl',  # Mathematica
        '*.sas', '*.stata', '*.do',  # Statistics
        '*.ipynb',  # Jupyter notebooks

        # === MOBILE DEVELOPMENT ===
        '*.dart',  # Dart/Flutter
        '*.kt', '*.kts',  # Kotlin (Android)
        '*.java',  # Java (Android)

        # === GAME DEVELOPMENT ===
        '*.cs',  # Unity C#
        '*.gd', '*.tres', '*.tscn',  # Godot
        '*.lua',  # Love2D, World of Warcraft addons
        '*.as', '*.mxml',  # ActionScript/Flex
        '*.hlsl', '*.glsl', '*.cg', '*.shader',  # Shaders

        # === DATABASE ===
        '*.sql', '*.sqlite', '*.db', '*.mysql', '*.pgsql', '*.plsql',
        '*.cypher', '*.cql', '*.sparql', '*.graphql', '*.gql',

        # === CONFIGURATION FORMATS ===
        '*.json', '*.json5', '*.jsonc', '*.jsonl', '*.ndjson',
        '*.toml', '*.yaml', '*.yml', '*.ini', '*.cfg', '*.conf', '*.config',
        '*.properties', '*.env', '*.dotenv', '*.editorconfig',
        '*.hcl', '*.tf', '*.tfvars',  # Terraform
        '*.dhall', '*.nix',  # Nix
        '*.hocon',  # HOCON (Typesafe Config)

        # === MARKUP & DOCUMENTATION ===
        '*.md', '*.markdown', '*.mdown', '*.mkd', '*.mdx',
        '*.rst', '*.adoc', '*.asciidoc', '*.txt', '*.text',
        '*.tex', '*.latex', '*.ltx', '*.cls', '*.sty',
        '*.org', '*.wiki', '*.textile',

        # === DATA FORMATS ===
        '*.csv', '*.tsv', '*.psv', '*.ssv',
        '*.parquet', '*.avro', '*.orc', '*.arrow',
        '*.proto', '*.protobuf',  # Protocol Buffers
        '*.thrift',  # Apache Thrift
        '*.capnp',  # Cap'n Proto

        # === SCIENTIFIC COMPUTING ===
        '*.cu', '*.cuh',  # CUDA
        '*.opencl', '*.cl',  # OpenCL
        '*.sage', '*.magma',  # Mathematical software

        # === EMERGING LANGUAGES ===
        '*.move',  # Move (Diem/Aptos)
        '*.sol',  # Solidity (Ethereum)
        '*.cairo',  # Cairo (StarkNet)
        '*.fe',  # Fe (Ethereum)
        '*.gleam',  # Gleam
        '*.roc',  # Roc
        '*.grain',  # Grain
        '*.red',  # Red
        '*.io',  # Io
        '*.pony',  # Pony
        '*.chapel', '*.chpl',  # Chapel

        # === DOMAIN-SPECIFIC LANGUAGES ===
        '*.vhdl', '*.vhd',  # VHDL
        '*.v', '*.sv',  # Verilog/SystemVerilog
        '*.asl', '*.dsl',  # Domain-specific languages
        '*.feature',  # Gherkin/Cucumber
        '*.story',  # JBehave
        '*.bdd',  # Behavior-driven development

        # === LEGACY & SPECIALTY ===
        '*.pas', '*.pp',  # Pascal
        '*.cob', '*.cbl', '*.cpy',  # COBOL
        '*.for', '*.f77',  # FORTRAN 77
        '*.asm', '*.s', '*.S',  # Assembly
        '*.awk',  # AWK
        '*.sed',  # Sed scripts
        '*.regex', '*.re',  # Regex files

        # === AUTOMATION & TESTING ===
        '*.robot',  # Robot Framework
        '*.feature',  # Cucumber/Gherkin
        '*.spec', '*.test',  # Test specifications
        '*.e2e', '*.integration',  # Test files

        # === BUILD & INFRASTRUCTURE ===
        '*.bazel', '*.bzl',  # Bazel
        '*.buck',  # Buck
        '*.ninja',  # Ninja
        '*.gyp', '*.gypi',  # GYP

        # === VIRTUALIZATION & CONTAINERS ===
        '*.dockerfile',  # Dockerfile variants
        '*.containerfile',
        '*.vagrantfile',
    ]

    DEFAULT_SPECIAL_FILES = [
        # === BUILD SYSTEMS ===
        'Dockerfile', 'Containerfile', 'docker-compose.yml', 'docker-compose.yaml',
        'Makefile', 'makefile', 'GNUmakefile', 'Makefile.am', 'Makefile.in',
        'CMakeLists.txt', 'cmake.txt', 'meson.build', 'meson_options.txt',
        'SConstruct', 'SConscript', 'wscript', 'waf',
        'BUILD', 'BUILD.bazel', 'WORKSPACE', 'WORKSPACE.bazel',
        'buck', 'BUCK', 'TARGETS',
        'ninja.build', 'build.ninja',

        # === JAVASCRIPT/NODE.JS ECOSYSTEM ===
        'package.json', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
        'bower.json', 'component.json', 'npm-shrinkwrap.json',
        'webpack.config.js', 'webpack.config.ts', 'webpack.common.js',
        'vite.config.js', 'vite.config.ts', 'rollup.config.js', 'rollup.config.ts',
        'parcel.config.js', 'snowpack.config.js', 'esbuild.config.js',
        'tsconfig.json', 'jsconfig.json', 'tsconfig.build.json',
        'babel.config.js', 'babel.config.json', '.babelrc', '.babelrc.js',
        'postcss.config.js', 'tailwind.config.js', 'tailwind.config.ts',
        '.eslintrc.js', '.eslintrc.json', '.eslintrc.yml', '.eslintrc.yaml',
        '.prettierrc', '.prettierrc.js', '.prettierrc.json', '.prettierignore',
        'jest.config.js', 'jest.config.ts', 'vitest.config.js', 'vitest.config.ts',
        'playwright.config.js', 'playwright.config.ts', 'cypress.config.js',

        # === PYTHON ECOSYSTEM ===
        'requirements.txt', 'requirements-dev.txt', 'requirements-test.txt',
        'Pipfile', 'Pipfile.lock', 'poetry.lock', 'pdm.lock',
        'pyproject.toml', 'setup.py', 'setup.cfg', 'manifest.in',
        'tox.ini', 'pytest.ini', 'conftest.py', '.coveragerc', 'coverage.ini',
        'mypy.ini', '.mypy.ini', 'pyrightconfig.json',
        'flake8.cfg', '.flake8', 'pylintrc', '.pylintrc',
        'black.toml', 'isort.cfg', '.isort.cfg', 'bandit.yaml',
        'environment.yml', 'conda.yml', 'environment.yaml',

        # === RUST ECOSYSTEM ===
        'Cargo.toml', 'Cargo.lock', 'rust-toolchain', 'rust-toolchain.toml',
        'clippy.toml', 'rustfmt.toml', '.rustfmt.toml',

        # === GO ECOSYSTEM ===
        'go.mod', 'go.sum', 'go.work', 'go.work.sum',

        # === JAVA ECOSYSTEM ===
        'pom.xml', 'build.gradle', 'build.gradle.kts', 'settings.gradle',
        'gradle.properties', 'gradlew', 'gradlew.bat',
        'build.xml', 'ivy.xml', 'build.sbt', 'project.clj',

        # === .NET ECOSYSTEM ===
        '*.csproj', '*.vbproj', '*.fsproj', '*.sln', '*.proj',
        'packages.config', 'nuget.config', 'global.json',
        'Directory.Build.props', 'Directory.Build.targets',

        # === RUBY ECOSYSTEM ===
        'Gemfile', 'Gemfile.lock', 'Rakefile', '.ruby-version',
        'config.ru', '.rspec', '.rubocop.yml',

        # === PHP ECOSYSTEM ===
        'composer.json', 'composer.lock', 'phpunit.xml', 'phpunit.xml.dist',
        '.php_cs', '.php_cs.dist', 'phpstan.neon', 'psalm.xml',

        # === INFRASTRUCTURE AS CODE ===
        'terraform.tf', 'main.tf', 'variables.tf', 'outputs.tf',
        'terraform.tfvars', 'terraform.tfvars.json',
        'ansible.cfg', 'playbook.yml', 'hosts', 'inventory',
        'Vagrantfile', 'Berksfile', 'Policyfile.rb',
        'docker-stack.yml', 'docker-swarm.yml',

        # === KUBERNETES ===
        'kustomization.yaml', 'kustomization.yml',
        'deployment.yaml', 'service.yaml', 'ingress.yaml',
        'configmap.yaml', 'secret.yaml', 'namespace.yaml',

        # === CI/CD ===
        '.gitlab-ci.yml', '.gitlab-ci.yaml',
        'Jenkinsfile', 'jenkins.yml', 'jenkins.yaml',
        '.circleci/config.yml', '.circle/config.yml',
        '.travis.yml', 'appveyor.yml', '.appveyor.yml',
        'azure-pipelines.yml', 'azure-pipelines.yaml',
        'bitbucket-pipelines.yml', 'drone.yml', '.drone.yml',
        'wercker.yml', 'shippable.yml', 'codefresh.yml',

        # === GITHUB ACTIONS ===
        '.github/workflows/*.yml', '.github/workflows/*.yaml',
        '.github/dependabot.yml', '.github/renovate.json',

        # === DOCUMENTATION ===
        'README', 'README.txt', 'README.md', 'README.rst',
        'CHANGELOG', 'CHANGELOG.md', 'CHANGELOG.txt', 'CHANGES',
        'CONTRIBUTING.md', 'CONTRIBUTING.rst', 'CONTRIBUTING.txt',
        'CODE_OF_CONDUCT.md', 'SECURITY.md', 'SUPPORT.md',
        'LICENSE', 'LICENSE.txt', 'LICENSE.md', 'COPYING',
        'AUTHORS', 'AUTHORS.txt', 'AUTHORS.md', 'MAINTAINERS',
        'NOTICE', 'ACKNOWLEDGMENTS', 'CREDITS',

        # === VERSION CONTROL ===
        '.gitignore', '.gitattributes', '.gitmodules', '.gitmessage',
        '.hgignore', '.svnignore', '.bzrignore',

        # === EDITOR CONFIGURATIONS ===
        '.editorconfig', '.dir-locals.el', '.projectile',
        '.vimrc', '.nvimrc', '.emacs', '.spacemacs',

        # === MOBILE DEVELOPMENT ===
        'pubspec.yaml', 'pubspec.lock',  # Dart/Flutter
        'android_app.yaml', 'ios_app.yaml',
        'Info.plist', 'AndroidManifest.xml',
        'build.gradle', 'proguard-rules.pro',

        # === GAME DEVELOPMENT ===
        'project.godot', 'export_presets.cfg',  # Godot
        'game.project', 'main.tscn',
        'love.exe', 'main.lua',  # Love2D

        # === DATA SCIENCE ===
        'requirements-dev.txt', 'environment.yml',
        'notebook.ipynb', '*.ipynb',
        '.RData', '.Rprofile', 'DESCRIPTION',

        # === DATABASE ===
        'schema.sql', 'migrations.sql', 'seeds.sql',
        'alembic.ini', 'flyway.conf',
        '.sequelizerc', 'knexfile.js',

        # === WEB FRAMEWORKS ===
        'next.config.js', 'nuxt.config.js', 'svelte.config.js',
        'gatsby-config.js', 'gridsome.config.js',
        'quasar.conf.js', 'vue.config.js', 'angular.json',
        'ember-cli-build.js', '.ember-cli',

        # === TESTING ===
        'karma.conf.js', 'protractor.conf.js', 'wdio.conf.js',
        'codecept.conf.js', 'nightwatch.conf.js',
        'testcafe.json', 'cucumber.js',

        # === MONITORING & OBSERVABILITY ===
        'prometheus.yml', 'grafana.json', 'jaeger.yml',
        'newrelic.yml', 'datadog.yaml', 'sentry.properties',

        # === MISC CONFIGURATION ===
        'nodemon.json', 'browserslist', '.nvmrc', '.node-version',
        'lerna.json', 'rush.json', 'nx.json', 'workspace.json',
        'bit.json', '.bitmap', 'now.json', 'vercel.json',
        'netlify.toml', '_redirects', '_headers',
        'firebase.json', '.firebaserc', 'app.yaml', 'cron.yaml',
    ]

    def __init__(self, repo_url: str, shallow: bool = False, depth: Optional[int] = None):
        """
        Initializes the crawler with the repository URL.

        Args:
            repo_url: The Git repository URL to clone
            shallow: Whether to perform a shallow clone (faster, less history)
            depth: Depth for shallow clone (only used if shallow=True)
        """
        self.repo_url = repo_url
        self.shallow = shallow
        self.depth = depth or 1 if shallow else None
        self.temp_dir_handle = tempfile.TemporaryDirectory()
        self.repo_path = pathlib.Path(self.temp_dir_handle.name)
        self._file_paths_cache: Optional[List[pathlib.Path]] = None
        self._current_ref: Optional[str] = None

    def __enter__(self):
        """
        Enters the context manager, cloning the repository.
        """
        logger.info(f"Cloning {self.repo_url} into {self.repo_path}")
        self._clone_repo()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Exits the context manager, cleaning up resources.
        """
        logger.info("Cleaning up resources")
        self.cleanup()

    def _run_git_command(self, cmd: List[str], check: bool = True, capture_output: bool = True) -> subprocess.CompletedProcess:
        """
        Helper method to run git commands with consistent error handling.

        Args:
            cmd: Git command as list of strings
            check: Whether to raise CalledProcessError on non-zero exit
            capture_output: Whether to capture stdout/stderr

        Returns:
            CompletedProcess result
        """
        try:
            result = subprocess.run(
                cmd,
                cwd=self.repo_path,
                check=check,
                capture_output=capture_output,
                text=True,
                timeout=300  # 5 minute timeout for git operations
            )
            return result
        except subprocess.TimeoutExpired:
            logger.error(f"Git command timed out: {' '.join(cmd)}")
            raise
        except subprocess.CalledProcessError as e:
            logger.error(f"Git command failed: {' '.join(cmd)}, Error: {e.stderr}")
            raise

    def _clone_repo(self):
        """
        Clones the git repository with optimized settings.
        """
        try:
            clone_cmd = ['git', 'clone']

            if self.shallow:
                clone_cmd.extend(['--depth', str(self.depth)])
                clone_cmd.append('--single-branch')

            clone_cmd.extend([self.repo_url, str(self.repo_path)])

            self._run_git_command(clone_cmd)

            if not self.shallow:
                # Fetch all tags and remote branches for full clones
                self._run_git_command(['git', 'fetch', 'origin', '--tags'])
                self._run_git_command(['git', 'remote', 'update'])

            # Get current branch/ref
            result = self._run_git_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
            self._current_ref = result.stdout.strip()

            logger.info(f"Repository cloned successfully on branch: {self._current_ref}")

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to clone repository: {e.stderr}")
            raise

    def get_current_ref(self) -> str:
        """
        Gets the currently checked out reference.

        Returns:
            Current branch/tag/commit hash
        """
        if self._current_ref is None:
            result = self._run_git_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
            self._current_ref = result.stdout.strip()
        return self._current_ref

    def list_branches(self, remote: bool = True) -> List[str]:
        """
        Lists available branches.

        Args:
            remote: Whether to include remote branches

        Returns:
            List of branch names
        """
        cmd = ['git', 'branch']
        if remote:
            cmd.append('-a')

        result = self._run_git_command(cmd)
        branches = []
        for line in result.stdout.strip().split('\n'):
            line = line.strip()
            if line.startswith('*'):
                line = line[1:].strip()
            if line and not line.startswith('('):
                branches.append(line)

        return branches

    def list_tags(self) -> List[str]:
        """
        Lists available tags.

        Returns:
            List of tag names
        """
        result = self._run_git_command(['git', 'tag', '-l'])
        return [tag.strip() for tag in result.stdout.strip().split('\n') if tag.strip()]

    def get_blame_for_file(self, target_file: str, line_range: Optional[tuple] = None) -> str:
        """
        Runs `git blame` on a specific file in the repo.

        Args:
            target_file: Path to the file relative to repo root
            line_range: Optional tuple of (start_line, end_line) for partial blame

        Returns:
            Git blame output or error message
        """
        file_full_path = self.repo_path / target_file
        if not file_full_path.exists():
            error_msg = f"Error from crawler: File '{target_file}' does not exist in the repository."
            logger.warning(error_msg)
            return error_msg

        try:
            logger.info(f"Running 'git blame' on {file_full_path}")

            cmd = ['git', 'blame', '--show-email']
            if line_range:
                start, end = line_range
                cmd.extend(['-L', f'{start},{end}'])
            cmd.append(str(file_full_path))

            result = self._run_git_command(cmd)
            return result.stdout

        except subprocess.CalledProcessError as e:
            error_message = f"Error running 'git blame' on '{target_file}': {e.stderr}"
            logger.error(error_message)
            return f"Error from crawler: {error_message}"

    def get_blame_data_for_file(self, file_path: str) -> List[Dict[str, str]]:
        """
        Runs `git blame --line-porcelain` on a specific file and parses the output 
        for machine processing. Returns structured blame data per line.

        Args:
            file_path: Path to the file relative to repo root

        Returns:
            List of dictionaries with blame data for each line, e.g.,
            [{'commit': '...', 'author': 'J. Doe', 'email': 'j.doe@example.com', 'line_num': 1}, ...]
        """
        file_full_path = self.repo_path / file_path
        if not file_full_path.exists():
            logger.warning(f"File '{file_path}' does not exist in the repository.")
            return []

        try:
            logger.debug(f"Running 'git blame --line-porcelain' on {file_full_path}")

            cmd = ['git', 'blame', '--line-porcelain', str(file_full_path)]
            result = self._run_git_command(cmd)
            
            return self._parse_porcelain_blame(result.stdout)

        except subprocess.CalledProcessError as e:
            logger.error(f"Error running 'git blame --line-porcelain' on '{file_path}': {e.stderr}")
            return []

    def _parse_porcelain_blame(self, porcelain_output: str) -> List[Dict[str, str]]:
        """
        Parses the --line-porcelain output from git blame into structured data.
        
        Args:
            porcelain_output: Raw output from git blame --line-porcelain
            
        Returns:
            List of dictionaries with blame data for each line
        """
        lines = porcelain_output.strip().split('\n')
        blame_data = []
        current_entry = {}
        line_number = 1
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            if not line:
                i += 1
                continue
                
            # First line of each entry: commit hash, original line number, final line number, group size
            if re.match(r'^[0-9a-f]{40}', line):
                # Parse the header line: commit_hash orig_line final_line [group_size]
                parts = line.split()
                current_entry = {
                    'commit': parts[0],
                    'line_num': line_number
                }
                line_number += 1
                
            elif line.startswith('author '):
                current_entry['author'] = line[7:]  # Remove 'author ' prefix
                
            elif line.startswith('author-mail '):
                # Extract email, removing angle brackets
                email = line[12:].strip()
                if email.startswith('<') and email.endswith('>'):
                    email = email[1:-1]
                current_entry['email'] = email
                
            elif line.startswith('\t'):
                # This is the actual source line, end of this entry
                blame_data.append(current_entry.copy())
                current_entry = {}
                
            i += 1
            
        return blame_data

    def get_diff_for_branch(self, ref_name: str, base_ref: str = 'main', stat_only: bool = False) -> str:
        """
        Gets the `git diff` between two refs (branch, tag, or commit).

        Args:
            ref_name: The reference to compare
            base_ref: The base reference to compare against
            stat_only: Whether to return only diff statistics

        Returns:
            Git diff output or error message
        """
        try:
            logger.info(f"Calculating diff for '{ref_name}' against base '{base_ref}'")

            cmd = ['git', 'diff']
            if stat_only:
                cmd.append('--stat')
            cmd.append(f'{base_ref}...{ref_name}')

            logger.debug(f"Running command: {' '.join(cmd)}")
            result = self._run_git_command(cmd)
            return result.stdout

        except subprocess.CalledProcessError:
            # Try with 'master' if 'main' fails
            if base_ref == 'main':
                logger.info("Diff against 'main' failed, trying 'master' as base")
                return self.get_diff_for_branch(ref_name, 'master', stat_only)

            error_message = f"Error running 'git diff' between '{base_ref}' and '{ref_name}'"
            logger.error(error_message)
            return f"Error from crawler: {error_message}"

    def checkout_ref(self, ref_name: str, create_branch: bool = False) -> bool:
        """
        Checks out a specific git reference (branch, tag, or commit hash).

        Args:
            ref_name: The name of the reference to check out
            create_branch: Whether to create a new branch if it doesn't exist

        Returns:
            True if checkout was successful, False otherwise
        """
        logger.info(f"Attempting to checkout ref '{ref_name}'")
        try:
            # Invalidate caches since the files will change
            self._file_paths_cache = None

            cmd = ['git', 'checkout']
            if create_branch:
                cmd.extend(['-b', ref_name])
            else:
                cmd.append(ref_name)

            self._run_git_command(cmd)
            self._current_ref = ref_name
            logger.info(f"Successfully checked out '{ref_name}'")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to checkout '{ref_name}': {e.stderr}")
            return False

    def get_file_content(self, file_path: str, encoding: str = 'utf-8') -> Optional[str]:
        """
        Reads the content of a file in the repository.

        Args:
            file_path: Path to the file relative to repo root
            encoding: File encoding (default: utf-8)

        Returns:
            File content or None if file doesn't exist/can't be read
        """
        full_path = self.repo_path / file_path
        try:
            return full_path.read_text(encoding=encoding)
        except (FileNotFoundError, UnicodeDecodeError, PermissionError) as e:
            logger.warning(f"Could not read file '{file_path}': {e}")
            return None

    def find_file_path(self, target_filename: str, case_sensitive: bool = True) -> Union[str, Dict, None]:
        """
        Searches the repository for a file by its name.

        Args:
            target_filename: Name of the file to search for
            case_sensitive: Whether to perform case-sensitive search

        Returns:
            File path, conflict dict with options, or None if not found
        """
        logger.info(f"Searching for file matching '{target_filename}'")
        all_files = self.get_file_paths()

        possible_matches = []
        search_name = target_filename if case_sensitive else target_filename.lower()

        for file_path in all_files:
            relative_path = file_path.relative_to(self.repo_path)
            file_name = relative_path.name if case_sensitive else relative_path.name.lower()
            path_str = str(relative_path) if case_sensitive else str(relative_path).lower()

            if file_name == search_name or path_str.endswith('/' + search_name):
                possible_matches.append(relative_path)

        if not possible_matches:
            logger.info(f"No match found for '{target_filename}'")
            return None

        if len(possible_matches) == 1:
            match = str(possible_matches[0])
            logger.info(f"Found unique match: {match}")
            return match

        logger.info(f"Found multiple matches, checking for root-level file")
        root_matches = [p for p in possible_matches if len(p.parts) == 1]

        if len(root_matches) == 1:
            match = str(root_matches[0])
            logger.info(f"Prioritized unique root match: {match}")
            return match

        logger.warning(f"Ambiguous path detected for '{target_filename}'")
        return {
            "error": "ambiguous_path",
            "message": f"Multiple files found matching '{target_filename}'. Please specify one.",
            "options": [str(p) for p in possible_matches]
        }

    def get_file_paths(self, custom_extensions: Optional[List[str]] = None,
                      custom_excluded_dirs: Optional[Set[str]] = None,
                      include_hidden: bool = False) -> List[pathlib.Path]:
        """
        Scans the cloned repo and returns a list of relevant files.
        Caches the result for performance.

        Args:
            custom_extensions: Override default file extensions to include
            custom_excluded_dirs: Override default directories to exclude
            include_hidden: Whether to include hidden files/directories

        Returns:
            List of file paths
        """
        # Use custom parameters if provided, otherwise use defaults
        if custom_extensions is not None or custom_excluded_dirs is not None or include_hidden:
            # Don't use cache for custom parameters
            return self._scan_files(custom_extensions, custom_excluded_dirs, include_hidden)

        if self._file_paths_cache is not None:
            return self._file_paths_cache

        logger.info("Scanning for relevant files...")
        self._file_paths_cache = self._scan_files()
        logger.info(f"Found and cached {len(self._file_paths_cache)} files to process")
        return self._file_paths_cache

    def _scan_files(self, custom_extensions: Optional[List[str]] = None,
                   custom_excluded_dirs: Optional[Set[str]] = None,
                   include_hidden: bool = False) -> List[pathlib.Path]:
        """
        Internal method to scan files with given parameters.
        """
        extensions = custom_extensions if custom_extensions is not None else (
            self.DEFAULT_INCLUDED_EXTENSIONS + self.DEFAULT_SPECIAL_FILES
        )
        excluded_dirs = custom_excluded_dirs if custom_excluded_dirs is not None else self.DEFAULT_EXCLUDED_DIRS

        files_to_process = []

        for file_path in self.repo_path.rglob('*'):
            relative_path = file_path.relative_to(self.repo_path)

            # Skip hidden files/directories unless explicitly included
            if not include_hidden and any(part.startswith('.') for part in relative_path.parts):
                # Allow certain dotfiles that are in our special files list
                if file_path.name not in self.DEFAULT_SPECIAL_FILES:
                    continue

            # Skip excluded directories
            if any(part in excluded_dirs for part in relative_path.parts):
                continue

            if file_path.is_file():
                # Check against extensions (patterns) and special filenames
                if any(file_path.match(ext) for ext in extensions):
                    files_to_process.append(file_path)

        return files_to_process

    def get_language_statistics(self) -> Dict[str, Dict[str, Union[int, List[str]]]]:
        """
        Enhanced method to get comprehensive language statistics.

        Returns:
            Dictionary with language statistics including file counts and examples
        """
        files = self.get_file_paths()

        # Comprehensive language mapping
        LANGUAGE_MAP = {
            # Web Technologies
            '.js': 'JavaScript', '.mjs': 'JavaScript', '.cjs': 'JavaScript',
            '.jsx': 'JavaScript (React)', '.ts': 'TypeScript', '.tsx': 'TypeScript (React)',
            '.vue': 'Vue.js', '.svelte': 'Svelte', '.astro': 'Astro',
            '.html': 'HTML', '.htm': 'HTML', '.css': 'CSS', '.scss': 'SASS',
            '.sass': 'SASS', '.less': 'LESS', '.styl': 'Stylus',

            # Backend Languages
            '.py': 'Python', '.pyx': 'Cython', '.pyi': 'Python Interface',
            '.java': 'Java', '.kt': 'Kotlin', '.scala': 'Scala', '.groovy': 'Groovy',
            '.cs': 'C#', '.vb': 'Visual Basic .NET', '.fs': 'F#',
            '.c': 'C', '.cpp': 'C++', '.cxx': 'C++', '.cc': 'C++', '.h': 'C/C++ Header',
            '.rs': 'Rust', '.go': 'Go', '.swift': 'Swift',
            '.rb': 'Ruby', '.php': 'PHP', '.pl': 'Perl', '.lua': 'Lua',

            # Functional Languages
            '.hs': 'Haskell', '.ml': 'OCaml', '.elm': 'Elm', '.ex': 'Elixir',
            '.erl': 'Erlang', '.clj': 'Clojure', '.scm': 'Scheme', '.lisp': 'Common Lisp',

            # Data Science
            '.r': 'R', '.jl': 'Julia', '.m': 'MATLAB', '.ipynb': 'Jupyter Notebook',

            # Mobile
            '.dart': 'Dart', '.gs': 'Google Apps Script',

            # Systems
            '.zig': 'Zig', '.nim': 'Nim', '.crystal': 'Crystal', '.d': 'D',

            # Markup and Config
            '.md': 'Markdown', '.rst': 'reStructuredText', '.tex': 'LaTeX',
            '.json': 'JSON', '.yaml': 'YAML', '.yml': 'YAML', '.toml': 'TOML',
            '.xml': 'XML', '.ini': 'INI', '.cfg': 'Config',

            # Database
            '.sql': 'SQL', '.graphql': 'GraphQL', '.gql': 'GraphQL',

            # Shell
            '.sh': 'Shell Script', '.bash': 'Bash', '.zsh': 'Zsh', '.fish': 'Fish',
            '.ps1': 'PowerShell', '.bat': 'Batch', '.cmd': 'Command Script',

            # Infrastructure
            '.tf': 'Terraform', '.hcl': 'HCL', '.dockerfile': 'Dockerfile',

            # Game Development
            '.gd': 'GDScript', '.hlsl': 'HLSL', '.glsl': 'GLSL', '.shader': 'Shader',

            # Emerging/Blockchain
            '.sol': 'Solidity', '.move': 'Move', '.cairo': 'Cairo',

            # Legacy
            '.pas': 'Pascal', '.cob': 'COBOL', '.for': 'FORTRAN', '.asm': 'Assembly',

            # Special cases
            'no_extension': 'No Extension'
        }

        language_stats = {}

        for file_path in files:
            ext = file_path.suffix.lower() or 'no_extension'
            language = LANGUAGE_MAP.get(ext, f'Unknown ({ext})')

            if language not in language_stats:
                language_stats[language] = {
                    'count': 0,
                    'extensions': set(),
                    'examples': []
                }

            language_stats[language]['count'] += 1
            language_stats[language]['extensions'].add(ext)

            # Add up to 3 example files
            if len(language_stats[language]['examples']) < 3:
                relative_path = str(file_path.relative_to(self.repo_path))
                language_stats[language]['examples'].append(relative_path)

        # Convert sets to lists for JSON serialization
        for lang_data in language_stats.values():
            lang_data['extensions'] = list(lang_data['extensions'])

        return language_stats

    def get_repo_stats(self) -> Dict[str, Union[int, str, List[str], Dict]]:
        """
        Gets comprehensive statistics about the repository including language breakdown.

        Returns:
            Dictionary with enhanced repo statistics
        """
        try:
            files = self.get_file_paths()
            language_stats = self.get_language_statistics()

            # Count files by extension (legacy compatibility)
            extensions = {}
            for file_path in files:
                ext = file_path.suffix.lower() or 'no_extension'
                extensions[ext] = extensions.get(ext, 0) + 1

            # Get commit count (if not shallow)
            commit_count = "N/A (shallow clone)"
            if not self.shallow:
                try:
                    result = self._run_git_command(['git', 'rev-list', '--count', 'HEAD'])
                    commit_count = int(result.stdout.strip())
                except:
                    pass

            # Detect primary language
            if language_stats:
                primary_language = max(language_stats.items(), key=lambda x: x[1]['count'])
                primary_lang_name = primary_language[0]
                primary_lang_count = primary_language[1]['count']
            else:
                primary_lang_name = "Unknown"
                primary_lang_count = 0

            return {
                'total_files': len(files),
                'primary_language': primary_lang_name,
                'primary_language_files': primary_lang_count,
                'language_breakdown': language_stats,
                'file_extensions': extensions,  # Legacy compatibility
                'current_ref': self.get_current_ref(),
                'commit_count': commit_count,
                'branches': self.list_branches() if not self.shallow else "N/A (shallow clone)",
                'tags': self.list_tags() if not self.shallow else "N/A (shallow clone)",
                'polyglot_support': True,
                'supported_languages': len(language_stats)
            }
        except Exception as e:
            logger.error(f"Error getting repo stats: {e}")
            return {'error': str(e)}

    def cleanup(self):
        """
        Removes the temporary directory and all its contents.
        """
        try:
            self.temp_dir_handle.cleanup()
            logger.info(f"Cleaned up temporary directory: {self.repo_path}")
        except Exception as e:
            logger.warning(f"Error during cleanup: {e}")

    # Enhanced utility methods
    def find_files_by_language(self, language: str) -> List[pathlib.Path]:
        """
        Find files by programming language name.

        Args:
            language: Language name (e.g., 'Python', 'JavaScript', 'Rust')

        Returns:
            List of file paths for that language
        """
        language_stats = self.get_language_statistics()
        matching_files = []

        if language in language_stats:
            extensions = language_stats[language]['extensions']
            all_files = self.get_file_paths()

            for file_path in all_files:
                ext = file_path.suffix.lower() or 'no_extension'
                if ext in extensions:
                    matching_files.append(file_path)

        return matching_files

    def get_project_type_hints(self) -> Dict[str, Union[str, List[str]]]:
        """
        Analyze the repository to determine likely project types and frameworks.

        Returns:
            Dictionary with project type information
        """
        files = self.get_file_paths()
        file_names = {f.name for f in files}
        language_stats = self.get_language_statistics()

        project_hints = {
            'primary_language': None,
            'frameworks': [],
            'build_systems': [],
            'package_managers': [],
            'project_types': []
        }

        # Determine primary language
        if language_stats:
            primary_lang = max(language_stats.items(), key=lambda x: x[1]['count'])
            project_hints['primary_language'] = primary_lang[0]

        # Framework detection
        framework_indicators = {
            'React': ['package.json', '.jsx', '.tsx'] + [f for f in file_names if 'react' in f.lower()],
            'Vue.js': ['vue.config.js', '.vue'] + [f for f in file_names if 'vue' in f.lower()],
            'Angular': ['angular.json', '.component.ts'] + [f for f in file_names if 'angular' in f.lower()],
            'Next.js': ['next.config.js'] + [f for f in file_names if 'next' in f.lower()],
            'Svelte': ['svelte.config.js', '.svelte'],
            'Django': ['manage.py', 'wsgi.py'] + [f for f in file_names if 'django' in f.lower()],
            'Flask': [f for f in file_names if 'flask' in f.lower()],
            'FastAPI': [f for f in file_names if 'fastapi' in f.lower()],
            'Spring': ['pom.xml'] + [f for f in file_names if 'spring' in f.lower()],
            'Express': [f for f in file_names if 'express' in f.lower()],
            'Ruby on Rails': ['Gemfile', 'config.ru'] + [f for f in file_names if 'rails' in f.lower()],
            'Laravel': ['composer.json'] + [f for f in file_names if 'laravel' in f.lower()],
            'Unity': [f for f in file_names if f.endswith('.unity')],
            'Godot': ['project.godot', '.gd'],
            'Flutter': ['pubspec.yaml', '.dart'],
            'React Native': ['metro.config.js'] + [f for f in file_names if 'react-native' in f.lower()],
        }

        for framework, indicators in framework_indicators.items():
            if any(indicator in file_names or any(f.endswith(indicator) for f in file_names) for indicator in indicators):
                project_hints['frameworks'].append(framework)

        # Build system detection
        build_systems = {
            'Make': ['Makefile', 'makefile', 'GNUmakefile'],
            'CMake': ['CMakeLists.txt'],
            'Gradle': ['build.gradle', 'gradlew'],
            'Maven': ['pom.xml'],
            'Cargo': ['Cargo.toml'],
            'npm': ['package.json'],
            'Webpack': ['webpack.config.js'],
            'Vite': ['vite.config.js'],
            'Bazel': ['WORKSPACE', 'BUILD'],
        }

        for build_system, files_list in build_systems.items():
            if any(f in file_names for f in files_list):
                project_hints['build_systems'].append(build_system)

        # Package manager detection
        package_managers = {
            'npm': ['package-lock.json'],
            'yarn': ['yarn.lock'],
            'pnpm': ['pnpm-lock.yaml'],
            'pip': ['requirements.txt'],
            'poetry': ['poetry.lock'],
            'pipenv': ['Pipfile'],
            'conda': ['environment.yml'],
            'cargo': ['Cargo.lock'],
            'composer': ['composer.lock'],
            'bundler': ['Gemfile.lock'],
        }

        for pm, files_list in package_managers.items():
            if any(f in file_names for f in files_list):
                project_hints['package_managers'].append(pm)

        # Project type classification
        if 'package.json' in file_names:
            project_hints['project_types'].append('Node.js Application')
        if any(f.endswith('.py') for f in file_names):
            project_hints['project_types'].append('Python Application')
        if 'Cargo.toml' in file_names:
            project_hints['project_types'].append('Rust Application')
        if any(f.endswith('.java') for f in file_names):
            project_hints['project_types'].append('Java Application')
        if 'go.mod' in file_names:
            project_hints['project_types'].append('Go Application')
        if any(f.endswith('.cs') for f in file_names):
            project_hints['project_types'].append('.NET Application')
        if 'Dockerfile' in file_names:
            project_hints['project_types'].append('Containerized Application')
        if any(f.endswith('.tf') for f in file_names):
            project_hints['project_types'].append('Infrastructure as Code')

        return project_hints

    # Backward compatibility aliases
    def find_files_by_extension(self, extension: str) -> List[pathlib.Path]:
        """
        Backward compatibility method to find files by extension.
        """
        all_files = self.get_file_paths()
        return [f for f in all_files if f.suffix.lower() == extension.lower()]

--- FILE_END: backend/ingestion/crawler.py ---

--- FILE_START: backend/build_parsers.py ---
#!/usr/bin/env python3
"""
Enhanced Tree-sitter Parser Builder for Lumiere Core
Builds parsers for all supported languages in your polyglot application.
Now handles grammars that require generation step.
"""

import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

try:
    from tree_sitter import Language
except ImportError:
    print("✗ 'tree_sitter' library not found. Install with:")
    print("  pip install tree-sitter==0.20.1")
    sys.exit(1)

# --- Enhanced Configuration ---
GRAMMARS_DIR = Path("lumiere_core/services/grammars")
BUILD_DIR = Path("lumiere_core/services/build")
LIBRARY_PATH = BUILD_DIR / "my-languages.so"

# Comprehensive language mappings based on your polyglot config
LANGUAGE_GRAMMARS = {
    # Web Technologies
    'javascript': {
        'url': 'https://github.com/tree-sitter/tree-sitter-javascript',
        'extensions': ['.js', '.jsx', '.mjs'],
        'priority': 'high'
    },
    # 'typescript': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-typescript',
    #     'extensions': ['.ts', '.tsx'],
    #     'priority': 'high',
    #     'subdirs': ['typescript', 'tsx']  # TypeScript repo has multiple grammars
    # },
    'html': {
        'url': 'https://github.com/tree-sitter/tree-sitter-html',
        'extensions': ['.html', '.htm'],
        'priority': 'medium'
    },
    'css': {
        'url': 'https://github.com/tree-sitter/tree-sitter-css',
        'extensions': ['.css', '.scss', '.sass'],
        'priority': 'medium'
    },
    # 'vue': {
    #     'url': 'https://github.com/ikatyang/tree-sitter-vue',
    #     'extensions': ['.vue'],
    #     'priority': 'medium'
    # },

    # Backend Languages
    'python': {
        'url': 'https://github.com/tree-sitter/tree-sitter-python',
        'extensions': ['.py', '.pyx', '.pyi'],
        'priority': 'high'
    },
    'java': {
        'url': 'https://github.com/tree-sitter/tree-sitter-java',
        'extensions': ['.java'],
        'priority': 'high'
    },
    'c_sharp': {
        'url': 'https://github.com/tree-sitter/tree-sitter-c-sharp',
        'extensions': ['.cs'],
        'priority': 'high'
    },
    'go': {
        'url': 'https://github.com/tree-sitter/tree-sitter-go',
        'extensions': ['.go'],
        'priority': 'high'
    },
    'rust': {
        'url': 'https://github.com/tree-sitter/tree-sitter-rust',
        'extensions': ['.rs'],
        'priority': 'high',
        'requires_generate': True  
    },
    'swift': {
        'url': 'https://github.com/tree-sitter/tree-sitter-swift',
        'extensions': ['.swift'],
        'priority': 'medium',
        'requires_generate': True
    },

    # Functional Languages
    'haskell': {
        'url': 'https://github.com/tree-sitter/tree-sitter-haskell',
        'extensions': ['.hs', '.lhs'],
        'priority': 'medium'
    },
    'elixir': {
        'url': 'https://github.com/elixir-lang/tree-sitter-elixir',
        'extensions': ['.ex', '.exs'],
        'priority': 'medium',
        'requires_generate': True
    },
    'clojure': {
        'url': 'https://github.com/sogaiu/tree-sitter-clojure',
        'extensions': ['.clj', '.cljs', '.cljc'],
        'priority': 'medium'
    },

    # Systems Languages
    'c': {
        'url': 'https://github.com/tree-sitter/tree-sitter-c',
        'extensions': ['.c', '.h'],
        'priority': 'high'
    },
    # 'cpp': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-cpp',
    #     'extensions': ['.cpp', '.cxx', '.cc', '.hpp', '.hxx'],
    #     'priority': 'high'
    # },

    # Scripting Languages
    'ruby': {
        'url': 'https://github.com/tree-sitter/tree-sitter-ruby',
        'extensions': ['.rb', '.rake'],
        'priority': 'medium'
    },
    # 'php': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-php',
    #     'extensions': ['.php'],
    #     'priority': 'medium',
    #     'requires_generate': True  # This grammar needs generation step
    # },

    # Mobile/Cross-platform
    'kotlin': {
        'url': 'https://github.com/fwcd/tree-sitter-kotlin',
        'extensions': ['.kt', '.kts'],
        'priority': 'medium',
        'requires_generate': True
    },
    'dart': {
        'url': 'https://github.com/UserNobody14/tree-sitter-dart',
        'extensions': ['.dart'],
        'priority': 'medium',
        'requires_generate': True
    },

    # Data Science
    'r': {
        'url': 'https://github.com/r-lib/tree-sitter-r',
        'extensions': ['.r', '.R'],
        'priority': 'low'
    },
    'julia': {
        'url': 'https://github.com/tree-sitter/tree-sitter-julia',
        'extensions': ['.jl'],
        'priority': 'low'
    },

    # Additional useful languages
    'bash': {
        'url': 'https://github.com/tree-sitter/tree-sitter-bash',
        'extensions': ['.sh', '.bash'],
        'priority': 'medium'
    },
    'json': {
        'url': 'https://github.com/tree-sitter/tree-sitter-json',
        'extensions': ['.json'],
        'priority': 'high'
    },
    # 'yaml': {
    #     'url': 'https://github.com/ikatyang/tree-sitter-yaml',
    #     'extensions': ['.yml', '.yaml'],
    #     'priority': 'medium'
    # },
    'toml': {
        'url': 'https://github.com/ikatyang/tree-sitter-toml',
        'extensions': ['.toml'],
        'priority': 'low'
    },
    # 'sql': {
    #     'url': 'https://github.com/derekstride/tree-sitter-sql',
    #     'extensions': ['.sql'],
    #     'priority': 'medium',
    #     'requires_generate': True
    # }
}

# Languages known to require generation or have build issues
REQUIRES_GENERATION = {'php', 'haskell', 'ocaml', 'agda', 'sql', 'swift', 'kotlin', 'dart', 'elixir', 'vue'}

class ParserBuilder:
    def __init__(self, languages_subset: Optional[List[str]] = None,
                 priority_filter: Optional[str] = None):
        self.languages_subset = languages_subset
        self.priority_filter = priority_filter
        self.failed_languages = []
        self.successful_languages = []
        self.skipped_languages = []

    def check_prerequisites(self) -> bool:
        """Check if all required tools are available."""
        logger.info("Checking prerequisites...")

        # Check if we're in the right directory
        if not Path("manage.py").exists() or not Path("lumiere_core").is_dir():
            logger.error("This script must be run from the 'backend' directory")
            logger.error("Please run: cd backend && python build_parsers.py")
            return False

        # Check Git
        try:
            subprocess.run(["git", "--version"], check=True,
                         capture_output=True, text=True)
            logger.info("✓ Git is available")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("✗ Git is required but not found in PATH")
            return False

        # Check for C/C++ compiler
        compilers = ['gcc', 'clang', 'cl']  # Windows, Unix
        compiler_found = False
        for compiler in compilers:
            try:
                subprocess.run([compiler, '--version'], check=True,
                             capture_output=True, text=True)
                logger.info(f"✓ C/C++ compiler found: {compiler}")
                compiler_found = True
                break
            except (subprocess.CalledProcessError, FileNotFoundError):
                continue

        if not compiler_found:
            logger.error("✗ No C/C++ compiler found")
            logger.error("Install build tools:")
            logger.error("  macOS: xcode-select --install")
            logger.error("  Ubuntu/Debian: sudo apt install build-essential")
            logger.error("  Fedora/CentOS: sudo dnf groupinstall 'Development Tools'")
            return False

        # Check Node.js/npm (needed for grammar generation)
        try:
            subprocess.run(["node", "--version"], check=True,
                         capture_output=True, text=True)
            subprocess.run(["npm", "--version"], check=True,
                         capture_output=True, text=True)
            logger.info("✓ Node.js and npm are available")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.warning("⚠ Node.js/npm not found - some grammars may fail to build")
            logger.warning("  Install from: https://nodejs.org/")

        return True

    def get_languages_to_build(self) -> Dict[str, dict]:
        """Get the filtered list of languages to build."""
        languages = LANGUAGE_GRAMMARS.copy()

        # Filter by subset if specified
        if self.languages_subset:
            languages = {k: v for k, v in languages.items()
                        if k in self.languages_subset}

        # Filter by priority if specified
        if self.priority_filter:
            languages = {k: v for k, v in languages.items()
                        if v.get('priority', 'medium') == self.priority_filter}

        return languages

    def clone_or_update_grammar(self, name: str, config: dict) -> bool:
        """Clone or update a single grammar repository."""
        lang_path = GRAMMARS_DIR / name
        url = config['url']

        try:
            if lang_path.is_dir():
                logger.info(f"Updating {name}...")
                result = subprocess.run(
                    ["git", "pull", "--quiet"],
                    cwd=lang_path,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                if result.returncode != 0:
                    logger.warning(f"Failed to update {name}: {result.stderr}")
                    return False
            else:
                logger.info(f"Cloning {name}...")
                result = subprocess.run(
                    ["git", "clone", "--depth", "1", "--quiet", url, str(lang_path)],
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                if result.returncode != 0:
                    logger.error(f"Failed to clone {name}: {result.stderr}")
                    return False

            return True

        except subprocess.TimeoutExpired:
            logger.error(f"Timeout while processing {name}")
            return False
        except Exception as e:
            logger.error(f"Error processing {name}: {e}")
            return False

    def generate_parser_if_needed(self, name: str, lang_path: Path) -> bool:
        """Generate parser.c if the grammar requires it."""
        # Check all common source locations for an existing parser file
        src_locations = ["src", "php/src", "php_only/src", "."]
        for loc in src_locations:
            if list((lang_path / loc).glob("parser.c")):
                # Found it, no need to generate
                return True

        # If not found, and it's a language that requires generation, then run the build steps.
        if name in REQUIRES_GENERATION or LANGUAGE_GRAMMARS[name].get('requires_generate'):
            logger.info(f"Generating parser for {name}...")

            if not (lang_path / "package.json").exists():
                logger.warning(f"Generation required for {name} but no package.json found. Skipping generation.")
                return False

            try:
                # Use npm ci for faster, more reliable installs if a lock file exists
                install_command = ["npm", "ci"] if (lang_path / "package-lock.json").exists() else ["npm", "install"]

                logger.info(f"Installing dependencies for {name} using '{' '.join(install_command)}'...")
                # We need to run with shell=True on Windows for npm.cmd, and it's generally safer for npm scripts.
                result = subprocess.run(
                    install_command,
                    cwd=lang_path,
                    check=True,
                    capture_output=True,
                    text=True,
                    timeout=300, # Increased timeout for slow installs
                    shell=sys.platform == 'win32'
                )
                if result.stdout: logger.debug(f"npm install stdout for {name}: {result.stdout}")
                if result.stderr: logger.warning(f"npm install stderr for {name}: {result.stderr}")

                # After installation, re-check for the generated parser file. Some packages generate on install.
                for loc in src_locations:
                    if list((lang_path / loc).glob("parser.c")):
                        logger.info(f"✓ Parser for {name} was generated during npm install.")
                        return True

                # If it wasn't generated during install, try the `generate` command explicitly.
                logger.info(f"Attempting explicit 'tree-sitter generate' for {name}...")
                subprocess.run(
                    ["npx", "tree-sitter", "generate"],
                    cwd=lang_path,
                    check=True,
                    capture_output=True,
                    text=True,
                    timeout=60,
                    shell=sys.platform == 'win32'
                )

                # Final check
                for loc in src_locations:
                    if list((lang_path / loc).glob("parser.c")):
                        logger.info(f"✓ Successfully generated parser for {name}")
                        return True

                logger.error(f"Failed to generate parser.c for {name} after all steps.")
                return False

            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to generate parser for {name}. Command: '{e.cmd}'. Stderr: {e.stderr}. Stdout: {e.stdout}")
                return False
            except subprocess.TimeoutExpired:
                logger.error(f"Timeout generating parser for {name}")
                return False

        # Grammar doesn't require generation, but we didn't find the file.
        logger.warning(f"No parser.c found for {name}, and it is not marked for generation.")
        return False

    def clone_grammars_parallel(self, languages: Dict[str, dict]) -> List[str]:
        """Clone/update all grammars in parallel for speed."""
        logger.info(f"Processing {len(languages)} language grammars...")
        GRAMMARS_DIR.mkdir(parents=True, exist_ok=True)

        successful_langs = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_lang = {
                executor.submit(self.clone_or_update_grammar, name, config): name
                for name, config in languages.items()
            }

            for future in as_completed(future_to_lang):
                lang_name = future_to_lang[future]
                try:
                    if future.result():
                        # Check if parser generation is needed
                        lang_path = GRAMMARS_DIR / lang_name
                        if self.generate_parser_if_needed(lang_name, lang_path):
                            successful_langs.append(lang_name)
                        else:
                            logger.warning(f"Skipping {lang_name} - parser generation failed")
                            self.skipped_languages.append(lang_name)
                    else:
                        self.failed_languages.append(lang_name)
                except Exception as e:
                    logger.error(f"Unexpected error with {lang_name}: {e}")
                    self.failed_languages.append(lang_name)

        return successful_langs

    def build_library(self, successful_languages: List[str]) -> bool:
        """Build the shared library from successfully cloned grammars."""
        if not successful_languages:
            logger.error("No languages available to build")
            return False

        logger.info(f"Building library with {len(successful_languages)} languages...")
        BUILD_DIR.mkdir(parents=True, exist_ok=True)

        # Prepare grammar paths, handling special cases
        grammar_paths = []
        for lang in successful_languages:
            lang_path = GRAMMARS_DIR / lang
            config = LANGUAGE_GRAMMARS[lang]

            # Handle languages with subdirectories (like TypeScript)
            if 'subdirs' in config:
                for subdir in config['subdirs']:
                    subdir_path = lang_path / subdir
                    if subdir_path.exists():
                        grammar_paths.append(str(subdir_path))
            else:
                grammar_paths.append(str(lang_path))

        # Remove any paths that don't exist
        grammar_paths = [p for p in grammar_paths if Path(p).exists()]

        if not grammar_paths:
            logger.error("No valid grammar paths found")
            return False

        # Try to build with all grammars first
        logger.info(f"Compiling {len(grammar_paths)} grammars into {LIBRARY_PATH}")

        try:
            Language.build_library(str(LIBRARY_PATH), grammar_paths)
            logger.info("✓ Successfully built language library")

            # Verify the library was created and has reasonable size
            if LIBRARY_PATH.exists():
                size_mb = LIBRARY_PATH.stat().st_size / (1024 * 1024)
                logger.info(f"Library size: {size_mb:.1f} MB")
                return True
            else:
                logger.error("Library file was not created")
                return False

        except Exception as e:
            logger.error(f"Failed to build library: {e}")

            # If it's a C++ compilation error and Vue is in the mix, try without Vue
            if "scanner.cc" in str(e) and any("vue" in p for p in grammar_paths):
                logger.warning("Retrying build without Vue grammar due to C++ compilation issue...")

                # Remove Vue from successful languages and grammar paths
                if 'vue' in successful_languages:
                    successful_languages.remove('vue')
                    self.skipped_languages.append('vue')

                grammar_paths = [p for p in grammar_paths if "vue" not in p]

                if grammar_paths:
                    try:
                        Language.build_library(str(LIBRARY_PATH), grammar_paths)
                        logger.info("✓ Successfully built language library (without Vue)")

                        if LIBRARY_PATH.exists():
                            size_mb = LIBRARY_PATH.stat().st_size / (1024 * 1024)
                            logger.info(f"Library size: {size_mb:.1f} MB")
                            return True
                    except Exception as e2:
                        logger.error(f"Second build attempt failed: {e2}")
                        return False

            logger.error("This usually indicates a missing C/C++ compiler or development headers")
            return False

    def generate_language_mapping(self, successful_languages: List[str]) -> None:
        """Generate a language mapping file for the application."""
        mapping_file = BUILD_DIR / "language_mapping.json"

        mapping = {}
        for lang in successful_languages:
            config = LANGUAGE_GRAMMARS[lang]
            for ext in config.get('extensions', []):
                mapping[ext] = lang

        try:
            with open(mapping_file, 'w') as f:
                json.dump(mapping, f, indent=2, sort_keys=True)
            logger.info(f"Generated language mapping: {mapping_file}")
        except Exception as e:
            logger.warning(f"Could not generate language mapping: {e}")

    def print_summary(self, successful_languages: List[str]) -> None:
        """Print build summary."""
        logger.info("=" * 60)
        logger.info("BUILD SUMMARY")
        logger.info("=" * 60)

        if successful_languages:
            logger.info(f"✓ Successfully built {len(successful_languages)} languages:")
            for lang in sorted(successful_languages):
                extensions = LANGUAGE_GRAMMARS[lang].get('extensions', [])
                logger.info(f"  {lang:<12} -> {', '.join(extensions)}")

        if self.skipped_languages:
            logger.warning(f"⚠ Skipped {len(self.skipped_languages)} languages (generation failed):")
            for lang in sorted(self.skipped_languages):
                logger.warning(f"  {lang}")

        if self.failed_languages:
            logger.warning(f"✗ Failed to build {len(self.failed_languages)} languages:")
            for lang in sorted(self.failed_languages):
                logger.warning(f"  {lang}")

        logger.info("=" * 60)
        logger.info("Ready to start your server with: ./run_server.sh")

    def build(self) -> bool:
        """Main build process."""
        if not self.check_prerequisites():
            return False

        languages = self.get_languages_to_build()
        if not languages:
            logger.error("No languages selected for building")
            return False

        successful_languages = self.clone_grammars_parallel(languages)

        if not successful_languages:
            logger.error("No languages were successfully downloaded")
            return False

        if not self.build_library(successful_languages):
            return False

        self.generate_language_mapping(successful_languages)
        self.print_summary(successful_languages)

        return True

def main():
    """Main entry point with CLI argument support."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Build Tree-sitter parsers for Lumiere Core"
    )
    parser.add_argument(
        '--languages',
        nargs='+',
        help='Specific languages to build (default: all)'
    )
    parser.add_argument(
        '--priority',
        choices=['high', 'medium', 'low'],
        help='Build only languages with specific priority'
    )
    parser.add_argument(
        '--list-languages',
        action='store_true',
        help='List all available languages and exit'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Clean existing grammars and build directories'
    )
    parser.add_argument(
        '--skip-problematic',
        action='store_true',
        help='Skip languages that commonly fail (PHP, Haskell, etc.)'
    )

    args = parser.parse_args()

    if args.list_languages:
        print("Available languages:")
        for name, config in sorted(LANGUAGE_GRAMMARS.items()):
            priority = config.get('priority', 'medium')
            extensions = ', '.join(config.get('extensions', []))
            needs_gen = " [requires generation]" if config.get('requires_generate') or name in REQUIRES_GENERATION else ""
            print(f"  {name:<12} [{priority:<6}] -> {extensions}{needs_gen}")
        return

    if args.clean:
        logger.info("Cleaning existing directories...")
        for path in [GRAMMARS_DIR, BUILD_DIR]:
            if path.exists():
                shutil.rmtree(path)
                logger.info(f"Removed {path}")

    # Handle skip-problematic flag
    languages_to_skip = set()
    if args.skip_problematic:
        languages_to_skip = REQUIRES_GENERATION.copy()
        logger.info(f"Skipping problematic languages: {', '.join(sorted(languages_to_skip))}")

    # Filter out skipped languages
    if languages_to_skip and not args.languages:
        for lang in languages_to_skip:
            if lang in LANGUAGE_GRAMMARS:
                del LANGUAGE_GRAMMARS[lang]

    builder = ParserBuilder(
        languages_subset=args.languages,
        priority_filter=args.priority
    )

    success = builder.build()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

--- FILE_END: backend/build_parsers.py ---

--- FILE_START: backend/requirements.txt ---
# In backend/requirements.txt

# Django Core
Django
djangorestframework

# LLM & Vector Database
ollama
faiss-cpu
numpy
tqdm
networkx

# Web Scraping (Legacy, but keep for now)
requests
beautifulsoup4

# GitHub API Client
PyGithub

# --- loading .env files ---
python-dotenv

# --- The Conductor CLI ---
typer[all]
rich
prompt-toolkit

# --- The Crucible service ---
docker

# --- Google Gemini API ---
google-generativeai

#  --- The Polyglot Cartographer ---
tree_sitter==0.20.1

# Enhanced BOM Parser Dependencies
toml
PyYAML>=6.0                     # For parsing docker-compose.yml files
dataclasses-json>=0.5.7         # For enhanced dataclass serialization (Python < 3.7)

--- FILE_END: backend/requirements.txt ---

--- FILE_START: backend/lumiere_core/asgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/asgi.py
"""
ASGI config for backend project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_asgi_application()

--- FILE_END: backend/lumiere_core/asgi.py ---

--- FILE_START: backend/lumiere_core/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/__init__.py

--- FILE_END: backend/lumiere_core/__init__.py ---

--- FILE_START: backend/lumiere_core/.gitignore ---
# In ~/lumiere_semantique/backend/lumiere_core/.gitignore
# Python
__pycache__/
*.pyc

# Virtual Environment
venv/

# Django
db.sqlite3
*.log

# Environment variables
.env

--- FILE_END: backend/lumiere_core/.gitignore ---

--- FILE_START: backend/lumiere_core/.env ---
ANTHROPIC_API_KEY="sk-ant-your-api-key-here"

GITHUB_ACCESS_TOKEN="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"

# The GitHub username the agent will use to fork repos and create PRs.
GITHUB_FORK_USERNAME="LatchyCat"

--- FILE_END: backend/lumiere_core/.env ---

--- FILE_START: backend/lumiere_core/settings.py ---
# In ~/lumiere_semantique/backend/lumiere_core/settings.py
"""
Django settings for lumiere_core project.

Generated by 'django-admin startproject' using Django 5.2.3.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.2/ref/settings/
"""

from pathlib import Path
from dotenv import load_dotenv

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent

load_dotenv(BASE_DIR / '.env')

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = "django-insecure-7f#&l-vg3lb9%s5lkx!352hf2^&!w%ro6wa97*kqm@8+d94*67"

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",

    # --- Third-party apps ---
    'rest_framework',

    # --- Our local apps ---
    'ingestion',
    'api',
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
]

# This should already be correct from our previous fixes.
ROOT_URLCONF = 'lumiere_core.urls'

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

# This should also be correct, but ensure it points to 'lumiere_core'.
WSGI_APPLICATION = "lumiere_core.wsgi.application"


# Database
# https://docs.djangoproject.com/en/5.2/ref/settings/#databases

DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.sqlite3",
        "NAME": BASE_DIR / "db.sqlite3",
    }
}


# Password validation
# https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        "NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",
    },
]


# Internationalization
# https://docs.djangoproject.com/en/5.2/topics/i18n/

LANGUAGE_CODE = "en-us"

TIME_ZONE = "UTC"

USE_I18N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/5.2/howto/static-files/

STATIC_URL = "static/"

# Default primary key field type
# https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"

# --- Add this section for Django REST Framework ---
# This allows DRF to have sensible default settings.
REST_FRAMEWORK = {
    'DEFAULT_RENDERER_CLASSES': [
        'rest_framework.renderers.JSONRenderer',
    ],
    # Use BrowsableAPIRenderer only during development for easier debugging
    'DEFAULT_PARSER_CLASSES': [
        'rest_framework.parsers.JSONParser',
    ]
}

--- FILE_END: backend/lumiere_core/settings.py ---

--- FILE_START: backend/lumiere_core/urls.py ---
# In ~/lumiere_semantique/backend/lumiere_core/urls.py
# In lumiere_core/urls.py

from django.contrib import admin
from django.urls import path, include # <-- Make sure 'include' is imported

urlpatterns = [
    path('admin/', admin.site.urls),

    # This line tells Django that any URL starting with 'api/v1/'
    # should be handled by the URL patterns defined in our 'api.urls' file.
    path('api/v1/', include('api.urls')),
]

--- FILE_END: backend/lumiere_core/urls.py ---

--- FILE_START: backend/lumiere_core/services/diff_parser.py ---
# backend/lumiere_core/services/diff_parser.py

import re
import logging
from typing import Dict, List, Set, Tuple, Optional, Union
from dataclasses import dataclass
from pathlib import Path

# Set up logging
logger = logging.getLogger(__name__)

@dataclass
class DiffStats:
    """Statistics about a parsed diff."""
    total_files_changed: int
    total_lines_added: int
    total_lines_removed: int
    affected_nodes: int
    files_with_unknown_nodes: List[str]

@dataclass
class FileChange:
    """Represents changes to a single file."""
    file_path: str
    lines_added: int
    lines_removed: int
    changed_lines: Set[int]
    is_new_file: bool = False
    is_deleted_file: bool = False

class DiffParseError(Exception):
    """Custom exception for diff parsing errors."""
    pass

def get_changed_files_from_diff(diff_text: str) -> List[str]:
    """
    Extracts a list of file paths that were changed in the diff.

    Args:
        diff_text: The full text of a git diff

    Returns:
        Sorted list of unique file paths that were modified

    Raises:
        DiffParseError: If the diff text is malformed or empty
    """
    if not diff_text or not diff_text.strip():
        logger.warning("Empty or whitespace-only diff text provided")
        return []

    try:
        # Enhanced pattern to handle various diff formats
        pattern = re.compile(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', re.MULTILINE)
        matches = pattern.findall(diff_text)

        # Filter out /dev/null (used for new/deleted files) and empty matches
        file_paths = [
            match.strip() for match in matches
            if match.strip() and match.strip() != '/dev/null'
        ]

        if not file_paths:
            logger.info("No file changes detected in diff")

        return sorted(list(set(file_paths)))

    except re.error as e:
        raise DiffParseError(f"Regex compilation error: {e}") from e
    except Exception as e:
        raise DiffParseError(f"Unexpected error parsing diff: {e}") from e

def get_changed_files_with_stats(diff_text: str) -> List[FileChange]:
    """
    Enhanced version that returns detailed information about each changed file.

    Args:
        diff_text: The full text of a git diff

    Returns:
        List of FileChange objects with detailed statistics
    """
    if not diff_text or not diff_text.strip():
        return []

    file_changes = {}

    # Split diff by file sections
    file_sections = re.split(r'^diff --git', diff_text, flags=re.MULTILINE)

    for section in file_sections:
        if not section.strip():
            continue

        # Extract file path
        file_match = re.search(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', section, re.MULTILINE)
        if not file_match:
            continue

        file_path = file_match.group(1).strip()
        if file_path == '/dev/null':
            continue

        # Check if it's a new or deleted file
        is_new_file = '/dev/null' in re.search(r'^---\s(.+?)(?:\s|$)', section, re.MULTILINE).group(1) if re.search(r'^---\s(.+?)(?:\s|$)', section, re.MULTILINE) else False
        is_deleted_file = file_path == '/dev/null' or '--- a/' + file_path in section and '+++ /dev/null' in section

        # Count line changes and extract changed line numbers
        lines_added = len(re.findall(r'^\+(?!\+)', section, re.MULTILINE))
        lines_removed = len(re.findall(r'^-(?!-)', section, re.MULTILINE))

        # Extract changed line numbers
        changed_lines = set()
        hunk_headers = re.findall(r'^@@\s-\d+(?:,\d+)?\s\+(\d+)(?:,(\d+))?\s@@', section, re.MULTILINE)

        for start_line_str, length_str in hunk_headers:
            start_line = int(start_line_str)
            length = int(length_str) if length_str else 1
            changed_lines.update(range(start_line, start_line + length))

        file_changes[file_path] = FileChange(
            file_path=file_path,
            lines_added=lines_added,
            lines_removed=lines_removed,
            changed_lines=changed_lines,
            is_new_file=is_new_file,
            is_deleted_file=is_deleted_file
        )

    return sorted(file_changes.values(), key=lambda x: x.file_path)

def parse_diff_to_nodes(diff_text: str, file_to_node_map: Dict[str, List[Dict]]) -> List[str]:
    """
    Parses a git diff and, using a map of files to their nodes (functions, classes),
    identifies which specific nodes were modified.

    Args:
        diff_text: The full text of a `git diff`.
        file_to_node_map: A dictionary mapping file paths to a list of their contained
                          nodes, where each node has a 'name', 'start_line', and 'end_line'.
                          e.g., {'src/main.py': [{'id': '...', 'start_line': 10, 'end_line': 25}]}

    Returns:
        A list of unique node IDs that were affected by the changes in the diff.

    Raises:
        DiffParseError: If the diff text is malformed
        ValueError: If file_to_node_map has invalid structure
    """
    if not diff_text or not diff_text.strip():
        logger.warning("Empty diff text provided to parse_diff_to_nodes")
        return []

    if not isinstance(file_to_node_map, dict):
        raise ValueError("file_to_node_map must be a dictionary")

    # Validate node map structure
    _validate_node_map(file_to_node_map)

    affected_node_ids: Set[str] = set()

    try:
        # Split the diff by file sections - more robust approach
        file_sections = re.split(r'^diff --git', diff_text, flags=re.MULTILINE)

        for section in file_sections:
            if not section.strip():
                continue

            # Extract file path with better error handling
            file_path_match = re.search(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', section, re.MULTILINE)
            if not file_path_match:
                continue

            current_file = file_path_match.group(1).strip()

            # Skip /dev/null (new/deleted files)
            if current_file == '/dev/null':
                continue

            # If we don't have a map for this file, log it and continue
            if current_file not in file_to_node_map:
                logger.debug(f"No node mapping found for file: {current_file}")
                continue

            # Extract changed lines more efficiently
            changed_lines = _extract_changed_lines(section)

            if not changed_lines:
                continue

            # Find affected nodes
            affected_nodes = _find_affected_nodes(
                file_to_node_map[current_file],
                changed_lines
            )
            affected_node_ids.update(affected_nodes)

        result = sorted(list(affected_node_ids))
        logger.info(f"Found {len(result)} affected nodes from diff")
        return result

    except re.error as e:
        raise DiffParseError(f"Regex error while parsing diff: {e}") from e
    except Exception as e:
        raise DiffParseError(f"Unexpected error parsing diff to nodes: {e}") from e

def parse_diff_to_nodes_with_stats(
    diff_text: str,
    file_to_node_map: Dict[str, List[Dict]]
) -> Tuple[List[str], DiffStats]:
    """
    Enhanced version that returns both affected nodes and detailed statistics.

    Args:
        diff_text: The full text of a git diff
        file_to_node_map: Dictionary mapping file paths to their nodes

    Returns:
        Tuple of (affected_node_ids, diff_stats)
    """
    if not diff_text or not diff_text.strip():
        return [], DiffStats(0, 0, 0, 0, [])

    file_changes = get_changed_files_with_stats(diff_text)
    affected_node_ids = parse_diff_to_nodes(diff_text, file_to_node_map)

    files_with_unknown_nodes = [
        fc.file_path for fc in file_changes
        if fc.file_path not in file_to_node_map
    ]

    total_lines_added = sum(fc.lines_added for fc in file_changes)
    total_lines_removed = sum(fc.lines_removed for fc in file_changes)

    stats = DiffStats(
        total_files_changed=len(file_changes),
        total_lines_added=total_lines_added,
        total_lines_removed=total_lines_removed,
        affected_nodes=len(affected_node_ids),
        files_with_unknown_nodes=files_with_unknown_nodes
    )

    return affected_node_ids, stats

def filter_nodes_by_change_type(
    diff_text: str,
    file_to_node_map: Dict[str, List[Dict]],
    change_types: Set[str] = {'added', 'modified', 'deleted'}
) -> Dict[str, List[str]]:
    """
    Categorizes affected nodes by the type of change.

    Args:
        diff_text: The full text of a git diff
        file_to_node_map: Dictionary mapping file paths to their nodes
        change_types: Set of change types to include ('added', 'modified', 'deleted')

    Returns:
        Dictionary mapping change types to lists of affected node IDs
    """
    result = {change_type: [] for change_type in change_types}

    if not diff_text or not diff_text.strip():
        return result

    file_changes = get_changed_files_with_stats(diff_text)

    for file_change in file_changes:
        if file_change.file_path not in file_to_node_map:
            continue

        nodes = file_to_node_map[file_change.file_path]

        if file_change.is_new_file and 'added' in change_types:
            # All nodes in new files are considered added
            result['added'].extend(node['id'] for node in nodes)
        elif file_change.is_deleted_file and 'deleted' in change_types:
            # All nodes in deleted files are considered deleted
            result['deleted'].extend(node['id'] for node in nodes)
        elif 'modified' in change_types:
            # Find nodes that intersect with changed lines
            affected_nodes = _find_affected_nodes(nodes, file_change.changed_lines)
            result['modified'].extend(affected_nodes)

    # Remove duplicates and sort
    for change_type in result:
        result[change_type] = sorted(list(set(result[change_type])))

    return result

def _validate_node_map(file_to_node_map: Dict[str, List[Dict]]) -> None:
    """Validates the structure of the file_to_node_map."""
    for file_path, nodes in file_to_node_map.items():
        if not isinstance(nodes, list):
            raise ValueError(f"Nodes for file {file_path} must be a list")

        for i, node in enumerate(nodes):
            if not isinstance(node, dict):
                raise ValueError(f"Node {i} in file {file_path} must be a dictionary")

            required_keys = {'id', 'start_line', 'end_line'}
            if not required_keys.issubset(node.keys()):
                missing = required_keys - node.keys()
                raise ValueError(f"Node {i} in file {file_path} missing keys: {missing}")

            if not isinstance(node['start_line'], int) or not isinstance(node['end_line'], int):
                raise ValueError(f"Node {i} in file {file_path} line numbers must be integers")

            if node['start_line'] > node['end_line']:
                raise ValueError(f"Node {i} in file {file_path} has start_line > end_line")

def _extract_changed_lines(file_diff_section: str) -> Set[int]:
    """Extracts the set of changed line numbers from a file's diff section."""
    changed_lines: Set[int] = set()

    # Find all hunk headers like '@@ -15,7 +15,9 @@'
    hunk_headers = re.findall(
        r'^@@\s-\d+(?:,\d+)?\s\+(\d+)(?:,(\d+))?\s@@',
        file_diff_section,
        re.MULTILINE
    )

    for start_line_str, length_str in hunk_headers:
        start_line = int(start_line_str)
        # Length defaults to 1 if not specified
        length = int(length_str) if length_str else 1

        # Add all lines in this hunk to the changed lines set
        changed_lines.update(range(start_line, start_line + length))

    return changed_lines

def _find_affected_nodes(nodes: List[Dict], changed_lines: Set[int]) -> List[str]:
    """Finds nodes that are affected by the given changed lines."""
    affected_node_ids = []

    for node in nodes:
        node_line_range = range(node['start_line'], node['end_line'] + 1)

        # Check if any changed line falls within this node's range
        if any(line in node_line_range for line in changed_lines):
            affected_node_ids.append(node['id'])

    return affected_node_ids

# Backward compatibility aliases
def get_files_from_diff(diff_text: str) -> List[str]:
    """Deprecated: Use get_changed_files_from_diff instead."""
    logger.warning("get_files_from_diff is deprecated, use get_changed_files_from_diff")
    return get_changed_files_from_diff(diff_text)

# Utility functions for common use cases
def is_valid_diff(diff_text: str) -> bool:
    """
    Checks if the provided text appears to be a valid git diff.

    Args:
        diff_text: Text to validate

    Returns:
        True if the text appears to be a valid diff, False otherwise
    """
    if not diff_text or not diff_text.strip():
        return False

    # Look for common diff indicators
    diff_indicators = [
        r'^diff --git',
        r'^---\s',
        r'^\+\+\+\s',
        r'^@@.*@@'
    ]

    return any(re.search(pattern, diff_text, re.MULTILINE) for pattern in diff_indicators)

def get_diff_summary(diff_text: str) -> str:
    """
    Returns a human-readable summary of the diff.

    Args:
        diff_text: The full text of a git diff

    Returns:
        A summary string describing the changes
    """
    if not is_valid_diff(diff_text):
        return "Invalid or empty diff"

    file_changes = get_changed_files_with_stats(diff_text)

    if not file_changes:
        return "No file changes detected"

    total_files = len(file_changes)
    total_added = sum(fc.lines_added for fc in file_changes)
    total_removed = sum(fc.lines_removed for fc in file_changes)
    new_files = sum(1 for fc in file_changes if fc.is_new_file)
    deleted_files = sum(1 for fc in file_changes if fc.is_deleted_file)

    parts = [f"{total_files} file{'s' if total_files != 1 else ''} changed"]

    if total_added > 0:
        parts.append(f"{total_added} insertion{'s' if total_added != 1 else ''}")

    if total_removed > 0:
        parts.append(f"{total_removed} deletion{'s' if total_removed != 1 else ''}")

    if new_files > 0:
        parts.append(f"{new_files} new file{'s' if new_files != 1 else ''}")

    if deleted_files > 0:
        parts.append(f"{deleted_files} deleted file{'s' if deleted_files != 1 else ''}")

    return ", ".join(parts)

--- FILE_END: backend/lumiere_core/services/diff_parser.py ---

--- FILE_START: backend/lumiere_core/services/documentation.py ---
# backend/lumiere_core/services/documentation.py
from typing import Dict
# --- All services should import the master llm_service ---
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output


def generate_docstring_for_code(repo_id: str, new_code: str, instruction: str, model_identifier: str) -> Dict[str, str]:
    """
    The core logic for the Chronicler Agent (Documentation).
    It finds existing docstring patterns in the repo and uses them as a style guide
    to generate a new docstring for the provided code.

    Args:
        repo_id: Identifier for the repository
        new_code: The code that needs documentation
        instruction: Instructions for docstring generation
        model_identifier: The model to use for generation

    Returns:
        Dict containing the generated docstring
    """
    print(f"✒️  Initiating Chronicler Agent for repo '{repo_id}'")

    # Step 1: Find Existing Documentation Patterns with RAG
    print("   -> Step 1: Finding existing docstring patterns with RAG...")
    search_query = f"Example docstrings in Python code for a function about: {instruction}"

    try:
        # --- CORRECTED CALL: Pass repo_id directly ---
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=5
        )
    except Exception as e:
        print(f"   -> Warning: RAG search failed for Chronicler: {e}. Proceeding without context examples.")
        context_chunks = []

    doc_context_string = ""
    found_files = set()

    for chunk in context_chunks:
        text = chunk.get('text', '').strip()
        file_path = chunk.get('file_path', '')
        if text.startswith(('def ', 'class ')) and file_path not in found_files:
            doc_context_string += f"--- Example from file \"{file_path}\" ---\n{text}\n\n"
            found_files.add(file_path)

    if not doc_context_string:
        doc_context_string = "No specific docstring styles found. Please generate a standard Google-style docstring."
        print("   -> Warning: No existing docstring examples found via RAG.")
    else:
        print(f"   -> Found docstring patterns from files: {list(found_files)}")

    # Step 2: Construct the Docstring Generation Prompt
    print("   -> Step 2: Constructing docstring generation prompt...")
    prompt = f"""You are an expert technical writer specializing in Python documentation.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING DOCSTRING EXAMPLES"** to learn the project's documentation style (e.g., Google, reStructuredText, numpy). Pay attention to sections like `Args:`, `Returns:`, `Raises:`.
2. **Analyze the "CODE TO BE DOCUMENTED"** to understand its parameters, logic, and what it returns.
3. **Write a complete and professional docstring** for the provided code. It is CRITICAL that you exactly match the style of the examples.
4. **Output ONLY the docstring itself.** Do not include the function definition or any other text, just the \"\"\"...\"\"\" block.

---
### EXISTING DOCSTRING EXAMPLES
{doc_context_string}

---
### CODE TO BE DOCUMENTED
```python
{new_code}
```

Now, generate ONLY the docstring for the code above."""

    # Step 3: Generate and Clean the Docstring
    print(f"   -> Step 3: Sending request to model '{model_identifier}'...")
    raw_docstring = llm_service.generate_text(prompt, model_identifier=model_identifier)

    # Step 4: Clean the docstring output
    print("   -> Step 4: Cleaning and finalizing the docstring...")
    final_docstring = clean_llm_code_output(raw_docstring)

    # Remove surrounding triple quotes if present
    if final_docstring.startswith('"""') and final_docstring.endswith('"""'):
        final_docstring = final_docstring[3:-3].strip()

    return {"docstring": final_docstring}

--- FILE_END: backend/lumiere_core/services/documentation.py ---

--- FILE_START: backend/lumiere_core/services/impact_analyzer.py ---
"""
Impact Analyzer Service

This service calculates the "blast radius" or impact scope for potential code changes
by analyzing the architectural graph and performing bidirectional traversals from seed files.
This is essential for the "Good First Issue" Scout feature to identify issues suitable for newcomers.
"""

import json
import pathlib
from typing import Dict, List, Set, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ImpactAnalyzer:
    """
    Service to analyze the potential impact of code changes by calculating blast radius
    from seed files using the architectural graph generated by the Cartographer.
    """
    
    def __init__(self, cortex_directory: Optional[pathlib.Path] = None):
        """
        Initialize the Impact Analyzer.
        
        Args:
            cortex_directory: Optional path to the cortex directory. 
                            If None, will use default backend/cloned_repositories/
        """
        self.cortex_directory = cortex_directory or pathlib.Path("backend/cloned_repositories")
        
    def analyze_blast_radius(self, repo_id: str, seed_files: List[str], max_depth: int = 2) -> Dict[str, Any]:
        """
        Calculate the blast radius for a potential code change starting from seed files.
        
        Args:
            repo_id: Repository identifier
            seed_files: List of file paths to start the analysis from
            max_depth: Maximum depth for bidirectional traversal (default: 2)
            
        Returns:
            Dictionary containing blast radius analysis:
            {
                'blast_radius': int,           # Number of unique affected files
                'affected_nodes': List[str],   # List of affected file paths
                'depth': int,                  # Actual traversal depth used
                'seed_files': List[str],       # Original seed files
                'upstream_nodes': List[str],   # Dependencies (files that seed files depend on)
                'downstream_nodes': List[str], # Dependents (files that depend on seed files)
                'analysis_successful': bool    # Whether analysis completed successfully
            }
        """
        try:
            # Load the architectural graph
            architectural_graph = self._load_architectural_graph(repo_id)
            
            if not architectural_graph:
                logger.warning(f"No architectural graph found for repo {repo_id}")
                return {
                    'blast_radius': 0,
                    'affected_nodes': [],
                    'depth': 0,
                    'seed_files': seed_files,
                    'upstream_nodes': [],
                    'downstream_nodes': [],
                    'analysis_successful': False,
                    'error': 'No architectural graph available'
                }
            
            # Perform bidirectional graph traversal
            result = self._perform_bidirectional_traversal(
                architectural_graph, seed_files, max_depth
            )
            
            # Calculate blast radius
            all_affected = set(seed_files) | set(result['upstream_nodes']) | set(result['downstream_nodes'])
            
            return {
                'blast_radius': len(all_affected),
                'affected_nodes': sorted(list(all_affected)),
                'depth': max_depth,
                'seed_files': seed_files,
                'upstream_nodes': result['upstream_nodes'],
                'downstream_nodes': result['downstream_nodes'],
                'analysis_successful': True
            }
            
        except Exception as e:
            logger.error(f"Error analyzing blast radius for {repo_id}: {e}")
            return {
                'blast_radius': len(seed_files),  # Fallback to just seed files
                'affected_nodes': seed_files,
                'depth': 0,
                'seed_files': seed_files,
                'upstream_nodes': [],
                'downstream_nodes': [],
                'analysis_successful': False,
                'error': str(e)
            }
    
    def _load_architectural_graph(self, repo_id: str) -> Optional[Dict[str, Any]]:
        """
        Load the architectural graph from the Cortex file.
        
        Args:
            repo_id: Repository identifier
            
        Returns:
            Architectural graph dictionary or None if not found
        """
        cortex_file = self.cortex_directory / repo_id / "cortex.json"
        
        if not cortex_file.exists():
            logger.warning(f"Cortex file not found: {cortex_file}")
            return None
            
        try:
            with open(cortex_file, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)
                
            return cortex_data.get('architectural_graph')
            
        except Exception as e:
            logger.error(f"Error loading cortex file {cortex_file}: {e}")
            return None
    
    def _perform_bidirectional_traversal(
        self, 
        graph: Dict[str, Any], 
        seed_files: List[str], 
        max_depth: int
    ) -> Dict[str, List[str]]:
        """
        Perform bidirectional graph traversal from seed files.
        
        Args:
            graph: Architectural graph structure
            seed_files: Starting files for traversal
            max_depth: Maximum depth to traverse
            
        Returns:
            Dictionary with upstream and downstream nodes
        """
        # Extract nodes and edges from the graph
        nodes = set()
        dependencies = {}  # file -> list of files it depends on
        dependents = {}    # file -> list of files that depend on it
        
        # Parse the graph structure
        if 'nodes' in graph and 'edges' in graph:
            # Graph format: {"nodes": [...], "edges": [...]}
            for node in graph['nodes']:
                file_path = self._extract_file_path_from_node(node)
                if file_path:
                    nodes.add(file_path)
                    dependencies[file_path] = []
                    dependents[file_path] = []
            
            for edge in graph['edges']:
                source = self._extract_file_path_from_edge(edge, 'source')
                target = self._extract_file_path_from_edge(edge, 'target')
                
                if source and target:
                    # source depends on target
                    dependencies.setdefault(source, []).append(target)
                    # target is depended upon by source
                    dependents.setdefault(target, []).append(source)
                    
        elif isinstance(graph, dict):
            # Assume direct adjacency list format: {"file1": ["dep1", "dep2"], ...}
            for file_path, deps in graph.items():
                nodes.add(file_path)
                dependencies[file_path] = deps if isinstance(deps, list) else []
                
                # Build reverse dependencies
                for dep in dependencies[file_path]:
                    dependents.setdefault(dep, []).append(file_path)
        
        # Perform upstream traversal (dependencies)
        upstream_nodes = self._traverse_graph(dependencies, seed_files, max_depth)
        
        # Perform downstream traversal (dependents) 
        downstream_nodes = self._traverse_graph(dependents, seed_files, max_depth)
        
        return {
            'upstream_nodes': upstream_nodes,
            'downstream_nodes': downstream_nodes
        }
    
    def _extract_file_path_from_node(self, node: Any) -> Optional[str]:
        """Extract file path from a graph node."""
        if isinstance(node, str):
            return node
        elif isinstance(node, dict):
            # Try common keys
            for key in ['id', 'file_path', 'path', 'name']:
                if key in node:
                    return node[key]
        return None
    
    def _extract_file_path_from_edge(self, edge: Any, direction: str) -> Optional[str]:
        """Extract file path from a graph edge."""
        if isinstance(edge, dict):
            node_ref = edge.get(direction)
            if isinstance(node_ref, str):
                return node_ref
            elif isinstance(node_ref, dict):
                return self._extract_file_path_from_node(node_ref)
        return None
    
    def _traverse_graph(
        self, 
        adjacency_list: Dict[str, List[str]], 
        start_nodes: List[str], 
        max_depth: int
    ) -> List[str]:
        """
        Traverse the graph from start nodes up to max_depth.
        
        Args:
            adjacency_list: Graph adjacency list
            start_nodes: Starting nodes for traversal
            max_depth: Maximum depth to traverse
            
        Returns:
            List of visited nodes (excluding start nodes)
        """
        visited = set()
        queue = [(node, 0) for node in start_nodes if node in adjacency_list]
        
        while queue:
            current_node, depth = queue.pop(0)
            
            if current_node in visited or depth >= max_depth:
                continue
                
            visited.add(current_node)
            
            # Add neighbors to queue
            neighbors = adjacency_list.get(current_node, [])
            for neighbor in neighbors:
                if neighbor not in visited:
                    queue.append((neighbor, depth + 1))
        
        # Return all visited nodes except the original start nodes
        return [node for node in visited if node not in start_nodes]
    
    def calculate_impact_score(self, blast_radius: int, file_count: int = 100) -> float:
        """
        Calculate a normalized impact score based on blast radius.
        
        Args:
            blast_radius: Number of affected files
            file_count: Total files in repository (for normalization)
            
        Returns:
            Impact score between 0.0 and 1.0 (higher = more impact)
        """
        if file_count <= 0:
            return 0.0
            
        # Normalize blast radius by total file count
        normalized_radius = min(blast_radius / file_count, 1.0)
        
        # Apply logarithmic scaling to avoid penalizing medium-impact changes too heavily
        import math
        return math.sqrt(normalized_radius)
    
    def calculate_onboarding_suitability_score(self, blast_radius: int, max_suitable_radius: int = 100) -> float:
        """
        Calculate onboarding suitability score (inverse of impact).
        Lower blast radius = higher suitability for newcomers.
        
        Args:
            blast_radius: Number of affected files
            max_suitable_radius: Maximum blast radius considered suitable
            
        Returns:
            Suitability score between 0.0 and 100.0 (higher = more suitable)
        """
        if blast_radius <= 0:
            return 100.0
            
        # Invert the blast radius - smaller changes are more suitable
        clamped_radius = min(blast_radius, max_suitable_radius)
        suitability = 100.0 - (clamped_radius / max_suitable_radius * 100.0)
        
        return max(0.0, suitability)


# Convenience functions for external use
def analyze_blast_radius(repo_id: str, seed_files: List[str], max_depth: int = 2) -> Dict[str, Any]:
    """
    Convenience function to analyze blast radius.
    
    Args:
        repo_id: Repository identifier
        seed_files: List of file paths to start analysis from
        max_depth: Maximum traversal depth
        
    Returns:
        Blast radius analysis results
    """
    analyzer = ImpactAnalyzer()
    return analyzer.analyze_blast_radius(repo_id, seed_files, max_depth)


def calculate_onboarding_suitability(blast_radius: int) -> float:
    """
    Convenience function to calculate onboarding suitability score.
    
    Args:
        blast_radius: Number of affected files
        
    Returns:
        Suitability score (0-100, higher = more suitable for newcomers)
    """
    analyzer = ImpactAnalyzer()
    return analyzer.calculate_onboarding_suitability_score(blast_radius)
--- FILE_END: backend/lumiere_core/services/impact_analyzer.py ---

--- FILE_START: backend/lumiere_core/services/review_service.py ---
# backend/lumiere_core/services/review_service.py

import logging
import json
from pathlib import Path

from . import (
    github, llm_service, graph_differ, oracle_service, cartographer,
    diff_parser, scaffolding
)
from .llm_service import TaskType
from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier

logger = logging.getLogger(__name__)

def _generate_graph_for_ref(crawler: IntelligentCrawler) -> dict:
    """Helper function to generate a full architectural graph for the current state of the crawler."""
    files_to_process = crawler.get_file_paths()
    if not files_to_process:
        return {}
    jsonifier = Jsonifier(
        file_paths=files_to_process,
        repo_root=crawler.repo_path,
        repo_id="dynamic_analysis"
    )
    project_cortex = jsonifier.generate_cortex()
    return project_cortex.get("architectural_graph", {})


def inquire_pr(pr_url: str) -> dict:
    """
    Main orchestration logic for "The Inquisitor" dynamic code review agent.
    The model_identifier is no longer needed.
    """
    logger.info(f"--- INQUISITOR AGENT ACTIVATED for PR: {pr_url} ---")

    # 1. On-Demand Workspace Creation & PR Detail Fetching
    try:
        pr_details = github.scrape_github_issue(pr_url)
        if not pr_details:
            return {"error": "Failed to scrape PR details."}
        repo_url = pr_details['repo_url']
        parsed_url = github._parse_github_issue_url(pr_url)
        if not parsed_url:
            return {"error": "Could not parse PR URL."}
        owner, repo_name, pr_number = parsed_url
        gh_repo = github.g.get_repo(f"{owner}/{repo_name}")
        pr_obj = gh_repo.get_pull(pr_number)
        base_ref = pr_obj.base.ref
        head_ref = pr_obj.head.ref
        logger.info(f"Analyzing PR #{pr_number}: merging '{head_ref}' into '{base_ref}'")
    except Exception as e:
        logger.error(f"Failed to get PR details from GitHub API: {e}", exc_info=True)
        return {"error": f"Failed to get PR details from GitHub API: {e}"}

    with IntelligentCrawler(repo_url=repo_url) as crawler:
        # 2. Analyze Base Branch
        logger.info(f"[1/3] Checking out base branch '{base_ref}' and generating graph...")
        if not crawler.checkout_ref(base_ref):
            return {"error": f"Could not checkout base branch '{base_ref}'."}
        base_graph_data = _generate_graph_for_ref(crawler)
        base_graph = oracle_service._build_knowledge_graph(base_graph_data)

        # 3. Analyze Head Branch
        head_sha = pr_obj.head.sha
        logger.info(f"[2/3] Checking out head commit '{head_sha}' and generating graph...")

        # --- FIX: Fetch PR commits from origin to handle forks ---
        try:
            # This fetches the specific commits associated with the PR's head.
            # Using the private method is acceptable here to avoid major refactoring.
            crawler._run_git_command(['git', 'fetch', 'origin', f'pull/{pr_number}/head'], check=True)
            logger.info(f"Successfully fetched commits for PR #{pr_number}")
        except Exception as e:
            logger.warning(f"Could not fetch PR head directly: {e}. The commit might already exist locally.")
            pass # We can still try to checkout the sha

        # Now, checkout the specific commit hash, which is more reliable than branch names.
        if not crawler.checkout_ref(head_sha):
            return {"error": f"Could not checkout head commit SHA '{head_sha}'. The branch might have been deleted or force-pushed."}

        head_graph_data = _generate_graph_for_ref(crawler)
        head_graph = oracle_service._build_knowledge_graph(head_graph_data)

        # 4. Perform Architectural Graph Differencing
        logger.info("[3/3] Comparing architectural graphs...")
        architectural_delta = graph_differ.compare_graphs(base_graph, head_graph)
        delta_str = json.dumps(architectural_delta, indent=2)
        # --- FIX: Use the reliable SHA for the diff calculation ---
        text_diff = crawler.get_diff_for_branch(ref_name=head_sha, base_ref=base_ref)
        if "Error from crawler" in text_diff:
            return {"error": f"Failed to get git diff. {text_diff}"}

        # 5. Synthesize the Inquisitive Review
        prompt_parts = [
            "You are The Inquisitor, an AI agent expert in software architecture. Your task is to review a Pull Request by analyzing how it changes the project's structure.",
            f"\n\n**Pull Request Details:**\n- Title: \"{pr_details.get('title', '')}\"\n- Description: \"{pr_details.get('description', '')}\"",
            "\n\n**ARCHITECTURAL DELTA (Summary of Structural Changes):**\nThis report shows what was added to or removed from the codebase's architecture.",
            f"```json\n{delta_str}\n```",
            f"\n\n**CODE CHANGES (Text Diff):**\n```diff\n{text_diff[:4000]}\n```",
            "\n\n---\n\n**YOUR TASK:**\nBased on the **Architectural Delta**, write a code review.",
            "- Focus on the *structural implications* of the changes.",
            "- Highlight any new dependencies, removed functions that might be used elsewhere, or significant changes in how components connect.",
            "- Use the text diff for specific line context.",
            "- Start with a summary of the most important architectural changes.",
            "- Provide clear, constructive feedback."
        ]
        synthesis_prompt = "\n".join(prompt_parts)

        # --- THE CHANGE IS HERE ---
        review_text = llm_service.generate_text(
            synthesis_prompt,
            task_type=TaskType.COMPLEX_REASONING
        )
        logger.info("--- INQUISITOR AGENT MISSION COMPLETE ---")

        return {"review": review_text, "metadata": {"delta": architectural_delta}}

def harmonize_pr_fix(pr_url: str, review_text: str) -> dict:
    """
    Orchestrates the fix-generation process based on a review from the Inquisitor.
    """
    logger.info(f"--- HARMONIZER AGENT ACTIVATED for PR: {pr_url} ---")

    # 1. Get PR details
    try:
        pr_details = github.scrape_github_issue(pr_url)
        if not pr_details:
            return {"error": "Failed to scrape PR details."}
        repo_url = pr_details['repo_url']
        parsed_url = github._parse_github_issue_url(pr_url)
        owner, repo_name, pr_number = parsed_url
        gh_repo = github.g.get_repo(f"{owner}/{repo_name}")
        pr_obj = gh_repo.get_pull(pr_number)
        base_ref = pr_obj.base.ref
        head_ref = pr_obj.head.ref
        repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")
    except Exception as e:
        logger.error(f"Failed to get PR details for Harmonizer: {e}", exc_info=True)
        return {"error": "Failed to get PR details for Harmonizer.", "details": str(e)}

    # 2. Get working copy of the code and diff
    with IntelligentCrawler(repo_url=repo_url) as crawler:
        if not crawler.checkout_ref(head_ref):
            return {"error": f"Could not checkout head branch '{head_ref}' for fixing."}
        all_files_in_pr = {str(p.relative_to(crawler.repo_path)): p.read_text(encoding='utf-8', errors='ignore') for p in crawler.get_file_paths()}
        text_diff = crawler.get_diff_for_branch(ref_name=head_ref, base_ref=base_ref)
        if "Error from crawler" in text_diff:
            return {"error": "Failed to get git diff for Harmonizer.", "details": text_diff}
        target_files = diff_parser.get_changed_files_from_diff(text_diff)
        if not target_files:
            return {"error": "Harmonizer could not determine which files to fix from the PR diff."}
        original_contents_for_fix = {fp: all_files_in_pr.get(fp, "") for fp in target_files}

    # 3. Call the Scaffolder
    logger.info(f"Harmonizer is calling the Scaffolder with {len(target_files)} target files.")

    # Scaffolder no longer needs a model passed in; it will use the Task Router.
    scaffold_result = scaffolding.generate_scaffold(
        repo_id=repo_id,
        target_files=target_files,
        instruction=f"Fix the issues identified in the following code review for PR titled '{pr_details.get('title')}'.",
        rca_report=review_text,
        refinement_history=[]
    )

    if "error" in scaffold_result:
        return scaffold_result

    scaffold_result["original_contents"] = original_contents_for_fix
    logger.info("--- HARMONIZER MISSION COMPLETE ---")
    return scaffold_result

--- FILE_END: backend/lumiere_core/services/review_service.py ---

--- FILE_START: backend/lumiere_core/services/sentinel_service.py ---
# backend/lumiere_core/services/sentinel_service.py

import os
import ast
import logging
import networkx as nx
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)

# --- Constants ---
TEST_KEYWORDS = ("test", "spec")
IMPL_PREFIXES = ("src/", "lib/")
IMPL_SUFFIXES = (".py", ".js", ".rs", ".go")


# --- Metric Calculation Helpers ---

def _calculate_code_to_test_ratio(files_data: List[Dict[str, str]]) -> float:
    """Calculate the ratio of test files to implementation files."""
    test_files = 0
    impl_files = 0

    for file_info in files_data:
        path = file_info.get("file_path", "").lower()
        if any(keyword in path for keyword in TEST_KEYWORDS):
            test_files += 1
        elif path.startswith(IMPL_PREFIXES) or path.endswith(IMPL_SUFFIXES):
            impl_files += 1

    if impl_files == 0:
        return 0.0
    return round(test_files / impl_files, 3)


def _has_docstring(node: ast.AST) -> bool:
    """Check if the first statement of a node is a docstring."""
    if hasattr(node, "body") and node.body:
        first_stmt = node.body[0]
        return isinstance(first_stmt, ast.Expr) and isinstance(getattr(first_stmt, "value", None), ast.Str)
    return False


def _calculate_documentation_coverage(graph: nx.DiGraph) -> float:
    """Calculate the percentage of functions, methods, and classes with a docstring."""
    documented = 0
    total = 0

    for _, data in graph.nodes(data=True):
        if data.get("type") in ("function", "method", "class"):
            total += 1
            raw = data.get("raw_content", "")
            if isinstance(raw, str):
                try:
                    parsed = ast.parse(raw)
                    if parsed.body and _has_docstring(parsed.body[0]):
                        documented += 1
                except (SyntaxError, IndexError, TypeError) as e:
                    logger.debug(f"Docstring parse error: {e}")

    if total == 0:
        return 0.0
    return round((documented / total) * 100, 2)


def _build_graph(graph_data: Dict[str, Any]) -> nx.DiGraph:
    """Builds a NetworkX directed graph from graph_data dictionary."""
    graph = nx.DiGraph()
    nodes = graph_data.get("nodes", {})
    edges = graph_data.get("edges", [])

    graph.add_nodes_from(nodes.keys())
    graph.add_edges_from((e["source"], e["target"]) for e in edges if "source" in e and "target" in e)

    return graph


# --- Main Service Function ---

def calculate_snapshot_metrics(repo_path: Path, graph_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calculates a snapshot of project health metrics at a given point in time.

    Args:
        repo_path: Root path of the repository.
        graph_data: Architectural graph data from the cartographer.

    Returns:
        A dictionary with calculated health metrics.
    """
    logger.info(f"Sentinel: Calculating snapshot metrics for {repo_path}")
    metrics: Dict[str, Any] = {}

    # --- Architectural Metrics ---
    graph = _build_graph(graph_data) if graph_data else nx.DiGraph()

    metrics["total_nodes"] = graph.number_of_nodes()
    metrics["total_edges"] = graph.number_of_edges()
    metrics["coupling_factor"] = (
        round(metrics["total_edges"] / metrics["total_nodes"], 3)
        if metrics["total_nodes"] > 0 else 0
    )
    metrics["average_cyclomatic_complexity"] = 5.0  # Placeholder

    # --- Code Quality Metrics ---
    all_files_info = [
        {"file_path": str(Path(root) / file)}
        for root, _, files in os.walk(repo_path)
        for file in files
    ]

    metrics["code_to_test_ratio"] = _calculate_code_to_test_ratio(all_files_info)
    metrics["documentation_coverage"] = _calculate_documentation_coverage(graph)
    metrics["timestamp"] = datetime.utcnow().isoformat()

    logger.info(f"Sentinel: Metrics calculated: {metrics}")
    return metrics

--- FILE_END: backend/lumiere_core/services/sentinel_service.py ---

--- FILE_START: backend/lumiere_core/services/rca_service.py ---
# backend/lumiere_core/services/rca_service.py

import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum

from . import llm_service, github
from .ollama import search_index

# Configure logging
logger = logging.getLogger(__name__)

class ComplexityLevel(Enum):
    """Enumeration for complexity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"

class FileCategory(Enum):
    """Enumeration for file categories."""
    BACKEND = "backend"
    FRONTEND = "frontend"
    STYLING = "styling"
    CONFIG = "config"
    DOCS = "docs"
    DATABASE = "database"
    INFRASTRUCTURE = "infrastructure"
    BUILD = "build"
    TESTING = "testing"
    OTHER = "other"

@dataclass
class FileRelationships:
    """Data class for file relationship analysis results."""
    total_files: int
    file_types: Dict[str, List[str]]
    type_distribution: Dict[str, int]
    cross_layer_issue: bool
    complexity_indicator: ComplexityLevel
    primary_category: Optional[str] = None
    secondary_categories: List[str] = None

@dataclass
class AnalysisMetadata:
    """Enhanced metadata for analysis results."""
    total_context_chunks: int
    search_time_ms: Optional[float] = None
    analysis_time_ms: Optional[float] = None
    confidence_score: Optional[float] = None
    architectural_context_available: bool = False

def _get_repo_id_from_url(repo_url: str) -> str:
    """
    Helper to derive a filesystem-safe repo_id from a URL.

    Args:
        repo_url: The GitHub repository URL

    Returns:
        Filesystem-safe repository identifier

    Raises:
        ValueError: If the URL format is invalid
    """
    if not repo_url or not isinstance(repo_url, str):
        raise ValueError("Repository URL must be a non-empty string")

    if not repo_url.startswith("https://github.com/"):
        raise ValueError("Repository URL must be a valid GitHub URL")

    try:
        return repo_url.replace("https://github.com/", "").replace("/", "_")
    except Exception as e:
        raise ValueError(f"Failed to parse repository URL: {e}")

def _classify_file_by_extension(file_path: str) -> FileCategory:
    """
    Classify files into broad categories for better analysis context.

    Args:
        file_path: Path to the file

    Returns:
        FileCategory enum value
    """
    if not file_path:
        return FileCategory.OTHER

    # Enhanced extension mapping with more comprehensive coverage
    extension_map = {
        # Backend languages
        '.py': FileCategory.BACKEND, '.rb': FileCategory.BACKEND,
        '.java': FileCategory.BACKEND, '.go': FileCategory.BACKEND,
        '.php': FileCategory.BACKEND, '.cs': FileCategory.BACKEND,
        '.cpp': FileCategory.BACKEND, '.c': FileCategory.BACKEND,
        '.rs': FileCategory.BACKEND, '.kt': FileCategory.BACKEND,

        # Frontend
        '.js': FileCategory.FRONTEND, '.ts': FileCategory.FRONTEND,
        '.jsx': FileCategory.FRONTEND, '.tsx': FileCategory.FRONTEND,
        '.html': FileCategory.FRONTEND, '.htm': FileCategory.FRONTEND,
        '.vue': FileCategory.FRONTEND, '.svelte': FileCategory.FRONTEND,

        # Styling
        '.css': FileCategory.STYLING, '.scss': FileCategory.STYLING,
        '.sass': FileCategory.STYLING, '.less': FileCategory.STYLING,
        '.styl': FileCategory.STYLING,

        # Configuration
        '.json': FileCategory.CONFIG, '.yaml': FileCategory.CONFIG,
        '.yml': FileCategory.CONFIG, '.toml': FileCategory.CONFIG,
        '.ini': FileCategory.CONFIG, '.conf': FileCategory.CONFIG,
        '.env': FileCategory.CONFIG, '.properties': FileCategory.CONFIG,

        # Documentation
        '.md': FileCategory.DOCS, '.rst': FileCategory.DOCS,
        '.txt': FileCategory.DOCS, '.adoc': FileCategory.DOCS,

        # Database
        '.sql': FileCategory.DATABASE, '.graphql': FileCategory.DATABASE,
        '.gql': FileCategory.DATABASE,

        # Infrastructure
        '.tf': FileCategory.INFRASTRUCTURE, '.hcl': FileCategory.INFRASTRUCTURE,
        '.sh': FileCategory.INFRASTRUCTURE, '.bash': FileCategory.INFRASTRUCTURE,
        '.ps1': FileCategory.INFRASTRUCTURE, '.bat': FileCategory.INFRASTRUCTURE,
    }

    # Special filename mappings
    filename_map = {
        'dockerfile': FileCategory.INFRASTRUCTURE,
        'dockerfile.dev': FileCategory.INFRASTRUCTURE,
        'dockerfile.prod': FileCategory.INFRASTRUCTURE,
        'requirements.txt': FileCategory.BUILD,
        'package.json': FileCategory.BUILD,
        'package-lock.json': FileCategory.BUILD,
        'yarn.lock': FileCategory.BUILD,
        'pipfile': FileCategory.BUILD,
        'pipfile.lock': FileCategory.BUILD,
        'gemfile': FileCategory.BUILD,
        'gemfile.lock': FileCategory.BUILD,
        'composer.json': FileCategory.BUILD,
        'composer.lock': FileCategory.BUILD,
        'pom.xml': FileCategory.BUILD,
        'build.gradle': FileCategory.BUILD,
        'cargo.toml': FileCategory.BUILD,
        'cargo.lock': FileCategory.BUILD,
        'makefile': FileCategory.BUILD,
        'cmake.txt': FileCategory.BUILD,
    }

    file_path_lower = file_path.lower()

    # Check for test files first (highest priority)
    test_patterns = ['.test.', '.spec.', '_test.', '/test/', '/tests/', '__test__', '__tests__']
    if any(pattern in file_path_lower for pattern in test_patterns):
        return FileCategory.TESTING

    # Check filename mappings
    filename = Path(file_path_lower).name
    if filename in filename_map:
        return filename_map[filename]

    # Check extension mappings
    suffix = Path(file_path_lower).suffix
    return extension_map.get(suffix, FileCategory.OTHER)

def _determine_complexity_level(file_count: int, type_count: int, cross_layer: bool) -> ComplexityLevel:
    """
    Determine complexity level based on multiple factors.

    Args:
        file_count: Number of files involved
        type_count: Number of different file types
        cross_layer: Whether the issue crosses multiple layers

    Returns:
        ComplexityLevel enum value
    """
    if cross_layer and file_count > 15:
        return ComplexityLevel.VERY_HIGH
    elif file_count > 12 or (cross_layer and type_count > 4):
        return ComplexityLevel.HIGH
    elif file_count > 6 or type_count > 3:
        return ComplexityLevel.MEDIUM
    else:
        return ComplexityLevel.LOW

def _analyze_file_relationships(context_chunks: List[Dict]) -> FileRelationships:
    """
    Analyze relationships between files found in RAG results to provide better context.

    Args:
        context_chunks: List of context chunks from RAG search

    Returns:
        FileRelationships object with comprehensive analysis
    """
    if not context_chunks:
        return FileRelationships(
            total_files=0,
            file_types={},
            type_distribution={},
            cross_layer_issue=False,
            complexity_indicator=ComplexityLevel.LOW,
            primary_category=None,
            secondary_categories=[]
        )

    file_types = defaultdict(list)
    file_paths = set()
    category_counts = defaultdict(int)

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        if file_path:
            file_paths.add(file_path)
            file_category = _classify_file_by_extension(file_path)
            category_str = file_category.value
            file_types[category_str].append(file_path)
            category_counts[category_str] += 1

    # Determine primary and secondary categories
    sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
    primary_category = sorted_categories[0][0] if sorted_categories else None
    secondary_categories = [cat for cat, count in sorted_categories[1:4] if count > 1]

    total_files = len(file_paths)
    type_count = len(file_types)
    cross_layer_issue = type_count > 2
    complexity = _determine_complexity_level(total_files, type_count, cross_layer_issue)

    return FileRelationships(
        total_files=total_files,
        file_types=dict(file_types),
        type_distribution={k: len(set(v)) for k, v in file_types.items()},
        cross_layer_issue=cross_layer_issue,
        complexity_indicator=complexity,
        primary_category=primary_category,
        secondary_categories=secondary_categories
    )

def _load_architectural_context(repo_id: str) -> Tuple[str, bool]:
    """
    Load architectural context from cortex file.

    Args:
        repo_id: Repository identifier

    Returns:
        Tuple of (architectural_context_string, context_available_bool)
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_path.exists():
            logger.debug(f"Cortex file not found at {cortex_path}")
            return "No architectural context available for this analysis.", False

        with open(cortex_path, 'r', encoding='utf-8') as f:
            cortex_data = json.load(f)

        graph_data = cortex_data.get('architectural_graph')
        if not graph_data:
            return "Architectural context file found but contains no graph data.", False

        nodes = graph_data.get('nodes', {})  # Default to empty dict
        edges = graph_data.get('edges', [])

        # Correctly get the first 5 nodes from the dictionary's values
        nodes_list = list(nodes.values())

        context = f"""Architectural graph available with {len(nodes)} components and {len(edges)} connections.
Key components identified: {', '.join([node.get('name', 'Unknown') for node in nodes_list[:5]])}
This provides insights into system architecture and component relationships."""

        logger.info(f"Loaded architectural context for {repo_id}: {len(nodes)} nodes, {len(edges)} edges")
        return context, True

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse cortex JSON for {repo_id}: {e}")
        return "Architectural context file found but contains invalid JSON.", False
    except Exception as e:
        logger.error(f"Failed to load architectural context for {repo_id}: {e}")
        return "Error loading architectural context.", False

def _format_context_for_prompt(context_chunks: List[Dict], relationships: FileRelationships) -> str:
    """
    Format context chunks for LLM prompt with improved organization.

    Args:
        context_chunks: List of context chunks
        relationships: File relationship analysis

    Returns:
        Formatted context string
    """
    if not context_chunks:
        return "No relevant code context found."

    # Group chunks by file category for better organization
    categorized_chunks = defaultdict(list)
    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        category = _classify_file_by_extension(file_path).value
        categorized_chunks[category].append(chunk)

    context_parts = []

    # Present primary category first
    if relationships.primary_category and relationships.primary_category in categorized_chunks:
        chunks = categorized_chunks[relationships.primary_category]
        context_parts.append(f"=== PRIMARY CATEGORY: {relationships.primary_category.upper()} FILES ===")
        for chunk in chunks:
            context_parts.append(
                f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
            )

    # Then secondary categories
    for category in relationships.secondary_categories:
        if category in categorized_chunks:
            chunks = categorized_chunks[category]
            context_parts.append(f"=== SECONDARY CATEGORY: {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    # Finally, remaining categories
    for category, chunks in categorized_chunks.items():
        if category != relationships.primary_category and category not in relationships.secondary_categories:
            context_parts.append(f"=== {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    return "\n\n".join(context_parts)

def generate_briefing(issue_url: str, model_identifier: str) -> Dict[str, Any]:
    """
    Generates a 'Pre-flight Briefing' for a GitHub issue using RAG.

    Args:
        issue_url: GitHub issue URL
        model_identifier: LLM model identifier

    Returns:
        Dictionary containing briefing and metadata
    """
    logger.info(f"Generating briefing for issue: {issue_url}")

    try:
        # Validate inputs
        if not issue_url or not isinstance(issue_url, str):
            return {"error": "Invalid issue URL provided"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        # Scrape GitHub issue
        issue_data = github.scrape_github_issue(issue_url)
        if not issue_data:
            return {"error": "Could not retrieve issue details from GitHub. Please check the URL and try again."}

        repo_id = _get_repo_id_from_url(issue_data['repo_url'])
        query = issue_data['full_text_query']

        if not query:
            return {"error": "No query text found in issue data"}

        # Perform RAG search
        try:
            context_chunks = search_index(
                query_text=query,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=7
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed for {repo_id}: {e}")
            return {"error": f"Failed to retrieve context from vector index: {str(e)}"}

        # Analyze relationships and format context
        relationships = _analyze_file_relationships(context_chunks)
        context_string = _format_context_for_prompt(context_chunks, relationships)

        # Enhanced prompt with better structure
        prompt = f"""You are Lumière Sémantique, an expert AI programming assistant.
Your mission is to provide a comprehensive "Pre-flight Briefing" for a developer about to work on this GitHub issue.

**CODEBASE ANALYSIS SUMMARY:**
- Files involved: {relationships.total_files} files across {len(relationships.file_types)} different categories
- Primary category: {relationships.primary_category or 'Unknown'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

**INSTRUCTIONS:**
Analyze the GitHub issue and the provided codebase context to generate a comprehensive briefing report.

The report must be clear, well-structured, and formatted in Markdown. Include these sections:

1. **🎯 Task Summary**
   - Concise rephrasing of the issue request
   - Key objectives and expected outcomes

2. **🏗️ Codebase Architecture**
   - Relevant system architecture and component relationships
   - How the affected components interact

3. **🔍 Current System Analysis**
   - How the system currently handles the functionality in question
   - Existing patterns and conventions

4. **📁 Key Files & Components**
   - Most important files and functions from the context
   - Their roles and relationships

5. **🚀 Suggested Approach**
   - High-level implementation strategy
   - Recommended order of operations
   - Potential challenges and considerations

6. **⚠️ Important Notes**
   - Dependencies and side effects to consider
   - Testing recommendations

--- CODEBASE CONTEXT ---
{context_string}
--- END CONTEXT ---

**GITHUB ISSUE DETAILS:**
{query}

Generate the comprehensive Pre-flight Briefing now:
"""

        # Generate briefing
        briefing_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare metadata
        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        metadata = {
            "file_relationships": relationships_dict,
            "issue_url": issue_url,
            "repo_id": repo_id,
            "context_chunks_count": len(context_chunks),
            "model_used": model_identifier
        }

        logger.info(f"Successfully generated briefing for {issue_url}")
        return {"briefing": briefing_report, "metadata": metadata}

    except ValueError as e:
        logger.error(f"Validation error in generate_briefing: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in generate_briefing: {e}")
        return {"error": f"An unexpected error occurred: {str(e)}"}

def perform_rca(
    repo_url: str,
    bug_description: str,
    model_identifier: str,
    advanced_analysis: bool = False,
    confidence_threshold: float = 0.7
) -> Dict[str, Any]:
    """
    Performs a multi-file, context-aware Root Cause Analysis using RAG.

    Args:
        repo_url: GitHub repository URL
        bug_description: Description of the bug to analyze
        model_identifier: LLM model identifier
        advanced_analysis: Whether to perform advanced analysis with more context
        confidence_threshold: Minimum confidence threshold for results

    Returns:
        Dictionary containing analysis results and metadata
    """
    logger.info(f"Performing Multi-file RCA for bug: '{bug_description[:100]}...'")

    try:
        # Validate inputs
        if not repo_url or not isinstance(repo_url, str):
            return {"error": "Invalid repository URL provided"}

        if not bug_description or not isinstance(bug_description, str):
            return {"error": "Bug description is required"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        if not 0.0 <= confidence_threshold <= 1.0:
            return {"error": "Confidence threshold must be between 0.0 and 1.0"}

        repo_id = _get_repo_id_from_url(repo_url)

        # Load architectural context
        architectural_context, arch_available = _load_architectural_context(repo_id)

        # Perform RAG search with enhanced parameters
        try:
            logger.debug("Searching for relevant code chunks...")
            initial_k = 25 if advanced_analysis else 15

            context_chunks = search_index(
                query_text=bug_description,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=initial_k
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed during RCA for {repo_id}: {e}")
            return {"error": f"RAG search failed during RCA: {str(e)}"}

        if not context_chunks:
            return {
                "analysis": "Could not find any relevant code context for the bug description. "
                           "Unable to perform RCA. Please try rephrasing the bug description or "
                           "ensure the repository has been properly indexed."
            }

        logger.debug("Analyzing file relationships and filtering context...")
        relationships = _analyze_file_relationships(context_chunks)

        # Format context with improved organization
        logger.debug("Synthesizing context from suspect files...")
        formatted_context = _format_context_for_prompt(context_chunks, relationships)

        # Generate complexity guidance
        complexity_guidance = ""
        if relationships.cross_layer_issue:
            complexity_guidance = """
**COMPLEXITY ALERT:** This issue spans multiple system layers. Pay special attention to:
- Interface boundaries and data contracts
- State management across components
- Error propagation paths
- Dependency chains and side effects"""

        if relationships.complexity_indicator in [ComplexityLevel.HIGH, ComplexityLevel.VERY_HIGH]:
            complexity_guidance += """
- Consider breaking down the analysis into smaller, focused areas
- Look for common patterns or shared dependencies
- Pay attention to configuration and environment differences"""

        # Enhanced RCA prompt
        prompt = f"""You are a world-class debugging expert performing a comprehensive Root Cause Analysis (RCA).

**BUG DESCRIPTION:**
{bug_description}

**SYSTEM CONTEXT:**
{architectural_context}

**CODEBASE ANALYSIS:**
- Relevant files: {relationships.total_files} files analyzed
- Primary category: {relationships.primary_category or 'Mixed'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

{complexity_guidance}

**ANALYSIS INSTRUCTIONS:**
You must analyze ALL provided context systematically. Use the code evidence to build a comprehensive understanding of the bug's root cause.

Your analysis must follow this structure:

## 🎯 Executive Summary
A clear, single-sentence explanation of the root cause.

## 🏗️ System Overview
Brief explanation of how the affected components are designed to interact and what the expected behavior should be.

## 🔍 Root Cause Analysis
Detailed breakdown of:
- What is happening vs. what should happen
- The specific mechanism causing the failure
- Why this particular scenario triggers the bug

## 📋 Evidence & Reasoning
Cite specific files, functions, and code snippets that support your analysis:
- Direct evidence from the code
- Logical connections between components
- Data flow analysis

## 💥 Impact Assessment
Explain the cascading effects:
- What breaks when this bug occurs
- Which users/systems are affected
- Performance or security implications

## 🛠️ Recommended Fix Strategy
High-level approach including:
- Which specific files need modification
- Order of operations for the fix
- Testing strategy to verify the fix
- Potential risks and mitigation strategies

## ⚠️ Prevention Recommendations
Suggestions to prevent similar issues in the future.

**RELEVANT CODE CONTEXT:**
{formatted_context}

Now generate your comprehensive Root Cause Analysis:
"""

        logger.debug("Generating comprehensive RCA report...")
        analysis_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare enhanced metadata
        metadata = AnalysisMetadata(
            total_context_chunks=len(context_chunks),
            architectural_context_available=arch_available,
            confidence_score=None  # Could be implemented based on context quality
        )

        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        result = {
            "analysis": analysis_report,
            "metadata": {
                **relationships_dict,
                **metadata.__dict__,
                "repo_id": repo_id,
                "advanced_analysis": advanced_analysis,
                "confidence_threshold": confidence_threshold
            }
        }

        logger.info(f"Successfully completed RCA for {repo_id}")
        return result

    except ValueError as e:
        logger.error(f"Validation error in perform_rca: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in perform_rca: {e}")
        return {"error": f"An unexpected error occurred during RCA: {str(e)}"}

def get_analysis_health_check(repo_id: str) -> Dict[str, Any]:
    """
    Perform a health check for RCA analysis capabilities.

    Args:
        repo_id: Repository identifier

    Returns:
        Dictionary containing health check results
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        health_status = {
            "repo_id": repo_id,
            "vector_index_available": False,
            "architectural_context_available": False,
            "cortex_file_exists": cortex_path.exists(),
            "recommendations": []
        }

        # Check vector index (this would need to be implemented based on your vector storage)
        try:
            # Placeholder for vector index check
            test_chunks = search_index(
                query_text="test query",
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=1
            )
            health_status["vector_index_available"] = len(test_chunks) > 0
        except FileNotFoundError:
            health_status["recommendations"].append(
                "Vector index not found. Please re-ingest the repository with embedding enabled."
            )
        except Exception as e:
            health_status["recommendations"].append(f"Vector index check failed: {str(e)}")

        # Check architectural context
        if cortex_path.exists():
            try:
                with open(cortex_path, 'r', encoding='utf-8') as f:
                    cortex_data = json.load(f)
                graph_data = cortex_data.get('architectural_graph')
                health_status["architectural_context_available"] = bool(graph_data)

                if not graph_data:
                    health_status["recommendations"].append(
                        "Cortex file exists but contains no architectural graph data."
                    )
            except Exception as e:
                health_status["recommendations"].append(f"Failed to parse cortex file: {str(e)}")
        else:
            health_status["recommendations"].append(
                "No architectural context available. Consider running architectural analysis."
            )

        # Overall health assessment
        if health_status["vector_index_available"] and health_status["architectural_context_available"]:
            health_status["overall_status"] = "excellent"
        elif health_status["vector_index_available"]:
            health_status["overall_status"] = "good"
        else:
            health_status["overall_status"] = "poor"

        return health_status

    except Exception as e:
        logger.error(f"Health check failed for {repo_id}: {e}")
        return {
            "repo_id": repo_id,
            "overall_status": "error",
            "error": str(e)
        }

--- FILE_END: backend/lumiere_core/services/rca_service.py ---

--- FILE_START: backend/lumiere_core/services/suggester_service.py ---
# backend/lumiere_core/services/suggester_service.py

import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

def suggest_next_actions(last_action: str, result_data: Dict = None) -> Dict:
    """
    Generates a list of suggested next actions based on the last command's result.

    Args:
        last_action: The command that was just executed (e.g., "review", "analyze").
        result_data: The JSON data returned by the last command.

    Returns:
        A dictionary containing a list of suggestions and a recommended choice.
    """
    if result_data is None:
        result_data = {}

    suggestions = []
    recommended_choice = None

    if last_action == "review":
        suggestions.append({
            "key": "1",
            "text": "🎵 Harmonize: Attempt an automated fix based on this review.",
            "command": "harmonize"
        })
        suggestions.append({
            "key": "2",
            "text": "🔮 Oracle: Ask a follow-up question about the PR.",
            "command": "ask"
        })
        recommended_choice = "1"

    elif last_action == "analyze":
        suggestions.append({
            "key": "1",
            "text": "🎯 List Issues: View the prioritized list of issues for this repo.",
            "command": "list"
        })
        suggestions.append({
            "key": "2",
            "text": "🗺️ Graph: Visualize the repository's architecture.",
            "command": "graph"
        })
        suggestions.append({
            "key": "3",
            "text": "🔮 Oracle: Ask a high-level question about the codebase.",
            "command": "ask"
        })
        recommended_choice = "1"

    elif last_action == "dashboard":
        # Check if the briefing text suggests negative trends
        briefing_text = result_data.get("briefing", "").lower()
        if "increase" in briefing_text or "jumped" in briefing_text or "dropped" in briefing_text:
            suggestions.append({
                "key": "1",
                "text": "🔍 Re-analyze: Run a fresh analysis to get the latest graph and issues.",
                "command": "analyze"
            })
            recommended_choice = "1"

    # Always add a way to go back
    back_key = str(len(suggestions) + 1)
    suggestions.append({
        "key": back_key,
        "text": "↩️ Main Menu: Return to the main command prompt.",
        "command": "back"
    })

    # If no specific recommendation, recommend going back.
    if not recommended_choice:
        recommended_choice = back_key

    return {
        "suggestions": suggestions,
        "recommended_choice": recommended_choice
    }

--- FILE_END: backend/lumiere_core/services/suggester_service.py ---

--- FILE_START: backend/lumiere_core/services/cartographer.py ---
# backend/lumiere_core/services/cartographer.py

import ast
import logging
import json
from collections import defaultdict
from typing import Dict, Any, List, Optional, Union
from pathlib import Path


# --- Tree-sitter imports ---
try:
    from tree_sitter import Language, Parser, Node
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False
    logging.warning("Tree-sitter not available. JavaScript analysis will be limited.")

# Configure logging
logger = logging.getLogger(__name__)


class TreeSitterConfig:
    """Centralized Tree-sitter configuration and initialization."""

    def __init__(self):
        # --- FIX: Calculate the absolute path to the library ---
        # This makes the path resilient to where the script is run from.
        # It finds the directory of this file (services/) and then navigates to the build directory.
        current_file_dir = Path(__file__).resolve().parent
        self.library_path = current_file_dir / 'build' / 'my-languages.so'

        self.js_language: Optional[Language] = None
        self.js_parser: Optional[Parser] = None
        self.is_ready = False

        if TREE_SITTER_AVAILABLE:
            self._initialize()

    def _initialize(self) -> None:
        """Initialize Tree-sitter parser for JavaScript."""
        if not self.library_path.exists():
            logger.warning(f"Language library not found at {self.library_path}")
            logger.warning("Please run 'python build_parsers.py' from the 'backend' directory.")
            return

        try:
            self.js_language = Language(str(self.library_path), 'javascript')
            self.js_parser = Parser()
            self.js_parser.set_language(self.js_language)
            self.is_ready = True
            logger.info("✓ Tree-sitter JavaScript parser is ready.")
        except Exception as e:
            logger.error(f"Failed to load JavaScript parser: {e}")
            self.is_ready = False


# Global Tree-sitter configuration
ts_config = TreeSitterConfig()


class PythonCartographerVisitor(ast.NodeVisitor):
    """Enhanced AST visitor for Python with better error handling and organization."""

    def __init__(self):
        self.imports: List[Dict[str, Any]] = []
        self.function_calls: List[Dict[str, Any]] = []
        self.class_defs: List[Dict[str, Any]] = []
        self.function_defs: List[Dict[str, Any]] = []
        self.current_context_stack: List[str] = []  # Track nested contexts

    @property
    def current_class(self) -> Optional[str]:
        """Get the current class context."""
        for context in reversed(self.current_context_stack):
            if context.startswith('class:'):
                return context[6:]  # Remove 'class:' prefix
        return None

    def visit_Import(self, node: ast.Import) -> None:
        """Process direct imports."""
        for alias in node.names:
            self.imports.append({
                'type': 'direct',
                'name': alias.name,
                'alias': alias.asname,
                'line': node.lineno
            })
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        """Process from imports."""
        module = node.module or '.'
        for alias in node.names:
            self.imports.append({
                'type': 'from',
                'module': module,
                'name': alias.name,
                'alias': alias.asname,
                'line': node.lineno
            })
        self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        """Process class definitions with context tracking."""
        self.current_context_stack.append(f'class:{node.name}')

        self.class_defs.append({
            'name': node.name,
            'inherits_from': [safe_unparse(base) for base in node.bases],
            'line': node.lineno,
            'decorators': [safe_unparse(dec) for dec in node.decorator_list]
        })

        self.generic_visit(node)
        self.current_context_stack.pop()

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """Process function definitions."""
        self.function_defs.append({
            'name': node.name,
            'class_context': self.current_class,
            'line': node.lineno,
            'args': [arg.arg for arg in node.args.args],
            'decorators': [safe_unparse(dec) for dec in node.decorator_list]
        })
        self.generic_visit(node)

    def visit_Call(self, node: ast.Call) -> None:
        """Process function calls."""
        call_name = safe_unparse(node.func)
        self.function_calls.append({
            'name': call_name,
            'class_context': self.current_class,
            'line': node.lineno
        })
        self.generic_visit(node)


def safe_unparse(node: ast.AST) -> str:
    """Enhanced version of ast.unparse with better error handling."""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        try:
            return f"{safe_unparse(node.value)}.{node.attr}"
        except Exception:
            return f"<complex>.{node.attr}"
    elif isinstance(node, ast.Constant):
        return str(node.value)

    try:
        return ast.unparse(node)
    except Exception as e:
        logger.debug(f"Failed to unparse AST node: {e}")
        return f"<unparseable:{type(node).__name__}>"


class JavaScriptMapper:
    """Handles JavaScript AST mapping with Tree-sitter."""

    QUERY_PATTERNS = {
        "requires": '(call_expression function: (identifier) @func (#eq? @func "require") arguments: (arguments (string (string_fragment) @module)))',
        "imports": '(import_statement source: (string (string_fragment) @module))',
        "functions": '(function_declaration name: (identifier) @name)',
        "arrow_functions": '(variable_declarator name: (identifier) @name value: (arrow_function))',
        "classes": '(class_declaration name: (identifier) @name)',
        "calls": '(call_expression function: [ (identifier) @name (member_expression property: (property_identifier) @name) ] )',
        "exports": '(export_statement declaration: (function_declaration name: (identifier) @name))'
    }

    @staticmethod
    def map_ast(file_path: str, content: str, nodes: defaultdict, edges: List[Dict]) -> None:
        """Maps JavaScript content to graph nodes and edges using Tree-sitter."""
        if not ts_config.is_ready:
            logger.warning(f"Skipping JavaScript analysis for {file_path} - parser not ready")
            return

        try:
            tree = ts_config.js_parser.parse(bytes(content, "utf8"))
            root_node = tree.root_node

            # Initialize file node
            nodes[file_path].update({
                "type": "file",
                "language": "javascript",
                "classes": [],
                "functions": [],
                "exports": []
            })

            JavaScriptMapper._process_queries(file_path, root_node, nodes, edges)

        except Exception as e:
            logger.error(f"Failed to parse JavaScript file {file_path}: {e}")

    @staticmethod
    def _process_queries(file_path: str, root_node: Node, nodes: defaultdict, edges: List[Dict]) -> None:
        """Process Tree-sitter queries for JavaScript analysis."""
        for query_name, pattern in JavaScriptMapper.QUERY_PATTERNS.items():
            try:
                query = ts_config.js_language.query(pattern)
                captures = query.captures(root_node)

                for node, name in captures:
                    text = node.text.decode('utf8')
                    JavaScriptMapper._handle_capture(query_name, name, text, file_path, nodes, edges)

            except Exception as e:
                logger.warning(f"Tree-sitter query '{query_name}' failed for {file_path}: {e}")

    @staticmethod
    def _handle_capture(query_name: str, capture_name: str, text: str,
                       file_path: str, nodes: defaultdict, edges: List[Dict]) -> None:
        """Handle individual Tree-sitter captures."""
        if capture_name == 'module':
            edges.append({"source": file_path, "target": text, "type": "IMPORTS"})
        elif capture_name == 'name':
            if query_name in ['functions', 'arrow_functions']:
                nodes[file_path]['functions'].append(text)
            elif query_name == 'classes':
                nodes[file_path]['classes'].append(text)
            elif query_name == 'calls':
                edges.append({"source": file_path, "target": text, "type": "CALLS"})
            elif query_name == 'exports':
                nodes[file_path]['exports'].append(text)


def map_python_ast(file_path: str, tree: ast.AST, nodes: defaultdict, edges: List[Dict]) -> None:
    """Enhanced Python AST mapping with better organization."""
    try:
        visitor = PythonCartographerVisitor()
        visitor.visit(tree)

        # Initialize file node
        nodes[file_path].update({
            "type": "file",
            "language": "python",
            "classes": [],
            "functions": [],
            "imports": len(visitor.imports)
        })

        # Process classes
        for class_def in visitor.class_defs:
            class_name = class_def['name']
            class_node_id = f"{file_path}::{class_name}"

            nodes[file_path]['classes'].append(class_name)
            nodes[class_node_id].update({
                "type": "class",
                "name": class_name,
                "file": file_path,
                "methods": [],
                "line": class_def['line'],
                "decorators": class_def['decorators']
            })

            # Add inheritance edges
            for base in class_def['inherits_from']:
                if base != '<unparseable:Name>':  # Skip unparseable bases
                    edges.append({"source": class_node_id, "target": base, "type": "INHERITS_FROM"})

        # Process functions and methods
        for func_def in visitor.function_defs:
            func_name = func_def['name']
            if func_def['class_context']:
                parent_class_id = f"{file_path}::{func_def['class_context']}"
                if parent_class_id in nodes:
                    nodes[parent_class_id]['methods'].append(func_name)
            else:
                nodes[file_path]['functions'].append(func_name)

        # Process imports
        for imp in visitor.imports:
            target = imp.get('module', imp['name'])
            edges.append({
                "source": file_path,
                "target": target,
                "type": "IMPORTS",
                "import_type": imp['type']
            })

        # Process function calls
        for call in visitor.function_calls:
            source_id = file_path
            if call['class_context']:
                source_id = f"{file_path}::{call['class_context']}"

            if source_id in nodes:
                edges.append({
                    "source": source_id,
                    "target": call['name'],
                    "type": "CALLS"
                })

    except Exception as e:
        logger.error(f"Failed to map Python AST for {file_path}: {e}")


def generate_graph(analyzed_files: Dict[str, Any]) -> Dict[str, Any]:
    """Enhanced orchestration of multi-language architectural analysis."""
    logger.info("--- ENHANCED POLYGLOT CARTOGRAPHER AGENT ACTIVATED ---")
    logger.info(f"   -> Mapping architecture for {len(analyzed_files)} files...")

    nodes = defaultdict(dict)
    edges = []
    languages_found = set()
    processing_stats = {"success": 0, "errors": 0}

    for file_path, analysis_data in analyzed_files.items():
        try:
            lang = analysis_data.get('language')
            if not lang:
                logger.warning(f"No language specified for {file_path}")
                continue

            languages_found.add(lang)
            logger.debug(f"      - Analyzing: {file_path} ({lang})")

            if lang == 'python' and 'ast' in analysis_data:
                map_python_ast(file_path, analysis_data['ast'], nodes, edges)
                processing_stats["success"] += 1
            elif lang == 'javascript' and 'content' in analysis_data:
                JavaScriptMapper.map_ast(file_path, analysis_data['content'], nodes, edges)
                processing_stats["success"] += 1
            else:
                logger.warning(f"Unsupported language or missing data for {file_path} ({lang})")

        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            processing_stats["errors"] += 1

    # Generate summary
    total_nodes = len(nodes)
    total_edges = len(edges)

    logger.info("✓ Enhanced Polyglot Cartographer mapping complete.")
    logger.info(f"   -> Generated {total_nodes} nodes and {total_edges} edges")
    logger.info(f"   -> Analyzed languages: {', '.join(filter(None, languages_found))}")
    logger.info(f"   -> Processing stats: {processing_stats['success']} successful, {processing_stats['errors']} errors")

    return {
        "nodes": dict(nodes),
        "edges": edges,
        "metadata": {
            "languages": list(languages_found),
            "stats": processing_stats,
            "tree_sitter_available": TREE_SITTER_AVAILABLE,
            "javascript_parser_ready": ts_config.is_ready
        }
    }


# Backward compatibility aliases
_PythonCartographerVisitor = PythonCartographerVisitor
_safe_unparse = safe_unparse
_map_python_ast = map_python_ast
_map_javascript_ast = JavaScriptMapper.map_ast

# Legacy global variables for backward compatibility
LANGUAGE_LIBRARY_PATH = ts_config.library_path
JS_LANGUAGE = ts_config.js_language
js_parser = ts_config.js_parser
JAVASCRIPT_PARSER_READY = ts_config.is_ready

--- FILE_END: backend/lumiere_core/services/cartographer.py ---

--- FILE_START: backend/lumiere_core/services/cortex_service.py ---
# backend/lumiere_core/services/cortex_service.py

import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)

# Constants
CORTEX_FILENAME_TEMPLATE = "{repo_id}_cortex.json"
CLONED_REPOS_DIR = "cloned_repositories"


class CortexFileNotFound(Exception):
    """Raised when the Cortex file is missing for a given repository."""


class CortexFileMalformed(Exception):
    """Raised when the Cortex file is unreadable or not valid JSON."""


def _get_cortex_path(repo_id: str) -> Path:
    """
    Constructs the full path to a repository's Cortex file.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Path object pointing to the expected Cortex file location.
    """
    base_dir = Path(__file__).resolve().parent.parent.parent
    return base_dir / CLONED_REPOS_DIR / repo_id / CORTEX_FILENAME_TEMPLATE.format(repo_id=repo_id)


def load_cortex_data(repo_id: str) -> Dict[str, Any]:
    """
    Loads and parses the cortex JSON file for a given repository.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Parsed JSON data as a dictionary.

    Raises:
        CortexFileNotFound: If the cortex file does not exist.
        CortexFileMalformed: If the file is not valid JSON.
    """
    cortex_path = _get_cortex_path(repo_id)

    if not cortex_path.exists():
        logger.error(f"Cortex file not found at: {cortex_path}")
        raise CortexFileNotFound(f"Cortex file not found for repo: {repo_id}")

    try:
        with cortex_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.exception(f"Failed to parse cortex file: {cortex_path}")
        raise CortexFileMalformed(f"Failed to load or parse cortex file: {e}") from e


def get_file_content(repo_id: str, file_path: str) -> Optional[str]:
    """
    Retrieves the raw content of a specific file from the repository's Cortex data.

    Args:
        repo_id: The unique ID of the repository.
        file_path: The relative path of the file within the repo.

    Returns:
        The raw file content as a string, or None if the file isn't found.
    """
    try:
        cortex_data = load_cortex_data(repo_id)
        for file_entry in cortex_data.get("files", []):
            if file_entry.get("file_path") == file_path:
                return file_entry.get("raw_content")
    except (CortexFileNotFound, CortexFileMalformed):
        return None

    return None


def get_node_line_map(repo_id: str) -> Dict[str, List[Dict]]:
    """
    Creates a map from file paths to a list of their contained nodes with line numbers.
    This is a critical helper for the Adjudicator's diff parser.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        A dictionary mapping filenames to lists of node info.
        e.g. {'src/main.py': [{'id': '...', 'start_line': 10, 'end_line': 25}]}
    """
    node_map = {}
    try:
        cortex_data = load_cortex_data(repo_id)
        graph = cortex_data.get("architectural_graph", {})
        nodes = graph.get("nodes", {})

        for node_id, node_data in nodes.items():
            # We are interested in nodes that have line numbers and a file path.
            # This typically means functions, classes, and methods.
            start_line = node_data.get("start_line")
            end_line = node_data.get("end_line")
            # The 'file' attribute is present on class/function/method nodes.
            file_path = node_data.get("file")

            if start_line and end_line and file_path:
                if file_path not in node_map:
                    node_map[file_path] = []

                node_map[file_path].append({
                    "id": node_id,
                    "start_line": start_line,
                    "end_line": end_line,
                })
    except (CortexFileNotFound, CortexFileMalformed) as e:
        logger.error(f"Could not generate node line map for {repo_id}: {e}")
        return {}

    return node_map


def get_bom_data(repo_id: str, format_type: str = "json") -> Optional[Dict[str, Any]]:
    """
    Get BOM data for a repository with different format options.

    Args:
        repo_id: Repository identifier
        format_type: Format type (json, summary, detailed)

    Returns:
        Dict containing BOM data in requested format or None if not available
    """
    try:
        cortex_data = load_cortex_data(repo_id)
        bom_data = cortex_data.get('tech_stack_bom')

        if not bom_data:
            return None

        if format_type == "summary":
            return {
                "repo_id": repo_id,
                "summary": bom_data.get('summary', {}),
                "primary_ecosystems": list(set(dep.get('ecosystem', 'unknown')
                                            for deps in bom_data.get('dependencies', {}).values()
                                            for dep in deps)),
                "service_count": len(bom_data.get('services', [])),
                "last_updated": bom_data.get('summary', {}).get('last_updated'),
                "generation_status": cortex_data.get('bom_generation_status', 'unknown')
            }

        elif format_type == "detailed":
            # Add enhanced analysis
            enhanced_bom = bom_data.copy()
            enhanced_bom['analysis'] = {
                'dependency_health': _analyze_dependency_health(bom_data),
                'security_insights': _generate_security_insights(bom_data),
                'architecture_patterns': _detect_architecture_patterns(bom_data),
                'modernization_opportunities': _suggest_modernization(bom_data)
            }
            enhanced_bom['repo_id'] = repo_id
            enhanced_bom['generation_status'] = cortex_data.get('bom_generation_status', 'unknown')
            return enhanced_bom

        # Default json format
        bom_data_copy = bom_data.copy()
        bom_data_copy['repo_id'] = repo_id
        bom_data_copy['generation_status'] = cortex_data.get('bom_generation_status', 'unknown')
        return bom_data_copy

    except (CortexFileNotFound, CortexFileMalformed):
        return None

def has_bom_data(repo_id: str) -> bool:
    """
    Check if repository has BOM data available.

    Args:
        repo_id: Repository identifier

    Returns:
        True if BOM data exists, False otherwise
    """
    try:
        cortex_data = load_cortex_data(repo_id)
        return 'tech_stack_bom' in cortex_data
    except (CortexFileNotFound, CortexFileMalformed):
        return False

def get_repository_metadata(repo_id: str) -> Optional[Dict[str, Any]]:
    """
    Get repository metadata including BOM summary if available.

    Args:
        repo_id: Repository identifier

    Returns:
        Metadata dictionary or None if repository not found
    """
    try:
        cortex_data = load_cortex_data(repo_id)

        metadata = {
            'repo_id': repo_id,
            'has_bom': 'tech_stack_bom' in cortex_data,
            'bom_status': cortex_data.get('bom_generation_status', 'not_available'),
            'version': cortex_data.get('version', '1.0.0'),
            'file_count': len(cortex_data.get('files', []))
        }

        # Add BOM summary if available
        if 'bom_summary' in cortex_data:
            metadata['bom_summary'] = cortex_data['bom_summary']
        elif 'tech_stack_bom' in cortex_data:
            bom_data = cortex_data['tech_stack_bom']
            metadata['bom_summary'] = {
                'primary_language': bom_data.get('summary', {}).get('primary_language', 'Unknown'),
                'total_dependencies': bom_data.get('summary', {}).get('total_dependencies', 0),
                'total_services': bom_data.get('summary', {}).get('total_services', 0),
                'ecosystems': bom_data.get('summary', {}).get('ecosystems', [])
            }

        return metadata

    except (CortexFileNotFound, CortexFileMalformed):
        return None

# Helper functions for BOM analysis
def _analyze_dependency_health(bom_data: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze the health of dependencies."""
    all_deps = []
    for deps in bom_data.get('dependencies', {}).values():
        all_deps.extend(deps)

    total_deps = len(all_deps)
    outdated_count = sum(1 for dep in all_deps if dep.get('deprecated', False))

    return {
        'total_dependencies': total_deps,
        'potentially_outdated': outdated_count,
        'health_score': max(0, 100 - (outdated_count / total_deps * 100)) if total_deps > 0 else 100,
        'ecosystems': list(set(dep.get('ecosystem', 'unknown') for dep in all_deps))
    }

def _generate_security_insights(bom_data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate security insights from BOM data."""
    security_analysis = bom_data.get('security_analysis', {})
    return {
        'risk_level': 'low',
        'vulnerable_dependencies': security_analysis.get('high_risk_dependencies', 0),
        'recommendations': [
            'Enable automated dependency scanning',
            'Set up security alerts for new vulnerabilities',
            'Regularly update dependencies'
        ]
    }

def _detect_architecture_patterns(bom_data: Dict[str, Any]) -> List[str]:
    """Detect architectural patterns from BOM data."""
    patterns = []
    services = bom_data.get('services', [])
    service_types = [s.get('service_type') for s in services]

    if 'database' in service_types and 'cache' in service_types:
        patterns.append('Caching Layer')
    if 'message_queue' in service_types:
        patterns.append('Message-Driven Architecture')
    if len([s for s in services if s.get('service_type') == 'database']) > 1:
        patterns.append('Polyglot Persistence')
    if len(services) > 3:
        patterns.append('Microservices Architecture')

    return patterns

def _suggest_modernization(bom_data: Dict[str, Any]) -> List[Dict[str, str]]:
    """Suggest modernization opportunities."""
    suggestions = []
    all_deps = []
    for deps in bom_data.get('dependencies', {}).values():
        all_deps.extend(deps)

    # Check for outdated frameworks
    python_deps = [d for d in all_deps if d.get('ecosystem') == 'python']
    if any(d.get('name') == 'Django' and d.get('version', '').startswith('2.') for d in python_deps):
        suggestions.append({
            'type': 'framework_upgrade',
            'title': 'Upgrade Django to version 4.x',
            'description': 'Django 2.x is no longer supported. Consider upgrading to Django 4.x for security updates and new features.'
        })

    # Check for containerization opportunities
    if not bom_data.get('infrastructure', {}).get('containerized', False):
        suggestions.append({
            'type': 'containerization',
            'title': 'Consider containerizing your application',
            'description': 'Containerization can improve deployment consistency and scalability.'
        })

    return suggestions

--- FILE_END: backend/lumiere_core/services/cortex_service.py ---

--- FILE_START: backend/lumiere_core/services/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/__init__.py

--- FILE_END: backend/lumiere_core/services/__init__.py ---

--- FILE_START: backend/lumiere_core/services/code_surgery.py ---
# backend/lumiere_core/services/code_surgery.py

import re
import ast
from typing import Dict, Tuple, Optional, List, Set, Any

# ==============================================================================
# SECTION 1: CORE EXECUTOR / ROUTER
# ==============================================================================

def execute_surgical_plan(
    original_contents: Dict[str, str],
    plan: List[Dict[str, Any]]
) -> Tuple[Dict[str, str], Dict[str, Any]]:
    """
    Main entry point for the Code Surgery agent.
    Iterates through a surgical plan and applies the specified operations.
    """
    modified_contents = original_contents.copy()
    report = {
        "operations_attempted": len(plan),
        "operations_succeeded": 0,
        "operations_failed": 0,
        "errors": []
    }

    for op in plan:
        operation_type = op.get("operation")
        file_path = op.get("file_path")

        try:
            if operation_type == "CREATE_FILE":
                modified_contents[file_path] = _handle_create_file(op)

            elif operation_type == "REPLACE_BLOCK":
                current_content = modified_contents.get(file_path, "")
                modified_contents[file_path] = _handle_replace_block(current_content, op)

            elif operation_type == "ADD_FIELD_TO_STRUCT":
                current_content = modified_contents.get(file_path, "")
                modified_contents[file_path] = _handle_add_field_to_struct(current_content, op)

            else:
                raise ValueError(f"Unknown operation type: '{operation_type}'")

            print(f"  ✓ Operation '{operation_type}' on '{file_path}' succeeded.")
            report["operations_succeeded"] += 1

        except Exception as e:
            error_msg = f"Operation '{operation_type}' on '{file_path}' failed: {e}"
            print(f"  ❌ {error_msg}")
            report["errors"].append(error_msg)
            report["operations_failed"] += 1

    return modified_contents, report


# ==============================================================================
# SECTION 2: OPERATION HANDLERS
# ==============================================================================

def _handle_create_file(operation: Dict[str, Any]) -> str:
    """Handles the CREATE_FILE operation. Returns the new file content."""
    return operation.get("content", "")

def _handle_replace_block(original_content: str, operation: Dict[str, Any]) -> str:
    """Handles the REPLACE_BLOCK operation for functions, methods, or classes."""
    target_id = operation.get("target_identifier")
    new_content = operation.get("content", "")
    file_path = operation.get("file_path", "")

    language = 'python' if file_path.endswith('.py') else 'rust' if file_path.endswith('.rs') else 'markdown' if file_path.endswith('.md') else 'javascript'

    boundaries = _find_block_boundaries(original_content, target_id, language)
    if not boundaries:
        raise ValueError(f"Could not find function/block '{target_id}' to replace.")

    start_pos, end_pos = boundaries
    # Ensure a newline after the replacement, unless it's the end of the file
    new_code_with_newline = new_content + "\n" if end_pos < len(original_content) else new_content
    return original_content[:start_pos] + new_code_with_newline + original_content[end_pos:]

def _handle_add_field_to_struct(original_content: str, operation: Dict[str, Any]) -> str:
    """Handles the ADD_FIELD_TO_STRUCT operation, specifically for Rust/C-like languages."""
    target_id = operation.get("target_identifier")
    new_field_line = operation.get("content", "")

    pattern = re.compile(
        rf"(struct\s+{re.escape(target_id)}\s*{{)(.*?)(\}})",
        re.DOTALL | re.MULTILINE
    )
    match = pattern.search(original_content)

    if not match:
        raise ValueError(f"Could not find struct definition for '{target_id}'.")

    struct_header, struct_body, struct_footer = match.groups()
    lines = struct_body.strip().split('\n')
    indentation = "    "
    if lines and lines[-1].strip():
        last_line = lines[-1]
        indentation = " " * (len(last_line) - len(last_line.lstrip()))

    new_body = struct_body.rstrip() + "\n" + indentation + new_field_line.strip() + "\n"
    new_struct_code = struct_header + new_body + struct_footer
    return original_content.replace(match.group(0), new_struct_code)


# ==============================================================================
# SECTION 3: THE FINAL, UPGRADED "FINDER"
# ==============================================================================

def _find_block_boundaries(content: str, block_name: str, language: str) -> Optional[Tuple[int, int]]:
    """
    Find the start and end text positions of a function/method/block in the content.
    --- THIS IS THE ENHANCED VERSION ---
    """
    if language == 'markdown':
        # ... (markdown logic is fine, no changes needed here) ...
        # For brevity, I'm omitting the markdown part. The code is in your file.
        heading_level = block_name.count('#')
        safe_block_name = re.escape(block_name.replace('#', '').strip())
        pattern = re.compile(rf"^(#{'{'}{heading_level}{'}'}\s*{safe_block_name}.*?)$", re.MULTILINE | re.IGNORECASE)
        match = pattern.search(content)

        if not match:
            return None

        start_pos = match.start()
        next_heading_pattern = re.compile(rf"^(#{'{1,'}{heading_level}{'}'}\s+.*)$", re.MULTILINE)
        next_match = next_heading_pattern.search(content, pos=match.end())

        end_pos = next_match.start() if next_match else len(content)
        return (start_pos, end_pos)

    # --- Start of significant changes ---
    if language == 'python':
        try:
            tree = ast.parse(content)
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)) and node.name == block_name:
                    # AST parsing logic remains the same
                    start_pos = -1
                    # This logic correctly finds the start and end using AST, which is reliable.
                    # No changes needed here.
                    if hasattr(node, 'decorator_list') and node.decorator_list:
                        # Find the start of the first decorator
                        start_pos = content.find(ast.get_source_segment(content, node.decorator_list[0]))
                    else:
                        # No decorators, find the start of the node itself
                        start_pos = content.find(ast.get_source_segment(content, node))

                    if start_pos != -1:
                        end_pos = start_pos + len(ast.get_source_segment(content, node))
                        return (start_pos, end_pos)
            return None # Explicitly return None if not found in AST walk
        except (SyntaxError, ValueError):
            # Fallback to regex if AST parsing fails
            pass

    # Generic Regex-based search for Rust, JS, etc.
    # Escape the block name to be safe in regex, but keep ` ` as a flexible spacer
    flexible_name = re.escape(block_name).replace(r'\ ', r'\s+')

    # NEW, more robust regex patterns for Rust `impl` blocks
    patterns = [
        # Rust `impl Trait for Struct` (e.g., `impl Default for ConfigFile`)
        rf"^(?:pub(?:\(.*\))?\s+)?impl\s+{flexible_name}\s*{{",
        # Standard impl block (e.g., `impl ConfigFile`)
        rf"^(?:pub(?:\(.*\))?\s+)?impl(?:<[^>]*>)?\s+{flexible_name}\s*{{",
        # Functions
        rf"^(?:pub(?:\(.*\))?\s+)?(?:async\s+)?fn\s+{flexible_name}\s*\(",
        # Structs
        rf"^(?:pub(?:\(.*\))?\s+)?struct\s+{flexible_name}\s*{{",
        # JS/TS/Python Fallbacks
        rf"^(?:export\s+)?(?:async\s+)?function\s+{flexible_name}\s*\(",
        rf"^\s*@?.*\s*def\s+{flexible_name}\s*\(",
        rf"^\s*class\s+{flexible_name}",
    ]

    for pattern_str in patterns:
        pattern = re.compile(pattern_str, re.MULTILINE)
        match = pattern.search(content)
        if match:
            start_pos = match.start()
            # Find the end of the block by matching braces `{}`
            if '{' in content[start_pos:match.end()]:
                pos = content.find('{', start_pos)
                brace_count = 1
                while pos < len(content) - 1:
                    pos += 1
                    if content[pos] == '{':
                        brace_count += 1
                    elif content[pos] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            return (start_pos, pos + 1)
            # Find the end of Python block by indentation
            elif ':' in content[start_pos:match.end()]:
                lines = content[start_pos:].splitlines()
                if not lines: return None
                base_indent = len(lines[0]) - len(lines[0].lstrip())
                end_line_index = 0
                for i, line in enumerate(lines[1:]):
                    if line.strip() and (len(line) - len(line.lstrip()) <= base_indent):
                        end_line_index = i
                        break
                else:
                    end_line_index = len(lines) -1

                end_pos = start_pos + len("\n".join(lines[:end_line_index + 1]))
                return (start_pos, end_pos)

    return None


def get_relevant_code_from_cortex(content: str, rca_report: str, file_path: str) -> str:
    """
    Extracts relevant code sections from a file's content based on an RCA report.
    """
    if not content or not rca_report:
        return content

    keywords = _extract_keywords_from_rca(rca_report)
    if not keywords:
        return content

    if not file_path.endswith('.py'):
        relevant_lines = []
        for line in content.splitlines():
            if any(kw in line for kw in keywords):
                relevant_lines.append(line)
        return "\n".join(relevant_lines) if relevant_lines else content

    try:
        tree = ast.parse(content)
    except SyntaxError:
        return content

    relevant_nodes = []
    import_nodes = [node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))]

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            if node.name in keywords:
                relevant_nodes.append(node)
                if isinstance(node, ast.ClassDef):
                    for sub_node in node.body:
                        if isinstance(sub_node, (ast.FunctionDef, ast.AsyncFunctionDef)) and sub_node.name in keywords and sub_node not in relevant_nodes:
                            relevant_nodes.append(sub_node)

    if not relevant_nodes:
        return content

    code_parts = [ast.get_source_segment(content, n) for n in import_nodes if n]
    if code_parts and relevant_nodes:
        code_parts.append("\n... # (Code imports)\n")

    added_sources = set()
    for node in sorted(relevant_nodes, key=lambda n: n.lineno):
        source_segment = ast.get_source_segment(content, node)
        if source_segment and source_segment not in added_sources:
            code_parts.append(source_segment)
            added_sources.add(source_segment)

    separator = "\n\n... # (Code omitted for brevity)\n\n"
    final_code = separator.join(part for part in code_parts if part)

    print(f"  🧠 Compressed code for '{file_path}'. Original: {len(content)} chars, Compressed: {len(final_code)} chars.")
    return final_code


def _extract_keywords_from_rca(rca_report: str) -> Set[str]:
    """Extracts potential function, class, and variable names from the RCA report."""
    keywords = set()
    keywords.update(re.findall(r'`([^`]+)`', rca_report))
    keywords.update(re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', rca_report))
    keywords.update(re.findall(r'\b([A-Z][a-z]+(?:[A-Z][a-z]+)*|[a-z]+(?:_[a-z]+)+)\b', rca_report))

    cleaned_keywords = set()
    for kw in keywords:
        cleaned = kw.split('.')[-1].split('::')[-1]
        if len(cleaned) > 2:
            cleaned_keywords.add(cleaned)

    return cleaned_keywords

--- FILE_END: backend/lumiere_core/services/code_surgery.py ---

--- FILE_START: backend/lumiere_core/services/ambassador.py ---
# In backend/lumiere_core/services/ambassador.py

import os
import re
import time
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dotenv import load_dotenv

from github import Github, GithubException

from .github import scrape_github_issue, _parse_github_issue_url

from . import llm_service
from .llm_service import TaskType
from ingestion.crawler import IntelligentCrawler

# --- Configuration ---
load_dotenv(dotenv_path=Path(__file__).resolve().parent.parent / '.env')
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
GITHUB_USERNAME = os.getenv("GITHUB_FORK_USERNAME")

if not GITHUB_TOKEN or not GITHUB_USERNAME:
    raise ValueError("GITHUB_ACCESS_TOKEN and GITHUB_FORK_USERNAME must be set in the .env file.")

g = Github(GITHUB_TOKEN)
user = g.get_user(GITHUB_USERNAME)

def _sanitize_branch_name(text: str) -> str:
    """Creates a URL- and git-safe branch name from a string."""
    text = text.lower()
    text = re.sub(r'[\s/]+', '-', text)
    text = re.sub(r'[^a-z0-9-]', '', text)
    text = text.strip('-')
    return text[:60]

def _validate_file_changes(modified_files: Dict[str, str]) -> List[str]:
    """Validates the file changes dictionary and returns any validation errors."""
    errors = []

    if not modified_files:
        errors.append("No files provided for modification")
        return errors

    for file_path, content in modified_files.items():
        if not isinstance(file_path, str) or not file_path.strip():
            errors.append(f"Invalid file path: {repr(file_path)}")

        if not isinstance(content, str):
            errors.append(f"Invalid content type for {file_path}: expected string, got {type(content)}")

        # Check for potentially dangerous paths
        if file_path.startswith('/') or '..' in file_path:
            errors.append(f"Potentially unsafe file path: {file_path}")

    return errors

def _write_files_safely(repo_path: Path, modified_files: Dict[str, str]) -> List[str]:
    """
    Safely writes files to the repository with proper error handling.
    Returns a list of successfully written files.
    """
    successfully_written = []

    for file_path, new_content in modified_files.items():
        try:
            full_target_path = repo_path / file_path

            # Ensure parent directories exist
            full_target_path.parent.mkdir(parents=True, exist_ok=True)

            # Create backup if file exists
            backup_path = None
            if full_target_path.exists():
                backup_path = full_target_path.with_suffix(full_target_path.suffix + '.bak')
                full_target_path.rename(backup_path)

            try:
                # Write new content with explicit encoding
                full_target_path.write_text(new_content, encoding='utf-8')

                # Stage the file
                subprocess.run(
                    ['git', 'add', str(full_target_path)],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    check=True
                )

                successfully_written.append(file_path)
                print(f"✓ Staged changes for: {file_path}")

                # Remove backup if successful
                if backup_path and backup_path.exists():
                    backup_path.unlink()

            except Exception as e:
                # Restore backup if write failed
                if backup_path and backup_path.exists():
                    if full_target_path.exists():
                        full_target_path.unlink()
                    backup_path.rename(full_target_path)
                raise e

        except Exception as e:
            print(f"⚠ Failed to write and stage {file_path}: {e}")
            # Continue with other files rather than failing completely
            continue

    return successfully_written

def _run_git_command(command: List[str], repo_path: Path, description: str) -> bool:
    """
    Runs a git command with proper error handling and logging.
    Returns True if successful, False otherwise.
    """
    try:
        subprocess.run(
            command,
            cwd=repo_path,
            capture_output=True,
            text=True,
            check=True
        )
        print(f"✓ {description}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"✗ {description} failed: {e.stderr.strip()}")
        return False

def _generate_pr_body(issue_data: Dict[str, Any], issue_number: int, modified_files: List[str]) -> str:
    """Generates a comprehensive PR body with file change summary."""
    pr_body = f"""
This pull request was automatically generated and approved by the user via the Lumière Sémantique 'Socratic Dialogue' interface to address Issue #{issue_number}.

## Issue Summary
> {issue_data.get('body', 'No description provided.')[:500]}{'...' if len(issue_data.get('body', '')) > 500 else ''}

## Changes in this PR
This PR modifies **{len(modified_files)}** file(s) to resolve the issue:

"""

    for file_path in sorted(modified_files):
        pr_body += f"- `{file_path}`\n"

    pr_body += "\n---\n*This fix was validated by The Crucible against the project's existing test suite.*"

    return pr_body

def dispatch_pr(
    issue_url: str,
    modified_files: Dict[str, str],
    custom_commit_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Orchestrates the git operations and PR creation for a multi-file change set.
    """
    print("--- AMBASSADOR AGENT ACTIVATED (MULTI-FILE MODE) ---")
    validation_errors = _validate_file_changes(modified_files)
    if validation_errors:
        return {"error": f"Validation failed: {'; '.join(validation_errors)}"}
    print("\n[Step 1/4] Gathering Intel...")
    try:
        issue_data = scrape_github_issue(issue_url)
        if not issue_data: raise ValueError("Failed to scrape issue data.")
        parsed_url = _parse_github_issue_url(issue_url)
        if not parsed_url: raise ValueError("Could not parse issue URL.")
        owner, repo_name, issue_number = parsed_url
        repo_full_name = f"{owner}/{repo_name}"
    except Exception as e:
        return {"error": f"Intel gathering failed: {e}"}
    print("\n[Step 2/4] Preparing Repository...")
    try:
        upstream_repo = g.get_repo(repo_full_name)
        fork_name = f"{GITHUB_USERNAME}/{repo_name}"
        try:
            working_repo = g.get_repo(fork_name)
            print(f"✓ Using existing fork: {fork_name}")
        except GithubException:
            print("Creating fork...")
            working_repo = upstream_repo.create_fork()
            time.sleep(15)
        with IntelligentCrawler(repo_url=working_repo.clone_url) as crawler:
            repo_path = crawler.repo_path
            branch_name = f"lumiere-fix/{issue_number}-{_sanitize_branch_name(issue_data['title'])}"
            default_branch = upstream_repo.default_branch
            print(f"\n[Step 3/4] Applying Fix on new branch '{branch_name}'...")
            if not _run_git_command(['git', 'checkout', default_branch], repo_path, f"Switched to {default_branch}"):
                return {"error": "Failed to checkout default branch"}
            if not _run_git_command(['git', 'pull', upstream_repo.clone_url, default_branch], repo_path, f"Pulled latest from upstream {default_branch}"):
                 return {"error": "Failed to pull latest upstream changes"}
            if not _run_git_command(['git', 'checkout', '-b', branch_name], repo_path, f"Created branch {branch_name}"):
                _run_git_command(['git', 'checkout', branch_name], repo_path, f"Switched to existing branch {branch_name}")
            successfully_written = _write_files_safely(repo_path, modified_files)
            if not successfully_written:
                return {"error": "No files were successfully written"}
            if custom_commit_message:
                commit_message = custom_commit_message
            else:
                commit_prompt = f"Based on the issue title '{issue_data['title']}' and modified files: {', '.join(successfully_written)}, write a concise one-line Conventional Commits message. Output ONLY the message line."

                # --- THE CHANGE IS HERE ---
                commit_message = llm_service.generate_text(
                    commit_prompt,
                    task_type=TaskType.SIMPLE
                ).strip()

            if not _run_git_command(['git', 'commit', '-m', commit_message], repo_path, "Committed changes"):
                return {"error": "Failed to commit changes. Nothing to commit or git error."}
            if not _run_git_command(['git', 'push', '--set-upstream', 'origin', branch_name, '--force'], repo_path, "Pushed branch"):
                return {"error": "Failed to push branch"}
            print("\n[Step 4/4] Creating Pull Request...")
            pr_title = f"fix: {issue_data['title']} (resolves #{issue_number})"
            pr_body = _generate_pr_body(issue_data, issue_number, successfully_written)
            head_branch = f"{working_repo.owner.login}:{branch_name}"
            pull_request = upstream_repo.create_pull(
                title=pr_title, body=pr_body, head=head_branch, base=default_branch
            )
            print(f"✓ Pull Request created: {pull_request.html_url}")
            return {"status": "success", "pull_request_url": pull_request.html_url}
    except Exception as e:
        error_details = str(e)
        if isinstance(e, GithubException): error_details = e.data.get('message', str(e))
        return {"error": f"Operation failed: {error_details}"}

--- FILE_END: backend/lumiere_core/services/ambassador.py ---

--- FILE_START: backend/lumiere_core/services/scaffolding.py ---
# backend/lumiere_core/services/scaffolding.py

import json
import re
import traceback
from pathlib import Path
from typing import Dict, Optional, List, Any, Tuple
from enum import Enum

from . import llm_service
from .utils import clean_llm_code_output
from . import code_surgery

# Enhanced language configuration with comprehensive polyglot support
class TaskType(Enum):
    CODE_GENERATION = "code_generation"
    COMPLEX_REASONING = "complex_reasoning"
    ANALYSIS = "analysis"

POLYGLOT_LANGUAGE_CONFIG = {
    # Web Technologies
    '.js': {
        'name': 'JavaScript',
        'family': 'web',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['dynamic', 'interpreted', 'event-driven'],
        'frameworks': ['React', 'Vue', 'Angular', 'Express', 'Node.js'],
        'common_patterns': ['async/await', 'promises', 'closures', 'prototypes']
    },
    '.ts': {
        'name': 'TypeScript',
        'family': 'web',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'object-oriented'],
        'frameworks': ['Angular', 'React', 'Vue', 'NestJS'],
        'common_patterns': ['interfaces', 'generics', 'decorators', 'modules']
    },
    '.jsx': {
        'name': 'JavaScript (React)',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'jsx',
        'features': ['virtual-dom', 'component-lifecycle', 'hooks'],
        'frameworks': ['React', 'Next.js', 'Gatsby'],
        'common_patterns': ['JSX', 'hooks', 'state management', 'props']
    },
    '.tsx': {
        'name': 'TypeScript (React)',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'jsx',
        'features': ['static-typing', 'virtual-dom', 'component-lifecycle'],
        'frameworks': ['React', 'Next.js'],
        'common_patterns': ['typed props', 'interfaces', 'generic components']
    },
    '.vue': {
        'name': 'Vue.js',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'template-based',
        'features': ['reactive', 'template-driven', 'single-file-components'],
        'frameworks': ['Vue', 'Nuxt.js', 'Quasar'],
        'common_patterns': ['template', 'script', 'style', 'composition API']
    },

    # Backend Languages
    '.py': {
        'name': 'Python',
        'family': 'general-purpose',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'indented',
        'features': ['dynamic', 'interpreted', 'duck-typing', 'comprehensive-stdlib'],
        'frameworks': ['Django', 'Flask', 'FastAPI', 'Pyramid'],
        'common_patterns': ['list comprehensions', 'decorators', 'context managers', 'generators']
    },
    '.java': {
        'name': 'Java',
        'family': 'enterprise',
        'paradigm': 'object-oriented',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'garbage-collected', 'platform-independent'],
        'frameworks': ['Spring', 'Spring Boot', 'Hibernate', 'Struts'],
        'common_patterns': ['dependency injection', 'annotations', 'interfaces', 'inheritance']
    },
    '.cs': {
        'name': 'C#',
        'family': 'enterprise',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'garbage-collected', 'linq'],
        'frameworks': ['ASP.NET', '.NET Core', 'Entity Framework', 'Blazor'],
        'common_patterns': ['properties', 'events', 'delegates', 'async/await']
    },
    '.go': {
        'name': 'Go',
        'family': 'systems',
        'paradigm': 'procedural',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'concurrent', 'simple'],
        'frameworks': ['Gin', 'Echo', 'Fiber', 'Buffalo'],
        'common_patterns': ['goroutines', 'channels', 'interfaces', 'composition']
    },
    '.rs': {
        'name': 'Rust',
        'family': 'systems',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['memory-safe', 'zero-cost-abstractions', 'ownership', 'pattern-matching'],
        'frameworks': ['Actix', 'Rocket', 'Warp', 'Axum'],
        'common_patterns': ['ownership', 'borrowing', 'traits', 'match expressions']
    },
    '.swift': {
        'name': 'Swift',
        'family': 'mobile',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'modern',
        'features': ['type-safe', 'memory-safe', 'performant', 'expressive'],
        'frameworks': ['SwiftUI', 'UIKit', 'Vapor', 'Perfect'],
        'common_patterns': ['optionals', 'closures', 'protocols', 'extensions']
    },

    # Functional Languages
    '.hs': {
        'name': 'Haskell',
        'family': 'functional',
        'paradigm': 'purely-functional',
        'syntax_style': 'mathematical',
        'features': ['lazy-evaluation', 'immutable', 'type-inference', 'monads'],
        'frameworks': ['Yesod', 'Snap', 'Happstack'],
        'common_patterns': ['monads', 'functors', 'type classes', 'pattern matching']
    },
    '.ex': {
        'name': 'Elixir',
        'family': 'functional',
        'paradigm': 'functional',
        'syntax_style': 'ruby-like',
        'features': ['actor-model', 'fault-tolerant', 'concurrent', 'distributed'],
        'frameworks': ['Phoenix', 'Nerves', 'Broadway'],
        'common_patterns': ['pattern matching', 'pipe operator', 'GenServer', 'supervision trees']
    },
    '.clj': {
        'name': 'Clojure',
        'family': 'functional',
        'paradigm': 'functional',
        'syntax_style': 'lisp',
        'features': ['immutable', 'jvm-hosted', 'concurrent', 'homoiconic'],
        'frameworks': ['Ring', 'Compojure', 'Luminus'],
        'common_patterns': ['s-expressions', 'persistent data structures', 'multimethods', 'macros']
    },

    # Systems Languages
    '.c': {
        'name': 'C',
        'family': 'systems',
        'paradigm': 'procedural',
        'syntax_style': 'c-like',
        'features': ['low-level', 'manual-memory', 'portable', 'efficient'],
        'frameworks': ['glib', 'SDL', 'OpenGL'],
        'common_patterns': ['pointers', 'manual memory management', 'header files', 'macros']
    },
    '.cpp': {
        'name': 'C++',
        'family': 'systems',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['object-oriented', 'template-metaprogramming', 'raii', 'zero-overhead'],
        'frameworks': ['Qt', 'Boost', 'POCO', 'FLTK'],
        'common_patterns': ['classes', 'templates', 'RAII', 'smart pointers']
    },

    # Scripting Languages
    '.rb': {
        'name': 'Ruby',
        'family': 'scripting',
        'paradigm': 'object-oriented',
        'syntax_style': 'natural',
        'features': ['dynamic', 'expressive', 'metaprogramming', 'blocks'],
        'frameworks': ['Rails', 'Sinatra', 'Hanami'],
        'common_patterns': ['blocks', 'mixins', 'metaprogramming', 'duck typing']
    },
    '.php': {
        'name': 'PHP',
        'family': 'web',
        'paradigm': 'imperative',
        'syntax_style': 'c-like',
        'features': ['web-focused', 'dynamic', 'interpreted', 'embedded'],
        'frameworks': ['Laravel', 'Symfony', 'CodeIgniter', 'Zend'],
        'common_patterns': ['superglobals', 'include/require', 'associative arrays', 'traits']
    },

    # Mobile/Cross-platform
    '.dart': {
        'name': 'Dart',
        'family': 'mobile',
        'paradigm': 'object-oriented',
        'syntax_style': 'c-like',
        'features': ['widget-based', 'hot-reload', 'ahead-of-time', 'just-in-time'],
        'frameworks': ['Flutter', 'AngularDart'],
        'common_patterns': ['widgets', 'futures', 'streams', 'isolates']
    },
    '.kt': {
        'name': 'Kotlin',
        'family': 'mobile',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'modern',
        'features': ['null-safe', 'interoperable', 'concise', 'expressive'],
        'frameworks': ['Android SDK', 'Ktor', 'Spring'],
        'common_patterns': ['null safety', 'data classes', 'extension functions', 'coroutines']
    },

    # Data Science
    '.r': {
        'name': 'R',
        'family': 'statistical',
        'paradigm': 'functional',
        'syntax_style': 'domain-specific',
        'features': ['statistical', 'vectorized', 'data-analysis', 'visualization'],
        'frameworks': ['Shiny', 'ggplot2', 'dplyr', 'tidyverse'],
        'common_patterns': ['data frames', 'vectorization', 'pipes', 'factors']
    },
    '.jl': {
        'name': 'Julia',
        'family': 'scientific',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'mathematical',
        'features': ['high-performance', 'scientific', 'multiple-dispatch', 'metaprogramming'],
        'frameworks': ['Genie.jl', 'Flux.jl', 'DifferentialEquations.jl'],
        'common_patterns': ['multiple dispatch', 'macros', 'broadcasting', 'type system']
    }
}

def _get_enhanced_language_config(file_path: str) -> Dict[str, Any]:
    """Get comprehensive language configuration including paradigm and features."""
    ext = Path(file_path).suffix.lower()
    config = POLYGLOT_LANGUAGE_CONFIG.get(ext, {
        'name': 'Unknown',
        'family': 'unknown',
        'paradigm': 'unknown',
        'syntax_style': 'unknown',
        'features': [],
        'frameworks': [],
        'common_patterns': []
    })

    # Add file extension for reference
    config['extension'] = ext
    return config

def _detect_polyglot_context(target_files: List[str], cortex_data: Dict) -> Dict[str, Any]:
    """
    Enhanced context detection for polyglot projects.
    Analyzes multiple languages and provides comprehensive project context.
    """
    if not target_files:
        return _get_enhanced_language_config('')

    language_analysis = {}
    framework_hints = set()
    project_patterns = set()

    # Analyze each target file
    for file_path in target_files:
        config = _get_enhanced_language_config(file_path)
        language_name = config['name']

        if language_name not in language_analysis:
            language_analysis[language_name] = {
                'files': [],
                'config': config,
                'weight': 0
            }

        language_analysis[language_name]['files'].append(file_path)
        language_analysis[language_name]['weight'] += 1

        # Collect framework hints from cortex if available
        if cortex_data and 'files' in cortex_data:
            for file_cortex in cortex_data['files']:
                if file_cortex['file_path'] == file_path:
                    if 'framework_hints' in file_cortex:
                        framework_hints.update(file_cortex['framework_hints'])
                    break

        # Add common patterns
        project_patterns.update(config['common_patterns'])

    # Determine primary language by weight and importance
    if language_analysis:
        # Weight by file count and language importance
        language_priority = {
            'Python': 10, 'JavaScript': 9, 'TypeScript': 9, 'Java': 8, 'C#': 8,
            'Go': 7, 'Rust': 7, 'Swift': 6, 'C++': 6, 'Ruby': 5, 'PHP': 5,
            'Dart': 4, 'Kotlin': 4, 'Haskell': 3, 'Elixir': 3, 'Clojure': 3
        }

        weighted_scores = {}
        for lang_name, data in language_analysis.items():
            file_weight = data['weight']
            priority_weight = language_priority.get(lang_name, 1)
            weighted_scores[lang_name] = file_weight * priority_weight

        primary_language = max(weighted_scores, key=weighted_scores.get)
        primary_config = language_analysis[primary_language]['config']
    else:
        primary_language = 'Unknown'
        primary_config = _get_enhanced_language_config('')

    return {
        'primary_language': primary_language,
        'primary_config': primary_config,
        'all_languages': language_analysis,
        'detected_frameworks': list(framework_hints),
        'project_patterns': list(project_patterns),
        'is_polyglot': len(language_analysis) > 1,
        'polyglot_complexity': len(language_analysis)
    }

def _extract_json_from_llm(raw_text: str) -> Optional[str]:
    """Enhanced JSON extraction with better error handling."""
    # Try multiple patterns for JSON extraction
    patterns = [
        r'```(?:json)?\s*(\[.*?\])\s*```',  # Standard markdown code block
        r'```(?:json)?\s*(\{.*?\})\s*```',  # Object in code block
        r'(\[(?:[^[\]]|(?1))*\])',          # Recursive bracket matching
        r'(\{(?:[^{}]|(?1))*\})'            # Recursive brace matching
    ]

    for pattern in patterns:
        match = re.search(pattern, raw_text, re.DOTALL)
        if match:
            return match.group(1)

    # Fallback: try to find JSON-like structures
    try:
        start = raw_text.index('[')
        end = raw_text.rindex(']') + 1
        return raw_text[start:end]
    except ValueError:
        try:
            start = raw_text.index('{')
            end = raw_text.rindex('}') + 1
            return raw_text[start:end]
        except ValueError:
            return None

def _validate_and_parse_surgical_plan(json_str: str, language_context: Dict[str, Any]) -> Tuple[Optional[List[Dict]], str]:
    """Enhanced validation with language-specific checks."""
    if not json_str:
        return None, "AI response was empty or did not contain a JSON object."

    try:
        plan = json.loads(json_str)
    except json.JSONDecodeError as e:
        return None, f"AI response was not valid JSON. Parser error: {e}"

    if not isinstance(plan, list):
        return None, f"AI response was not a list of operations. Found type: {type(plan).__name__}."

    # Enhanced operation validation
    valid_operations = {
        "REPLACE_BLOCK": {
            "required": ["file_path", "target_identifier", "content"],
            "description": "Replace an entire function, method, class, or code block"
        },
        "ADD_FIELD_TO_STRUCT": {
            "required": ["file_path", "target_identifier", "content"],
            "description": "Add a new field to a struct (Rust/C/Go)"
        },
        "CREATE_FILE": {
            "required": ["file_path", "content"],
            "description": "Create a new file"
        },
        "INSERT_CODE_AT": {
            "required": ["file_path", "line_number", "content"],
            "description": "Insert code at a specific line number"
        },
        "ADD_IMPORT": {
            "required": ["file_path", "import_statement"],
            "description": "Add an import/require/using statement"
        },
        "REPLACE_FUNCTION": {
            "required": ["file_path", "function_name", "content"],
            "description": "Replace a specific function"
        },
        "ADD_METHOD_TO_CLASS": {
            "required": ["file_path", "class_name", "method_content"],
            "description": "Add a method to an existing class"
        }
    }

    for i, op in enumerate(plan):
        op_num = i + 1
        if not isinstance(op, dict):
            return None, f"Operation #{op_num} is not a valid object."

        operation_type = op.get("operation")
        if not operation_type:
            return None, f"Operation #{op_num} is missing the required 'operation' field."

        if operation_type not in valid_operations:
            return None, f"Operation #{op_num} has an unknown operation type: '{operation_type}'. Valid operations: {list(valid_operations.keys())}"

        required_fields = valid_operations[operation_type]["required"]
        missing = [field for field in required_fields if field not in op]
        if missing:
            return None, f"Operation #{op_num} ('{operation_type}') is missing required fields: {', '.join(missing)}."

        # Language-specific validation
        file_path = op.get("file_path", "")
        if file_path:
            file_config = _get_enhanced_language_config(file_path)
            language_name = file_config['name']

            # Validate operation compatibility with language
            if operation_type == "ADD_FIELD_TO_STRUCT":
                if language_name not in ['Rust', 'C', 'C++', 'Go']:
                    return None, f"Operation #{op_num}: ADD_FIELD_TO_STRUCT is not applicable to {language_name}. Use ADD_METHOD_TO_CLASS for object-oriented languages."

    return plan, ""

def _scout_expand_scope(
    original_contents: Dict[str, str],
    surgical_plan: List[Dict],
    full_file_map: Dict[str, str]
) -> Dict[str, str]:
    """
    Enhanced Scout Service with polyglot awareness.
    Ensures the file scope matches the AI's plan by loading any missing files.
    """
    print("🛰️  Activating Enhanced Scout: Verifying and expanding polyglot file scope...")

    plan_files = {op['file_path'] for op in surgical_plan if 'file_path' in op}

    updated_contents = original_contents.copy()
    expanded_files_loaded = 0
    language_stats = {}

    for file_path in plan_files:
        if file_path not in updated_contents:
            updated_contents[file_path] = full_file_map.get(file_path, "")
            print(f"  → Scout expanded scope to include: {file_path}")
            expanded_files_loaded += 1

            # Track language diversity
            config = _get_enhanced_language_config(file_path)
            language_name = config['name']
            language_stats[language_name] = language_stats.get(language_name, 0) + 1

    if expanded_files_loaded > 0:
        print(f"✓ Scout successfully expanded scope with {expanded_files_loaded} new file(s).")
        if language_stats:
            print(f"  → Languages in expanded scope: {', '.join(language_stats.keys())}")
    else:
        print("✓ File scope is consistent with the AI's plan.")

    return updated_contents

def _generate_language_aware_prompt(
    target_files: List[str],
    instruction: str,
    rca_report: str,
    language_context: Dict[str, Any],
    file_content_section: str,
    refinement_context: str = ""
) -> str:
    """
    Generate a sophisticated, language-aware prompt for the AI.
    """
    primary_language = language_context['primary_language']
    primary_config = language_context['primary_config']
    is_polyglot = language_context['is_polyglot']

    # Build language expertise section
    language_expertise = f"""You are an expert software architect specializing in {primary_language}"""

    if is_polyglot:
        all_languages = list(language_context['all_languages'].keys())
        language_expertise += f" and polyglot development with {', '.join(all_languages)}"

    language_expertise += "."

    # Add language-specific context
    language_context_section = f"""
### LANGUAGE CONTEXT ###
Primary Language: {primary_language}
- Paradigm: {primary_config.get('paradigm', 'unknown')}
- Syntax Style: {primary_config.get('syntax_style', 'unknown')}
- Key Features: {', '.join(primary_config.get('features', []))}
- Common Patterns: {', '.join(primary_config.get('common_patterns', []))}
"""

    if primary_config.get('frameworks'):
        language_context_section += f"- Popular Frameworks: {', '.join(primary_config['frameworks'])}\n"

    if is_polyglot:
        language_context_section += f"""
This is a polyglot project with {language_context['polyglot_complexity']} languages:
"""
        for lang_name, lang_data in language_context['all_languages'].items():
            files = lang_data['files']
            language_context_section += f"- {lang_name}: {len(files)} file(s) - {', '.join(files)}\n"

    if language_context['detected_frameworks']:
        language_context_section += f"Detected Frameworks: {', '.join(language_context['detected_frameworks'])}\n"

    # Enhanced operation examples based on language
    operation_examples = _get_language_specific_operation_examples(primary_language, primary_config)

    surgical_prompt = f"""{language_expertise} Your task is to generate a precise surgical plan to fix a bug in this {'polyglot' if is_polyglot else primary_language} project.

{language_context_section}

<Goal>{instruction}</Goal>
<RCA_Report>{rca_report}</RCA_Report>
{refinement_context}
{file_content_section}

### YOUR TASK ###
Based on all the provided information, create a step-by-step surgical plan as a JSON array that follows {primary_language} best practices{' and handles the polyglot nature of this project' if is_polyglot else ''}.

### AVAILABLE OPERATIONS ###
You can use the following operations in your plan:

1. `"operation": "CREATE_FILE"`: Creates a new file.
   - Required fields: `file_path`, `content`.
   - Use for: New modules, configuration files, or missing dependencies.

2. `"operation": "REPLACE_BLOCK"`: Replaces an entire function, method, class, or other code block.
   - Required fields: `file_path`, `target_identifier` (the unique name/signature), `content`.
   - Use for: Complete rewrites of functions, classes, or major code blocks.

3. `"operation": "ADD_FIELD_TO_STRUCT"`: (For Rust/C/Go) Adds a new field to a struct.
   - Required fields: `file_path`, `target_identifier` (struct name), `content`.
   - Use for: Adding new data fields to existing structures.

4. `"operation": "INSERT_CODE_AT"`: Inserts code at a specific line number.
   - Required fields: `file_path`, `line_number`, `content`.
   - Use for: Adding code at precise locations.

5. `"operation": "ADD_IMPORT"`: Adds an import/require/using statement.
   - Required fields: `file_path`, `import_statement`.
   - Use for: Adding dependencies or modules.

6. `"operation": "REPLACE_FUNCTION"`: Replaces a specific function.
   - Required fields: `file_path`, `function_name`, `content`.
   - Use for: Function-specific changes.

7. `"operation": "ADD_METHOD_TO_CLASS"`: Adds a method to an existing class.
   - Required fields: `file_path`, `class_name`, `method_content`.
   - Use for: Extending classes with new functionality.

{operation_examples}

### IMPORTANT GUIDELINES ###
- Follow {primary_language} naming conventions and style guidelines
- Ensure type safety and error handling appropriate for {primary_language}
- Use idiomatic {primary_language} patterns: {', '.join(primary_config.get('common_patterns', [])[:3])}
{f'- Consider cross-language compatibility in this polyglot project' if is_polyglot else ''}
- Maintain consistency with existing code architecture
- Include proper documentation/comments in the target language style

### RESPONSE FORMAT ###
- You MUST respond with ONLY a valid JSON array `[...]`.
- Do not include any explanations, markdown fences, or other text.
- If you need to modify a file not in the provided context, add an operation for that file.

Generate the {primary_language} surgical plan now."""

    return surgical_prompt

def _get_language_specific_operation_examples(language: str, config: Dict[str, Any]) -> str:
    """Generate language-specific operation examples."""
    examples = f"\n### {language.upper()} SPECIFIC EXAMPLES ###\n"

    if language == "Python":
        examples += """
<Python_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/main.py",
    "import_statement": "from typing import Optional, Dict"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/utils.py",
    "function_name": "process_data",
    "content": "def process_data(data: Dict[str, Any]) -> Optional[str]:\\n    \\"\\"\\"Process data with type safety.\\"\\"\\"\\n    if not data:\\n        return None\\n    return str(data.get('result', ''))"
  }
]
```
</Python_Examples>"""

    elif language in ["JavaScript", "TypeScript"]:
        examples += """
<JavaScript_TypeScript_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/utils.js",
    "import_statement": "import { validateInput } from './validators';"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/api.js",
    "function_name": "fetchData",
    "content": "async function fetchData(url) {\\n  try {\\n    const response = await fetch(url);\\n    if (!response.ok) throw new Error('Network error');\\n    return await response.json();\\n  } catch (error) {\\n    console.error('Fetch failed:', error);\\n    throw error;\\n  }\\n}"
  }
]
```
</JavaScript_TypeScript_Examples>"""

    elif language == "Java":
        examples += """
<Java_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/main/java/Main.java",
    "import_statement": "import java.util.Optional;"
  },
  {
    "operation": "ADD_METHOD_TO_CLASS",
    "file_path": "src/main/java/UserService.java",
    "class_name": "UserService",
    "method_content": "    public Optional<User> findUserById(Long id) {\\n        if (id == null || id <= 0) {\\n            return Optional.empty();\\n        }\\n        return userRepository.findById(id);\\n    }"
  }
]
```
</Java_Examples>"""

    elif language == "Go":
        examples += """
<Go_Examples>
```json
[
  {
    "operation": "ADD_FIELD_TO_STRUCT",
    "file_path": "internal/models/user.go",
    "target_identifier": "User",
    "content": "    Email    string `json:\\"email\\" validate:\\"required,email\\"`"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "internal/handlers/user.go",
    "function_name": "CreateUser",
    "content": "func CreateUser(w http.ResponseWriter, r *http.Request) {\\n    var user User\\n    if err := json.NewDecoder(r.Body).Decode(&user); err != nil {\\n        http.Error(w, \\"Invalid JSON\\", http.StatusBadRequest)\\n        return\\n    }\\n    // Process user creation\\n    w.WriteHeader(http.StatusCreated)\\n}"
  }
]
```
</Go_Examples>"""

    elif language == "Rust":
        examples += """
<Rust_Examples>
```json
[
  {
    "operation": "ADD_FIELD_TO_STRUCT",
    "file_path": "src/models/user.rs",
    "target_identifier": "User",
    "content": "    pub email: Option<String>,"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/lib.rs",
    "function_name": "process_data",
    "content": "pub fn process_data(input: &str) -> Result<String, Box<dyn std::error::Error>> {\\n    if input.is_empty() {\\n        return Err(\\"Input cannot be empty\\".into());\\n    }\\n    Ok(input.to_uppercase())\\n}"
  }
]
```
</Rust_Examples>"""

    else:
        # Generic example for other languages
        examples += f"""
<{language}_Examples>
```json
[
  {{
    "operation": "REPLACE_BLOCK",
    "file_path": "src/main{config.get('extension', '')}",
    "target_identifier": "main_function",
    "content": "// Enhanced main function with proper error handling\\n// TODO: Implement based on {language} best practices"
  }}
]
```
</{language}_Examples>"""

    return examples

def generate_scaffold(
    repo_id: str,
    target_files: List[str],
    instruction: str,
    rca_report: str,
    refinement_history: Optional[List[Dict[str, str]]] = None
) -> Dict[str, Any]:
    """
    Enhanced Code Scaffolding with comprehensive polyglot support and dynamic scope expansion.

    This version provides:
    - Multi-language project detection and analysis
    - Language-aware prompt generation
    - Framework-specific code generation hints
    - Enhanced operation validation
    - Polyglot project complexity handling
    """
    print(f"🔧 Initiating Enhanced Polyglot Surgical Scaffolding for {target_files} in repo '{repo_id}'")

    try:
        # Load and validate cortex data
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"
        if not cortex_path.exists():
            return {"error": "Cortex file not found", "details": f"Expected path: {cortex_path}"}

        with open(cortex_path, 'r', encoding='utf-8') as f:
            cortex_data = json.load(f)

        file_map = {file['file_path']: file['raw_content'] for file in cortex_data.get('files', [])}
        original_contents = {fp: file_map.get(fp, "") for fp in target_files}

        # Enhanced polyglot context detection
        language_context = _detect_polyglot_context(target_files, cortex_data)

        print(f"  🌐 Detected project context:")
        print(f"     • Primary language: {language_context['primary_language']}")
        print(f"     • Project type: {'Polyglot' if language_context['is_polyglot'] else 'Monoglot'}")
        if language_context['is_polyglot']:
            print(f"     • Languages involved: {list(language_context['all_languages'].keys())}")
        if language_context['detected_frameworks']:
            print(f"     • Frameworks detected: {', '.join(language_context['detected_frameworks'])}")

        # --- STEP 1: GENERATE THE ENHANCED SURGICAL PLAN ---
        file_content_prompt_section = "\n\n### RELEVANT EXISTING CODE\n"
        for path, content in original_contents.items():
            if content:
                # Enhanced compression with language awareness
                compressed_content = code_surgery.get_relevant_code_from_cortex(content, rca_report, path)
                language_config = _get_enhanced_language_config(path)
                file_content_prompt_section += f"<file path=\"{path}\" language=\"{language_config['name']}\">\n{compressed_content}\n</file>\n\n"

        refinement_context = ""
        if refinement_history:
            refinement_context = "\n\n### PREVIOUS REFINEMENT ATTEMPTS\n"
            for i, refinement in enumerate(refinement_history[-2:]):
                feedback = refinement.get("feedback", "No feedback provided.")
                refinement_context += f"Attempt {i+1} Feedback: {feedback}\n"
            refinement_context += "\nPlease learn from the previous feedback and generate a better plan that follows language-specific best practices.\n"

        # Generate sophisticated language-aware prompt
        surgical_prompt = _generate_language_aware_prompt(
            target_files, instruction, rca_report, language_context,
            file_content_prompt_section, refinement_context
        )

        max_retries = 3
        surgical_plan = None
        last_llm_response = ""

        for attempt in range(max_retries):
            print(f"  🤖 Attempt {attempt + 1}: Calling LLM for {language_context['primary_language']} surgical plan...")

            # Enhanced LLM call with appropriate task type
            task_type = TaskType.CODE_GENERATION
            if language_context['is_polyglot'] or language_context['polyglot_complexity'] > 2:
                task_type = TaskType.COMPLEX_REASONING

            llm_response = llm_service.generate_text(
                surgical_prompt,
                task_type=task_type
            )
            last_llm_response = llm_response

            if not llm_response or not llm_response.strip():
                print("  ⚠️ Empty response, retrying...")
                continue

            # Enhanced JSON extraction and validation
            json_str = _extract_json_from_llm(llm_response)
            if json_str:
                plan, error_msg = _validate_and_parse_surgical_plan(json_str, language_context)
                if plan:
                    surgical_plan = plan
                    print(f"  ✓ Successfully parsed {language_context['primary_language']} surgical plan with {len(plan)} operations.")

                    # Log plan summary
                    operation_summary = {}
                    for op in plan:
                        op_type = op.get('operation', 'unknown')
                        operation_summary[op_type] = operation_summary.get(op_type, 0) + 1
                    print(f"    → Operations: {', '.join(f'{count}x {op}' for op, count in operation_summary.items())}")
                    break
                else:
                    print(f"  ❌ Blueprint Rejected (Attempt {attempt+1}/{max_retries}): {error_msg}")
                    print(f"  📝 Faulty JSON received: {json_str[:250]}...")
            else:
                print(f"  ❌ Could not extract JSON from LLM response (Attempt {attempt+1}/{max_retries}).")

        if not surgical_plan:
            return {
                "error": f"Failed to generate a valid {language_context['primary_language']} surgical plan after {max_retries} attempts.",
                "details": "The AI's final proposed plan was malformed or incomplete. This may be due to the complexity of the polyglot project or language-specific constraints.",
                "llm_response": last_llm_response,
                "language_context": language_context,
            }

        # Enhanced Scout with polyglot awareness
        final_contents_for_surgery = _scout_expand_scope(
            original_contents,
            surgical_plan,
            file_map
        )

        # Execute surgical plan with enhanced error handling
        print("🔬 Executing enhanced surgical plan...")
        modified_files, surgery_report = code_surgery.execute_surgical_plan(
            final_contents_for_surgery,
            surgical_plan
        )

        if surgery_report.get("errors"):
            return {
                "error": "Enhanced Code Surgery failed to apply the plan.",
                "details": surgery_report["errors"],
                "language_context": language_context,
                "partial_results": surgery_report.get("successes", [])
            }

        # Enhanced success reporting
        print("🎉 Enhanced polyglot surgical scaffolding completed successfully!")
        print(f"  ✓ Modified {len(modified_files)} files")
        print(f"  ✓ Applied {len(surgical_plan)} operations")
        print(f"  ✓ Primary language: {language_context['primary_language']}")

        if language_context['is_polyglot']:
            print(f"  ✓ Handled polyglot complexity: {language_context['polyglot_complexity']} languages")

        return {
            "modified_files": modified_files,
            "original_contents": final_contents_for_surgery,
            "plan_executed": surgical_plan,
            "surgery_report": surgery_report,
            "language_context": language_context,
            "polyglot_summary": {
                "primary_language": language_context['primary_language'],
                "is_polyglot": language_context['is_polyglot'],
                "languages_involved": list(language_context['all_languages'].keys()),
                "frameworks_detected": language_context['detected_frameworks'],
                "complexity_score": language_context['polyglot_complexity']
            }
        }

    except Exception as e:
        print(f"❌ Critical error in enhanced polyglot scaffolding: {str(e)}")
        return {
            "error": "A critical error occurred in the enhanced polyglot scaffolding service.",
            "details": str(e),
            "traceback": traceback.format_exc(),
        }

# --- Enhanced utility functions for polyglot support ---

def analyze_code_complexity(target_files: List[str], cortex_data: Dict) -> Dict[str, Any]:
    """
    Analyze the complexity of a polyglot codebase for better scaffolding decisions.
    """
    if not cortex_data or 'files' not in cortex_data:
        return {"error": "Invalid cortex data"}

    complexity_analysis = {
        "total_files": len(target_files),
        "languages": {},
        "frameworks": set(),
        "complexity_indicators": {},
        "recommendations": []
    }

    for file_path in target_files:
        config = _get_enhanced_language_config(file_path)
        language = config['name']

        if language not in complexity_analysis["languages"]:
            complexity_analysis["languages"][language] = {
                "file_count": 0,
                "total_lines": 0,
                "paradigm": config.get('paradigm', 'unknown'),
                "features": config.get('features', [])
            }

        complexity_analysis["languages"][language]["file_count"] += 1

        # Find corresponding file in cortex
        for file_cortex in cortex_data['files']:
            if file_cortex['file_path'] == file_path:
                lines = len(file_cortex['raw_content'].splitlines())
                complexity_analysis["languages"][language]["total_lines"] += lines

                if 'framework_hints' in file_cortex:
                    complexity_analysis["frameworks"].update(file_cortex['framework_hints'])
                break

    # Convert set to list for JSON serialization
    complexity_analysis["frameworks"] = list(complexity_analysis["frameworks"])

    # Generate complexity indicators
    language_count = len(complexity_analysis["languages"])
    if language_count > 1:
        complexity_analysis["complexity_indicators"]["polyglot"] = True
        complexity_analysis["complexity_indicators"]["language_diversity"] = language_count

    total_lines = sum(lang["total_lines"] for lang in complexity_analysis["languages"].values())
    complexity_analysis["complexity_indicators"]["total_lines"] = total_lines

    if total_lines > 10000:
        complexity_analysis["complexity_indicators"]["large_codebase"] = True

    # Generate recommendations
    if language_count > 3:
        complexity_analysis["recommendations"].append(
            "High language diversity detected. Consider breaking down changes into language-specific phases."
        )

    if len(complexity_analysis["frameworks"]) > 2:
        complexity_analysis["recommendations"].append(
            "Multiple frameworks detected. Ensure cross-framework compatibility."
        )

    return complexity_analysis

def get_language_specific_best_practices(language: str) -> List[str]:
    """
    Return language-specific best practices for code generation.
    """
    practices = {
        "Python": [
            "Follow PEP 8 style guidelines",
            "Use type hints for better code documentation",
            "Implement proper exception handling",
            "Use list comprehensions and generator expressions where appropriate",
            "Follow the principle of 'Pythonic' code"
        ],
        "JavaScript": [
            "Use const and let instead of var",
            "Implement proper error handling with try-catch",
            "Use async/await for asynchronous operations",
            "Follow ESLint recommendations",
            "Use destructuring and modern ES6+ features"
        ],
        "TypeScript": [
            "Leverage strong typing features",
            "Use interfaces and type definitions",
            "Implement proper generic types",
            "Use strict mode configuration",
            "Follow Angular/React specific patterns if applicable"
        ],
        "Java": [
            "Follow Java naming conventions",
            "Use dependency injection patterns",
            "Implement proper exception handling",
            "Use Optional for nullable values",
            "Follow SOLID principles"
        ],
        "Go": [
            "Follow Go formatting standards (gofmt)",
            "Use goroutines and channels for concurrency",
            "Implement proper error handling",
            "Keep interfaces small and focused",
            "Use composition over inheritance"
        ],
        "Rust": [
            "Leverage ownership and borrowing system",
            "Use Result and Option types for error handling",
            "Implement proper trait patterns",
            "Use pattern matching effectively",
            "Follow Rust naming conventions"
        ]
    }

    return practices.get(language, [
        "Follow language-specific style guidelines",
        "Implement proper error handling",
        "Use idiomatic patterns for the language",
        "Ensure code readability and maintainability"
    ])

def detect_framework_patterns(file_content: str, language: str) -> List[str]:
    """
    Detect framework-specific patterns in code for better scaffolding.
    """
    patterns = []
    content_lower = file_content.lower()

    framework_signatures = {
        "react": ["usestate", "useeffect", "jsx", "react.component"],
        "vue": ["vue.component", "template>", "@click", "v-if"],
        "angular": ["@component", "@injectable", "ngmodule", "ngoninit"],
        "express": ["app.get", "app.post", "express()", "router."],
        "flask": ["@app.route", "flask()", "request."],
        "django": ["models.model", "views.view", "urls.py", "django."],
        "spring": ["@controller", "@service", "@autowired", "@requestmapping"],
        "laravel": ["route::", "eloquent", "blade.php", "artisan"]
    }

    for framework, signatures in framework_signatures.items():
        if any(sig in content_lower for sig in signatures):
            patterns.append(framework)

    return patterns

# --- Backward Compatibility ---
def generate_scaffold_legacy(repo_id: str, target_files: List[str], instruction: str, rca_report: str) -> Dict[str, Any]:
    """Legacy interface for backward compatibility."""
    return generate_scaffold(repo_id, target_files, instruction, rca_report)

--- FILE_END: backend/lumiere_core/services/scaffolding.py ---

--- FILE_START: backend/lumiere_core/services/dance_service.py ---
# backend/lumiere_core/services/dance_service.py

import logging
import json
import networkx as nx
from typing import Dict, Any, List, Optional, Tuple, Set
from collections import defaultdict, deque
from pathlib import Path
import xml.etree.ElementTree as ET
import time

from . import cortex_service, oracle_service
from .oracle_service import OracleService

logger = logging.getLogger(__name__)


class DanceService:
    """The Masked Dancer - reveals the secret choreography of running applications."""
    
    def __init__(self):
        self.oracle = OracleService()
    
    def trace_dance(self, repo_id: str, starting_node_id: str, max_depth: int = 10) -> Dict[str, Any]:
        """
        Core function to trace execution flow starting from a given node.
        
        Args:
            repo_id: Repository identifier
            starting_node_id: The node to start tracing from
            max_depth: Maximum depth for the trace to prevent infinite recursion
            
        Returns:
            Dictionary containing trace steps and metadata
        """
        try:
            # Load cortex data
            cortex_data = cortex_service.load_cortex_data(repo_id)
            graph_data = cortex_data.get("architectural_graph")
            
            if not graph_data:
                return {"error": "No architectural graph found for this repository"}
            
            # Build networkx graph
            graph = self._build_graph(graph_data)
            
            if starting_node_id not in graph:
                return {"error": f"Starting node '{starting_node_id}' not found in graph"}
            
            # Perform depth-first search to trace execution flow
            trace_steps = []
            visited_in_path = set()
            recursion_stack = []
            
            self._dfs_trace(
                graph, starting_node_id, trace_steps, 
                visited_in_path, recursion_stack, 0, max_depth
            )
            
            return {
                "starting_node": starting_node_id,
                "trace_steps": trace_steps,
                "total_steps": len(trace_steps),
                "max_depth_reached": max(step.get("depth", 0) for step in trace_steps) if trace_steps else 0,
                "nodes_visited": len(set(step["target"] for step in trace_steps)),
                "has_recursion": any(step.get("is_recursive", False) for step in trace_steps)
            }
            
        except Exception as e:
            logger.error(f"Error tracing dance for {repo_id}: {e}")
            return {"error": str(e)}
    
    def _build_graph(self, graph_data: Dict[str, Any]) -> nx.DiGraph:
        """Build networkx graph from cortex data."""
        G = nx.DiGraph()
        
        # Add nodes
        nodes = graph_data.get("nodes", {})
        for node_id, node_data in nodes.items():
            G.add_node(node_id, **node_data)
        
        # Add edges
        edges = graph_data.get("edges", [])
        for edge in edges:
            source = edge.get("source")
            target = edge.get("target")
            edge_type = edge.get("type", "unknown")
            
            if source and target and source in G:
                G.add_edge(source, target, type=edge_type)
        
        return G
    
    def _dfs_trace(self, graph: nx.DiGraph, current_node: str, trace_steps: List[Dict],
                   visited_in_path: Set[str], recursion_stack: List[str], 
                   depth: int, max_depth: int):
        """
        Depth-first search to trace execution flow.
        """
        if depth >= max_depth:
            return
        
        if current_node in visited_in_path:
            # Recursion detected
            trace_steps.append({
                "source": recursion_stack[-1] if recursion_stack else current_node,
                "target": current_node,
                "depth": depth,
                "type": "RECURSIVE_CALL",
                "is_recursive": True
            })
            return
        
        visited_in_path.add(current_node)
        recursion_stack.append(current_node)
        
        # Get outgoing edges (function calls, dependencies, etc.)
        for target_node in graph.successors(current_node):
            edge_data = graph.get_edge_data(current_node, target_node)
            edge_type = edge_data.get("type", "CALLS") if edge_data else "CALLS"
            
            # Only follow certain edge types for execution flow
            if edge_type in ["CALLS", "INHERITS_FROM", "IMPLEMENTS", "USES"]:
                trace_steps.append({
                    "source": current_node,
                    "target": target_node,
                    "depth": depth + 1,
                    "type": edge_type,
                    "is_recursive": False
                })
                
                # Continue tracing
                self._dfs_trace(
                    graph, target_node, trace_steps, 
                    visited_in_path, recursion_stack, depth + 1, max_depth
                )
        
        visited_in_path.remove(current_node)
        recursion_stack.pop()
    
    def find_starting_node(self, repo_id: str, query: str) -> Dict[str, Any]:
        """
        Use The Oracle to identify the best starting function/method for a dance.
        
        Args:
            repo_id: Repository identifier
            query: Natural language description of what to trace
            
        Returns:
            Dictionary with suggested starting node and confidence
        """
        oracle_prompt = f"""Identify the single best starting function/method node in the graph for this concept: '{query}'.
        
        Look for:
        - API endpoints or route handlers
        - Main entry points
        - Controller methods
        - Service functions
        - Event handlers
        
        Return ONLY the node identifier (function name or class.method format).
        If you're not confident, return the most likely candidate with an explanation."""
        
        try:
            # Use Oracle to search for relevant code
            search_result = oracle_service.perform_semantic_search(
                repo_id, query, {"k": 10, "use_graph": True}
            )
            
            # Extract potential starting points from search results
            candidates = []
            for result in search_result.get("results", []):
                text = result.get("text", "")
                file_path = result.get("file_path", "")
                
                # Look for function definitions, class methods, API routes
                function_matches = self._extract_function_names(text)
                for func_name in function_matches:
                    candidates.append({
                        "node_id": func_name,
                        "file_path": file_path,
                        "confidence": result.get("relevance_score", 0.5),
                        "context": text[:200]
                    })
            
            if not candidates:
                return {"error": "No suitable starting points found for the query"}
            
            # Sort by confidence and return best match
            candidates.sort(key=lambda x: x["confidence"], reverse=True)
            best_candidate = candidates[0]
            
            return {
                "suggested_node": best_candidate["node_id"],
                "file_path": best_candidate["file_path"],
                "confidence": best_candidate["confidence"],
                "context": best_candidate["context"],
                "alternatives": candidates[1:5]  # Top 5 alternatives
            }
            
        except Exception as e:
            logger.error(f"Error finding starting node: {e}")
            return {"error": str(e)}
    
    def _extract_function_names(self, text: str) -> List[str]:
        """Extract function and method names from code text."""
        import re
        
        function_names = []
        
        # Python function definitions
        python_funcs = re.findall(r'def\s+([a-zA-Z_]\w*)\s*\(', text)
        function_names.extend(python_funcs)
        
        # Python class methods
        class_methods = re.findall(r'class\s+(\w+).*?def\s+([a-zA-Z_]\w*)', text, re.DOTALL)
        for class_name, method_name in class_methods:
            function_names.append(f"{class_name}.{method_name}")
        
        # JavaScript/TypeScript functions
        js_funcs = re.findall(r'function\s+([a-zA-Z_]\w*)\s*\(', text)
        function_names.extend(js_funcs)
        
        # API route decorators
        routes = re.findall(r'@app\.route\([\'"]([^\'\"]+)[\'"].*?\ndef\s+([a-zA-Z_]\w*)', text, re.DOTALL)
        for route, func_name in routes:
            function_names.append(func_name)
        
        return list(set(function_names))  # Remove duplicates
    
    def render_tree_cli(self, trace_data: Dict[str, Any]) -> str:
        """
        Render trace as a beautiful rich.Tree for CLI display.
        
        Args:
            trace_data: Result from trace_dance()
            
        Returns:
            Formatted tree string for console display
        """
        if "error" in trace_data:
            return f"❌ Dance failed: {trace_data['error']}"
        
        starting_node = trace_data.get("starting_node", "unknown")
        trace_steps = trace_data.get("trace_steps", [])
        
        if not trace_steps:
            return f"💃 The Dance of '{starting_node}' - No execution flow found"
        
        # Build tree structure
        tree_lines = [f"💃 The Dance of '{starting_node}'"]
        
        # Group steps by depth for proper tree rendering
        depth_groups = defaultdict(list)
        for step in trace_steps:
            depth_groups[step["depth"]].append(step)
        
        # Render tree with proper indentation
        for depth in sorted(depth_groups.keys()):
            for step in depth_groups[depth]:
                indent = "│   " * (depth - 1) + "├── " if depth > 0 else "└── "
                
                # Choose emoji based on edge type
                emoji = "📞"
                if step["type"] == "INHERITS_FROM":
                    emoji = "🧬"
                elif step["type"] == "IMPLEMENTS":
                    emoji = "⚙️"
                elif step["type"] == "USES":
                    emoji = "🔗"
                elif step.get("is_recursive", False):
                    emoji = "🔄"
                
                target = step["target"]
                edge_type = step["type"].lower().replace("_", " ")
                
                tree_lines.append(f"{indent}{emoji} {edge_type} {target}")
        
        # Add metadata
        metadata = [
            f"",
            f"📊 Dance Statistics:",
            f"   • Total steps: {trace_data.get('total_steps', 0)}",
            f"   • Max depth: {trace_data.get('max_depth_reached', 0)}",
            f"   • Nodes visited: {trace_data.get('nodes_visited', 0)}",
            f"   • Has recursion: {'Yes' if trace_data.get('has_recursion', False) else 'No'}"
        ]
        
        return "\n".join(tree_lines + metadata)
    
    def generate_svg_animation(self, trace_data: Dict[str, Any], 
                              output_file: Optional[str] = None) -> str:
        """
        Generate an animated SVG visualization of the execution flow.
        
        Args:
            trace_data: Result from trace_dance()
            output_file: Optional file path to save the SVG
            
        Returns:
            SVG content as string
        """
        if "error" in trace_data:
            return f"<svg><text>Error: {trace_data['error']}</text></svg>"
        
        starting_node = trace_data.get("starting_node", "unknown")
        trace_steps = trace_data.get("trace_steps", [])
        
        # Collect all unique nodes
        all_nodes = set([starting_node])
        for step in trace_steps:
            all_nodes.add(step["source"])
            all_nodes.add(step["target"])
        
        # Layout nodes in a hierarchical structure
        node_positions = self._calculate_node_positions(list(all_nodes), trace_steps)
        
        # Generate SVG
        svg_content = self._build_svg_content(
            starting_node, trace_steps, node_positions
        )
        
        # Save to file if requested
        if output_file:
            try:
                with open(output_file, 'w') as f:
                    f.write(svg_content)
                logger.info(f"SVG animation saved to {output_file}")
            except Exception as e:
                logger.error(f"Failed to save SVG: {e}")
        
        return svg_content
    
    def _calculate_node_positions(self, nodes: List[str], 
                                 trace_steps: List[Dict]) -> Dict[str, Tuple[int, int]]:
        """Calculate positions for nodes in the SVG layout."""
        positions = {}
        
        # Simple hierarchical layout
        levels = defaultdict(list)
        node_depths = {}
        
        # Calculate depth for each node based on trace
        for step in trace_steps:
            depth = step.get("depth", 0)
            target = step["target"]
            if target not in node_depths or depth < node_depths[target]:
                node_depths[target] = depth
        
        # Group nodes by depth level
        for node in nodes:
            depth = node_depths.get(node, 0)
            levels[depth].append(node)
        
        # Position nodes
        y_spacing = 80
        x_spacing = 200
        
        for depth, level_nodes in levels.items():
            y = depth * y_spacing + 50
            x_start = 50
            
            for i, node in enumerate(level_nodes):
                x = x_start + (i * x_spacing)
                positions[node] = (x, y)
        
        return positions
    
    def _build_svg_content(self, starting_node: str, trace_steps: List[Dict],
                          node_positions: Dict[str, Tuple[int, int]]) -> str:
        """Build the complete SVG content with animations."""
        
        # Calculate SVG dimensions
        max_x = max(pos[0] for pos in node_positions.values()) + 150
        max_y = max(pos[1] for pos in node_positions.values()) + 100
        
        svg_parts = [
            f'<svg width="{max_x}" height="{max_y}" xmlns="http://www.w3.org/2000/svg">',
            '<style>',
            '.node { fill: #4a90e2; stroke: #2c5aa0; stroke-width: 2; }',
            '.node-text { fill: white; font-family: Arial; font-size: 12px; text-anchor: middle; }',
            '.edge { stroke: #666; stroke-width: 2; fill: none; opacity: 0; }',
            '.edge-animated { stroke-dasharray: 5,5; }',
            '@keyframes flow { from { stroke-dashoffset: 10; } to { stroke-dashoffset: 0; } }',
            '</style>',
            '',
            '<!-- Nodes -->'
        ]
        
        # Add nodes
        for node, (x, y) in node_positions.items():
            node_id = node.replace('.', '_').replace('/', '_')
            svg_parts.extend([
                f'<g id="node_{node_id}">',
                f'  <circle cx="{x}" cy="{y}" r="25" class="node"/>',
                f'  <text x="{x}" y="{y + 5}" class="node-text">{node[:10]}</text>',
                '</g>'
            ])
        
        svg_parts.append('\n<!-- Edges -->')
        
        # Add edges with animation
        for i, step in enumerate(trace_steps):
            source = step["source"]
            target = step["target"]
            
            if source in node_positions and target in node_positions:
                x1, y1 = node_positions[source]
                x2, y2 = node_positions[target]
                
                edge_id = f"edge_{i}"
                animation_delay = i * 0.5  # Stagger animations
                
                svg_parts.extend([
                    f'<path id="{edge_id}" d="M {x1} {y1} L {x2} {y2}" class="edge edge-animated">',
                    f'  <animate attributeName="opacity" values="0;1;1" dur="0.5s" begin="{animation_delay}s" fill="freeze"/>',
                    f'  <animateTransform attributeName="stroke-dashoffset" values="10;0" dur="1s" begin="{animation_delay}s" fill="freeze"/>',
                    '</path>'
                ])
        
        # Add JavaScript for interactivity
        svg_parts.extend([
            '',
            '<script type="text/javascript"><![CDATA[',
            '// SVG is self-contained and animated',
            'console.log("Lumière Sémantique - Dance visualization loaded");',
            ']]></script>',
            '</svg>'
        ])
        
        return '\n'.join(svg_parts)


# Global instance
_dance_service = None

def get_dance_service() -> DanceService:
    """Get or create the global Dance service instance."""
    global _dance_service
    if _dance_service is None:
        _dance_service = DanceService()
    return _dance_service

# Public API
def trace_execution_flow(repo_id: str, starting_point: str, max_depth: int = 10) -> Dict[str, Any]:
    """
    Public API to trace execution flow starting from a given point.
    """
    service = get_dance_service()
    return service.trace_dance(repo_id, starting_point, max_depth)

def find_entry_point(repo_id: str, description: str) -> Dict[str, Any]:
    """
    Public API to find the best entry point for tracing based on description.
    """
    service = get_dance_service()
    return service.find_starting_node(repo_id, description)

def visualize_dance(repo_id: str, starting_point: str, format_type: str = "cli", 
                   output_file: Optional[str] = None) -> str:
    """
    Public API to visualize execution flow.
    
    Args:
        repo_id: Repository identifier
        starting_point: Function/method to start tracing from
        format_type: "cli" for rich.Tree or "svg" for animated SVG
        output_file: Optional file path for SVG output
        
    Returns:
        Formatted visualization string
    """
    service = get_dance_service()
    
    # Get trace data
    trace_data = service.trace_dance(repo_id, starting_point)
    
    if format_type == "svg":
        return service.generate_svg_animation(trace_data, output_file)
    else:
        return service.render_tree_cli(trace_data)
--- FILE_END: backend/lumiere_core/services/dance_service.py ---

--- FILE_START: backend/lumiere_core/services/bom_parser.py ---
# backend/lumiere_core/services/bom_parser.py
import json
import re
import toml
import xml.etree.ElementTree as ET
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class DependencyType(Enum):
    APPLICATION = "application"
    DEVELOPMENT = "development"
    BUILD_TOOL = "build_tool"
    TESTING = "testing"
    PEER = "peer"
    OPTIONAL = "optional"

class SecurityRisk(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class EnumEncoder(json.JSONEncoder):
    """ Custom JSON encoder to handle Enum objects. """
    def default(self, obj):
        if isinstance(obj, Enum):
            return obj.value # Convert Enum to its string value
        return super().default(obj)

@dataclass
class Dependency:
    name: str
    version: str
    source: str
    dependency_type: DependencyType
    ecosystem: str
    license: Optional[str] = None
    security_risk: Optional[SecurityRisk] = None
    deprecated: bool = False
    description: Optional[str] = None
    homepage: Optional[str] = None
    repository: Optional[str] = None
    vulnerability_count: int = 0
    last_updated: Optional[str] = None

@dataclass
class Service:
    name: str
    version: str
    source: str
    service_type: str  # database, cache, message_queue, etc.
    ports: List[int] = None
    environment: Dict[str, str] = None
    volumes: List[str] = None
    networks: List[str] = None

@dataclass
class BuildTool:
    name: str
    version: str
    source: str
    purpose: str  # bundler, compiler, test_runner, etc.
    configuration: Dict[str, Any] = None

@dataclass
class TechStackBOM:
    summary: Dict[str, Any]
    dependencies: Dict[str, List[Dependency]]
    services: List[Service]
    build_tools: List[BuildTool]
    languages: Dict[str, Dict[str, Any]]
    infrastructure: Dict[str, Any]
    security_analysis: Dict[str, Any]
    metadata: Dict[str, Any]

    # THE FIX IS HERE: This method converts the object to a JSON-serializable dictionary.
    def to_dict(self) -> Dict[str, Any]:
        """Converts the dataclass instance to a dictionary, correctly handling Enums."""
            # asdict converts the dataclass to a dict, but enums are still objects.
            # json.dumps with our custom encoder turns enums into strings.
            # json.loads turns the resulting JSON string back into a final, clean dictionary.
        return json.loads(json.dumps(asdict(self), cls=EnumEncoder))



def _parse_requirements_txt(file_path: Path, dep_type: DependencyType = DependencyType.APPLICATION) -> List[Dependency]:
    """Enhanced requirements.txt parser with better version handling."""
    dependencies = []
    if not file_path.exists():
        return dependencies

    # Enhanced pattern to capture package name, version specifiers, and extras
    pattern = re.compile(r"^\s*([a-zA-Z0-9\-_.]+)(\[.*\])?\s*([<>=!~^].*)?(?:\s*#.*)?$")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('-'):
                    # Handle -e editable installs
                    if line.startswith('-e '):
                        continue

                    match = pattern.match(line)
                    if match:
                        name, extras, version = match.groups()
                        dependencies.append(Dependency(
                            name=name,
                            version=version.strip() if version else "any",
                            source=file_path.name,
                            dependency_type=dep_type,
                            ecosystem="python"
                        ))
    except Exception as e:
        print(f"Warning: Could not parse {file_path.name}: {e}")

    return dependencies

def _parse_python_deps(repo_root: Path) -> Tuple[List[Dependency], List[BuildTool], Dict[str, Any]]:
    """Enhanced Python dependency parser with better Poetry/PDM support."""
    dependencies = []
    build_tools = []
    lang_info = {
        'version': None,
        'build_system': None,
        'package_manager': None,
        'virtual_env': None
    }

    # Parse pyproject.toml (Poetry, PDM, setuptools)
    pyproject_paths = list(repo_root.rglob("pyproject.toml"))
    for pyproject_path in pyproject_paths:
        try:
            data = toml.load(pyproject_path)

            # Detect build system
            build_system = data.get("build-system", {})
            if build_system:
                lang_info['build_system'] = build_system.get("build-backend", "unknown")
                requires = build_system.get("requires", [])
                for req in requires:
                    # --- THE FIX: Make this parsing more robust ---
                    # The original line caused `list index out of range` on simple requirements like 'hatchling'
                    # The corrected version safely handles various formats.
                    name_part = req.split(">=")[0].split("==")[0].split("~=")[0].split("<=")[0].strip()
                    if name_part:
                        build_tools.append(BuildTool(
                            name=name_part,
                            version="any",
                            source="pyproject.toml",
                            purpose="build_system"
                        ))
                    # --- END OF FIX ---

            # Poetry dependencies
            poetry_config = data.get("tool", {}).get("poetry", {})
            if poetry_config:
                lang_info['package_manager'] = 'poetry'

                # Parse Python version requirement
                python_version = poetry_config.get("dependencies", {}).get("python")
                if python_version:
                    lang_info['version'] = python_version

                # Application dependencies
                app_deps = poetry_config.get("dependencies", {})
                for name, version_info in app_deps.items():
                    if name.lower() == "python":
                        continue

                    version = version_info if isinstance(version_info, str) else version_info.get("version", "any")
                    optional = version_info.get("optional", False) if isinstance(version_info, dict) else False

                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source="pyproject.toml",
                        dependency_type=DependencyType.OPTIONAL if optional else DependencyType.APPLICATION,
                        ecosystem="python"
                    ))

                # Development dependencies
                dev_deps = poetry_config.get("group", {}).get("dev", {}).get("dependencies", {})
                for name, version_info in dev_deps.items():
                    version = version_info if isinstance(version_info, str) else version_info.get("version", "any")
                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source="pyproject.toml",
                        dependency_type=DependencyType.DEVELOPMENT,
                        ecosystem="python"
                    ))

                build_tools.append(BuildTool(
                    name="poetry",
                    version="any",
                    source="pyproject.toml",
                    purpose="package_manager"
                ))

            # PDM dependencies
            pdm_config = data.get("tool", {}).get("pdm", {})
            if pdm_config:
                lang_info['package_manager'] = 'pdm'
                build_tools.append(BuildTool(
                    name="pdm",
                    version="any",
                    source="pyproject.toml",
                    purpose="package_manager"
                ))

        except Exception as e:
            print(f"Warning: Could not parse pyproject.toml: {e}")

    # Parse requirements files
    requirements_files_patterns = [
        "requirements.txt",
        "requirements-dev.txt",
        "requirements-test.txt",
        "dev-requirements.txt",
        "test-requirements.txt",
    ]

    for pattern in requirements_files_patterns:
        for req_file_path in repo_root.rglob(pattern):
            dependencies.extend(_parse_requirements_txt(req_file_path, DependencyType.APPLICATION)) # Default to APPLICATION, can be refined later

    # Check for virtual environment indicators
    if (repo_root / "Pipfile").exists():
        lang_info['package_manager'] = 'pipenv'
        lang_info['virtual_env'] = 'pipenv'
    elif (repo_root / "poetry.lock").exists():
        lang_info['virtual_env'] = 'poetry'
    elif (repo_root / "requirements.txt").exists():
        lang_info['virtual_env'] = 'pip'

    return dependencies, build_tools, lang_info

def _parse_npm_deps(repo_root: Path) -> Tuple[List[Dependency], List[BuildTool], Dict[str, Any]]:
    """Enhanced Node.js dependency parser with better framework detection."""
    dependencies = []
    build_tools = []
    lang_info = {
        'version': None,
        'package_manager': None,
        'frameworks': [],
        'bundler': None
    }

    package_json_paths = list(repo_root.rglob("package.json"))
    if not package_json_paths:
        return dependencies, build_tools, lang_info

    for package_json_path in package_json_paths:
        try:
            with open(package_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Detect Node.js version
            engines = data.get("engines", {})
            if "node" in engines:
                lang_info['version'] = engines["node"]

            # Detect package manager
            if (package_json_path.parent / "yarn.lock").exists():
                lang_info['package_manager'] = 'yarn'
            elif (package_json_path.parent / "pnpm-lock.yaml").exists():
                lang_info['package_manager'] = 'pnpm'
            elif (package_json_path.parent / "package-lock.json").exists():
                lang_info['package_manager'] = 'npm'
            else:
                lang_info['package_manager'] = 'npm'

            # Parse dependencies
            dep_sections = [
                ("dependencies", DependencyType.APPLICATION),
                ("devDependencies", DependencyType.DEVELOPMENT),
                ("peerDependencies", DependencyType.PEER),
                ("optionalDependencies", DependencyType.OPTIONAL)
            ]

            framework_indicators = {
                'react': 'React',
                '@angular/core': 'Angular',
                'vue': 'Vue.js',
                'svelte': 'Svelte',
                'next': 'Next.js',
                'nuxt': 'Nuxt.js',
                'express': 'Express.js',
                'fastify': 'Fastify',
                'koa': 'Koa.js',
                'nestjs': 'NestJS'
            }

            for section, dep_type in dep_sections:
                for name, version in data.get(section, {}).items():
                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source=package_json_path.name,
                        dependency_type=dep_type,
                        ecosystem="javascript"
                    ))

                    # Detect frameworks
                    for indicator, framework in framework_indicators.items():
                        if indicator in name.lower():
                            if framework not in lang_info['frameworks']:
                                lang_info['frameworks'].append(framework)

            # Parse scripts for build tools
            scripts = data.get("scripts", {})
            build_tool_indicators = {
                'webpack': 'webpack',
                'vite': 'vite',
                'rollup': 'rollup',
                'esbuild': 'esbuild',
                'parcel': 'parcel',
                'babel': 'babel',
                'tsc': 'typescript',
                'eslint': 'eslint',
                'prettier': 'prettier',
                'jest': 'jest',
                'vitest': 'vitest',
                'cypress': 'cypress',
                'playwright': 'playwright'
            }

            detected_tools = set()
            for script_name, script_content in scripts.items():
                for tool_indicator, tool_name in build_tool_indicators.items():
                    if tool_indicator in script_content.lower():
                        detected_tools.add(tool_name)

            for tool in detected_tools:
                purpose = "bundler" if tool in ['webpack', 'vite', 'rollup', 'esbuild', 'parcel'] else "build_tool"
                if purpose == "bundler":
                    lang_info['bundler'] = tool

                build_tools.append(BuildTool(
                    name=tool,
                    version="any",
                    source=package_json_path.name,
                    purpose=purpose
                ))

        except Exception as e:
            print(f"Warning: Could not parse {package_json_path.name}: {e}")

    return dependencies, build_tools, lang_info

def parse_all_manifests(repo_root: Path) -> Optional[TechStackBOM]:
    """
    Enhanced main orchestrator function that creates a comprehensive BOM.
    """
    logger.info(f"BOM Parser: Starting analysis for repository root: {repo_root}")
    all_dependencies = []
    all_services = []
    all_build_tools = []
    language_info = {}

    # Parse different ecosystems
    parsers = {
        "Python": _parse_python_deps,
        "Node.js": _parse_npm_deps,
    }

    for ecosystem, parser_func in parsers.items():
        logger.info(f"BOM Parser: Running {ecosystem} parser...")
        try:
            deps, tools, lang_info = parser_func(repo_root)
            all_dependencies.extend(deps)
            all_build_tools.extend(tools)
            if lang_info:
                language_info[ecosystem.lower()] = lang_info
        except Exception as e:
            logger.error(f"BOM Parser: Error running '{ecosystem}' parser: {e}")

    # Parse Docker services
    logger.info("BOM Parser: Running Docker services parser...")
    try:
        services, base_images = _parse_docker_services(repo_root)
        all_services.extend(services)
        all_dependencies.extend(base_images)
    except Exception as e:
        logger.error(f"BOM Parser: Error parsing Docker services: {e}")

    detected_languages = _detect_languages(repo_root)

    primary_language = "Unknown"
    if detected_languages:
        primary_language = max(detected_languages.keys(),
                             key=lambda x: detected_languages.get(x, {}).get('lines', 0))

    categorized_deps = {
        'application': [d for d in all_dependencies if d.dependency_type == DependencyType.APPLICATION],
        'development': [d for d in all_dependencies if d.dependency_type == DependencyType.DEVELOPMENT],
        'testing': [d for d in all_dependencies if d.dependency_type == DependencyType.TESTING],
        'build': [d for d in all_dependencies if d.dependency_type == DependencyType.BUILD_TOOL],
        'peer': [d for d in all_dependencies if d.dependency_type == DependencyType.PEER],
        'optional': [d for d in all_dependencies if d.dependency_type == DependencyType.OPTIONAL]
    }

    security_analysis = _analyze_security_risks(all_dependencies)

    infrastructure = {
        'containerized': len([s for s in all_services if 'docker' in s.source.lower()]) > 0,
        'databases': [s for s in all_services if s.service_type == 'database'],
        'caches': [s for s in all_services if s.service_type == 'cache'],
        'web_servers': [s for s in all_services if s.service_type == 'web_server'],
        'message_queues': [s for s in all_services if s.service_type == 'message_queue']
    }

    bom = TechStackBOM(
        summary={
            'primary_language': primary_language,
            'total_dependencies': len(all_dependencies),
            'total_services': len(all_services),
            'total_build_tools': len(all_build_tools),
            'languages_detected': len(detected_languages),
            'ecosystems': list(set(d.ecosystem for d in all_dependencies)),
            'last_updated': None
        },
        dependencies={k: [asdict(d) for d in v] for k, v in categorized_deps.items()},
        services=[asdict(s) for s in all_services],
        build_tools=[asdict(t) for t in all_build_tools],
        languages=detected_languages,
        infrastructure=infrastructure,
        security_analysis=security_analysis,
        metadata={
            'parser_version': '2.1.0', # Version bump for robustness fix
            'parsing_errors': [],
            'repository_size': sum(1 for _ in repo_root.rglob('*') if _.is_file()),
            'config_files_found': _count_config_files(repo_root)
        }
    )

    logger.info(f"BOM Parser: Finished analysis. Found {len(all_dependencies)} total dependencies.")
    return bom

def _count_config_files(repo_root: Path) -> Dict[str, int]:
    """Count different types of configuration files."""
    config_patterns = {
        'package_managers': ['package.json', 'pyproject.toml', 'Cargo.toml', 'pom.xml'],
        'docker': ['Dockerfile*', 'docker-compose*.yml', 'docker-compose*.yaml'],
        'ci_cd': ['.github/workflows/*.yml', '.gitlab-ci.yml', 'Jenkinsfile'],
        'environment': ['.env*', 'config/*.yml', 'config/*.yaml'],
        'build_tools': ['webpack.config.js', 'vite.config.js', 'rollup.config.js']
    }

    counts = {}
    for category, patterns in config_patterns.items():
        count = 0
        for pattern in patterns:
            try:
                # Use rglob for patterns that might be in subdirectories
                if '*' in pattern or '?' in pattern:
                     count += len(list(repo_root.rglob(pattern)))
                else: # Use glob for top-level files
                     count += len(list(repo_root.glob(pattern)))
            except re.error:
                 logger.warning(f"BOM Parser: Invalid pattern in BOM parser config: {pattern}")
                 continue
        counts[category] = count

    return counts

def _parse_requirements_txt(file_path: Path, dep_type: DependencyType = DependencyType.APPLICATION) -> List[Dependency]:
    """Enhanced requirements.txt parser with better version handling."""
    dependencies = []
    logger.debug(f"BOM Parser: Parsing requirements.txt: {file_path}")
    if not file_path.exists():
        logger.debug(f"BOM Parser: requirements.txt not found: {file_path}")
        return dependencies

    # Enhanced pattern to capture package name, version specifiers, and extras
    pattern = re.compile(r"^\s*([a-zA-Z0-9\-_.]+)(\[.*\])?\s*([<>=!~^].*)?(?:\s*#.*)?$")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('-'):
                    # Handle -e editable installs
                    if line.startswith('-e '):
                        continue

                    match = pattern.match(line)
                    if match:
                        name, extras, version = match.groups()
                        dependencies.append(Dependency(
                            name=name,
                            version=version.strip() if version else "any",
                            source=file_path.name,
                            dependency_type=dep_type,
                            ecosystem="python"
                        ))
                        logger.debug(f"BOM Parser: Found Python dependency: {name} {version} in {file_path.name}")
    except Exception as e:
        logger.warning(f"BOM Parser: Could not parse {file_path.name}: {e}")

    return dependencies

def _parse_python_deps(repo_root: Path) -> Tuple[List[Dependency], List[BuildTool], Dict[str, Any]]:
    """Enhanced Python dependency parser with better Poetry/PDM support."""
    dependencies = []
    build_tools = []
    lang_info = {
        'version': None,
        'build_system': None,
        'package_manager': None,
        'virtual_env': None
    }

    # Parse pyproject.toml (Poetry, PDM, setuptools)
    pyproject_paths = list(repo_root.rglob("pyproject.toml"))
    logger.debug(f"BOM Parser: Found pyproject.toml files: {pyproject_paths}")
    for pyproject_path in pyproject_paths:
        logger.debug(f"BOM Parser: Parsing pyproject.toml: {pyproject_path}")
        try:
            data = toml.load(pyproject_path)

            # Detect build system
            build_system = data.get("build-system", {})
            if build_system:
                lang_info['build_system'] = build_system.get("build-backend", "unknown")
                requires = build_system.get("requires", [])
                for req in requires:
                    # --- THE FIX: Make this parsing more robust ---
                    # The original line caused `list index out of range` on simple requirements like 'hatchling'
                    # The corrected version safely handles various formats.
                    name_part = req.split(">=")[0].split("==")[0].split("~=")[0].split("<=")[0].strip()
                    if name_part:
                        build_tools.append(BuildTool(
                            name=name_part,
                            version="any",
                            source="pyproject.toml",
                            purpose="build_system"
                        ))
                        logger.debug(f"BOM Parser: Found Python build tool: {name_part} in {pyproject_path.name}")

            # Poetry dependencies
            poetry_config = data.get("tool", {}).get("poetry", {})
            if poetry_config:
                lang_info['package_manager'] = 'poetry'

                # Parse Python version requirement
                python_version = poetry_config.get("dependencies", {}).get("python")
                if python_version:
                    lang_info['version'] = python_version

                # Application dependencies
                app_deps = poetry_config.get("dependencies", {})
                for name, version_info in app_deps.items():
                    if name.lower() == "python":
                        continue

                    version = version_info if isinstance(version_info, str) else version_info.get("version", "any")
                    optional = version_info.get("optional", False) if isinstance(version_info, dict) else False

                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source="pyproject.toml",
                        dependency_type=DependencyType.OPTIONAL if optional else DependencyType.APPLICATION,
                        ecosystem="python"
                    ))
                    logger.debug(f"BOM Parser: Found Poetry dependency: {name} {version} in {pyproject_path.name}")

                # Development dependencies
                dev_deps = poetry_config.get("group", {}).get("dev", {}).get("dependencies", {})
                for name, version_info in dev_deps.items():
                    version = version_info if isinstance(version_info, str) else version_info.get("version", "any")
                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source="pyproject.toml",
                        dependency_type=DependencyType.DEVELOPMENT,
                        ecosystem="python"
                    ))
                    logger.debug(f"BOM Parser: Found Poetry dev dependency: {name} {version} in {pyproject_path.name}")

                build_tools.append(BuildTool(
                    name="poetry",
                    version="any",
                    source="pyproject.toml",
                    purpose="package_manager"
                ))

            # PDM dependencies
            pdm_config = data.get("tool", {}).get("pdm", {})
            if pdm_config:
                lang_info['package_manager'] = 'pdm'
                build_tools.append(BuildTool(
                    name="pdm",
                    version="any",
                    source="pyproject.toml",
                    purpose="package_manager"
                ))
                logger.debug(f"BOM Parser: Found PDM config in {pyproject_path.name}")

        except Exception as e:
            logger.warning(f"BOM Parser: Could not parse pyproject.toml: {e}")

    # Parse requirements files
    requirements_files_patterns = [
        "requirements.txt",
        "requirements-dev.txt",
        "requirements-test.txt",
        "dev-requirements.txt",
        "test-requirements.txt",
    ]

    for pattern in requirements_files_patterns:
        req_file_paths = list(repo_root.rglob(pattern))
        logger.debug(f"BOM Parser: Found {pattern} files: {req_file_paths}")
        for req_file_path in req_file_paths:
            dependencies.extend(_parse_requirements_txt(req_file_path, DependencyType.APPLICATION)) # Default to APPLICATION, can be refined later

    # Check for virtual environment indicators
    if (repo_root / "Pipfile").exists():
        lang_info['package_manager'] = 'pipenv'
        lang_info['virtual_env'] = 'pipenv'
        logger.debug(f"BOM Parser: Found Pipfile in {repo_root}")
    elif (repo_root / "poetry.lock").exists():
        lang_info['virtual_env'] = 'poetry'
        logger.debug(f"BOM Parser: Found poetry.lock in {repo_root}")
    elif (repo_root / "requirements.txt").exists():
        lang_info['virtual_env'] = 'pip'
        logger.debug(f"BOM Parser: Found requirements.txt in {repo_root}")

    return dependencies, build_tools, lang_info

def _parse_npm_deps(repo_root: Path) -> Tuple[List[Dependency], List[BuildTool], Dict[str, Any]]:
    """Enhanced Node.js dependency parser with better framework detection."""
    dependencies = []
    build_tools = []
    lang_info = {
        'version': None,
        'package_manager': None,
        'frameworks': [],
        'bundler': None
    }

    package_json_paths = list(repo_root.rglob("package.json"))
    logger.debug(f"BOM Parser: Found package.json files: {package_json_paths}")
    if not package_json_paths:
        return dependencies, build_tools, lang_info

    for package_json_path in package_json_paths:
        logger.debug(f"BOM Parser: Parsing package.json: {package_json_path}")
        try:
            with open(package_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Detect Node.js version
            engines = data.get("engines", {})
            if "node" in engines:
                lang_info['version'] = engines["node"]
                logger.debug(f"BOM Parser: Found Node.js version: {engines['node']} in {package_json_path.name}")

            # Detect package manager
            if (package_json_path.parent / "yarn.lock").exists():
                lang_info['package_manager'] = 'yarn'
                logger.debug(f"BOM Parser: Found yarn.lock in {package_json_path.parent}")
            elif (package_json_path.parent / "pnpm-lock.yaml").exists():
                lang_info['package_manager'] = 'pnpm'
                logger.debug(f"BOM Parser: Found pnpm-lock.yaml in {package_json_path.parent}")
            elif (package_json_path.parent / "package-lock.json").exists():
                lang_info['package_manager'] = 'npm'
                logger.debug(f"BOM Parser: Found package-lock.json in {package_json_path.parent}")
            else:
                lang_info['package_manager'] = 'npm'
                logger.debug(f"BOM Parser: Defaulting to npm package manager for {package_json_path.name}")

            # Parse dependencies
            dep_sections = [
                ("dependencies", DependencyType.APPLICATION),
                ("devDependencies", DependencyType.DEVELOPMENT),
                ("peerDependencies", DependencyType.PEER),
                ("optionalDependencies", DependencyType.OPTIONAL)
            ]

            framework_indicators = {
                'react': 'React',
                '@angular/core': 'Angular',
                'vue': 'Vue.js',
                'svelte': 'Svelte',
                'next': 'Next.js',
                'nuxt': 'Nuxt.js',
                'express': 'Express.js',
                'fastify': 'Fastify',
                'koa': 'Koa.js',
                'nestjs': 'NestJS'
            }

            for section, dep_type in dep_sections:
                for name, version in data.get(section, {}).items():
                    dependencies.append(Dependency(
                        name=name,
                        version=version,
                        source=package_json_path.name,
                        dependency_type=dep_type,
                        ecosystem="javascript"
                    ))
                    logger.debug(f"BOM Parser: Found JS dependency: {name} {version} in {package_json_path.name}")

                    # Detect frameworks
                    for indicator, framework in framework_indicators.items():
                        if indicator in name.lower():
                            if framework not in lang_info['frameworks']:
                                lang_info['frameworks'].append(framework)
                                logger.debug(f"BOM Parser: Detected framework: {framework} in {package_json_path.name}")

            # Parse scripts for build tools
            scripts = data.get("scripts", {})
            build_tool_indicators = {
                'webpack': 'webpack',
                'vite': 'vite',
                'rollup': 'rollup',
                'esbuild': 'esbuild',
                'parcel': 'parcel',
                'babel': 'babel',
                'tsc': 'typescript',
                'eslint': 'eslint',
                'prettier': 'prettier',
                'jest': 'jest',
                'vitest': 'vitest',
                'cypress': 'cypress',
                'playwright': 'playwright'
            }

            detected_tools = set()
            for script_name, script_content in scripts.items():
                for tool_indicator, tool_name in build_tool_indicators.items():
                    if tool_indicator in script_content.lower():
                        detected_tools.add(tool_name)

            for tool in detected_tools:
                purpose = "bundler" if tool in ['webpack', 'vite', 'rollup', 'esbuild', 'parcel'] else "build_tool"
                if purpose == "bundler":
                    lang_info['bundler'] = tool

                build_tools.append(BuildTool(
                    name=tool,
                    version="any",
                    source=package_json_path.name,
                    purpose=purpose
                ))
                logger.debug(f"BOM Parser: Found JS build tool: {tool} in {package_json_path.name}")

        except Exception as e:
            logger.warning(f"BOM Parser: Could not parse {package_json_path.name}: {e}")

    return dependencies, build_tools, lang_info

def _parse_docker_services(repo_root: Path) -> Tuple[List[Service], List[Dependency]]:
    """Enhanced Docker parser with service analysis."""
    services = []
    base_images = []

    # Parse docker-compose files
    compose_files = list(repo_root.rglob('**/docker-compose*.yml')) + list(repo_root.rglob('**/docker-compose*.yaml'))
    logger.debug(f"BOM Parser: Found docker-compose files: {compose_files}")

    for compose_file in compose_files:
        logger.debug(f"BOM Parser: Parsing docker-compose file: {compose_file}")
        try:
            with open(compose_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)

            compose_services = data.get('services', {})
            for service_name, service_config in compose_services.items():
                image = service_config.get('image', '')
                if image:
                    name, version = (image.split(':') + ['latest'])[:2]

                    # Determine service type
                    service_type = _determine_service_type(name)

                    services.append(Service(
                        name=name,
                        version=version,
                        source=compose_file.name,
                        service_type=service_type,
                        ports=service_config.get('ports', []),
                        environment=service_config.get('environment', {}),
                        volumes=service_config.get('volumes', []),
                        networks=list(service_config.get('networks', {}).keys()) if isinstance(service_config.get('networks'), dict) else service_config.get('networks', [])
                    ))
                    logger.debug(f"BOM Parser: Found Docker service: {name} {version} in {compose_file.name}")

        except Exception as e:
            logger.warning(f"BOM Parser: Could not parse {compose_file.name}: {e}")

    # Parse Dockerfiles
    dockerfiles = list(repo_root.rglob('**/Dockerfile*'))
    logger.debug(f"BOM Parser: Found Dockerfiles: {dockerfiles}")
    for dockerfile in dockerfiles:
        logger.debug(f"BOM Parser: Parsing Dockerfile: {dockerfile}")
        try:
            content = dockerfile.read_text(encoding='utf-8')
            from_matches = re.findall(r'^\s*FROM\s+([^\s]+)', content, re.MULTILINE | re.IGNORECASE)

            for match in from_matches:
                # Skip build stages
                if ' as ' in match.lower():
                    match = match.split(' as ')[0]

                if ':' in match:
                    name, version = match.split(':', 1)
                else:
                    name, version = match, 'latest'

                # Skip obvious build stages
                if name.isalpha() and '.' not in name and '/' not in name:
                    continue

                base_images.append(Dependency(
                    name=name,
                    version=version,
                    source=dockerfile.name,
                    dependency_type=DependencyType.APPLICATION,
                    ecosystem="docker"
                ))
                logger.debug(f"BOM Parser: Found Docker base image: {name} {version} in {dockerfile.name}")

        except Exception as e:
            logger.warning(f"BOM Parser: Could not parse {dockerfile.name}: {e}")

    return services, base_images

def _determine_service_type(image_name: str) -> str:
    """Determine service type based on image name."""
    service_types = {
        'postgres': 'database',
        'mysql': 'database',
        'mongodb': 'database',
        'redis': 'cache',
        'memcached': 'cache',
        'nginx': 'web_server',
        'apache': 'web_server',
        'rabbitmq': 'message_queue',
        'kafka': 'message_queue',
        'elasticsearch': 'search',
        'solr': 'search',
        'prometheus': 'monitoring',
        'grafana': 'monitoring',
        'jaeger': 'tracing',
        'zipkin': 'tracing'
    }

    for key, service_type in service_types.items():
        if key in image_name.lower():
            return service_type

    return 'application'

def _analyze_security_risks(dependencies: List[Dependency]) -> Dict[str, Any]:
    """Analyze security risks (placeholder for future vulnerability scanning)."""
    total_deps = len(dependencies)
    high_risk_count = 0  # Placeholder

    return {
        'total_dependencies': total_deps,
        'high_risk_dependencies': high_risk_count,
        'last_scan': None,
        'recommendations': []
    }

def _detect_languages(repo_root: Path) -> Dict[str, Dict[str, Any]]:
    """Detect programming languages used in the repository."""
    from collections import defaultdict

    # This mapping can be expanded as needed
    LANGUAGE_MAP = {
        '.py': {'name': 'Python'},
        '.js': {'name': 'JavaScript'},
        '.ts': {'name': 'TypeScript'},
        '.java': {'name': 'Java'},
        '.go': {'name': 'Go'},
        '.rs': {'name': 'Rust'},
    }

    language_stats = defaultdict(lambda: {'files': 0, 'lines': 0})

    for file_path in repo_root.rglob('*'):
        if file_path.is_file() and file_path.suffix in LANGUAGE_MAP:
            lang_info = LANGUAGE_MAP[file_path.suffix]
            lang_name = lang_info['name']

            try:
                line_count = len(file_path.read_text(encoding='utf-8', errors='ignore').splitlines())
                language_stats[lang_name]['files'] += 1
                language_stats[lang_name]['lines'] += line_count
                language_stats[lang_name]['config'] = lang_info
            except Exception:
                continue

    return dict(language_stats)

def parse_all_manifests(repo_root: Path) -> Optional[TechStackBOM]:
    """
    Enhanced main orchestrator function that creates a comprehensive BOM.
    """
    all_dependencies = []
    all_services = []
    all_build_tools = []
    language_info = {}

    # Parse different ecosystems
    parsers = {
        "Python": _parse_python_deps,
        "Node.js": _parse_npm_deps,
    }

    for ecosystem, parser_func in parsers.items():
        try:
            deps, tools, lang_info = parser_func(repo_root)
            all_dependencies.extend(deps)
            all_build_tools.extend(tools)
            if lang_info:
                language_info[ecosystem.lower()] = lang_info
        except Exception as e:
            print(f"Error running '{ecosystem}' parser: {e}")

    # Parse Docker services
    try:
        services, base_images = _parse_docker_services(repo_root)
        all_services.extend(services)
        all_dependencies.extend(base_images)
    except Exception as e:
        print(f"Error parsing Docker services: {e}")

    detected_languages = _detect_languages(repo_root)

    primary_language = "Unknown"
    if detected_languages:
        primary_language = max(detected_languages.keys(),
                             key=lambda x: detected_languages.get(x, {}).get('lines', 0))

    categorized_deps = {
        'application': [d for d in all_dependencies if d.dependency_type == DependencyType.APPLICATION],
        'development': [d for d in all_dependencies if d.dependency_type == DependencyType.DEVELOPMENT],
        'testing': [d for d in all_dependencies if d.dependency_type == DependencyType.TESTING],
        'build': [d for d in all_dependencies if d.dependency_type == DependencyType.BUILD_TOOL],
        'peer': [d for d in all_dependencies if d.dependency_type == DependencyType.PEER],
        'optional': [d for d in all_dependencies if d.dependency_type == DependencyType.OPTIONAL]
    }

    security_analysis = _analyze_security_risks(all_dependencies)

    infrastructure = {
        'containerized': len([s for s in all_services if 'docker' in s.source.lower()]) > 0,
        'databases': [s for s in all_services if s.service_type == 'database'],
        'caches': [s for s in all_services if s.service_type == 'cache'],
        'web_servers': [s for s in all_services if s.service_type == 'web_server'],
        'message_queues': [s for s in all_services if s.service_type == 'message_queue']
    }

    bom = TechStackBOM(
        summary={
            'primary_language': primary_language,
            'total_dependencies': len(all_dependencies),
            'total_services': len(all_services),
            'total_build_tools': len(all_build_tools),
            'languages_detected': len(detected_languages),
            'ecosystems': list(set(d.ecosystem for d in all_dependencies)),
            'last_updated': None
        },
        dependencies={k: [asdict(d) for d in v] for k, v in categorized_deps.items()},
        services=[asdict(s) for s in all_services],
        build_tools=[asdict(t) for t in all_build_tools],
        languages=detected_languages,
        infrastructure=infrastructure,
        security_analysis=security_analysis,
        metadata={
            'parser_version': '2.1.0', # Version bump for robustness fix
            'parsing_errors': [],
            'repository_size': sum(1 for _ in repo_root.rglob('*') if _.is_file()),
            'config_files_found': _count_config_files(repo_root)
        }
    )

    return bom

def _count_config_files(repo_root: Path) -> Dict[str, int]:
    """Count different types of configuration files."""
    config_patterns = {
        'package_managers': ['package.json', 'pyproject.toml', 'Cargo.toml', 'pom.xml'],
        'docker': ['Dockerfile*', 'docker-compose*.yml', 'docker-compose*.yaml'],
        'ci_cd': ['.github/workflows/*.yml', '.gitlab-ci.yml', 'Jenkinsfile'],
        'environment': ['.env*', 'config/*.yml', 'config/*.yaml'],
        'build_tools': ['webpack.config.js', 'vite.config.js', 'rollup.config.js']
    }

    counts = {}
    for category, patterns in config_patterns.items():
        count = 0
        for pattern in patterns:
            try:
                # Use rglob for patterns that might be in subdirectories
                if '*' in pattern or '?' in pattern:
                     count += len(list(repo_root.rglob(pattern)))
                else: # Use glob for top-level files
                     count += len(list(repo_root.glob(pattern)))
            except re.error:
                 print(f"Warning: Invalid pattern in BOM parser config: {pattern}")
                 continue
        counts[category] = count

    return counts

--- FILE_END: backend/lumiere_core/services/bom_parser.py ---

--- FILE_START: backend/lumiere_core/services/diplomat.py ---
# In backend/lumiere_core/services/diplomat.py

import os
import re
import requests
from typing import Dict, Any, List
from github import Github, GithubException, Issue

from . import llm_service
from .utils import clean_llm_code_output

# --- Configuration ---
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    raise ValueError("GITHUB_ACCESS_TOKEN must be set in the .env file.")

g = Github(GITHUB_TOKEN)

def _get_pr_for_issue(issue: Issue) -> Dict[str, Any] | None:
    """Finds the Pull Request that closed a given issue."""
    try:
        for event in issue.get_timeline():
            if event.event == "closed" and event.source and event.source.issue:
                pr = event.source.issue
                return {
                    "url": pr.html_url,
                    "title": pr.title,
                    "diff_url": pr.diff_url,
                }
    except GithubException as e:
        print(f"   -> API error while fetching timeline for {issue.html_url}: {e}")
    return None

def find_similar_solved_issues(issue_title: str, issue_body: str, model_identifier: str) -> Dict[str, Any]:
    """
    The main logic for The Diplomat agent.
    Searches GitHub for similar, solved issues and synthesizes the findings.
    """
    print("--- DIPLOMAT AGENT ACTIVATED ---")
    print(f"Using model: {model_identifier}")

    print("\n[Step 1/3] Generating a targeted search query from issue details...")
    query_generation_prompt = f"""
You are an expert GitHub search querycrafter. Based on the following issue title and body, generate a concise, powerful search query for finding similar issues.
Focus on extracting key library names, error messages, and critical function names.
For example, for a "TypeError" in "requests", the query might be: `requests "TypeError: timeout value must be a float"`.

ISSUE TITLE: {issue_title}
ISSUE BODY:
{issue_body}

Now, provide ONLY the search query string. Do not include any of your own commentary or XML tags.
"""
    raw_query = llm_service.generate_text(query_generation_prompt, model_identifier)
    search_query = clean_llm_code_output(raw_query).replace('"', '')
    print(f"✓ Generated Search Query: '{search_query}'")

    print("\n[Step 2/3] Searching GitHub for similar, solved issues...")
    qualified_query = f'{search_query} is:issue is:closed stars:>100 in:body'

    try:
        issues = g.search_issues(query=qualified_query, order="desc")
        print(f"✓ Found {issues.totalCount} potential matches. Analyzing the top 5...")

        # --- FIX: Check if there are any results before trying to iterate ---
        if issues.totalCount == 0:
            return {
                "summary": "The Diplomat was unable to find relevant, solved issues on GitHub for this specific problem.",
                "evidence": []
            }

        evidence = []
        # We can now safely iterate over the slice
        for issue in issues[:5]:
            print(f"   -> Analyzing: {issue.html_url}")
            closing_pr = _get_pr_for_issue(issue)
            if closing_pr:
                evidence.append({
                    "issue_title": issue.title,
                    "issue_url": issue.html_url,
                    "repo_name": issue.repository.full_name,
                    "solution_url": closing_pr['url'],
                    "diff_url": closing_pr['diff_url'],
                })

        if not evidence:
            return {
                "summary": "The Diplomat found some potentially related issues, but none had a clear linked Pull Request to analyze for a solution.",
                "evidence": []
            }

    except GithubException as e:
        return {"error": f"An error occurred while searching GitHub: {e.data.get('message', str(e))}"}

    print("\n[Step 3/3] Synthesizing findings into an intelligence briefing...")
    evidence_str = ""
    for item in evidence:
        evidence_str += f"- Issue in **{item['repo_name']}**: \"{item['issue_title']}\"\n"
        evidence_str += f"  - Issue Link: {item['issue_url']}\n"
        evidence_str += f"  - Solved by PR: {item['solution_url']}\n"

    synthesis_prompt = f"""
You are "The Diplomat," an AI agent for Lumière Sémantique.
You have found several solved issues on GitHub that are similar to the user's current problem.
Your mission is to write a concise intelligence briefing summarizing your findings. Do NOT tell the user how to fix their code. Instead, highlight the PATTERNS you found in the solutions.

Example summary format:
"This appears to be a known configuration issue. I found similar reports in `psf/requests` and `org/project` that were solved by changing a specific parameter. This strengthens the case for a configuration-based fix."

Here is the evidence you collected:
{evidence_str}

Now, generate the "Diplomat Intelligence Briefing" in Markdown.
"""
    summary = llm_service.generate_text(synthesis_prompt, model_identifier)

    print("--- DIPLOMAT AGENT MISSION COMPLETE ---")
    return {"summary": summary, "evidence": evidence}

--- FILE_END: backend/lumiere_core/services/diplomat.py ---

--- FILE_START: backend/lumiere_core/services/llm_service.py ---
# backend/lumiere_core/services/llm_service.py

from typing import List, Dict, Any

from . import ollama_service
from . import gemini_service

# --- The Lumière Task Router Configuration ---
# This is our Python equivalent of the config.json from claude-code-router.
# We will start with models we know work in our system.
TASK_ROUTER_CONFIG = {
    # For simple, non-critical tasks that require instruction following (like JSON output).
    "ROUTER_TASK_SIMPLE": "gemini/gemini-1.5-flash-latest",

    # For complex reasoning, code generation, and architectural analysis.
    "ROUTER_TASK_COMPLEX_REASONING": "gemini/gemini-1.5-flash-latest",

    # For tasks requiring a huge context window, like summarizing a whole repo.
    "ROUTER_TASK_LONG_CONTEXT": "gemini/gemini-1.5-pro-latest",

    # For code-specific generation tasks.
    "ROUTER_TASK_CODE_GENERATION": "gemini/gemini-1.5-flash-latest",
}


class TaskType:
    """Defines the types of tasks our system can perform."""
    SIMPLE = "ROUTER_TASK_SIMPLE"
    COMPLEX_REASONING = "ROUTER_TASK_COMPLEX_REASONING"
    LONG_CONTEXT = "ROUTER_TASK_LONG_CONTEXT"
    CODE_GENERATION = "ROUTER_TASK_CODE_GENERATION"


def generate_text(prompt: str, task_type: str = TaskType.SIMPLE) -> str:
    """
    The new intelligent entry point. It routes the prompt to the best model for the job.

    Args:
        prompt: The text prompt for the model.
        task_type: The type of task, used to select the right model from the router config.

    Returns:
        The generated text from the chosen model.
    """
    # 1. Select the model based on the task type
    model_identifier = TASK_ROUTER_CONFIG.get(task_type)
    if not model_identifier:
        # Fallback to a simple model if the task type is unknown
        model_identifier = TASK_ROUTER_CONFIG[TaskType.SIMPLE]
        print(f"Warning: Unknown task_type '{task_type}'. Defaulting to {model_identifier}.")

    print(f"Task Router: Routing task '{task_type}' to model '{model_identifier}'")

    # 2. Split the identifier to find the provider
    parts = model_identifier.split('/', 1)
    if len(parts) != 2:
        return f"Error: Invalid model identifier format '{model_identifier}'."

    provider, model_name = parts

    # 3. Call the appropriate provider service
    if provider == "ollama":
        return ollama_service.generate_text(prompt, model_name)
    elif provider == "gemini":
        return gemini_service.generate_text(prompt, model_name)
    else:
        return f"Error: Unknown LLM provider '{provider}'."

def list_available_models() -> List[Dict[str, Any]]:
    """
    Aggregates available models from all configured providers. (Unchanged)
    """
    all_models = []
    all_models.extend(ollama_service.list_models())
    all_models.extend(gemini_service.list_models())
    return all_models

--- FILE_END: backend/lumiere_core/services/llm_service.py ---

--- FILE_START: backend/lumiere_core/services/expertise_service.py ---
"""
Expertise Service

This service identifies knowledgeable contributors for specific files or modules
by analyzing git blame data and commit history. This powers the "Find an Expert" feature
of the Onboarding Concierge.
"""

import json
import pathlib
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ExpertiseService:
    """
    Service to calculate and rank experts for given code artifacts based on
    contribution patterns from git blame and commit history.
    """
    
    def __init__(self, cortex_directory: Optional[pathlib.Path] = None):
        """
        Initialize the Expertise Service.
        
        Args:
            cortex_directory: Optional path to the cortex directory.
                            If None, will use default backend/cloned_repositories/
        """
        self.cortex_directory = cortex_directory or pathlib.Path("backend/cloned_repositories")
    
    def find_experts_for_file(self, repo_id: str, file_path: str) -> List[Dict[str, Any]]:
        """
        Find and rank experts for a specific file based on contribution metrics.
        
        Args:
            repo_id: Repository identifier
            file_path: Path to the file relative to repository root
            
        Returns:
            List of experts sorted by expertise score, each containing:
            {
                'author': str,           # Author name
                'email': str,            # Author email
                'score': float,          # Expertise score (0-100)
                'details': {
                    'blame_lines': int,     # Lines attributed via git blame
                    'commits': int,         # Number of commits (if available)
                    'percentage': float     # Percentage of total lines
                }
            }
        """
        try:
            # Load blame cache data
            blame_data = self._load_blame_cache(repo_id)
            
            if not blame_data or file_path not in blame_data:
                logger.warning(f"No blame data found for {file_path} in {repo_id}")
                return []
            
            file_blame = blame_data[file_path]
            
            if not file_blame:
                logger.warning(f"Empty blame data for {file_path}")
                return []
            
            # Calculate total lines for percentage calculation
            total_lines = sum(file_blame.values())
            
            # Score each contributor
            experts = []
            for email, line_count in file_blame.items():
                # Extract name from email if available, otherwise use email
                author_name = self._extract_name_from_email(email)
                
                # Calculate base score from line contribution
                line_percentage = (line_count / total_lines) * 100 if total_lines > 0 else 0
                
                # For now, use blame data as primary metric
                # TODO: In Phase 3.1 enhancement, add commit history data
                expertise_score = self._calculate_expertise_score(
                    blame_lines=line_count,
                    total_lines=total_lines,
                    commits=0  # Placeholder for future GitHub API integration
                )
                
                expert_data = {
                    'author': author_name,
                    'email': email,
                    'score': round(expertise_score, 1),
                    'details': {
                        'blame_lines': line_count,
                        'commits': 0,  # Placeholder
                        'percentage': round(line_percentage, 1)
                    }
                }
                
                experts.append(expert_data)
            
            # Sort by expertise score descending
            experts.sort(key=lambda x: x['score'], reverse=True)
            
            # Limit to top 10 experts
            return experts[:10]
            
        except Exception as e:
            logger.error(f"Error finding experts for {file_path} in {repo_id}: {e}")
            return []
    
    def find_experts_for_module(self, repo_id: str, module_pattern: str) -> List[Dict[str, Any]]:
        """
        Find experts for a module or directory pattern.
        
        Args:
            repo_id: Repository identifier
            module_pattern: Pattern to match files (e.g., "src/services/", "*.py")
            
        Returns:
            Aggregated list of experts across all matching files
        """
        try:
            blame_data = self._load_blame_cache(repo_id)
            
            if not blame_data:
                return []
            
            # Find matching files
            matching_files = []
            for file_path in blame_data.keys():
                if self._matches_pattern(file_path, module_pattern):
                    matching_files.append(file_path)
            
            if not matching_files:
                logger.warning(f"No files matching pattern '{module_pattern}' found in {repo_id}")
                return []
            
            # Aggregate contributions across all matching files
            aggregated_contributions = {}
            total_lines_across_files = 0
            
            for file_path in matching_files:
                file_blame = blame_data[file_path]
                for email, line_count in file_blame.items():
                    aggregated_contributions[email] = aggregated_contributions.get(email, 0) + line_count
                    total_lines_across_files += line_count
            
            # Generate expert rankings
            experts = []
            for email, total_line_count in aggregated_contributions.items():
                author_name = self._extract_name_from_email(email)
                
                expertise_score = self._calculate_expertise_score(
                    blame_lines=total_line_count,
                    total_lines=total_lines_across_files,
                    commits=0  # Placeholder
                )
                
                percentage = (total_line_count / total_lines_across_files) * 100 if total_lines_across_files > 0 else 0
                
                expert_data = {
                    'author': author_name,
                    'email': email,
                    'score': round(expertise_score, 1),
                    'details': {
                        'blame_lines': total_line_count,
                        'commits': 0,  # Placeholder
                        'percentage': round(percentage, 1),
                        'files_involved': len([f for f in matching_files if email in blame_data[f]]),
                        'module_pattern': module_pattern
                    }
                }
                
                experts.append(expert_data)
            
            experts.sort(key=lambda x: x['score'], reverse=True)
            return experts[:10]
            
        except Exception as e:
            logger.error(f"Error finding experts for module '{module_pattern}' in {repo_id}: {e}")
            return []
    
    def _load_blame_cache(self, repo_id: str) -> Optional[Dict[str, Dict[str, int]]]:
        """
        Load the blame cache file for a repository.
        
        Args:
            repo_id: Repository identifier
            
        Returns:
            Blame cache data or None if not found
        """
        blame_cache_file = self.cortex_directory / repo_id / f"{repo_id}_blame_cache.json"
        
        if not blame_cache_file.exists():
            logger.warning(f"Blame cache file not found: {blame_cache_file}")
            return None
        
        try:
            with open(blame_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading blame cache {blame_cache_file}: {e}")
            return None
    
    def _extract_name_from_email(self, email: str) -> str:
        """
        Extract a display name from an email address.
        
        Args:
            email: Email address
            
        Returns:
            Display name or email if extraction fails
        """
        if '@' in email:
            local_part = email.split('@')[0]
            # Convert common patterns to readable names
            name = local_part.replace('.', ' ').replace('_', ' ').replace('-', ' ')
            # Capitalize words
            return ' '.join(word.capitalize() for word in name.split())
        return email
    
    def _calculate_expertise_score(self, blame_lines: int, total_lines: int, commits: int = 0) -> float:
        """
        Calculate expertise score using weighted formula.
        
        Args:
            blame_lines: Number of lines attributed to this author
            total_lines: Total lines in the file/module
            commits: Number of commits (placeholder for future enhancement)
            
        Returns:
            Expertise score between 0 and 100
        """
        # Current implementation focuses on blame data
        # Future enhancement will incorporate commit history
        
        if total_lines == 0:
            return 0.0
        
        # Base score from line contribution percentage
        line_percentage = (blame_lines / total_lines) * 100
        
        # Apply logarithmic scaling to avoid overly penalizing smaller contributors
        import math
        
        # Score based on contribution percentage with diminishing returns
        if line_percentage <= 0:
            score = 0.0
        elif line_percentage >= 50:
            score = 90.0 + (line_percentage - 50) * 0.2  # Slow growth after 50%
        else:
            score = line_percentage * 1.8  # Linear growth up to 50%
        
        # Future: Add commit frequency bonus
        # commit_bonus = min(commits * 2, 10)  # Up to 10 bonus points
        # score += commit_bonus
        
        return min(score, 100.0)
    
    def _matches_pattern(self, file_path: str, pattern: str) -> bool:
        """
        Check if a file path matches a given pattern.
        
        Args:
            file_path: File path to check
            pattern: Pattern to match (supports * wildcards and directory matching)
            
        Returns:
            True if file matches pattern
        """
        import fnmatch
        
        # Handle directory patterns
        if pattern.endswith('/'):
            return file_path.startswith(pattern) or f"/{pattern}" in file_path
        
        # Handle file extension patterns
        if pattern.startswith('*.'):
            return file_path.endswith(pattern[1:])
        
        # Handle general glob patterns
        return fnmatch.fnmatch(file_path, pattern)
    
    def get_repository_experts_summary(self, repo_id: str) -> Dict[str, Any]:
        """
        Get an overall summary of expertise distribution in the repository.
        
        Args:
            repo_id: Repository identifier
            
        Returns:
            Summary with top contributors and statistics
        """
        try:
            blame_data = self._load_blame_cache(repo_id)
            
            if not blame_data:
                return {'error': 'No blame data available'}
            
            # Aggregate all contributions
            global_contributions = {}
            total_files = len(blame_data)
            total_lines = 0
            
            for file_path, file_blame in blame_data.items():
                for email, line_count in file_blame.items():
                    if email not in global_contributions:
                        global_contributions[email] = {
                            'total_lines': 0,
                            'files_contributed': 0
                        }
                    
                    global_contributions[email]['total_lines'] += line_count
                    global_contributions[email]['files_contributed'] += 1
                    total_lines += line_count
            
            # Calculate top contributors
            top_contributors = []
            for email, data in global_contributions.items():
                author_name = self._extract_name_from_email(email)
                contribution_percentage = (data['total_lines'] / total_lines) * 100 if total_lines > 0 else 0
                
                contributor = {
                    'author': author_name,
                    'email': email,
                    'total_lines': data['total_lines'],
                    'files_contributed': data['files_contributed'],
                    'contribution_percentage': round(contribution_percentage, 1)
                }
                
                top_contributors.append(contributor)
            
            # Sort by total lines contributed
            top_contributors.sort(key=lambda x: x['total_lines'], reverse=True)
            
            return {
                'repository': repo_id,
                'total_files_analyzed': total_files,
                'total_lines_of_code': total_lines,
                'total_contributors': len(global_contributions),
                'top_contributors': top_contributors[:10],
                'analysis_timestamp': self._get_current_timestamp()
            }
            
        except Exception as e:
            logger.error(f"Error generating repository experts summary for {repo_id}: {e}")
            return {'error': str(e)}
    
    def _get_current_timestamp(self) -> str:
        """Get current timestamp in ISO format."""
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()


# Convenience functions for external use
def find_experts_for_file(repo_id: str, file_path: str) -> List[Dict[str, Any]]:
    """
    Convenience function to find experts for a specific file.
    
    Args:
        repo_id: Repository identifier
        file_path: Path to the file
        
    Returns:
        List of experts ranked by expertise score
    """
    service = ExpertiseService()
    return service.find_experts_for_file(repo_id, file_path)


def find_experts_for_module(repo_id: str, module_pattern: str) -> List[Dict[str, Any]]:
    """
    Convenience function to find experts for a module or pattern.
    
    Args:
        repo_id: Repository identifier
        module_pattern: File/directory pattern to match
        
    Returns:
        List of experts ranked by expertise score
    """
    service = ExpertiseService()
    return service.find_experts_for_module(repo_id, module_pattern)
--- FILE_END: backend/lumiere_core/services/expertise_service.py ---

--- FILE_START: backend/lumiere_core/services/gemini_service.py ---
# In backend/lumiere_core/services/gemini_service.py

import os
import google.generativeai as genai
from typing import List, Dict

# Configure the Gemini client from environment variables
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

def is_configured() -> bool:
    """Check if the Gemini service is ready to be used."""
    return GEMINI_API_KEY is not None

def generate_text(prompt: str, model_name: str) -> str:
    """
    Sends a prompt to the Google Gemini API.

    Args:
        prompt: The full prompt to send.
        model_name: The specific Gemini model to use (e.g., 'gemini-1.5-pro-latest').
    """
    if not is_configured():
        return "Error: GEMINI_API_KEY is not configured in the environment."

    print(f"Sending prompt to Google Gemini model: '{model_name}'...")
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"An error occurred while communicating with the Gemini API: {e}")
        return f"Error from Gemini API: {e}"

def list_models() -> List[Dict[str, str]]:
    """Lists available Gemini models that support text generation."""
    if not is_configured():
        return []

    print("Fetching available Google Gemini models...")
    available = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                model_id = f"gemini/{m.name.replace('models/', '')}"
                available.append({
                    "id": model_id,
                    "provider": "gemini",
                    "name": m.display_name
                })
        return available
    except Exception as e:
        print(f"Could not fetch Gemini models: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/gemini_service.py ---

--- FILE_START: backend/lumiere_core/services/utils.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/utils.py
# In lumiere_core/services/utils.py
import re

def clean_llm_code_output(raw_code: str) -> str:
    """
    [Robustness] Removes Markdown code fences and extraneous whitespace from LLM output.

    This function uses a regular expression to find and remove
    common Markdown code block fences (like ```python or ```) from the start
    and end of the string. It also strips any leading or trailing whitespace.
    This is a shared utility to ensure all code-generating agents produce
    clean, machine-readable output.
    """
    # This regex matches an optional language specifier (like 'python', 'toml')
    # and the code fences themselves at the start and end of the string.
    code_fence_pattern = r"^\s*```[a-zA-Z]*\n?|```\s*$"
    cleaned_code = re.sub(code_fence_pattern, '', raw_code)
    return cleaned_code.strip()

--- FILE_END: backend/lumiere_core/services/utils.py ---

--- FILE_START: backend/lumiere_core/services/oracle_service.py ---
# backend/lumiere_core/services/oracle_service.py

import logging
import json
import networkx as nx
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from collections import defaultdict, Counter
import re
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import hashlib
import time
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from functools import lru_cache
import threading
from queue import Queue, Empty
import asyncio
from contextlib import contextmanager

from . import llm_service, cortex_service, ollama
from .llm_service import TaskType
from .utils import clean_llm_code_output

logger = logging.getLogger(__name__)

# --- Exceptions ---
class OracleServiceError(Exception):
    """Base exception for Oracle service errors."""
    pass

class GraphBuildingError(OracleServiceError):
    """Raised when graph building fails."""
    pass

class EntityExtractionError(OracleServiceError):
    """Raised when entity extraction fails."""
    pass

class GraphAnalysisError(OracleServiceError):
    """Raised when graph analysis fails."""
    pass

class RAGSearchError(OracleServiceError):
    """Raised when RAG search fails."""
    pass

# --- Enhanced Configuration ---
class SearchStrategy(Enum):
    """Search strategies for different query types."""
    SIMPLE = "simple"
    EXPANDED = "expanded"
    MULTI_HOP = "multi_hop"
    HYBRID = "hybrid"
    GRAPH_GUIDED = "graph_guided"

@dataclass
class RAGConfig:
    """Enhanced configuration for RAG search behavior."""
    # Search parameters
    default_k: int = 20
    max_k: int = 50
    min_k: int = 5

    # Reranking thresholds
    similarity_threshold: float = 0.7
    diversity_threshold: float = 0.3
    relevance_decay: float = 0.95

    # Query expansion
    max_expansions: int = 5
    max_synonyms: int = 3
    expansion_depth: int = 2

    # Multi-hop settings
    max_hops: int = 3
    hop_decay: float = 0.8
    min_hop_relevance: float = 0.5

    # Parallel execution
    max_workers: int = 5
    timeout_seconds: int = 30

    # Caching
    cache_ttl: int = 3600  # 1 hour
    max_cache_size: int = 1000

    # Advanced features
    use_semantic_clustering: bool = True
    use_entity_linking: bool = True
    use_code_understanding: bool = True
    enable_self_reflection: bool = True

# Global config instance
rag_config = RAGConfig()

# --- Cache Management ---
class SearchCache:
    """Thread-safe cache for search results."""
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self._cache = {}
        self._timestamps = {}
        self._lock = threading.RLock()
        self._max_size = max_size
        self._ttl = ttl

    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < self._ttl:
                    return self._cache[key]
                else:
                    del self._cache[key]
                    del self._timestamps[key]
            return None

    def put(self, key: str, value: Any):
        with self._lock:
            if len(self._cache) >= self._max_size:
                # Remove oldest entry
                oldest_key = min(self._timestamps, key=self._timestamps.get)
                del self._cache[oldest_key]
                del self._timestamps[oldest_key]

            self._cache[key] = value
            self._timestamps[key] = time.time()

    def clear(self):
        with self._lock:
            self._cache.clear()
            self._timestamps.clear()

# Global cache instance
search_cache = SearchCache(rag_config.max_cache_size, rag_config.cache_ttl)

# --- Advanced Entity Processing ---
@dataclass
class Entity:
    """Represents an extracted entity with metadata."""
    text: str
    type: str
    confidence: float = 1.0
    context: str = ""
    aliases: List[str] = field(default_factory=list)
    related: List[str] = field(default_factory=list)

class EntityExtractor:
    """Advanced entity extraction with multiple strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self._pattern_cache = {}
        self._init_patterns()

    def _init_patterns(self):
        """Initialize regex patterns for code entity extraction."""
        self._pattern_cache = {
            'import': re.compile(r'(?:from|import)\s+([a-zA-Z_][\w\.]*)', re.MULTILINE),
            'class': re.compile(r'class\s+([A-Z]\w*)', re.MULTILINE),
            'function': re.compile(r'def\s+([a-z_]\w*)', re.MULTILINE),
            'variable': re.compile(r'([a-z_]\w*)\s*=', re.MULTILINE),
            'file_path': re.compile(r'[\'"]([^\'"\s]+\.[a-z]+)[\'"]', re.MULTILINE),
            'url': re.compile(r'https?://[^\s]+', re.MULTILINE),
            'api_endpoint': re.compile(r'[\'"][/][\w/\-{}]+[\'"]', re.MULTILINE),
        }

    def extract(self, text: str, use_llm: bool = True) -> Dict[str, List[Entity]]:
        """Extract entities using both pattern matching and LLM."""
        entities = defaultdict(list)

        # Pattern-based extraction
        if self.config.use_code_understanding:
            entities.update(self._extract_with_patterns(text))

        # LLM-based extraction
        if use_llm:
            try:
                llm_entities = self._extract_with_llm(text)
                self._merge_entities(entities, llm_entities)
            except Exception as e:
                logger.warning(f"LLM entity extraction failed: {e}")

        # Entity linking and expansion
        if self.config.use_entity_linking:
            entities = self._link_entities(entities)

        return dict(entities)

    def _extract_with_patterns(self, text: str) -> Dict[str, List[Entity]]:
        """Extract entities using regex patterns."""
        entities = defaultdict(list)

        for pattern_name, pattern in self._pattern_cache.items():
            matches = pattern.findall(text)
            entity_type = self._pattern_to_entity_type(pattern_name)

            for match in matches:
                if isinstance(match, str) and match.strip():
                    entity = Entity(
                        text=match.strip(),
                        type=entity_type,
                        confidence=0.8,
                        context=self._extract_context(text, match)
                    )
                    entities[entity_type].append(entity)

        return entities

    def _extract_with_llm(self, text: str) -> Dict[str, List[Entity]]:
        """Extract entities using LLM with structured output."""
        prompt = f"""Extract software development entities from this text.
Return a JSON object with these keys: functions, classes, files, technologies, patterns, concepts.
Each value should be a list of objects with: text, confidence (0-1), and context.

Text: "{text[:1000]}"

JSON Output:"""

        try:
            response = llm_service.generate_text(prompt, task_type=TaskType.SIMPLE)
            cleaned = clean_llm_code_output(response)
            data = json.loads(cleaned)

            entities = defaultdict(list)
            for entity_type, entity_list in data.items():
                if isinstance(entity_list, list):
                    for item in entity_list:
                        if isinstance(item, dict) and 'text' in item:
                            entity = Entity(
                                text=item['text'],
                                type=entity_type,
                                confidence=item.get('confidence', 0.9),
                                context=item.get('context', '')
                            )
                            entities[entity_type].append(entity)

            return entities
        except Exception as e:
            logger.error(f"Failed to parse LLM entity response: {e}")
            return {}

    def _merge_entities(self, base: Dict[str, List[Entity]],
                       new: Dict[str, List[Entity]]) -> None:
        """Merge entity lists, avoiding duplicates."""
        for entity_type, entity_list in new.items():
            existing_texts = {e.text.lower() for e in base.get(entity_type, [])}

            for entity in entity_list:
                if entity.text.lower() not in existing_texts:
                    base[entity_type].append(entity)
                    existing_texts.add(entity.text.lower())

    def _link_entities(self, entities: Dict[str, List[Entity]]) -> Dict[str, List[Entity]]:
        """Link related entities and find aliases."""
        # Build entity graph
        entity_graph = defaultdict(set)

        for entity_type, entity_list in entities.items():
            for entity in entity_list:
                # Find potential aliases (e.g., MyClass vs my_class)
                aliases = self._find_aliases(entity.text, entities)
                entity.aliases = aliases

                # Link related entities
                for alias in aliases:
                    entity_graph[entity.text].add(alias)

        return entities

    def _find_aliases(self, text: str, all_entities: Dict[str, List[Entity]]) -> List[str]:
        """Find potential aliases for an entity."""
        aliases = []
        text_lower = text.lower()
        text_snake = self._to_snake_case(text)
        text_camel = self._to_camel_case(text)

        for entity_list in all_entities.values():
            for entity in entity_list:
                if entity.text != text:
                    other_lower = entity.text.lower()
                    if (other_lower == text_lower or
                        other_lower == text_snake or
                        other_lower == text_camel):
                        aliases.append(entity.text)

        return aliases

    def _extract_context(self, text: str, match: str, window: int = 50) -> str:
        """Extract context around a match."""
        index = text.find(match)
        if index == -1:
            return ""

        start = max(0, index - window)
        end = min(len(text), index + len(match) + window)
        return text[start:end].strip()

    def _pattern_to_entity_type(self, pattern_name: str) -> str:
        """Map pattern names to entity types."""
        mapping = {
            'import': 'technologies',
            'class': 'classes',
            'function': 'functions',
            'variable': 'variables',
            'file_path': 'files',
            'url': 'urls',
            'api_endpoint': 'endpoints'
        }
        return mapping.get(pattern_name, 'general_terms')

    def _to_snake_case(self, text: str) -> str:
        """Convert to snake_case."""
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def _to_camel_case(self, text: str) -> str:
        """Convert to CamelCase."""
        components = text.split('_')
        return ''.join(x.title() for x in components)

# --- Query Processing and Expansion ---
class QueryProcessor:
    """Advanced query processing with multiple expansion strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self._synonym_cache = {}

    def process(self, query: str, entities: Dict[str, List[Entity]],
                strategy: SearchStrategy = SearchStrategy.EXPANDED) -> List[str]:
        """Process and expand query based on strategy."""
        if strategy == SearchStrategy.SIMPLE:
            return [query]
        elif strategy == SearchStrategy.EXPANDED:
            return self._expand_query(query, entities)
        elif strategy == SearchStrategy.MULTI_HOP:
            return self._generate_hop_queries(query, entities)
        elif strategy == SearchStrategy.HYBRID:
            return self._hybrid_expansion(query, entities)
        else:
            return [query]

    def _expand_query(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Expand query with entity variations and synonyms."""
        expansions = [query]

        # Add entity-based expansions
        for entity_type, entity_list in entities.items():
            for entity in entity_list[:2]:  # Limit to avoid explosion
                # Add direct entity queries
                expansions.append(f"{entity.text} implementation")
                expansions.append(f"usage of {entity.text}")

                # Add alias queries
                for alias in entity.aliases[:2]:
                    expansions.append(f"{alias} {entity_type[:-1]}")

        # Add pattern-based expansions
        patterns = self._extract_query_patterns(query)
        for pattern in patterns:
            expansions.extend(self._expand_pattern(pattern, entities))

        # Deduplicate and limit
        seen = set()
        unique = []
        for exp in expansions:
            if exp not in seen and len(exp) > 3:
                seen.add(exp)
                unique.append(exp)

        return unique[:self.config.max_expansions]

    def _generate_hop_queries(self, query: str, entities: Dict[str, List[Entity]],
                             hop: int = 0) -> List[str]:
        """Generate queries for multi-hop search."""
        if hop == 0:
            return [query]

        hop_queries = []

        # Focus on different aspects based on hop
        if hop == 1:
            # Look for implementations
            for entity in self._get_top_entities(entities, 3):
                hop_queries.append(f"implementation of {entity.text}")
                hop_queries.append(f"{entity.text} definition")
        elif hop == 2:
            # Look for usages and connections
            for entity in self._get_top_entities(entities, 2):
                hop_queries.append(f"calls to {entity.text}")
                hop_queries.append(f"{entity.text} dependencies")
        else:
            # Look for related concepts
            for entity in self._get_top_entities(entities, 2):
                hop_queries.append(f"related to {entity.text}")

        return hop_queries[:self.config.max_expansions]

    def _hybrid_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Combine multiple expansion strategies."""
        expansions = []

        # Simple expansions
        expansions.extend(self._expand_query(query, entities))

        # Semantic expansions
        if self.config.use_semantic_clustering:
            expansions.extend(self._semantic_expansion(query, entities))

        # Code-aware expansions
        if self.config.use_code_understanding:
            expansions.extend(self._code_aware_expansion(query, entities))

        # Deduplicate and rank
        ranked = self._rank_expansions(expansions, query)
        return ranked[:self.config.max_expansions * 2]

    def _semantic_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Generate semantically related queries."""
        expansions = []

        # Extract key concepts
        concepts = self._extract_concepts(query)

        for concept in concepts:
            # Add conceptual queries
            expansions.append(f"examples of {concept}")
            expansions.append(f"{concept} pattern")
            expansions.append(f"best practices for {concept}")

        return expansions

    def _code_aware_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Generate code-aware query expansions."""
        expansions = []

        # Look for code patterns in query
        if "api" in query.lower():
            expansions.extend(["API routes", "endpoint definitions", "REST endpoints"])
        if "database" in query.lower():
            expansions.extend(["schema", "models", "queries"])
        if "test" in query.lower():
            expansions.extend(["unit tests", "test cases", "test fixtures"])

        return expansions

    def _extract_query_patterns(self, query: str) -> List[str]:
        """Extract patterns from query."""
        patterns = []

        # Question patterns
        question_words = ["what", "how", "where", "when", "why", "which"]
        for word in question_words:
            if word in query.lower():
                patterns.append(word)

        # Action patterns
        action_words = ["implement", "use", "call", "create", "update", "delete"]
        for word in action_words:
            if word in query.lower():
                patterns.append(word)

        return patterns

    def _expand_pattern(self, pattern: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Expand based on pattern type."""
        expansions = []

        if pattern in ["what", "where"]:
            for entity in self._get_top_entities(entities, 2):
                expansions.append(f"{pattern} is {entity.text}")
        elif pattern in ["how"]:
            for entity in self._get_top_entities(entities, 2):
                expansions.append(f"{pattern} to use {entity.text}")

        return expansions

    def _extract_concepts(self, query: str) -> List[str]:
        """Extract high-level concepts from query."""
        # Simple concept extraction - could be enhanced with NLP
        words = query.lower().split()
        concepts = []

        # Look for noun phrases
        for i, word in enumerate(words):
            if len(word) > 4:  # Simple heuristic
                if i + 1 < len(words):
                    concept = f"{word} {words[i+1]}"
                    if len(words[i+1]) > 3:
                        concepts.append(concept)
                concepts.append(word)

        return concepts[:3]

    def _rank_expansions(self, expansions: List[str], original: str) -> List[str]:
        """Rank expansions by relevance to original query."""
        scored = []

        for exp in expansions:
            score = self._calculate_relevance(exp, original)
            scored.append((exp, score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [exp for exp, _ in scored]

    def _calculate_relevance(self, expansion: str, original: str) -> float:
        """Calculate relevance score between expansion and original."""
        # Simple word overlap score
        original_words = set(original.lower().split())
        expansion_words = set(expansion.lower().split())

        overlap = len(original_words & expansion_words)
        total = len(original_words | expansion_words)

        return overlap / total if total > 0 else 0

    def _get_top_entities(self, entities: Dict[str, List[Entity]], n: int) -> List[Entity]:
        """Get top N entities by confidence."""
        all_entities = []
        for entity_list in entities.values():
            all_entities.extend(entity_list)

        all_entities.sort(key=lambda e: e.confidence, reverse=True)
        return all_entities[:n]

# --- Advanced Search and Reranking ---
class SearchOrchestrator:
    """Orchestrates complex search operations with multiple strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.entity_extractor = EntityExtractor(config)
        self.query_processor = QueryProcessor(config)
        self._executor = ThreadPoolExecutor(max_workers=config.max_workers)

    def search(self, repo_id: str, query: str,
               strategy: SearchStrategy = SearchStrategy.HYBRID,
               graph: Optional[nx.DiGraph] = None) -> Dict[str, Any]:
        """Perform advanced search with specified strategy."""
        start_time = time.time()

        # Check cache
        cache_key = self._generate_cache_key(repo_id, query, strategy)
        cached_result = search_cache.get(cache_key)
        if cached_result:
            logger.info(f"Cache hit for query: {query}")
            return cached_result

        # Extract entities
        entities = self.entity_extractor.extract(query)

        # Choose search strategy
        if strategy == SearchStrategy.SIMPLE:
            results = self._simple_search(repo_id, query)
        elif strategy == SearchStrategy.EXPANDED:
            results = self._expanded_search(repo_id, query, entities)
        elif strategy == SearchStrategy.MULTI_HOP:
            results = self._multi_hop_search(repo_id, query, entities)
        elif strategy == SearchStrategy.GRAPH_GUIDED and graph:
            results = self._graph_guided_search(repo_id, query, entities, graph)
        else:
            results = self._hybrid_search(repo_id, query, entities, graph)

        # Post-process results
        results = self._post_process_results(results, query, entities)

        # Add metadata
        results['metadata'] = {
            'strategy': strategy.value,
            'entities_found': {k: len(v) for k, v in entities.items()},
            'search_time': time.time() - start_time,
            'total_results': len(results.get('chunks', []))
        }

        # Cache results
        search_cache.put(cache_key, results)

        return results

    def _simple_search(self, repo_id: str, query: str) -> Dict[str, Any]:
        """Perform simple single-query search."""
        try:
            chunks = ollama.search_index(
                query,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=self.config.default_k
            )
            return {'chunks': chunks or []}
        except Exception as e:
            logger.error(f"Simple search failed: {e}")
            return {'chunks': [], 'error': str(e)}

    def _expanded_search(self, repo_id: str, query: str,
                        entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Perform expanded search with query variations."""
        expansions = self.query_processor.process(query, entities, SearchStrategy.EXPANDED)

        all_chunks = []
        futures = []

        for expansion in expansions:
            future = self._executor.submit(
                ollama.search_index,
                expansion,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=self.config.default_k // len(expansions)
            )
            futures.append((future, expansion))

        for future, expansion in futures:
            try:
                chunks = future.result(timeout=self.config.timeout_seconds)
                if chunks:
                    for chunk in chunks:
                        chunk['query_source'] = expansion
                    all_chunks.extend(chunks)
            except Exception as e:
                logger.warning(f"Search failed for expansion '{expansion}': {e}")

        return {'chunks': all_chunks, 'expansions': expansions}

    def _multi_hop_search(self, repo_id: str, query: str,
                         entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Perform multi-hop search following references."""
        all_chunks = []
        hop_results = []
        visited_queries = set()

        current_queries = [query]
        current_entities = entities

        for hop in range(self.config.max_hops):
            if not current_queries:
                break

            hop_chunks = []
            next_queries = []
            next_entities = defaultdict(list)

            # Search current queries
            for q in current_queries:
                if q in visited_queries:
                    continue

                visited_queries.add(q)

                # Search
                result = self._expanded_search(repo_id, q, current_entities)
                chunks = result.get('chunks', [])

                if chunks:
                    # Apply hop decay to scores
                    decay = self.config.hop_decay ** hop
                    for chunk in chunks:
                        chunk['hop'] = hop
                        chunk['relevance_score'] = chunk.get('relevance_score', 1.0) * decay

                    hop_chunks.extend(chunks)

                    # Extract new entities for next hop
                    for chunk in chunks[:5]:  # Analyze top chunks
                        text = chunk.get('text', '')
                        new_entities = self.entity_extractor.extract(text, use_llm=False)
                        for entity_type, entity_list in new_entities.items():
                            next_entities[entity_type].extend(entity_list)

            if hop_chunks:
                all_chunks.extend(hop_chunks)
                hop_results.append({
                    'hop': hop,
                    'queries': current_queries,
                    'chunks_found': len(hop_chunks)
                })

            # Generate next hop queries
            if hop < self.config.max_hops - 1:
                for entity_type, entity_list in next_entities.items():
                    for entity in entity_list[:2]:  # Limit expansion
                        if entity.confidence > self.config.min_hop_relevance:
                            next_queries.append(entity.text)

            current_queries = list(set(next_queries))[:self.config.max_expansions]
            current_entities = dict(next_entities)

        return {
            'chunks': all_chunks,
            'hop_results': hop_results,
            'total_hops': len(hop_results)
        }

    def _graph_guided_search(self, repo_id: str, query: str,
                           entities: Dict[str, List[Entity]],
                           graph: nx.DiGraph) -> Dict[str, Any]:
        """Use graph structure to guide search."""
        # Find relevant nodes in graph
        relevant_nodes = self._find_relevant_nodes(graph, entities)

        if not relevant_nodes:
            # Fallback to expanded search
            return self._expanded_search(repo_id, query, entities)

        all_chunks = []
        node_results = []

        # Search for each relevant node and its neighbors
        for node_id, relevance_score in relevant_nodes[:10]:
            node_data = graph.nodes[node_id]

            # Generate targeted query
            node_query = self._generate_node_query(node_id, node_data, query)

            # Search
            chunks = ollama.search_index(
                node_query,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=5
            )

            if chunks:
                for chunk in chunks:
                    chunk['graph_node'] = node_id
                    chunk['node_relevance'] = relevance_score

                all_chunks.extend(chunks)

                node_results.append({
                    'node': node_id,
                    'query': node_query,
                    'chunks_found': len(chunks)
                })

        return {
            'chunks': all_chunks,
            'node_results': node_results,
            'graph_nodes_searched': len(relevant_nodes)
        }

    def _hybrid_search(self, repo_id: str, query: str,
                      entities: Dict[str, List[Entity]],
                      graph: Optional[nx.DiGraph] = None) -> Dict[str, Any]:
        """Combine multiple search strategies."""
        all_results = []

        # Parallel execution of different strategies
        futures = [
            (self._executor.submit(self._expanded_search, repo_id, query, entities), 'expanded'),
            (self._executor.submit(self._multi_hop_search, repo_id, query, entities), 'multi_hop')
        ]

        if graph:
            futures.append(
                (self._executor.submit(self._graph_guided_search, repo_id, query, entities, graph), 'graph_guided')
            )

        strategy_results = {}
        all_chunks = []

        for future, strategy_name in futures:
            try:
                result = future.result(timeout=self.config.timeout_seconds)
                strategy_results[strategy_name] = result
                all_chunks.extend(result.get('chunks', []))
            except Exception as e:
                logger.warning(f"Strategy {strategy_name} failed: {e}")

        # Merge and deduplicate results
        unique_chunks = self._deduplicate_chunks(all_chunks)

        return {
            'chunks': unique_chunks,
            'strategy_results': strategy_results,
            'strategies_used': list(strategy_results.keys())
        }

    def _find_relevant_nodes(self, graph: nx.DiGraph,
                           entities: Dict[str, List[Entity]]) -> List[Tuple[str, float]]:
        """Find nodes in graph relevant to entities."""
        node_scores = []

        for node_id, node_data in graph.nodes(data=True):
            score = 0.0

            # Score based on entity matches
            for entity_type, entity_list in entities.items():
                for entity in entity_list:
                    if entity.text.lower() in node_id.lower():
                        score += entity.confidence * 2
                    if entity.text.lower() in str(node_data.get('name', '')).lower():
                        score += entity.confidence

                    # Check aliases
                    for alias in entity.aliases:
                        if alias.lower() in node_id.lower():
                            score += entity.confidence * 0.5

            if score > 0:
                node_scores.append((node_id, score))

        # Sort by score
        node_scores.sort(key=lambda x: x[1], reverse=True)
        return node_scores

    def _generate_node_query(self, node_id: str, node_data: Dict[str, Any],
                           original_query: str) -> str:
        """Generate a query specific to a graph node."""
        node_name = node_data.get('name', node_id)
        node_type = node_data.get('type', '')
        file_path = node_data.get('file', '')

        # Combine node info with original query intent
        if file_path:
            return f"{node_name} in {file_path} {original_query}"
        else:
            return f"{node_type} {node_name} {original_query}"

    def _post_process_results(self, results: Dict[str, Any], query: str,
                            entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Post-process search results with reranking and clustering."""
        chunks = results.get('chunks', [])

        if not chunks:
            return results

        # Deduplicate
        chunks = self._deduplicate_chunks(chunks)

        # Rerank
        chunks = self._rerank_chunks(chunks, query, entities)

        # Cluster if enabled
        if self.config.use_semantic_clustering and len(chunks) > 10:
            clusters = self._cluster_chunks(chunks)
            results['clusters'] = clusters

        # Select top chunks with diversity
        final_chunks = self._select_diverse_chunks(chunks, self.config.default_k)

        results['chunks'] = final_chunks
        results['total_before_filtering'] = len(chunks)

        return results

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Advanced deduplication using content similarity."""
        if not chunks:
            return []

        unique_chunks = []
        seen_hashes = set()
        seen_content = []

        for chunk in chunks:
            # Quick hash check
            chunk_hash = self._calculate_chunk_hash(chunk)
            if chunk_hash in seen_hashes:
                continue

            # Semantic similarity check
            if self._is_duplicate(chunk, seen_content):
                continue

            seen_hashes.add(chunk_hash)
            seen_content.append(chunk)
            unique_chunks.append(chunk)

        return unique_chunks

    def _calculate_chunk_hash(self, chunk: Dict[str, Any]) -> str:
        """Calculate hash for chunk."""
        text = chunk.get('text', '')
        file_path = chunk.get('file_path', '')
        # Use more of the content for better deduplication
        content = f"{file_path}:{text[:200]}"
        return hashlib.md5(content.encode()).hexdigest()

    def _is_duplicate(self, chunk: Dict[str, Any],
                     seen_content: List[Dict[str, Any]]) -> bool:
        """Check if chunk is semantically similar to seen content."""
        chunk_text = chunk.get('text', '').lower()
        chunk_words = set(chunk_text.split())

        for seen_chunk in seen_content:
            seen_text = seen_chunk.get('text', '').lower()
            seen_words = set(seen_text.split())

            # Calculate Jaccard similarity
            intersection = len(chunk_words & seen_words)
            union = len(chunk_words | seen_words)

            if union > 0:
                similarity = intersection / union
                if similarity > self.config.similarity_threshold:
                    return True

        return False

    def _rerank_chunks(self, chunks: List[Dict[str, Any]], query: str,
                      entities: Dict[str, List[Entity]]) -> List[Dict[str, Any]]:
        """Advanced reranking with multiple signals."""
        # Extract ranking features
        for chunk in chunks:
            features = self._extract_ranking_features(chunk, query, entities)

            # Calculate composite score
            score = (
                features['text_relevance'] * 0.3 +
                features['entity_coverage'] * 0.3 +
                features['code_quality'] * 0.2 +
                features['freshness'] * 0.1 +
                features['structural_relevance'] * 0.1
            )

            # Apply existing scores
            if 'relevance_score' in chunk:
                score = score * 0.7 + chunk['relevance_score'] * 0.3

            chunk['final_score'] = score
            chunk['ranking_features'] = features

        # Sort by final score
        chunks.sort(key=lambda x: x.get('final_score', 0), reverse=True)

        return chunks

    def _extract_ranking_features(self, chunk: Dict[str, Any], query: str,
                                entities: Dict[str, List[Entity]]) -> Dict[str, float]:
        """Extract features for ranking."""
        text = chunk.get('text', '').lower()
        file_path = chunk.get('file_path', '').lower()

        features = {
            'text_relevance': self._calculate_text_relevance(text, query),
            'entity_coverage': self._calculate_entity_coverage(text, entities),
            'code_quality': self._estimate_code_quality(text),
            'freshness': self._calculate_freshness(chunk),
            'structural_relevance': self._calculate_structural_relevance(chunk)
        }

        return features

    def _calculate_text_relevance(self, text: str, query: str) -> float:
        """Calculate text relevance to query."""
        query_terms = set(query.lower().split())
        text_terms = set(text.split())

        # Term frequency
        term_freq = sum(1 for term in query_terms if term in text_terms)

        # Normalize by query length
        relevance = term_freq / len(query_terms) if query_terms else 0

        # Boost for exact phrase match
        if query.lower() in text:
            relevance += 0.5

        return min(relevance, 1.0)

    def _calculate_entity_coverage(self, text: str,
                                 entities: Dict[str, List[Entity]]) -> float:
        """Calculate how many entities are covered in text."""
        total_entities = sum(len(v) for v in entities.values())
        if total_entities == 0:
            return 0.5  # Neutral score

        covered = 0
        for entity_list in entities.values():
            for entity in entity_list:
                if entity.text.lower() in text:
                    covered += entity.confidence
                # Check aliases
                for alias in entity.aliases:
                    if alias.lower() in text:
                        covered += entity.confidence * 0.5
                        break

        return min(covered / total_entities, 1.0)

    def _estimate_code_quality(self, text: str) -> float:
        """Estimate code quality based on heuristics."""
        quality_score = 0.5  # Base score

        # Check for code indicators
        code_indicators = [
            (r'def\s+\w+\s*\(', 0.1),  # Functions
            (r'class\s+\w+', 0.1),      # Classes
            (r'import\s+\w+', 0.05),    # Imports
            (r'"""[\s\S]+?"""', 0.1),   # Docstrings
            (r'#\s*\w+', 0.05),         # Comments
        ]

        for pattern, score in code_indicators:
            if re.search(pattern, text):
                quality_score += score

        # Penalty for too long or too short
        length = len(text)
        if length < 50:
            quality_score *= 0.5
        elif length > 2000:
            quality_score *= 0.8

        return min(quality_score, 1.0)

    def _calculate_freshness(self, chunk: Dict[str, Any]) -> float:
        """Calculate freshness score (placeholder for timestamp-based scoring)."""
        # Could be enhanced with actual file timestamps
        return 0.5

    def _calculate_structural_relevance(self, chunk: Dict[str, Any]) -> float:
        """Calculate relevance based on structural properties."""
        score = 0.5

        # Boost for certain file types
        file_path = chunk.get('file_path', '').lower()
        if file_path.endswith(('.py', '.js', '.java')):
            score += 0.2
        elif file_path.endswith(('.md', '.txt', '.rst')):
            score += 0.1

        # Boost for specific directories
        important_dirs = ['src', 'core', 'api', 'routes', 'models', 'services']
        for dir_name in important_dirs:
            if dir_name in file_path:
                score += 0.1
                break

        return min(score, 1.0)

    def _cluster_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Cluster chunks by semantic similarity."""
        # Simple clustering based on shared entities
        clusters = []
        clustered = set()

        for i, chunk in enumerate(chunks):
            if i in clustered:
                continue

            cluster = {
                'id': f'cluster_{i}',
                'chunks': [chunk],
                'representative': chunk,
                'size': 1
            }

            # Find similar chunks
            for j, other_chunk in enumerate(chunks[i+1:], i+1):
                if j in clustered:
                    continue

                if self._chunks_similar(chunk, other_chunk):
                    cluster['chunks'].append(other_chunk)
                    cluster['size'] += 1
                    clustered.add(j)

            clusters.append(cluster)

        return clusters

    def _chunks_similar(self, chunk1: Dict[str, Any],
                       chunk2: Dict[str, Any]) -> bool:
        """Check if two chunks are similar enough to cluster."""
        # File-based clustering
        if chunk1.get('file_path') == chunk2.get('file_path'):
            return True

        # Content-based clustering
        text1 = set(chunk1.get('text', '').lower().split())
        text2 = set(chunk2.get('text', '').lower().split())

        if len(text1) == 0 or len(text2) == 0:
            return False

        overlap = len(text1 & text2)
        smaller = min(len(text1), len(text2))

        return (overlap / smaller) > 0.5

    def _select_diverse_chunks(self, chunks: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """Select top k chunks with diversity."""
        if len(chunks) <= k:
            return chunks

        selected = []
        remaining = chunks.copy()

        # Select best chunk
        if remaining:
            selected.append(remaining.pop(0))

        # Select diverse chunks
        while len(selected) < k and remaining:
            # Find chunk most different from selected
            best_chunk = None
            best_diversity = -1

            for chunk in remaining:
                diversity = self._calculate_diversity(chunk, selected)
                if diversity > best_diversity:
                    best_diversity = diversity
                    best_chunk = chunk

            if best_chunk:
                selected.append(best_chunk)
                remaining.remove(best_chunk)

        return selected

    def _calculate_diversity(self, chunk: Dict[str, Any],
                           selected: List[Dict[str, Any]]) -> float:
        """Calculate diversity of chunk compared to selected chunks."""
        if not selected:
            return 1.0

        chunk_text = set(chunk.get('text', '').lower().split())

        min_similarity = 1.0
        for sel_chunk in selected:
            sel_text = set(sel_chunk.get('text', '').lower().split())

            if len(chunk_text) == 0 or len(sel_text) == 0:
                similarity = 0
            else:
                overlap = len(chunk_text & sel_text)
                union = len(chunk_text | sel_text)
                similarity = overlap / union if union > 0 else 0

            min_similarity = min(min_similarity, similarity)

        return 1.0 - min_similarity

    def _generate_cache_key(self, repo_id: str, query: str,
                          strategy: SearchStrategy) -> str:
        """Generate cache key for search results."""
        content = f"{repo_id}:{query}:{strategy.value}"
        return hashlib.md5(content.encode()).hexdigest()

    def close(self):
        """Clean up resources."""
        self._executor.shutdown(wait=True)

# --- Graph Analysis ---
def _build_knowledge_graph(graph_data: Dict[str, Any]) -> nx.DiGraph:
    """Builds a networkx DiGraph from the architectural graph data."""
    if not isinstance(graph_data, dict):
        raise GraphBuildingError(f"Graph data must be a dictionary, got {type(graph_data)}")

    try:
        G = nx.DiGraph()
        nodes = graph_data.get("nodes", {})
        edges = graph_data.get("edges", [])

        if not isinstance(nodes, dict):
            logger.warning(f"Nodes data is not a dictionary, got {type(nodes)}. Using empty dict.")
            nodes = {}

        if not isinstance(edges, list):
            logger.warning(f"Edges data is not a list, got {type(edges)}. Using empty list.")
            edges = []

        # Add nodes with validation
        nodes_added = 0
        for node_id, node_data in nodes.items():
            try:
                if not isinstance(node_id, str):
                    logger.warning(f"Skipping node with invalid ID type: {type(node_id)}")
                    continue

                if not isinstance(node_data, dict):
                    logger.warning(f"Node {node_id} has invalid data type: {type(node_data)}. Using empty dict.")
                    node_data = {}

                G.add_node(node_id, **node_data)
                nodes_added += 1
            except Exception as e:
                logger.warning(f"Failed to add node {node_id}: {e}")
                continue

        # Add edges with validation
        edges_added = 0
        for edge in edges:
            try:
                if not isinstance(edge, dict):
                    logger.warning(f"Skipping invalid edge: {edge}")
                    continue

                source = edge.get("source")
                target = edge.get("target")

                if not source or not target:
                    logger.warning(f"Edge missing source or target: {edge}")
                    continue

                if not isinstance(source, str) or not isinstance(target, str):
                    logger.warning(f"Edge source/target must be strings: {edge}")
                    continue

                # Only add edge if source exists in graph
                if source in G:
                    edge_type = edge.get("type", "unknown")
                    G.add_edge(source, target, type=edge_type)
                    edges_added += 1
                else:
                    logger.warning(f"Source node {source} not found in graph for edge to {target}")
            except Exception as e:
                logger.warning(f"Failed to add edge {edge}: {e}")
                continue

        logger.info(f"Built knowledge graph with {nodes_added} nodes and {edges_added} edges.")

        if nodes_added == 0:
            raise GraphBuildingError("No valid nodes were added to the graph")

        return G

    except Exception as e:
        if isinstance(e, GraphBuildingError):
            raise
        raise GraphBuildingError(f"Failed to build knowledge graph: {e}")

# --- Context Building ---
class ContextBuilder:
    """Builds context for LLM synthesis from search results."""

    def __init__(self, config: RAGConfig):
        self.config = config

    def build(self, search_results: Dict[str, Any],
              question: str,
              is_specific: bool = False) -> str:
        """Build context string from search results."""
        chunks = search_results.get('chunks', [])

        if not chunks:
            return "No relevant code context was found for your question."

        # Group chunks by file for better organization
        chunks_by_file = defaultdict(list)
        for chunk in chunks:
            file_path = chunk.get('file_path', 'unknown')
            chunks_by_file[file_path].append(chunk)

        # Build context with structure
        context_parts = []

        # Add metadata if available
        if 'metadata' in search_results:
            meta = search_results['metadata']
            context_parts.append(f"[Search performed using {meta.get('strategy', 'unknown')} strategy, found {meta.get('total_results', 0)} results in {meta.get('search_time', 0):.2f}s]")

        # Add clustered results if available
        if 'clusters' in search_results:
            context_parts.append(self._format_clusters(search_results['clusters']))
        else:
            # Format chunks by file
            for file_path, file_chunks in chunks_by_file.items():
                context_parts.append(self._format_file_chunks(file_path, file_chunks))

        # Add hop information if multi-hop search
        if 'hop_results' in search_results:
            context_parts.append(self._format_hop_results(search_results['hop_results']))

        return "\n\n".join(context_parts)

    def _format_file_chunks(self, file_path: str, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks from a single file."""
        parts = [f"=== File: {file_path} ==="]

        for i, chunk in enumerate(chunks[:5]):  # Limit chunks per file
            text = chunk.get('text', '').strip()
            if text:
                parts.append(f"\n[Chunk {i+1}]")
                if 'final_score' in chunk:
                    parts.append(f"Relevance: {chunk['final_score']:.2f}")
                parts.append(text)

        return "\n".join(parts)

    def _format_clusters(self, clusters: List[Dict[str, Any]]) -> str:
        """Format clustered results."""
        parts = ["=== Clustered Results ==="]

        for cluster in clusters[:5]:  # Limit clusters
            parts.append(f"\n[Cluster: {cluster['id']} - {cluster['size']} related chunks]")

            # Show representative chunk
            rep = cluster['representative']
            parts.append(f"File: {rep.get('file_path', 'unknown')}")
            parts.append(rep.get('text', '')[:500])  # Truncate

            if cluster['size'] > 1:
                parts.append(f"... and {cluster['size'] - 1} similar chunks")

        return "\n".join(parts)

    def _format_hop_results(self, hop_results: List[Dict[str, Any]]) -> str:
        """Format multi-hop search results."""
        parts = ["=== Multi-hop Search Path ==="]

        for hop in hop_results:
            parts.append(f"\nHop {hop['hop'] + 1}:")
            parts.append(f"Queries: {', '.join(hop['queries'][:3])}")
            parts.append(f"Found: {hop['chunks_found']} results")

        return "\n".join(parts)

# --- Main Orchestration ---
class OracleService:
    """Enhanced Oracle service with PhD-level RAG capabilities."""

    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or rag_config
        self.search_orchestrator = SearchOrchestrator(self.config)
        self.context_builder = ContextBuilder(self.config)
        self.entity_extractor = EntityExtractor(self.config)

    def answer_question(self, repo_id: str, question: str,
                       use_enhanced_rag: bool = True) -> Dict[str, Any]:
        """
        Main entry point for answering questions with enhanced RAG.
        Maintains backward compatibility while adding advanced features.
        """
        # Input validation
        if not repo_id or not isinstance(repo_id, str):
            return {"error": "Invalid repository ID provided"}

        if not question or not isinstance(question, str):
            return {"error": "Invalid question provided"}

        logger.info(f"The Oracle received a question for repo '{repo_id}': '{question}'")

        # Load repository data
        try:
            cortex_data = cortex_service.load_cortex_data(repo_id)
            if not isinstance(cortex_data, dict):
                return {"error": "Invalid cortex data format"}

            graph_data = cortex_data.get("architectural_graph")
            if not graph_data:
                return {"error": "Architectural graph not found for this repository."}

        except cortex_service.CortexFileNotFound:
            return {"error": f"Cortex file for repo '{repo_id}' not found. Please ingest the repo first."}
        except cortex_service.CortexFileMalformed:
            return {"error": f"Cortex file for repo '{repo_id}' is corrupted. Please re-ingest the repo."}
        except Exception as e:
            logger.error(f"Error loading cortex data for repo {repo_id}: {e}")
            return {"error": "Failed to load repository data. Please try again later."}

        # Build graph
        try:
            graph = _build_knowledge_graph(graph_data)
        except GraphBuildingError as e:
            logger.error(f"Graph building failed: {e}")
            return {"error": "Failed to analyze repository structure. The repository data may be corrupted."}
        except Exception as e:
            logger.error(f"Unexpected error building graph: {e}")
            return {"error": "An unexpected error occurred while analyzing the repository structure."}

        # Extract entities
        try:
            entities_dict = self.entity_extractor.extract(question)
            # Convert to old format for compatibility
            entities = self._convert_entities_format(entities_dict)
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            entities = self._get_default_entities()

        # Determine search strategy
        strategy = self._determine_strategy(question, entities, use_enhanced_rag)

        # Perform search
        try:
            search_results = self.search_orchestrator.search(
                repo_id, question, strategy, graph
            )
        except Exception as e:
            logger.error(f"Search failed: {e}")
            search_results = {'chunks': [], 'error': str(e)}

        # Build context
        context_string = self.context_builder.build(
            search_results, question,
            is_specific=self._is_specific_question(entities)
        )

        # Determine prompt heading
        if self._is_specific_question(entities):
            prompt_heading = "ARCHITECTURAL IMPACT ANALYSIS"
        else:
            prompt_heading = "RELEVANT CODE & COMMENTARY"

        # Generate final answer
        try:
            answer = self._synthesize_answer(
                question, context_string, prompt_heading,
                entities, search_results
            )

            response = {"answer": answer}

            # Add metadata if enhanced RAG was used
            if use_enhanced_rag and 'metadata' in search_results:
                response["search_metadata"] = search_results['metadata']

            return response

        except Exception as e:
            logger.error(f"Final synthesis failed: {e}")
            return {"error": "An unexpected error occurred while generating the final answer."}

    def _determine_strategy(self, question: str, entities: Dict[str, List[str]],
                          use_enhanced: bool) -> SearchStrategy:
        """Determine the best search strategy for the question."""
        if not use_enhanced:
            return SearchStrategy.SIMPLE

        # Check question complexity
        question_lower = question.lower()

        # Multi-hop indicators
        if any(word in question_lower for word in ["how does", "workflow", "process", "trace"]):
            return SearchStrategy.MULTI_HOP

        # Graph-guided indicators
        if any(word in question_lower for word in ["impact", "depends", "affects", "uses"]):
            return SearchStrategy.GRAPH_GUIDED

        # Complex questions need hybrid approach
        if len(question.split()) > 15 or len(entities.get('general_terms', [])) > 3:
            return SearchStrategy.HYBRID

        # Default to expanded search
        return SearchStrategy.EXPANDED

    def _convert_entities_format(self, entities_dict: Dict[str, List[Entity]]) -> Dict[str, List[str]]:
        """Convert new entity format to old format for compatibility."""
        old_format = {}
        for entity_type, entity_list in entities_dict.items():
            old_format[entity_type] = [e.text for e in entity_list]
        return old_format

    def _get_default_entities(self) -> Dict[str, List[str]]:
        """Returns default empty entity structure."""
        return {
            "functions": [],
            "classes": [],
            "files": [],
            "general_terms": [],
            "code_patterns": [],
            "technologies": []
        }

    def _is_specific_question(self, entities: Dict[str, List[str]]) -> bool:
        """Check if question is about specific code entities."""
        return any(entities.get(k) for k in ["functions", "classes", "files"])

    def _synthesize_answer(self, question: str, context: str,
                         prompt_heading: str, entities: Dict[str, List[str]],
                         search_results: Dict[str, Any]) -> str:
        """Synthesize final answer using LLM."""
        # Self-reflection prompt if enabled
        reflection = ""
        if self.config.enable_self_reflection and search_results.get('error'):
            reflection = f"""
**Search Challenges:**
The search encountered some difficulties: {search_results.get('error')}
This may affect the completeness of the answer.
"""

        synthesis_prompt = f"""You are The Oracle, an advanced AI system that understands code architecture at a PhD level.
Your purpose is to provide deep, insightful answers by combining architectural graph analysis with comprehensive source code understanding.

**DEVELOPER'S QUESTION:**
{question}

**{prompt_heading}:**
{context}

**EXTRACTED ENTITIES:**
- Functions: {entities.get('functions', [])}
- Classes: {entities.get('classes', [])}
- Files: {entities.get('files', [])}
- Technologies: {entities.get('technologies', [])}
- Patterns: {entities.get('code_patterns', [])}

{reflection}

---
Based on ALL the evidence above, provide a comprehensive answer that:
1. **Directly answers** the developer's question with specific details
2. **Cites evidence** from the code snippets to support your answer
3. **Explains architectural implications** and design patterns observed
4. **Identifies potential issues** or areas for improvement
5. **Suggests next steps** for deeper investigation if needed

Use clear markdown formatting with headers, code blocks, and lists where appropriate.
Be specific and technical, but also explain the "why" behind the code structure.
"""

        answer = llm_service.generate_text(synthesis_prompt, task_type=TaskType.COMPLEX_REASONING)
        return answer

    def close(self):
        """Clean up resources."""
        self.search_orchestrator.close()

# --- Global Oracle Instance ---
_oracle_instance = None

def get_oracle() -> OracleService:
    """Get or create the global Oracle instance."""
    global _oracle_instance
    if _oracle_instance is None:
        _oracle_instance = OracleService()
    return _oracle_instance

# --- Public API (Backward Compatible) ---
def answer_question(repo_id: str, question: str, use_enhanced_rag: bool = True) -> Dict[str, Any]:
    """
    Public API for answering questions. Maintains backward compatibility.
    """
    oracle = get_oracle()
    return oracle.answer_question(repo_id, question, use_enhanced_rag)

def answer_question_legacy(repo_id: str, question: str) -> Dict[str, Any]:
    """
    Legacy wrapper that uses the original RAG behavior.
    For backward compatibility with existing code.
    """
    return answer_question(repo_id, question, use_enhanced_rag=False)

def perform_semantic_search(repo_id: str, query: str, search_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Public interface for performing semantic search with enhanced RAG capabilities.
    """
    oracle = get_oracle()
    config = search_config or {}

    # Determine strategy from config
    strategy = SearchStrategy.EXPANDED
    if config.get('use_multi_hop'):
        strategy = SearchStrategy.MULTI_HOP
    elif config.get('use_graph'):
        strategy = SearchStrategy.GRAPH_GUIDED

    # Perform search
    results = oracle.search_orchestrator.search(repo_id, query, strategy)

    # Format for API
    return {
        "results": results.get('chunks', [])[:config.get('k', 15)],
        "total": len(results.get('chunks', [])),
        "query": query,
        "metadata": results.get('metadata', {})
    }

def analyze_code_impact(repo_id: str, file_path: str, function_or_class: str) -> Dict[str, Any]:
    """
    Analyzes the impact of changes to a specific code entity.
    """
    question = f"What is the impact of changing {function_or_class} in {file_path}?"
    oracle = get_oracle()

    # Use graph-guided search for impact analysis
    result = oracle.answer_question(repo_id, question, use_enhanced_rag=True)

    # Extract specific impact information
    return {
        "entity": function_or_class,
        "file": file_path,
        "analysis": result.get("answer", ""),
        "metadata": result.get("search_metadata", {})
    }

# --- Configuration Management ---
def update_config(new_config: Dict[str, Any]) -> None:
    """Update RAG configuration."""
    global rag_config
    for key, value in new_config.items():
        if hasattr(rag_config, key):
            setattr(rag_config, key, value)

    # Clear cache on config change
    search_cache.clear()

def get_config() -> RAGConfig:
    """Get current RAG configuration."""
    return rag_config

# --- Cleanup ---
def cleanup():
    """Clean up resources on shutdown."""
    global _oracle_instance
    if _oracle_instance:
        _oracle_instance.close()
        _oracle_instance = None

# Register cleanup
import atexit
atexit.register(cleanup)

--- FILE_END: backend/lumiere_core/services/oracle_service.py ---

--- FILE_START: backend/lumiere_core/services/ollama_service.py ---
# In backend/lumiere_core/services/ollama_service.py

import ollama
import requests 
from typing import Dict, List

def generate_text(prompt: str, model_name: str = 'qwen3:4b') -> str:
    """
    Sends a prompt to a local Ollama model and returns the response.

    Args:
        prompt: The full prompt to send to the model.
        model_name: The name of the Ollama model to use for generation.

    Returns:
        The generated text content from the model.
    """
    print(f"Sending prompt to local Ollama model: '{model_name}'...")
    try:
        client = ollama.Client()
        response = client.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return (f"An error occurred while communicating with the Ollama server: {e}\n"
                f"Please ensure the Ollama server is running and the model '{model_name}' is available.")

# === REPLACE THE ENTIRE list_models FUNCTION WITH THIS NEW VERSION ===
def list_models() -> List[Dict[str, str]]:
    """
    Fetches the list of locally available Ollama models by calling the API directly.
    This is more robust than relying on the library's internal list() parsing.
    """
    print("Fetching available local Ollama models via direct API call...")
    try:
        # Use requests to call the /api/tags endpoint directly
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        data = response.json()

        available = []
        # The structure from the API is {"models": [...]}, so we access the 'models' key
        for model_data in data.get('models', []):
            # The model's full name (e.g., "qwen3:4b") is in the 'name' key
            full_name = model_data.get('name')
            if not full_name:
                continue  # Skip if a model entry is malformed

            model_id = f"ollama/{full_name}"
            available.append({
                "id": model_id,
                "provider": "ollama",
                "name": full_name,  # Use the full name for display as well
            })

        if not available:
            print("Ollama API responded, but no local models were found.")
        else:
            print(f"✓ Found {len(available)} local Ollama models.")

        return available

    except requests.exceptions.ConnectionError:
        print("Could not fetch Ollama models. Is the Ollama server running?")
        return []
    except requests.exceptions.RequestException as e:
        print(f"An error occurred while calling the Ollama API: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/ollama_service.py ---

--- FILE_START: backend/lumiere_core/services/graph_differ.py ---
# backend/lumiere_core/services/graph_differ.py

import networkx as nx
from typing import Dict, List, Tuple, Iterable


def compare_graphs(base_graph: nx.DiGraph, head_graph: nx.DiGraph) -> Dict[str, List[str]]:
    """
    Compares two architectural graphs (base vs. head) and identifies changes.

    Args:
        base_graph (nx.DiGraph): Graph from the base branch.
        head_graph (nx.DiGraph): Graph from the head branch (e.g. a PR).

    Returns:
        Dict[str, List[str]]: A dictionary with:
            - 'nodes_added': New nodes in the head graph.
            - 'nodes_removed': Nodes missing from the head graph.
            - 'edges_added': New edges (formatted as 'A -> B').
            - 'edges_removed': Missing edges (formatted as 'A -> B').
    """
    def format_edges(edges: Iterable[Tuple[str, str]]) -> List[str]:
        return [f"{u} -> {v}" for u, v in sorted(edges)]

    base_nodes = set(base_graph.nodes)
    head_nodes = set(head_graph.nodes)

    base_edges = set(base_graph.edges)
    head_edges = set(head_graph.edges)

    return {
        "nodes_added": sorted(head_nodes - base_nodes),
        "nodes_removed": sorted(base_nodes - head_nodes),
        "edges_added": format_edges(head_edges - base_edges),
        "edges_removed": format_edges(base_edges - head_edges),
    }

--- FILE_END: backend/lumiere_core/services/graph_differ.py ---

--- FILE_START: backend/lumiere_core/services/onboarding_service.py ---
"""
Onboarding Service

This service orchestrates the generation of personalized onboarding paths for GitHub issues.
It combines impact analysis, expertise detection, and architectural understanding to create
comprehensive guides for new developers. This is the capstone feature of the Onboarding Concierge.
"""

import json
import pathlib
from typing import Dict, List, Any, Optional
import logging

from . import impact_analyzer
from . import expertise_service
from . import oracle_service
from . import ollama
from . import llm_service
from .llm_service import TaskType

logger = logging.getLogger(__name__)


class OnboardingService:
    """
    Service that orchestrates the generation of personalized onboarding paths
    by combining multiple analysis services into a comprehensive guide.
    """
    
    def __init__(self, cortex_directory: Optional[pathlib.Path] = None):
        """
        Initialize the Onboarding Service.
        
        Args:
            cortex_directory: Optional path to the cortex directory.
                            If None, will use default backend/cloned_repositories/
        """
        self.cortex_directory = cortex_directory or pathlib.Path("backend/cloned_repositories")
        self.impact_analyzer = impact_analyzer.ImpactAnalyzer(cortex_directory)
        self.expertise_service = expertise_service.ExpertiseService(cortex_directory)
    
    def generate_onboarding_path(self, repo_id: str, issue_number: int) -> Dict[str, Any]:
        """
        Generate a complete, step-by-step onboarding guide for a given GitHub issue.
        
        Args:
            repo_id: Repository identifier
            issue_number: GitHub issue number
            
        Returns:
            Comprehensive onboarding guide with learning path and expert contacts
        """
        try:
            logger.info(f"Generating onboarding path for issue #{issue_number} in {repo_id}")
            
            # Step 1: Identify Locus of Change using mock issue data
            # In a real implementation, this would fetch from GitHub API
            issue_data = self._get_issue_data(repo_id, issue_number)
            
            locus_files = self._identify_locus_of_change(
                repo_id, 
                issue_data.get('title', ''), 
                issue_data.get('description', '')
            )
            
            if not locus_files:
                return {
                    'error': 'Could not identify relevant files for this issue',
                    'issue_number': issue_number,
                    'repo_id': repo_id
                }
            
            # Step 2: Build Epistemic Subgraph & Learning Path
            learning_path = self._build_learning_path(repo_id, locus_files)
            
            # Step 3: Enrich Each Step with Oracle summaries and Expert contacts
            enriched_steps = []
            
            for step in learning_path:
                file_path = step['file_path']
                
                # Get file summary from Oracle
                file_summary = self._get_file_summary(repo_id, file_path)
                
                # Find experts for this file
                experts = self.expertise_service.find_experts_for_file(repo_id, file_path)
                top_expert = experts[0] if experts else None
                
                enriched_step = {
                    'file_path': file_path,
                    'step_number': step['step_number'],
                    'dependency_level': step['dependency_level'],
                    'summary': file_summary,
                    'top_expert': top_expert,
                    'learning_objective': self._generate_learning_objective(file_path, file_summary)
                }
                
                enriched_steps.append(enriched_step)
            
            # Step 4: Synthesize Final Report
            final_report = self._synthesize_final_report(
                repo_id, 
                issue_number, 
                issue_data, 
                enriched_steps,
                locus_files
            )
            
            return {
                'repository': repo_id,
                'issue_number': issue_number,
                'issue_title': issue_data.get('title', 'Unknown Issue'),
                'locus_files': locus_files,
                'learning_path_steps': len(enriched_steps),
                'onboarding_guide': final_report,
                'enriched_steps': enriched_steps,
                'generation_successful': True
            }
            
        except Exception as e:
            logger.error(f"Error generating onboarding path for issue #{issue_number}: {e}")
            return {
                'repository': repo_id,
                'issue_number': issue_number,
                'error': str(e),
                'generation_successful': False
            }
    
    def _get_issue_data(self, repo_id: str, issue_number: int) -> Dict[str, str]:
        """
        Get issue data. In the future, this would fetch from GitHub API.
        For now, return mock data.
        """
        # TODO: Integrate with GitHub service to fetch real issue data
        return {
            'title': f'Sample Issue #{issue_number}',
            'description': f'This is a sample issue description for issue #{issue_number} in repository {repo_id}. It involves fixing a bug or implementing a feature.'
        }
    
    def _identify_locus_of_change(self, repo_id: str, title: str, description: str) -> List[str]:
        """
        Identify the core files related to an issue using RAG search.
        
        Args:
            repo_id: Repository identifier
            title: Issue title
            description: Issue description
            
        Returns:
            List of file paths that are likely to be affected
        """
        try:
            # Combine title and description for search
            issue_text = f"{title}\n{description}"
            
            # Use RAG search to find relevant code chunks
            search_results = ollama.search_index(
                query_text=issue_text,
                model_name="snowflake-arctic-embed",
                repo_id=repo_id,
                k=5
            )
            
            # Extract unique file paths from search results
            locus_files = []
            for result in search_results:
                chunk_id = result.get('chunk_id', '')
                if chunk_id:
                    # Extract file path from chunk_id (format: repo_id_filepath_chunk_index)
                    parts = chunk_id.replace(f"{repo_id}_", "", 1).rsplit('_', 1)
                    if parts:
                        file_path = parts[0]
                        if file_path not in locus_files:
                            locus_files.append(file_path)
            
            return locus_files[:3]  # Return top 3 relevant files
            
        except Exception as e:
            logger.warning(f"RAG search failed for locus identification: {e}")
            return []
    
    def _build_learning_path(self, repo_id: str, locus_files: List[str]) -> List[Dict[str, Any]]:
        """
        Build a learning path by analyzing dependencies and creating a topological sort.
        
        Args:
            repo_id: Repository identifier
            locus_files: Core files identified for the issue
            
        Returns:
            Ordered list of files to learn, from least-dependent to most-dependent
        """
        try:
            # Load architectural graph
            architectural_graph = self._load_architectural_graph(repo_id)
            
            if not architectural_graph:
                # Fallback: simple ordering based on file paths
                return self._create_simple_learning_path(locus_files)
            
            # Create epistemic subgraph containing locus files and neighbors
            subgraph_files = self._create_epistemic_subgraph(architectural_graph, locus_files)
            
            # Perform topological sort to determine learning order
            learning_order = self._topological_sort(architectural_graph, subgraph_files)
            
            # Create learning path with metadata
            learning_path = []
            for i, file_path in enumerate(learning_order):
                step = {
                    'step_number': i + 1,
                    'file_path': file_path,
                    'dependency_level': self._calculate_dependency_level(architectural_graph, file_path),
                    'is_locus_file': file_path in locus_files
                }
                learning_path.append(step)
            
            return learning_path
            
        except Exception as e:
            logger.warning(f"Failed to build dependency-based learning path: {e}")
            return self._create_simple_learning_path(locus_files)
    
    def _load_architectural_graph(self, repo_id: str) -> Optional[Dict[str, Any]]:
        """Load architectural graph from cortex file."""
        cortex_file = self.cortex_directory / repo_id / "cortex.json"
        
        if not cortex_file.exists():
            return None
            
        try:
            with open(cortex_file, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)
            return cortex_data.get('architectural_graph')
        except Exception as e:
            logger.error(f"Error loading architectural graph: {e}")
            return None
    
    def _create_epistemic_subgraph(self, graph: Dict[str, Any], locus_files: List[str]) -> List[str]:
        """
        Create a subgraph containing locus files and their immediate neighbors.
        
        Args:
            graph: Full architectural graph
            locus_files: Core files for the issue
            
        Returns:
            List of files in the epistemic subgraph
        """
        subgraph_files = set(locus_files)
        
        # Extract edges from graph
        edges = graph.get('edges', [])
        
        # Add immediate neighbors (1-hop away)
        for edge in edges:
            source = self._extract_file_from_edge(edge, 'source')
            target = self._extract_file_from_edge(edge, 'target')
            
            if source in locus_files:
                subgraph_files.add(target)
            if target in locus_files:
                subgraph_files.add(source)
        
        return list(subgraph_files)
    
    def _extract_file_from_edge(self, edge: Any, direction: str) -> Optional[str]:
        """Extract file path from graph edge."""
        if isinstance(edge, dict):
            node_ref = edge.get(direction)
            if isinstance(node_ref, str):
                return node_ref
            elif isinstance(node_ref, dict):
                return node_ref.get('id') or node_ref.get('file_path')
        return None
    
    def _topological_sort(self, graph: Dict[str, Any], files: List[str]) -> List[str]:
        """
        Perform topological sort on the subgraph to determine learning order.
        
        Args:
            graph: Architectural graph
            files: Files to sort
            
        Returns:
            Files sorted from least-dependent to most-dependent
        """
        # Build adjacency list for the subgraph
        dependencies = {f: [] for f in files}
        
        edges = graph.get('edges', [])
        for edge in edges:
            source = self._extract_file_from_edge(edge, 'source')
            target = self._extract_file_from_edge(edge, 'target')
            
            if source in files and target in files:
                dependencies[source].append(target)
        
        # Kahn's algorithm for topological sorting
        in_degree = {f: 0 for f in files}
        for f in files:
            for dep in dependencies[f]:
                in_degree[dep] += 1
        
        queue = [f for f in files if in_degree[f] == 0]
        result = []
        
        while queue:
            current = queue.pop(0)
            result.append(current)
            
            for neighbor in dependencies[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        return result
    
    def _create_simple_learning_path(self, locus_files: List[str]) -> List[Dict[str, Any]]:
        """Fallback simple learning path when graph analysis fails."""
        learning_path = []
        for i, file_path in enumerate(locus_files):
            step = {
                'step_number': i + 1,
                'file_path': file_path,
                'dependency_level': 'unknown',
                'is_locus_file': True
            }
            learning_path.append(step)
        return learning_path
    
    def _calculate_dependency_level(self, graph: Dict[str, Any], file_path: str) -> str:
        """Calculate dependency level (low, medium, high) for a file."""
        edges = graph.get('edges', [])
        
        # Count incoming dependencies
        incoming = sum(1 for edge in edges if self._extract_file_from_edge(edge, 'target') == file_path)
        
        if incoming == 0:
            return 'low'
        elif incoming <= 3:
            return 'medium'
        else:
            return 'high'
    
    def _get_file_summary(self, repo_id: str, file_path: str) -> str:
        """Get a summary of the file's purpose using Oracle service."""
        try:
            question = f"Provide a concise, one-paragraph summary of the purpose of the file '{file_path}' for a new developer."
            
            response = oracle_service.answer_question(
                repo_id=repo_id,
                question=question,
                context_limit=3  # Limit context for focused summary
            )
            
            # Extract just the answer portion
            if isinstance(response, dict) and 'answer' in response:
                return response['answer']
            elif isinstance(response, str):
                return response
            else:
                return f"File: {file_path} - Purpose unknown (Oracle query failed)"
                
        except Exception as e:
            logger.warning(f"Failed to get Oracle summary for {file_path}: {e}")
            return f"File: {file_path} - Summary not available"
    
    def _generate_learning_objective(self, file_path: str, summary: str) -> str:
        """Generate a learning objective for a file."""
        # Extract key concepts from the summary for the learning objective
        if "service" in summary.lower():
            return f"Understand how the {file_path.split('/')[-1]} service works and its role in the system"
        elif "model" in summary.lower():
            return f"Learn the data structure and relationships defined in {file_path.split('/')[-1]}"
        elif "controller" in summary.lower() or "view" in summary.lower():
            return f"Understand the request handling and user interface logic in {file_path.split('/')[-1]}"
        elif "util" in summary.lower() or "helper" in summary.lower():
            return f"Learn the utility functions and helper methods in {file_path.split('/')[-1]}"
        else:
            return f"Understand the purpose and functionality of {file_path.split('/')[-1]}"
    
    def _synthesize_final_report(
        self, 
        repo_id: str, 
        issue_number: int, 
        issue_data: Dict[str, str], 
        enriched_steps: List[Dict[str, Any]],
        locus_files: List[str]
    ) -> str:
        """
        Synthesize the final onboarding report using LLM.
        
        Args:
            repo_id: Repository identifier
            issue_number: Issue number
            issue_data: Issue title and description
            enriched_steps: Learning path with summaries and experts
            locus_files: Core files for the issue
            
        Returns:
            Formatted Markdown report
        """
        try:
            # Prepare context for LLM
            steps_context = ""
            for step in enriched_steps:
                expert_info = ""
                if step.get('top_expert'):
                    expert = step['top_expert']
                    expert_info = f" (Expert: {expert['author']} - {expert['email']})"
                
                steps_context += f"""
**Step {step['step_number']}: {step['file_path']}**
- Learning Objective: {step['learning_objective']}
- Dependency Level: {step['dependency_level']}
- Summary: {step['summary'][:200]}...{expert_info}

"""
            
            # Create comprehensive prompt for LLM
            prompt = f"""You are an expert technical mentor creating a personalized onboarding guide for a new developer. 

Generate a friendly, encouraging, and well-structured Markdown report to help a newcomer understand how to approach this GitHub issue.

**Context:**
- Repository: {repo_id}
- Issue #{issue_number}: {issue_data.get('title', 'Unknown')}
- Issue Description: {issue_data.get('description', 'No description available')}
- Core files to focus on: {', '.join(locus_files)}

**Learning Path (ordered from foundational to advanced):**
{steps_context}

**Instructions:**
1. Create a warm, welcoming introduction explaining the issue and what they'll learn
2. Present the learning path as a clear, step-by-step guide
3. For each step, explain WHY they should understand this file before moving to the next
4. Include the expert contact information naturally in each step
5. Add encouragement and tips for new developers
6. End with next steps and how to get help

Format the response as clean Markdown with proper headers, bullet points, and emphasis.
Make it feel like a personal mentorship session, not a dry technical document.
"""
            
            # Generate the final report
            final_report = llm_service.generate_text(
                prompt,
                task_type=TaskType.COMPLEX_REASONING
            )
            
            return final_report
            
        except Exception as e:
            logger.error(f"Failed to synthesize final report: {e}")
            # Fallback simple report
            return self._create_fallback_report(repo_id, issue_number, issue_data, enriched_steps)
    
    def _create_fallback_report(self, repo_id: str, issue_number: int, issue_data: Dict[str, str], enriched_steps: List[Dict[str, Any]]) -> str:
        """Create a simple fallback report when LLM synthesis fails."""
        report = f"""# Onboarding Guide: Issue #{issue_number}

## Issue Overview
**Repository:** {repo_id}
**Issue:** {issue_data.get('title', 'Unknown Issue')}

{issue_data.get('description', 'No description available')}

## Learning Path

"""
        
        for step in enriched_steps:
            expert_info = ""
            if step.get('top_expert'):
                expert = step['top_expert']
                expert_info = f"\n**Expert Contact:** {expert['author']} ({expert['email']})"
            
            report += f"""### Step {step['step_number']}: {step['file_path']}

**Learning Objective:** {step['learning_objective']}

**Summary:** {step['summary'][:300]}...{expert_info}

---

"""
        
        report += """
## Next Steps
1. Review each file in the order presented above
2. Reach out to the experts if you have questions
3. Start with small changes and test frequently

Good luck with your contribution!
"""
        
        return report


# Convenience functions for external use
def generate_onboarding_path(repo_id: str, issue_number: int) -> Dict[str, Any]:
    """
    Convenience function to generate an onboarding path.
    
    Args:
        repo_id: Repository identifier
        issue_number: GitHub issue number
        
    Returns:
        Comprehensive onboarding guide
    """
    service = OnboardingService()
    return service.generate_onboarding_path(repo_id, issue_number)
--- FILE_END: backend/lumiere_core/services/onboarding_service.py ---

--- FILE_START: backend/lumiere_core/services/mage_service.py ---
# backend/lumiere_core/services/mage_service.py

import logging
import re
import ast
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path

from . import llm_service, cortex_service, code_surgery
from .llm_service import TaskType

logger = logging.getLogger(__name__)


class MageService:
    """The Mage - master of code transmutation and intelligent transformations."""
    
    def __init__(self):
        self.available_spells = {
            "translate_contract": self._spell_translate_contract,
            "transmute_implementation": self._spell_transmute_implementation,
            "refactor_pattern": self._spell_refactor_pattern,
            "modernize_syntax": self._spell_modernize_syntax,
            "add_type_hints": self._spell_add_type_hints,
            "extract_method": self._spell_extract_method,
            "inline_method": self._spell_inline_method,
            "convert_to_async": self._spell_convert_to_async
        }
    
    def list_spells(self) -> Dict[str, str]:
        """Return available spells and their descriptions."""
        return {
            "translate_contract": "Translate class/interface contracts between languages",
            "transmute_implementation": "Transform implementation style (e.g., procedural to functional)",
            "refactor_pattern": "Apply design patterns or refactor existing patterns",
            "modernize_syntax": "Update code to use modern language features",
            "add_type_hints": "Add type annotations to Python code",
            "extract_method": "Extract code into a separate method",
            "inline_method": "Inline a method's implementation",
            "convert_to_async": "Convert synchronous code to asynchronous"
        }
    
    def cast_spell(self, repo_id: str, spell_name: str, file_path: str, 
                   target_identifier: str, **kwargs) -> Dict[str, Any]:
        """
        Cast a spell to transform code.
        
        Args:
            repo_id: Repository identifier
            spell_name: Name of the spell to cast
            file_path: Path to the file containing the code
            target_identifier: Function/class/method name to transform
            **kwargs: Additional spell-specific parameters
            
        Returns:
            Dictionary with transformation result
        """
        try:
            if spell_name not in self.available_spells:
                return {"error": f"Unknown spell: {spell_name}. Available spells: {list(self.available_spells.keys())}"}
            
            # Get the current code
            file_content = cortex_service.get_file_content(repo_id, file_path)
            if not file_content:
                return {"error": f"Could not retrieve content for file: {file_path}"}
            
            # Find the target code block
            target_code, start_line, end_line = self._find_code_block(file_content, target_identifier)
            if not target_code:
                return {"error": f"Could not find '{target_identifier}' in {file_path}"}
            
            # Cast the spell
            spell_func = self.available_spells[spell_name]
            result = spell_func(target_code, target_identifier, file_path, **kwargs)
            
            if "error" in result:
                return result
            
            # Add metadata
            result.update({
                "original_code": target_code,
                "file_path": file_path,
                "target_identifier": target_identifier,
                "spell_name": spell_name,
                "start_line": start_line,
                "end_line": end_line
            })
            
            return result
            
        except Exception as e:
            logger.error(f"Error casting spell {spell_name}: {e}")
            return {"error": str(e)}
    
    def apply_transformation(self, repo_id: str, transformation_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Apply a transformation result to the actual file using code surgery.
        
        Args:
            repo_id: Repository identifier
            transformation_result: Result from cast_spell()
            
        Returns:
            Dictionary with application result
        """
        try:
            if "transformed_code" not in transformation_result:
                return {"error": "No transformed code to apply"}
            
            file_path = transformation_result["file_path"]
            target_identifier = transformation_result["target_identifier"]
            new_code = transformation_result["transformed_code"]
            
            # Use code surgery to apply the transformation
            surgery_result = code_surgery.replace_block(repo_id, file_path, target_identifier, new_code)
            
            if surgery_result.get("success"):
                return {
                    "success": True,
                    "message": f"Transformation applied successfully to {file_path}",
                    "changes": surgery_result.get("changes", {})
                }
            else:
                return {"error": f"Failed to apply transformation: {surgery_result.get('error', 'Unknown error')}"}
                
        except Exception as e:
            logger.error(f"Error applying transformation: {e}")
            return {"error": str(e)}
    
    def _find_code_block(self, file_content: str, identifier: str) -> Tuple[str, int, int]:
        """
        Find a code block (function, class, method) in the file content.
        
        Returns:
            Tuple of (code_block, start_line, end_line)
        """
        lines = file_content.split('\n')
        
        # Try to parse as Python first
        try:
            tree = ast.parse(file_content)
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                    if node.name == identifier:
                        start_line = node.lineno
                        end_line = node.end_lineno or start_line
                        code_block = '\n'.join(lines[start_line-1:end_line])
                        return code_block, start_line, end_line
                elif isinstance(node, ast.FunctionDef) and hasattr(node, 'parent'):
                    # Handle class methods
                    if f"{node.parent.name}.{node.name}" == identifier:
                        start_line = node.lineno
                        end_line = node.end_lineno or start_line
                        code_block = '\n'.join(lines[start_line-1:end_line])
                        return code_block, start_line, end_line
        except SyntaxError:
            # Fall back to regex if AST parsing fails
            pass
        
        # Fallback to regex-based search
        patterns = [
            rf'^(\s*)def\s+{re.escape(identifier)}\s*\(',  # Python function
            rf'^(\s*)class\s+{re.escape(identifier)}\s*\(',  # Python class
            rf'^(\s*)async\s+def\s+{re.escape(identifier)}\s*\(',  # Python async function
            rf'^(\s*)function\s+{re.escape(identifier)}\s*\(',  # JavaScript function
            rf'^(\s*){re.escape(identifier)}\s*:\s*function\s*\(',  # JavaScript method
        ]
        
        for i, line in enumerate(lines):
            for pattern in patterns:
                match = re.match(pattern, line, re.MULTILINE)
                if match:
                    # Find the end of the block by tracking indentation
                    start_line = i + 1
                    base_indent = len(match.group(1)) if match.groups() else 0
                    end_line = start_line
                    
                    # Simple block detection based on indentation
                    for j in range(i + 1, len(lines)):
                        line_content = lines[j].strip()
                        if not line_content:  # Skip empty lines
                            continue
                        
                        current_indent = len(lines[j]) - len(lines[j].lstrip())
                        if current_indent <= base_indent and line_content:
                            end_line = j
                            break
                        end_line = j + 1
                    
                    code_block = '\n'.join(lines[start_line-1:end_line])
                    return code_block, start_line, end_line
        
        return "", 0, 0
    
    def _spell_translate_contract(self, code: str, identifier: str, file_path: str, 
                                 target_language: str = "typescript", **kwargs) -> Dict[str, Any]:
        """Translate class/interface contracts between languages."""
        
        language_prompts = {
            "typescript": """You are an expert polyglot developer. Translate the following Python class into a TypeScript interface/type. 
            Ensure perfect type correspondence and idiomatic TypeScript syntax.
            
            Focus on:
            - Converting Python types to TypeScript equivalents
            - Handling optional vs required fields
            - Using appropriate TypeScript conventions
            - Maintaining the original intent and structure
            
            Python Code:
            {code}
            
            Return ONLY the TypeScript interface/type definition:""",
            
            "python": """You are an expert polyglot developer. Translate the following TypeScript interface/type into a Python class using Pydantic or dataclasses.
            
            Focus on:
            - Converting TypeScript types to Python type hints
            - Using appropriate Python conventions
            - Handling optional vs required fields
            - Maintaining the original intent and structure
            
            TypeScript Code:
            {code}
            
            Return ONLY the Python class definition:""",
            
            "java": """You are an expert polyglot developer. Translate the following code into a Java class/interface.
            
            Focus on:
            - Converting types to Java equivalents
            - Using appropriate Java conventions (getters/setters, builders)
            - Maintaining the original intent and structure
            
            Code:
            {code}
            
            Return ONLY the Java class/interface definition:"""
        }
        
        if target_language not in language_prompts:
            return {"error": f"Unsupported target language: {target_language}"}
        
        prompt = language_prompts[target_language].format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": f"translate_to_{target_language}",
                "description": f"Translated {identifier} to {target_language}"
            }
        except Exception as e:
            return {"error": f"Translation failed: {str(e)}"}
    
    def _spell_transmute_implementation(self, code: str, identifier: str, file_path: str,
                                      target_style: str = "functional", **kwargs) -> Dict[str, Any]:
        """Transform implementation style (e.g., procedural to functional)."""
        
        style_prompts = {
            "functional": """You are an expert in functional programming. Transform this code to use functional programming principles.
            
            Focus on:
            - Converting loops to map/filter/reduce operations
            - Eliminating side effects where possible
            - Using immutable data structures
            - Applying functional composition
            - Maintaining the same behavior and output
            
            Original Code:
            {code}
            
            Return ONLY the functionally-transformed code:""",
            
            "object_oriented": """You are an expert in object-oriented design. Transform this code to use object-oriented principles.
            
            Focus on:
            - Encapsulating data and behavior in classes
            - Applying appropriate design patterns
            - Using inheritance and composition where beneficial
            - Maintaining single responsibility principle
            - Keeping the same behavior and output
            
            Original Code:
            {code}
            
            Return ONLY the object-oriented code:""",
            
            "async": """You are an expert in asynchronous programming. Transform this code to use async/await patterns.
            
            Focus on:
            - Converting blocking operations to async
            - Using appropriate async libraries
            - Handling concurrent operations efficiently
            - Maintaining error handling
            - Keeping the same behavior and output
            
            Original Code:
            {code}
            
            Return ONLY the asynchronous code:"""
        }
        
        if target_style not in style_prompts:
            return {"error": f"Unsupported target style: {target_style}"}
        
        prompt = style_prompts[target_style].format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": f"transmute_to_{target_style}",
                "description": f"Transformed {identifier} to {target_style} style"
            }
        except Exception as e:
            return {"error": f"Transmutation failed: {str(e)}"}
    
    def _spell_refactor_pattern(self, code: str, identifier: str, file_path: str,
                               pattern: str = "strategy", **kwargs) -> Dict[str, Any]:
        """Apply design patterns or refactor existing patterns."""
        
        pattern_prompts = {
            "strategy": """Refactor this code to use the Strategy design pattern.
            
            Extract different algorithms/behaviors into separate strategy classes.
            Create a context class that uses these strategies.
            Maintain the same external interface and behavior.
            
            Original Code:
            {code}
            
            Return the refactored code using Strategy pattern:""",
            
            "factory": """Refactor this code to use the Factory design pattern.
            
            Extract object creation logic into factory methods or classes.
            Simplify the main code by delegating object creation.
            Maintain the same functionality and behavior.
            
            Original Code:
            {code}
            
            Return the refactored code using Factory pattern:""",
            
            "observer": """Refactor this code to use the Observer design pattern.
            
            Separate concerns by implementing observer/subject relationships.
            Allow multiple observers to react to changes.
            Maintain loose coupling between components.
            
            Original Code:
            {code}
            
            Return the refactored code using Observer pattern:"""
        }
        
        if pattern not in pattern_prompts:
            return {"error": f"Unsupported pattern: {pattern}"}
        
        prompt = pattern_prompts[pattern].format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": f"apply_{pattern}_pattern",
                "description": f"Applied {pattern} pattern to {identifier}"
            }
        except Exception as e:
            return {"error": f"Pattern application failed: {str(e)}"}
    
    def _spell_modernize_syntax(self, code: str, identifier: str, file_path: str, **kwargs) -> Dict[str, Any]:
        """Update code to use modern language features."""
        
        # Detect language from file extension
        extension = Path(file_path).suffix.lower()
        
        if extension == ".py":
            prompt = """Modernize this Python code to use the latest Python features and best practices.
            
            Focus on:
            - Using f-strings instead of .format() or %
            - Using pathlib instead of os.path
            - Using type hints where appropriate
            - Using dataclasses or attrs where beneficial
            - Using context managers (with statements)
            - Using list/dict comprehensions where appropriate
            - Using modern Python 3.8+ features (walrus operator, positional-only params, etc.)
            
            Original Code:
            {code}
            
            Return ONLY the modernized Python code:"""
        elif extension in [".js", ".ts"]:
            prompt = """Modernize this JavaScript/TypeScript code to use modern ES6+ features.
            
            Focus on:
            - Using arrow functions where appropriate
            - Using const/let instead of var
            - Using template literals instead of string concatenation
            - Using destructuring assignments
            - Using async/await instead of callbacks
            - Using modules (import/export)
            - Using modern array methods (map, filter, reduce)
            
            Original Code:
            {code}
            
            Return ONLY the modernized JavaScript/TypeScript code:"""
        else:
            return {"error": f"Language modernization not supported for {extension} files"}
        
        prompt = prompt.format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": "modernize_syntax",
                "description": f"Modernized syntax for {identifier}"
            }
        except Exception as e:
            return {"error": f"Modernization failed: {str(e)}"}
    
    def _spell_add_type_hints(self, code: str, identifier: str, file_path: str, **kwargs) -> Dict[str, Any]:
        """Add type annotations to Python code."""
        
        prompt = """Add comprehensive type hints to this Python code.
        
        Focus on:
        - Adding type hints to function parameters and return values
        - Using appropriate types from typing module (List, Dict, Optional, Union, etc.)
        - Inferring types from the code logic and usage
        - Adding docstrings with type information if not present
        - Using modern type hint syntax (Python 3.9+ where applicable)
        
        Original Code:
        {code}
        
        Return ONLY the code with complete type hints:"""
        
        prompt = prompt.format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": "add_type_hints",
                "description": f"Added type hints to {identifier}"
            }
        except Exception as e:
            return {"error": f"Type hint addition failed: {str(e)}"}
    
    def _spell_extract_method(self, code: str, identifier: str, file_path: str,
                             method_name: str = "extracted_method", **kwargs) -> Dict[str, Any]:
        """Extract code into a separate method."""
        
        prompt = """Extract the specified logic into a separate method to improve code organization.
        
        Focus on:
        - Identifying cohesive blocks of code that can be extracted
        - Creating a well-named method with appropriate parameters
        - Ensuring the extracted method has a single responsibility
        - Maintaining the same behavior and output
        - Adding appropriate documentation
        
        Method name to create: {method_name}
        
        Original Code:
        {code}
        
        Return the refactored code with the extracted method:"""
        
        prompt = prompt.format(code=code, method_name=method_name)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": "extract_method",
                "description": f"Extracted method '{method_name}' from {identifier}"
            }
        except Exception as e:
            return {"error": f"Method extraction failed: {str(e)}"}
    
    def _spell_inline_method(self, code: str, identifier: str, file_path: str, **kwargs) -> Dict[str, Any]:
        """Inline a method's implementation."""
        
        prompt = """Inline the method calls to simplify the code structure.
        
        Focus on:
        - Replacing method calls with the actual implementation
        - Maintaining the same behavior and output
        - Simplifying the code where appropriate
        - Preserving important comments and logic
        
        Original Code:
        {code}
        
        Return the code with methods inlined:"""
        
        prompt = prompt.format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": "inline_method",
                "description": f"Inlined method calls in {identifier}"
            }
        except Exception as e:
            return {"error": f"Method inlining failed: {str(e)}"}
    
    def _spell_convert_to_async(self, code: str, identifier: str, file_path: str, **kwargs) -> Dict[str, Any]:
        """Convert synchronous code to asynchronous."""
        
        prompt = """Convert this synchronous code to use async/await patterns.
        
        Focus on:
        - Converting blocking operations to async equivalents
        - Using appropriate async libraries (aiohttp, aiofiles, etc.)
        - Handling concurrent operations where beneficial
        - Maintaining proper error handling
        - Adding async/await keywords where needed
        - Keeping the same behavior and output
        
        Original Code:
        {code}
        
        Return ONLY the asynchronous version of the code:"""
        
        prompt = prompt.format(code=code)
        
        try:
            transformed_code = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            return {
                "transformed_code": self._clean_code_output(transformed_code),
                "transformation_type": "convert_to_async",
                "description": f"Converted {identifier} to async"
            }
        except Exception as e:
            return {"error": f"Async conversion failed: {str(e)}"}
    
    def _clean_code_output(self, llm_output: str) -> str:
        """Clean and format the LLM output to extract just the code."""
        # Remove code fence markers
        cleaned = re.sub(r'^```\w*\n', '', llm_output, flags=re.MULTILINE)
        cleaned = re.sub(r'\n```$', '', cleaned, flags=re.MULTILINE)
        
        # Remove leading/trailing whitespace but preserve internal formatting
        cleaned = cleaned.strip()
        
        # Remove common explanatory text
        lines = cleaned.split('\n')
        code_lines = []
        in_code = False
        
        for line in lines:
            # Skip explanatory lines before code
            if not in_code and any(keyword in line.lower() for keyword in 
                                 ['here is', 'here\'s', 'this is', 'the code', 'transformed', 'refactored']):
                continue
            
            # Start collecting code
            if not in_code and (line.strip().startswith(('def ', 'class ', 'function ', 'const ', 'let ', 'var ')) or
                               any(line.strip().startswith(keyword) for keyword in ['import ', 'from ', 'interface ', 'type '])):
                in_code = True
            
            if in_code:
                code_lines.append(line)
        
        return '\n'.join(code_lines) if code_lines else cleaned


# Global instance
_mage_service = None

def get_mage_service() -> MageService:
    """Get or create the global Mage service instance."""
    global _mage_service
    if _mage_service is None:
        _mage_service = MageService()
    return _mage_service

# Public API
def list_available_spells() -> Dict[str, str]:
    """Get list of available spells and their descriptions."""
    service = get_mage_service()
    return service.list_spells()

def cast_transformation_spell(repo_id: str, spell_name: str, file_path: str, 
                             target_identifier: str, **kwargs) -> Dict[str, Any]:
    """Cast a transformation spell on code."""
    service = get_mage_service()
    return service.cast_spell(repo_id, spell_name, file_path, target_identifier, **kwargs)

def apply_code_transformation(repo_id: str, transformation_result: Dict[str, Any]) -> Dict[str, Any]:
    """Apply a transformation to the actual source file."""
    service = get_mage_service()
    return service.apply_transformation(repo_id, transformation_result)
--- FILE_END: backend/lumiere_core/services/mage_service.py ---

--- FILE_START: backend/lumiere_core/services/crucible.py ---
# In backend/lumiere_core/services/crucible.py

import os
import uuid
import json
import time
from pathlib import Path
from typing import Dict, Tuple, Optional, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime

import docker
from docker.errors import BuildError, ContainerError, APIError, DockerException

from .utils import clean_llm_code_output
from ingestion.crawler import IntelligentCrawler

# --- Enhanced Data Structures ---
@dataclass
class ValidationResult:
    """Enhanced result structure with detailed metrics"""
    status: str
    logs: str
    execution_time: float = 0.0
    build_time: float = 0.0
    test_time: float = 0.0
    project_type: str = "unknown"
    image_size: Optional[int] = None
    warnings: List[str] = None
    detected_languages: List[str] = None
    build_tools: List[str] = None

    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
        if self.detected_languages is None:
            self.detected_languages = []
        if self.build_tools is None:
            self.build_tools = []

    def to_dict(self) -> Dict:
        """Convert to dictionary for backward compatibility"""
        result = {"status": self.status, "logs": self.logs}
        # Add enhanced fields only if they have meaningful values
        if self.execution_time > 0:
            result["execution_time"] = self.execution_time
        if self.build_time > 0:
            result["build_time"] = self.build_time
        if self.test_time > 0:
            result["test_time"] = self.test_time
        if self.project_type != "unknown":
            result["project_type"] = self.project_type
        if self.image_size:
            result["image_size_mb"] = round(self.image_size / (1024 * 1024), 2)
        if self.warnings:
            result["warnings"] = self.warnings
        if self.detected_languages:
            result["detected_languages"] = self.detected_languages
        if self.build_tools:
            result["build_tools"] = self.build_tools
        return result

# --- Enhanced Environment Detection (Polyglot Support) ---
def _detect_project_ecosystem(repo_path: Path) -> Tuple[str, List[str], List[str]]:
    """
    Enhanced project detection supporting multiple languages and ecosystems.
    Returns: (primary_project_type, detected_languages, build_tools)
    """
    detected_languages = []
    build_tools = []
    project_types = []

    # File-based detection patterns
    detection_patterns = {
        # Python ecosystem
        "python": {
            "files": ["requirements.txt", "pyproject.toml", "setup.py", "Pipfile",
                     "poetry.lock", "conda.yml", "environment.yml", "setup.cfg"],
            "extensions": [".py"],
            "build_tools": ["pip", "poetry", "pipenv", "conda"]
        },
        # Node.js ecosystem
        "node": {
            "files": ["package.json"],
            "extensions": [".js", ".ts", ".jsx", ".tsx"],
            "build_tools": ["npm", "yarn", "pnpm"]
        },
        # Java ecosystem
        "java": {
            "files": ["pom.xml", "build.gradle", "build.gradle.kts", "gradle.properties"],
            "extensions": [".java"],
            "build_tools": ["maven", "gradle"]
        },
        # .NET ecosystem
        "dotnet": {
            "files": ["*.csproj", "*.vbproj", "*.fsproj", "*.sln", "global.json"],
            "extensions": [".cs", ".vb", ".fs"],
            "build_tools": ["dotnet"]
        },
        # Go ecosystem
        "go": {
            "files": ["go.mod", "go.sum"],
            "extensions": [".go"],
            "build_tools": ["go"]
        },
        # Rust ecosystem
        "rust": {
            "files": ["Cargo.toml", "Cargo.lock"],
            "extensions": [".rs"],
            "build_tools": ["cargo"]
        },
        # C/C++ ecosystem
        "cpp": {
            "files": ["CMakeLists.txt", "Makefile", "makefile", "meson.build", "conanfile.txt"],
            "extensions": [".c", ".cpp", ".cxx", ".cc", ".h", ".hpp"],
            "build_tools": ["cmake", "make", "meson", "conan"]
        },
        # Swift ecosystem
        "swift": {
            "files": ["Package.swift", "*.xcodeproj", "*.xcworkspace"],
            "extensions": [".swift"],
            "build_tools": ["swift", "xcodebuild"]
        },
        # Ruby ecosystem
        "ruby": {
            "files": ["Gemfile", "Gemfile.lock", "Rakefile", "*.gemspec"],
            "extensions": [".rb"],
            "build_tools": ["bundler", "rake", "gem"]
        },
        # PHP ecosystem
        "php": {
            "files": ["composer.json", "composer.lock"],
            "extensions": [".php"],
            "build_tools": ["composer"]
        },
        # Dart/Flutter ecosystem
        "dart": {
            "files": ["pubspec.yaml", "pubspec.lock"],
            "extensions": [".dart"],
            "build_tools": ["pub", "flutter"]
        },
        # Kotlin ecosystem
        "kotlin": {
            "files": ["build.gradle.kts"],
            "extensions": [".kt", ".kts"],
            "build_tools": ["gradle"]
        },
        # Scala ecosystem
        "scala": {
            "files": ["build.sbt", "project/build.properties"],
            "extensions": [".scala"],
            "build_tools": ["sbt"]
        },
        # Clojure ecosystem
        "clojure": {
            "files": ["project.clj", "deps.edn", "build.boot"],
            "extensions": [".clj", ".cljs", ".cljc"],
            "build_tools": ["leiningen", "clojure"]
        },
        # Haskell ecosystem
        "haskell": {
            "files": ["*.cabal", "stack.yaml", "cabal.project"],
            "extensions": [".hs", ".lhs"],
            "build_tools": ["cabal", "stack"]
        },
        # Elixir ecosystem
        "elixir": {
            "files": ["mix.exs", "mix.lock"],
            "extensions": [".ex", ".exs"],
            "build_tools": ["mix"]
        },
        # R ecosystem
        "r": {
            "files": ["DESCRIPTION", "renv.lock", ".Rprofile"],
            "extensions": [".r", ".R"],
            "build_tools": ["R"]
        }
    }

    # Check for file indicators
    for ecosystem, config in detection_patterns.items():
        found_files = []
        for pattern in config["files"]:
            if "*" in pattern:
                found_files.extend(repo_path.glob(pattern))
            else:
                if (repo_path / pattern).exists():
                    found_files.append(pattern)

        if found_files:
            detected_languages.append(ecosystem)
            build_tools.extend(config["build_tools"])
            project_types.append(ecosystem)

    # Check for source code files if no config files found
    if not detected_languages:
        for ecosystem, config in detection_patterns.items():
            for ext in config["extensions"]:
                if list(repo_path.rglob(f"*{ext}")):
                    detected_languages.append(ecosystem)
                    build_tools.extend(config["build_tools"])
                    project_types.append(ecosystem)
                    break

    # Determine primary project type
    if project_types:
        # Priority order for mixed projects
        priority_order = ["python", "node", "java", "dotnet", "go", "rust", "cpp", "swift"]
        for priority_type in priority_order:
            if priority_type in project_types:
                primary_type = priority_type
                break
        else:
            primary_type = project_types[0]
    else:
        primary_type = "unknown"

    print(f"   -> Detected ecosystem: {primary_type}")
    print(f"   -> Languages found: {detected_languages}")
    print(f"   -> Build tools: {build_tools}")

    return primary_type, detected_languages, build_tools

def _get_enhanced_project_commands(project_type: str, repo_path: Path, detected_languages: List[str]) -> Tuple[str, str, str, Optional[str]]:
    """
    Enhanced command generation supporting polyglot projects.
    Returns: (install_command, test_command, base_image, dependency_file)
    """
    print(f"   -> Configuring build for project type: {project_type}")

    # Multi-stage build support for polyglot projects
    base_images = {
        "python": "python:3.11-slim",
        "node": "node:18-alpine",
        "java": "openjdk:17-jdk-slim",
        "dotnet": "mcr.microsoft.com/dotnet/sdk:7.0",
        "go": "golang:1.21-alpine",
        "rust": "rust:1.75-slim",
        "cpp": "gcc:12",
        "swift": "swift:5.9",
        "ruby": "ruby:3.2-alpine",
        "php": "php:8.2-cli",
        "dart": "dart:stable",
        "kotlin": "openjdk:17-jdk-slim",
        "scala": "openjdk:17-jdk-slim",
        "clojure": "clojure:lein-alpine",
        "haskell": "haskell:9.4",
        "elixir": "elixir:1.15-alpine",
        "r": "r-base:4.3.0"
    }

    # Primary language configuration
    if project_type == "python":
        install_cmd, dep_file = _configure_python_build(repo_path)
        test_cmd = _detect_python_test_runner(repo_path)
        base_image = base_images["python"]

    elif project_type == "node":
        install_cmd, dep_file = _configure_node_build(repo_path)
        test_cmd = _detect_node_test_runner(repo_path)
        base_image = base_images["node"]

    elif project_type == "java":
        install_cmd, dep_file = _configure_java_build(repo_path)
        test_cmd = _detect_java_test_runner(repo_path)
        base_image = base_images["java"]

    elif project_type == "dotnet":
        install_cmd, dep_file = _configure_dotnet_build(repo_path)
        test_cmd = _detect_dotnet_test_runner(repo_path)
        base_image = base_images["dotnet"]

    elif project_type == "go":
        install_cmd, dep_file = _configure_go_build(repo_path)
        test_cmd = _detect_go_test_runner(repo_path)
        base_image = base_images["go"]

    elif project_type == "rust":
        install_cmd, dep_file = _configure_rust_build(repo_path)
        test_cmd = _detect_rust_test_runner(repo_path)
        base_image = base_images["rust"]

    elif project_type == "cpp":
        install_cmd, dep_file = _configure_cpp_build(repo_path)
        test_cmd = _detect_cpp_test_runner(repo_path)
        base_image = base_images["cpp"]

    elif project_type == "swift":
        install_cmd, dep_file = _configure_swift_build(repo_path)
        test_cmd = _detect_swift_test_runner(repo_path)
        base_image = base_images["swift"]

    elif project_type == "ruby":
        install_cmd, dep_file = _configure_ruby_build(repo_path)
        test_cmd = _detect_ruby_test_runner(repo_path)
        base_image = base_images["ruby"]

    elif project_type == "php":
        install_cmd, dep_file = _configure_php_build(repo_path)
        test_cmd = _detect_php_test_runner(repo_path)
        base_image = base_images["php"]

    elif project_type == "dart":
        install_cmd, dep_file = _configure_dart_build(repo_path)
        test_cmd = _detect_dart_test_runner(repo_path)
        base_image = base_images["dart"]

    else:
        # Fallback for unknown or unsupported languages
        install_cmd = "echo 'Unknown project type, attempting generic build'"
        test_cmd = "echo 'No test runner detected for this project type'"
        base_image = "alpine:latest"
        dep_file = None

    return install_cmd, test_cmd, base_image, dep_file

# Language-specific build configuration functions
def _configure_python_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Python build commands and detect dependency file."""
    if (repo_path / "poetry.lock").exists():
        return "pip install poetry && poetry install", "pyproject.toml"
    elif (repo_path / "Pipfile").exists():
        return "pip install pipenv && pipenv install --system", "Pipfile"
    elif (repo_path / "requirements.txt").exists():
        return "pip install -r requirements.txt", "requirements.txt"
    elif (repo_path / "pyproject.toml").exists():
        return "pip install .", "pyproject.toml"
    elif (repo_path / "setup.py").exists():
        return "pip install -e .", "setup.py"
    elif (repo_path / "conda.yml").exists() or (repo_path / "environment.yml").exists():
        env_file = "conda.yml" if (repo_path / "conda.yml").exists() else "environment.yml"
        return f"conda env create -f {env_file} && conda activate $(head -1 {env_file} | cut -d' ' -f2)", env_file
    else:
        return "echo 'No Python dependencies to install'", None

def _detect_python_test_runner(repo_path: Path) -> str:
    """Detect appropriate Python test runner."""
    if (repo_path / "pytest.ini").exists() or (repo_path / "pyproject.toml").exists():
        content = ""
        if (repo_path / "pyproject.toml").exists():
            content = (repo_path / "pyproject.toml").read_text()
        if "pytest" in content or (repo_path / "pytest.ini").exists():
            return "python -m pytest"

    if (repo_path / "tox.ini").exists():
        return "tox"

    if any(repo_path.rglob("test_*.py")) or any(repo_path.rglob("*_test.py")):
        return "python -m pytest"

    if (repo_path / "tests").is_dir() or (repo_path / "test").is_dir():
        return "python -m unittest discover"

    return "python -m unittest discover"

def _configure_node_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Node.js build commands."""
    if (repo_path / "pnpm-lock.yaml").exists():
        return "pnpm install", "package.json"
    elif (repo_path / "yarn.lock").exists():
        return "yarn install --frozen-lockfile", "package.json"
    elif (repo_path / "package.json").exists():
        return "npm ci", "package.json"
    else:
        return "echo 'No Node.js dependencies found'", None

def _detect_node_test_runner(repo_path: Path) -> str:
    """Detect appropriate Node.js test runner."""
    if (repo_path / "package.json").exists():
        try:
            package_json = json.loads((repo_path / "package.json").read_text())
            scripts = package_json.get("scripts", {})

            if "test" in scripts:
                if (repo_path / "pnpm-lock.yaml").exists():
                    return "pnpm test"
                elif (repo_path / "yarn.lock").exists():
                    return "yarn test"
                else:
                    return "npm test"
        except:
            pass

    # Check for specific test framework files
    if (repo_path / "jest.config.js").exists() or (repo_path / "jest.config.ts").exists():
        return "npx jest"
    elif (repo_path / "vitest.config.js").exists() or (repo_path / "vitest.config.ts").exists():
        return "npx vitest run"
    elif (repo_path / "cypress.config.js").exists():
        return "npx cypress run"
    elif (repo_path / "playwright.config.js").exists():
        return "npx playwright test"

    return "npm test"

def _configure_java_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Java build commands."""
    if (repo_path / "pom.xml").exists():
        return "mvn compile", "pom.xml"
    elif (repo_path / "build.gradle").exists() or (repo_path / "build.gradle.kts").exists():
        gradle_file = "build.gradle.kts" if (repo_path / "build.gradle.kts").exists() else "build.gradle"
        return "./gradlew build", gradle_file
    else:
        return "echo 'No Java build file found'", None

def _detect_java_test_runner(repo_path: Path) -> str:
    """Detect appropriate Java test runner."""
    if (repo_path / "pom.xml").exists():
        return "mvn test"
    elif (repo_path / "build.gradle").exists() or (repo_path / "build.gradle.kts").exists():
        return "./gradlew test"
    else:
        return "echo 'No Java test runner found'"

def _configure_dotnet_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure .NET build commands."""
    sln_files = list(repo_path.glob("*.sln"))
    csproj_files = list(repo_path.glob("*.csproj"))

    if sln_files:
        return "dotnet restore && dotnet build", sln_files[0].name
    elif csproj_files:
        return "dotnet restore && dotnet build", csproj_files[0].name
    else:
        return "dotnet build", None

def _detect_dotnet_test_runner(repo_path: Path) -> str:
    """Detect appropriate .NET test runner."""
    return "dotnet test"

def _configure_go_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Go build commands."""
    if (repo_path / "go.mod").exists():
        return "go mod download && go build ./...", "go.mod"
    else:
        return "go build ./...", None

def _detect_go_test_runner(repo_path: Path) -> str:
    """Detect appropriate Go test runner."""
    return "go test ./..."

def _configure_rust_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Rust build commands."""
    if (repo_path / "Cargo.toml").exists():
        return "cargo build --release", "Cargo.toml"
    else:
        return "echo 'No Cargo.toml found'", None

def _detect_rust_test_runner(repo_path: Path) -> str:
    """Detect appropriate Rust test runner."""
    return "cargo test --release"

def _configure_cpp_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure C++ build commands."""
    if (repo_path / "CMakeLists.txt").exists():
        return "mkdir -p build && cd build && cmake .. && make", "CMakeLists.txt"
    elif (repo_path / "Makefile").exists() or (repo_path / "makefile").exists():
        makefile = "Makefile" if (repo_path / "Makefile").exists() else "makefile"
        return "make", makefile
    elif (repo_path / "meson.build").exists():
        return "meson setup builddir && meson compile -C builddir", "meson.build"
    else:
        return "echo 'No C++ build system found'", None

def _detect_cpp_test_runner(repo_path: Path) -> str:
    """Detect appropriate C++ test runner."""
    if (repo_path / "CMakeLists.txt").exists():
        return "cd build && ctest"
    elif (repo_path / "Makefile").exists():
        return "make test"
    else:
        return "echo 'No C++ test runner found'"

def _configure_swift_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Swift build commands."""
    if (repo_path / "Package.swift").exists():
        return "swift build", "Package.swift"
    else:
        return "echo 'No Package.swift found'", None

def _detect_swift_test_runner(repo_path: Path) -> str:
    """Detect appropriate Swift test runner."""
    return "swift test"

def _configure_ruby_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Ruby build commands."""
    if (repo_path / "Gemfile").exists():
        return "bundle install", "Gemfile"
    else:
        return "echo 'No Gemfile found'", None

def _detect_ruby_test_runner(repo_path: Path) -> str:
    """Detect appropriate Ruby test runner."""
    if (repo_path / "Rakefile").exists():
        return "bundle exec rake test"
    elif any(repo_path.rglob("*_spec.rb")):
        return "bundle exec rspec"
    elif any(repo_path.rglob("test_*.rb")):
        return "bundle exec ruby -Itest test/test_*.rb"
    else:
        return "bundle exec rake test"

def _configure_php_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure PHP build commands."""
    if (repo_path / "composer.json").exists():
        return "composer install", "composer.json"
    else:
        return "echo 'No composer.json found'", None

def _detect_php_test_runner(repo_path: Path) -> str:
    """Detect appropriate PHP test runner."""
    if (repo_path / "phpunit.xml").exists() or (repo_path / "phpunit.xml.dist").exists():
        return "vendor/bin/phpunit"
    elif (repo_path / "composer.json").exists():
        try:
            composer_json = json.loads((repo_path / "composer.json").read_text())
            scripts = composer_json.get("scripts", {})
            if "test" in scripts:
                return "composer test"
        except:
            pass
    return "vendor/bin/phpunit"

def _configure_dart_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Dart build commands."""
    if (repo_path / "pubspec.yaml").exists():
        # Check if it's a Flutter project
        pubspec_content = (repo_path / "pubspec.yaml").read_text()
        if "flutter:" in pubspec_content:
            return "flutter pub get && flutter build apk --debug", "pubspec.yaml"
        else:
            return "dart pub get", "pubspec.yaml"
    else:
        return "echo 'No pubspec.yaml found'", None

def _detect_dart_test_runner(repo_path: Path) -> str:
    """Detect appropriate Dart test runner."""
    if (repo_path / "pubspec.yaml").exists():
        pubspec_content = (repo_path / "pubspec.yaml").read_text()
        if "flutter:" in pubspec_content:
            return "flutter test"
        else:
            return "dart test"
    return "dart test"

def _generate_polyglot_dockerfile(install_command: str, test_command: str, base_image: str,
                                project_type: str = "unknown", detected_languages: List[str] = None) -> str:
    """
    Generate optimized Dockerfile with polyglot support and multi-stage builds when needed.
    """
    print("   -> Generating enhanced polyglot Dockerfile...")

    dockerfile_content = f"FROM {base_image}\n"
    dockerfile_content += "WORKDIR /app\n\n"

    # Enhanced security and optimization
    dockerfile_content += "# Security: Create non-root user\n"
    dockerfile_content += "RUN groupadd -r appuser && useradd -r -g appuser -d /app -s /sbin/nologin appuser\n\n"

    # Language-specific optimizations
    if project_type == "node":
        dockerfile_content += "# Node.js optimizations\n"
        dockerfile_content += "ENV NODE_ENV=production\n"
        dockerfile_content += "ENV CI=true\n"
        dockerfile_content += "COPY package*.json yarn.lock* pnpm-lock.yaml* ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "# Copy source code after dependency installation for better caching\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "python":
        dockerfile_content += "# Python optimizations\n"
        dockerfile_content += "ENV PYTHONUNBUFFERED=1\n"
        dockerfile_content += "ENV PYTHONDONTWRITEBYTECODE=1\n"
        dockerfile_content += "ENV PIP_NO_CACHE_DIR=1\n"
        dockerfile_content += "ENV PIP_DISABLE_PIP_VERSION_CHECK=1\n"

        # Copy dependency files first for better caching
        dependency_files = ["requirements*.txt", "pyproject.toml", "setup.py", "setup.cfg",
                          "Pipfile", "Pipfile.lock", "poetry.lock", "conda.yml", "environment.yml"]
        dockerfile_content += "# Copy dependency files\n"
        for dep_file in dependency_files:
            dockerfile_content += f"COPY {dep_file} ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "# Copy source code\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "java":
        dockerfile_content += "# Java optimizations\n"
        dockerfile_content += "ENV JAVA_OPTS=\"-Xmx512m -XX:+UseContainerSupport\"\n"
        dockerfile_content += "COPY pom.xml build.gradle* build.gradle.kts* gradle.properties* gradlew* ./\n"
        dockerfile_content += "COPY gradle/ gradle/ 2>/dev/null || true\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "go":
        dockerfile_content += "# Go optimizations\n"
        dockerfile_content += "ENV CGO_ENABLED=0\n"
        dockerfile_content += "ENV GOOS=linux\n"
        dockerfile_content += "COPY go.mod go.sum ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "rust":
        dockerfile_content += "# Rust optimizations\n"
        dockerfile_content += "ENV CARGO_NET_GIT_FETCH_WITH_CLI=true\n"
        dockerfile_content += "COPY Cargo.toml Cargo.lock ./\n"
        dockerfile_content += "# Pre-build dependencies for better caching\n"
        dockerfile_content += "RUN mkdir src && echo 'fn main() {}' > src/main.rs\n"
        dockerfile_content += "RUN cargo build --release && rm -rf src\n"
        dockerfile_content += "COPY . .\n"
        dockerfile_content += "RUN touch src/main.rs\n"
        dockerfile_content += "RUN cargo build --release\n\n"

    else:
        # Generic approach for other languages
        dockerfile_content += "# Copy source code\n"
        dockerfile_content += "COPY . .\n\n"
        dockerfile_content += f"# Install dependencies\n"
        dockerfile_content += f"RUN {install_command}\n\n"

    # Set proper ownership and switch to non-root user
    dockerfile_content += "# Set ownership and switch to non-root user\n"
    dockerfile_content += "RUN chown -R appuser:appuser /app\n"
    dockerfile_content += "USER appuser\n\n"

    # Health check for long-running tests
    dockerfile_content += "# Health check\n"
    dockerfile_content += "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n"
    dockerfile_content += "  CMD echo 'Health check: Tests running'\n\n"

    # Final command
    cmd_parts = test_command.split()
    cmd_json = ', '.join([f'"{part}"' for part in cmd_parts])
    dockerfile_content += f"# Run tests\n"
    dockerfile_content += f"CMD [{cmd_json}]\n"

    print(f"      - Enhanced polyglot Dockerfile generated for {project_type}")
    return dockerfile_content

def _analyze_polyglot_warnings(repo_path: Path, project_type: str, detected_languages: List[str]) -> List[str]:
    """Enhanced warning analysis for polyglot projects."""
    warnings = []

    # Multi-language project warnings
    if len(detected_languages) > 1:
        warnings.append(f"Multi-language project detected: {', '.join(detected_languages)}. Build complexity may be higher.")

    # Language-specific warnings
    for language in detected_languages:
        if language == "python":
            req_file = repo_path / "requirements.txt"
            if req_file.exists():
                content = req_file.read_text()
                if "==" not in content and ">=" not in content:
                    warnings.append("Python: Consider pinning package versions for reproducible builds")

        elif language == "node":
            package_json = repo_path / "package.json"
            if package_json.exists() and not (repo_path / "package-lock.json").exists() and not (repo_path / "yarn.lock").exists():
                warnings.append("Node.js: No lock file found. Consider using npm ci or yarn for reproducible builds")

        elif language == "java":
            if not any((repo_path / f).exists() for f in ["pom.xml", "build.gradle", "build.gradle.kts"]):
                warnings.append("Java: No standard build file found. Manual compilation may be required")

    # Security warnings
    security_files = [".env", "config.json", "secrets.yml"]
    for sec_file in security_files:
        if (repo_path / sec_file).exists():
            warnings.append(f"Security: {sec_file} found. Ensure sensitive data is not included in container")

    # Performance warnings
    large_files = []
    for file_path in repo_path.rglob("*"):
        if file_path.is_file() and file_path.stat().st_size > 50 * 1024 * 1024:  # 50MB
            large_files.append(file_path.name)

    if large_files:
        warnings.append(f"Performance: Large files detected: {', '.join(large_files[:3])}. Consider .dockerignore")

    # Dockerfile optimization warnings
    if not (repo_path / ".dockerignore").exists():
        warnings.append("Optimization: Consider adding .dockerignore to exclude unnecessary files")

    return warnings

# --- Enhanced Main Service Orchestrator ---
def validate_fix(repo_url: str, target_file: str, modified_code: str,
                 enhanced_output: bool = False) -> Dict[str, str]:
    """
    Enhanced Crucible agent with comprehensive polyglot support.

    Args:
        repo_url: Repository URL to validate
        target_file: Target file path to modify
        modified_code: Modified code content
        enhanced_output: Whether to return enhanced output with metrics

    Returns:
        Dictionary with validation results
    """
    print("\n--- ENHANCED POLYGLOT CRUCIBLE ACTIVATED ---")
    print(f"Validating fix for '{target_file}' in '{repo_url}'")

    start_time = time.time()
    result = ValidationResult(status="error", logs="Initialization failed")

    try:
        client = docker.from_env()
        client.ping()
        print("✓ Successfully connected to Docker daemon.")
    except DockerException:
        error_msg = "Crucible Error: Could not connect to the Docker daemon. Please ensure Docker Desktop is running."
        print(f"✗ {error_msg}")
        result.logs = error_msg
        return result.to_dict() if enhanced_output else {"status": result.status, "logs": result.logs}

    with IntelligentCrawler(repo_url=repo_url) as crawler:
        repo_path = crawler.repo_path
        print("[Step 1/7] Patching file in local clone...")
        full_target_path = repo_path / target_file
        if not full_target_path.exists():
            full_target_path.parent.mkdir(parents=True, exist_ok=True)
        full_target_path.write_text(modified_code, encoding='utf-8')
        print("✓ File patched.")

        print("[Step 2/7] Analyzing polyglot project environment...")
        project_type, detected_languages, build_tools = _detect_project_ecosystem(repo_path)
        result.project_type = project_type
        result.detected_languages = detected_languages
        result.build_tools = build_tools

        install_cmd, test_cmd, base_image, dep_file = _get_enhanced_project_commands(
            project_type, repo_path, detected_languages
        )

        # Generate enhanced warnings
        result.warnings = _analyze_polyglot_warnings(repo_path, project_type, detected_languages)

        dockerfile_str = _generate_polyglot_dockerfile(
            install_cmd, test_cmd, base_image, project_type, detected_languages
        )
        (repo_path / "Dockerfile.lumiere").write_text(dockerfile_str, encoding='utf-8')
        print("✓ Enhanced polyglot environment analysis complete.")

        print("[Step 3/7] Building polyglot validation image...")
        build_start = time.time()
        image_tag = f"lumiere-crucible-polyglot/{uuid.uuid4()}"
        image = None
        try:
            image, build_logs = client.images.build(
                path=str(repo_path),
                dockerfile="Dockerfile.lumiere",
                tag=image_tag,
                rm=True,
                forcerm=True,
                pull=True,
                platform="linux/amd64"  # Ensure consistent platform
            )
            result.build_time = time.time() - build_start
            result.image_size = image.attrs.get('Size', 0)
            print(f"✓ Polyglot image '{image_tag}' built in {result.build_time:.2f}s")
        except BuildError as e:
            print(f"✗ Build Failed: {e}")
            logs = "\n".join([log.get('stream', '').strip() for log in e.build_log if 'stream' in log])
            result.status = "failed"
            result.logs = f"Docker image build failed:\n{logs}"
            result.execution_time = time.time() - start_time
            return result.to_dict() if enhanced_output else {"status": result.status, "logs": result.logs}

        try:
            print(f"[Step 4/7] Running polyglot tests in container '{image_tag}'...")
            test_start = time.time()

            # Enhanced container run with better resource limits
            container_output = client.containers.run(
                image_tag,
                remove=True,
                mem_limit="2g",  # Increased memory for polyglot builds
                cpu_count=4,     # More CPU for complex builds
                network_disabled=True,
                security_opt=["no-new-privileges:true"],  # Enhanced security
                read_only=False,  # Some builds need write access
                tmpfs={"/tmp": "size=512m,noexec"}  # Secure tmp
            )

            result.test_time = time.time() - test_start
            print(f"✓ Polyglot tests PASSED in {result.test_time:.2f}s")
            result.status = "passed"
            result.logs = container_output.decode('utf-8')

        except ContainerError as e:
            result.test_time = time.time() - test_start if 'test_start' in locals() else 0
            print(f"✗ Polyglot tests FAILED. Exit code: {e.exit_status}")
            logs = e.stderr.decode('utf-8') if e.stderr else e.stdout.decode('utf-8')
            result.status = "failed"
            result.logs = logs

        finally:
            print("[Step 5/7] Cleaning up polyglot validation image...")
            if image:
                try:
                    client.images.remove(image.id, force=True)
                    print(f"✓ Image '{image_tag}' removed.")
                except APIError as e:
                    print(f"Warning: Could not remove image '{image_tag}'. Error: {e}")
                    result.warnings.append(f"Failed to cleanup image: {image_tag}")

    result.execution_time = time.time() - start_time
    print(f"[Step 6/7] Polyglot validation completed in {result.execution_time:.2f}s")
    print(f"[Step 7/7] Languages processed: {', '.join(result.detected_languages)}")
    print("--- ENHANCED POLYGLOT CRUCIBLE MISSION COMPLETE ---")

    # Return backward compatible format by default
    if enhanced_output:
        return result.to_dict()
    else:
        return {"status": result.status, "logs": result.logs}

# --- Enhanced API Functions ---
def validate_fix_enhanced(repo_url: str, target_file: str, modified_code: str) -> Dict:
    """Enhanced version that returns detailed polyglot metrics and warnings"""
    return validate_fix(repo_url, target_file, modified_code, enhanced_output=True)

def get_supported_project_types() -> List[str]:
    """Returns list of supported project types with enhanced polyglot support"""
    return [
        "python", "node", "java", "dotnet", "go", "rust", "cpp", "swift",
        "ruby", "php", "dart", "kotlin", "scala", "clojure", "haskell",
        "elixir", "r"
    ]

def analyze_project_ecosystem(repo_url: str) -> Dict[str, Any]:
    """Enhanced project analysis with comprehensive ecosystem detection"""
    with IntelligentCrawler(repo_url=repo_url) as crawler:
        repo_path = crawler.repo_path
        project_type, detected_languages, build_tools = _detect_project_ecosystem(repo_path)
        install_cmd, test_cmd, base_image, dep_file = _get_enhanced_project_commands(
            project_type, repo_path, detected_languages
        )
        warnings = _analyze_polyglot_warnings(repo_path, project_type, detected_languages)

        return {
            "primary_project_type": project_type,
            "detected_languages": detected_languages,
            "build_tools": build_tools,
            "install_command": install_cmd,
            "test_command": test_cmd,
            "base_image": base_image,
            "dependency_file": dep_file,
            "warnings": warnings,
            "polyglot_support": True,
            "supported": project_type in get_supported_project_types()
        }

# --- Backward Compatibility Aliases ---
def crucible_validate(repo_url: str, target_file: str, modified_code: str) -> Dict[str, str]:
    """Backward compatibility alias for validate_fix"""
    return validate_fix(repo_url, target_file, modified_code)

def analyze_project(repo_url: str) -> Dict[str, str]:
    """Backward compatibility alias for analyze_project_ecosystem"""
    result = analyze_project_ecosystem(repo_url)
    # Convert to old format for backward compatibility
    return {
        "project_type": result["primary_project_type"],
        "install_command": result["install_command"],
        "test_command": result["test_command"],
        "base_image": result["base_image"],
        "dependency_file": result["dependency_file"],
        "supported": result["supported"]
    }

--- FILE_END: backend/lumiere_core/services/crucible.py ---

--- FILE_START: backend/lumiere_core/services/strategist.py ---
# In backend/lumiere_core/services/strategist.py

import json
import re
from typing import Dict, List, Any

from . import github
from . import ollama
from . import impact_analyzer

from . import llm_service
from .llm_service import TaskType

def analyze_and_prioritize(repo_url: str) -> Dict[str, Any]:
    """
    The core logic for The Strategist agent.
    Fetches all open issues and uses an LLM to prioritize them.
    The 'model_identifier' is no longer passed in.
    """
    print("--- STRATEGIST AGENT ACTIVATED ---")
    print(f"Analyzing repository: {repo_url}")
    # The specific model used is now decided by the Task Router.

    # Step 1: Fetch and Enrich All Open Issues with Blast Radius Analysis
    print("\n[Step 1/4] Fetching and enriching all open issues...")
    match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if not match:
        return {"error": f"Could not parse repository name from URL: {repo_url}"}
    repo_full_name = f"{match.group(1)}/{match.group(2)}"
    
    # Extract repo_id for blast radius analysis (assuming repo_id follows pattern owner_repo)
    repo_id = repo_full_name.replace('/', '_')
    
    raw_issues = github.list_open_issues(repo_full_name)
    if not raw_issues:
        return {"analysis_summary": "No open issues found for this repository.", "prioritized_issues": []}
    
    enriched_issues = []
    issues_for_prompt = ""
    
    print(f"\n[Step 2/4] Performing blast radius analysis for onboarding suitability...")
    
    for issue_stub in raw_issues:
        issue_details = github.scrape_github_issue(issue_stub['url'])
        if issue_details:
            enriched_issue_data = {**issue_stub, **issue_details}
            
            # Perform blast radius analysis for onboarding suitability
            blast_analysis = _analyze_issue_blast_radius(
                repo_id, 
                enriched_issue_data.get('title', ''), 
                enriched_issue_data.get('description', '')
            )
            
            # Add blast radius data to the issue
            enriched_issue_data.update(blast_analysis)
            
            enriched_issues.append(enriched_issue_data)
            description = issue_details.get('description') or ""
            onboarding_info = f" [Onboarding Score: {blast_analysis.get('onboarding_suitability_score', 0):.1f}]"
            issues_for_prompt += f"### Issue #{issue_stub['number']}: {issue_stub['title']}{onboarding_info}\n{description}\n\n---\n\n"
    
    print(f"✓ Found and enriched {len(enriched_issues)} open issues with blast radius analysis.")

    # Step 3: Use LLM to score and justify prioritization
    print("\n[Step 3/4] Submitting issues to LLM for prioritization analysis...")
    prompt = f"""You are "The Strategist", an expert engineering manager. Your mission is to analyze a list of open GitHub issues and prioritize them.

You MUST produce a valid JSON array as your output. For each issue, create a JSON object with these exact fields:
- "issue_number": The integer issue number.
- "score": An integer from 0 to 100, where 100 is most critical.
- "justification": A concise, one-sentence explanation for your score.

ENHANCED SCORING CRITERIA:
- Critical (90-100): Crashes, data corruption, security vulnerabilities.
- High (70-89): Major feature bugs, performance problems.
- Medium (40-69): Minor bugs, UI/UX issues.
- Low (0-39): Feature requests, documentation, refactoring.

ONBOARDING CONSIDERATIONS:
Each issue includes an "Onboarding Score" (0-100) in brackets. Higher scores indicate better suitability for newcomers:
- Score 80-100: Excellent for newcomers (small blast radius, isolated changes)
- Score 60-79: Good for newcomers (moderate complexity)
- Score 40-59: Moderate difficulty (requires some experience)
- Score 0-39: Advanced (high complexity, wide impact)

When scoring issues that appear to be minor bugs or feature requests (typically low priority), 
consider boosting their score slightly if they have high onboarding suitability (score > 70) 
as these make excellent "good first issues" for new contributors.

Analyze the following issues and provide ONLY the JSON array as your response.
--- START OF ISSUES ---
{issues_for_prompt}
--- END OF ISSUES ---
"""
    # --- THE CHANGE IS HERE ---
    llm_response_str = llm_service.generate_text(
        prompt,
        task_type=TaskType.COMPLEX_REASONING
    )

    try:
        cleaned_str = re.sub(r'<think>.*?</think>', '', llm_response_str, flags=re.DOTALL)
        json_str_match = re.search(r'\[.*\]', cleaned_str, re.DOTALL)
        if not json_str_match:
            raise json.JSONDecodeError("No JSON array found in the LLM's cleaned response.", llm_response_str, 0)
        prioritization_data = json.loads(json_str_match.group(0))
        priority_map = {item['issue_number']: item for item in prioritization_data}
    except (json.JSONDecodeError, KeyError) as e:
        print(f"Error parsing LLM response: {e}\nLLM Response was:\n{llm_response_str}")
        return {"error": "Failed to parse prioritization data from LLM.", "llm_response": llm_response_str}

    print("✓ LLM analysis complete.")

    # Step 4: Merge data and sort with enhanced metrics
    print("\n[Step 4/4] Finalizing report...")
    final_ranked_list = []
    for issue in enriched_issues:
        issue_number = issue['number']
        if issue_number in priority_map:
            issue.update(priority_map[issue_number])
            final_ranked_list.append(issue)
    
    final_ranked_list.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    # Enhanced summary with onboarding metrics
    onboarding_suitable = [issue for issue in final_ranked_list 
                          if issue.get('onboarding_suitability_score', 0) > 70]
    
    summary = f"Analyzed {len(final_ranked_list)} open issues. Found {len(onboarding_suitable)} issues suitable for newcomers."
    
    for i, issue in enumerate(final_ranked_list):
        issue['rank'] = i + 1
        
    return {
        "repository": repo_full_name,
        "analysis_summary": summary,
        "prioritized_issues": final_ranked_list,
        "onboarding_suitable_count": len(onboarding_suitable),
        "total_issues_analyzed": len(final_ranked_list)
    }


def _analyze_issue_blast_radius(repo_id: str, title: str, description: str) -> Dict[str, Any]:
    """
    Analyze the blast radius for a GitHub issue by using RAG search to find relevant files
    and then calculating the impact scope.
    
    Args:
        repo_id: Repository identifier
        title: Issue title
        description: Issue description
        
    Returns:
        Dictionary with blast radius analysis results
    """
    try:
        # Combine title and description for RAG search
        issue_text = f"{title}\n{description}"
        
        # Use RAG search to find top 3-5 relevant code chunks
        try:
            search_results = ollama.search_index(
                query_text=issue_text,
                model_name="snowflake-arctic-embed",  # Use default embedding model
                repo_id=repo_id,
                k=5  # Get top 5 relevant chunks
            )
            
            # Extract file paths from search results
            seed_files = []
            for result in search_results:
                # Extract file path from chunk_id (typically format: repo_id_filepath_chunk_index)
                chunk_id = result.get('chunk_id', '')
                if chunk_id:
                    # Remove repo_id prefix and chunk index suffix to get file path
                    parts = chunk_id.replace(f"{repo_id}_", "", 1).rsplit('_', 1)
                    if parts:
                        file_path = parts[0]
                        if file_path not in seed_files:
                            seed_files.append(file_path)
            
        except Exception as e:
            print(f"  ⚠️  RAG search failed for issue analysis: {e}")
            # Fallback: use empty seed files which will result in minimal blast radius
            seed_files = []
        
        # If we found relevant files, analyze blast radius
        if seed_files:
            blast_analysis = impact_analyzer.analyze_blast_radius(
                repo_id=repo_id,
                seed_files=seed_files[:3],  # Use top 3 files
                max_depth=2
            )
            
            blast_radius = blast_analysis.get('blast_radius', len(seed_files))
            onboarding_score = impact_analyzer.calculate_onboarding_suitability(blast_radius)
            
            return {
                'blast_radius': blast_radius,
                'onboarding_suitability_score': onboarding_score,
                'affected_files': blast_analysis.get('affected_nodes', []),
                'seed_files_found': seed_files,
                'analysis_method': 'rag_search'
            }
        else:
            # No relevant files found - assume minimal impact
            return {
                'blast_radius': 1,
                'onboarding_suitability_score': 95.0,  # High suitability for unknown scope
                'affected_files': [],
                'seed_files_found': [],
                'analysis_method': 'fallback_minimal'
            }
            
    except Exception as e:
        print(f"  ⚠️  Blast radius analysis failed: {e}")
        # Safe fallback
        return {
            'blast_radius': 10,  # Moderate assumption
            'onboarding_suitability_score': 50.0,  # Neutral score
            'affected_files': [],
            'seed_files_found': [],
            'analysis_method': 'error_fallback',
            'error': str(e)
        }

--- FILE_END: backend/lumiere_core/services/strategist.py ---

--- FILE_START: backend/lumiere_core/services/github.py ---
# In backend/lumiere_core/services/github.py

import os
import re
from datetime import datetime, timezone
from typing import Dict, Optional, Tuple, List, Any
from github import Github, GithubException, PaginatedList
from dotenv import load_dotenv

load_dotenv()

GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    print("WARNING: GITHUB_ACCESS_TOKEN not found. API calls will be heavily rate-limited.")
    g = Github()
else:
    g = Github(GITHUB_TOKEN)


def _paginated_to_list(paginated_list: PaginatedList, max_items: int = 10) -> List[Dict[str, Any]]:
    items = []
    for i, item in enumerate(paginated_list):
        if i >= max_items:
            break
        item_data = {
            "name": item.name,
            "full_name": item.full_name,
            "description": item.description,
            "html_url": item.html_url,
            "language": item.language,
            "stargazers_count": item.stargazers_count
        }
        items.append(item_data)
    return items


def get_user_profile(username: str) -> Optional[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return {
            "login": user.login, "name": user.name, "bio": user.bio,
            "html_url": user.html_url, "public_repos": user.public_repos,
            "followers": user.followers, "following": user.following,
        }
    except GithubException:
        return None

def get_user_repos(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_repos(sort='updated'), max_items=10)
    except GithubException:
        return []

def get_user_starred(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_starred(), max_items=10)
    except GithubException:
        return []

def get_user_comment_threads(username: str) -> List[Dict[str, Any]]:
    threads = []
    try:
        user = g.get_user(username)
        events = user.get_events()
        # Increase check limit to ensure we find comment events
        max_events_to_check = 50
        comment_events_found = 0
        max_comments_to_process = 5

        for i, event in enumerate(events):
            if i >= max_events_to_check or comment_events_found >= max_comments_to_process:
                break

            if event.type in ['IssueCommentEvent', 'PullRequestReviewCommentEvent']:
                payload = event.payload
                comment_data = payload.get('comment')
                issue_data = payload.get('issue', payload.get('pull_request'))

                if not comment_data or not issue_data or comment_data['user']['login'] != username:
                    continue

                comment_events_found += 1
                repo_name, issue_number = event.repo.name, issue_data['number']

                try:
                    repo_obj = g.get_repo(repo_name)
                    issue_obj = repo_obj.get_issue(number=issue_number)

                    created_at_str = comment_data.get('created_at')
                    if not created_at_str: continue

                    # Correctly parse the ISO 8601 string into a timezone-aware datetime object
                    created_at_dt = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))

                    user_comment = {"id": comment_data['id'], "body": comment_data['body'], "html_url": comment_data['html_url']}

                    replies = []
                    # Fetch comments created *after* the user's comment
                    for reply_comment in issue_obj.get_comments(since=created_at_dt):
                        if reply_comment.user.login != username and reply_comment.id != user_comment['id']:
                            replies.append({"user": reply_comment.user.login, "body": reply_comment.body, "html_url": reply_comment.html_url})

                    threads.append({
                        "repo_name": repo_name, "issue_number": issue_number, "issue_title": issue_data['title'],
                        "issue_url": issue_data['html_url'], "user_comment": user_comment, "replies": replies
                    })
                except GithubException as ge:
                    print(f"Warning: Could not fully process event for {repo_name}#{issue_number}. Skipping. Reason: {ge}")
                    continue
        return threads
    except GithubException as e:
        print(f"GitHub API Error while fetching comment threads: {e}")
        return []

def _parse_github_issue_url(issue_url: str) -> Optional[Tuple[str, str, int]]:
    match = re.match(r"https://github\.com/([^/]+)/([^/]+)/(?:issues|pull)/(\d+)", issue_url)
    if match:
        owner, repo_name, issue_number_str = match.groups()
        return owner, repo_name, int(issue_number_str)
    return None

def scrape_github_issue(issue_url: str) -> Optional[Dict[str, str]]:
    print(f"Fetching GitHub issue via API: {issue_url}")
    parsed_url = _parse_github_issue_url(issue_url)
    if not parsed_url:
        print(f"Error: Could not parse GitHub issue URL: {issue_url}")
        return None
    owner, repo_name, issue_number = parsed_url
    repo_full_name = f"{owner}/{repo_name}"
    try:
        repo = g.get_repo(repo_full_name)
        issue = repo.get_issue(number=issue_number)
        title, description = issue.title, issue.body if issue.body else ""
        full_text_query, repo_url = f"Issue Title: {title}\n\nDescription:\n{description}", f"https://github.com/{owner}/{repo_name}"
        return {"title": title, "description": description, "full_text_query": full_text_query, "repo_url": repo_url}
    except GithubException as e:
        print(f"GitHub API Error: {e.status}, {e.data}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during GitHub API call: {e}")
        return None

def list_open_issues(repo_full_name: str) -> List[Dict[str, Any]]:
    """
    Fetches a list of all open issues for a given repository.
    """
    print(f"Fetching open issues for repository: {repo_full_name}")
    try:
        repo = g.get_repo(repo_full_name)
        open_issues = repo.get_issues(state='open')
        issues_list = []
        for issue in open_issues:
            if not issue.pull_request:
                issues_list.append({
                    "number": issue.number,
                    "title": issue.title,
                    "url": issue.html_url,
                    "author": issue.user.login,
                })
        return issues_list
    except GithubException as e:
        print(f"GitHub API Error while listing issues: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/github.py ---

--- FILE_START: backend/lumiere_core/services/testing.py ---
# backend/lumiere_core/services/testing.py
import logging
from typing import Dict, List, Optional, Any, Set
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output

# Set up logging for better debugging
logger = logging.getLogger(__name__)

def generate_tests_for_code(repo_id: str, new_code: str, instruction: str) -> Dict[str, str]:
    """
    The core logic for the Test Generation Agent.
    It finds existing test files in the repository to learn the project's testing
    style and then generates a new test function consistent with that style.

    Args:
        repo_id: The unique identifier for the repository.
        new_code: The new code for which tests need to be generated.
        instruction: A natural language description of the code's purpose.

    Returns:
        A dictionary containing the generated test code or an error message.
        Format: {"generated_tests": str, "error": str (optional)}
    """
    logger.info(f"Initiating Test Generation Agent for repo '{repo_id}'")

    # Input validation
    if not all([repo_id, new_code]):
        error_msg = "Missing required parameters: repo_id and new_code."
        logger.error(error_msg)
        return {"generated_tests": "", "error": error_msg}

    if not instruction:
        logger.warning("No instruction provided, using default description")
        instruction = "Generate appropriate tests for the given code"

    try:
        # --- Step 1: Find existing test patterns with RAG ---
        logger.info("Step 1: Finding existing test patterns with RAG...")
        test_context_string = _find_existing_test_patterns(repo_id, instruction)

        # --- Step 2: Construct the Reinforced Prompt ---
        logger.info("Step 2: Constructing reinforced test generation prompt...")
        prompt = _build_test_generation_prompt(test_context_string, new_code)

        # --- Step 3: Generate the Test Code ---
        logger.info("Step 3: Generating test code...")
        raw_generated_tests = _generate_test_code(prompt)

        # --- Step 4: Clean and validate the output ---
        logger.info("Step 4: Cleaning and validating the generated test code...")
        final_tests = clean_llm_code_output(raw_generated_tests)

        # Validate the generated tests
        validation_result = validate_test_code(final_tests)
        if not validation_result["is_valid"]:
            logger.warning(f"Generated test code has validation issues: {validation_result['syntax_errors']}")
            # Still return the code but include warning in logs

        logger.info("✓ Test generation completed successfully.")
        return {"generated_tests": final_tests}

    except Exception as e:
        error_msg = f"An unexpected error occurred during test generation: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {"generated_tests": "", "error": error_msg}


def _find_existing_test_patterns(repo_id: str, instruction: str) -> str:
    """
    Find existing test patterns using RAG search.

    Args:
        repo_id: Repository identifier
        instruction: Code description for search context

    Returns:
        String containing formatted test examples or default message
    """
    search_query = f"Example test cases for python code like this: {instruction}"

    try:
        # CORRECTED CALL: Pass repo_id directly to the centralized search function.
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=7  # Look for up to 7 relevant chunks
        )
    except Exception as e:
        logger.warning(f"RAG search failed for test generation in repo '{repo_id}': {e}")
        context_chunks = []

    test_context_string = ""
    found_test_files: Set[str] = set()

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        chunk_text = chunk.get('text', '')

        # Heuristic to identify test files and avoid duplicates
        if (_is_test_file(file_path) and
            file_path not in found_test_files and
            chunk_text and
            len(chunk_text.strip()) > 10):  # Ensure meaningful content

            test_context_string += f"--- Example test from file `{file_path}` ---\n```python\n{chunk_text}\n```\n\n"
            found_test_files.add(file_path)

    if not test_context_string:
        test_context_string = "No specific test patterns found. Please generate a standard `pytest` function using `assert`."
        logger.info("Warning: No existing test files found via RAG. Will generate a generic test.")
    else:
        logger.info(f"Found test patterns from files: {list(found_test_files)}")

    return test_context_string


def _is_test_file(file_path: str) -> bool:
    """Check if a file path indicates a test file."""
    if not file_path:
        return False

    file_path_lower = file_path.lower()
    return (
        'test' in file_path_lower or
        file_path_lower.endswith('_test.py') or
        file_path_lower.startswith('test_') or
        '/tests/' in file_path_lower
    )


def _build_test_generation_prompt(test_context_string: str, new_code: str) -> str:
    """
    Build the prompt for test generation.

    Args:
        test_context_string: Existing test examples
        new_code: Code to generate tests for

    Returns:
        Formatted prompt string
    """
    return f"""You are an expert QA Engineer and Python programmer. Your task is to write a new unit test for a piece of code.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING TEST EXAMPLES"** to understand the project's testing style. Pay close attention to:
   * Imports (e.g., `unittest`, `pytest`).
   * Structure: Are tests inside a `class`?
   * Assertions: Do they use `self.assertEqual` or plain `assert`?
   * Setup: Are there fixtures or `setUp` methods?

2. **Analyze the "NEW CODE TO BE TESTED"** to understand its functionality.

3. **Write a complete test function.** It is CRITICAL that you exactly match the style of the examples.
   If the examples are standalone functions (e.g., `def test_...():`), your test MUST also be a standalone function.
   DO NOT invent a class if the examples do not use one.

4. **Output ONLY raw Python code.** Do not include any explanations, commentary, or Markdown fences like ```python.

---
### EXISTING TEST EXAMPLES
{test_context_string}

---
### NEW CODE TO BE TESTED
```python
{new_code}
```

Now, generate ONLY the new, stylistically-consistent test function."""


def _generate_test_code(prompt: str) -> str:
    """
    Generate test code using the LLM service.

    Args:
        prompt: The formatted prompt for test generation

    Returns:
        Generated test code

    Raises:
        Exception: If LLM generation fails
    """
    model_to_use = "ollama/qwen2.5-coder:3b"
    logger.info(f"Sending request to code generation model '{model_to_use}'...")

    try:
        # Use the master LLM service and pass the full model identifier
        raw_generated_tests = llm_service.generate_text(prompt, model_identifier=model_to_use)

        if not raw_generated_tests or not raw_generated_tests.strip():
            raise ValueError("LLM returned empty response")

        return raw_generated_tests

    except Exception as e:
        logger.error(f"LLM generation failed for tests: {e}")
        raise Exception(f"LLM generation failed: {str(e)}")


def validate_test_code(test_code: str) -> Dict[str, Any]:
    """
    Validates the generated test code for basic syntax and structure.

    Args:
        test_code: The generated test code to validate

    Returns:
        Dictionary with validation results containing:
        - is_valid: bool
        - has_test_function: bool
        - syntax_errors: List[str]
    """
    validation_result = {
        "is_valid": False,
        "has_test_function": False,
        "syntax_errors": [],
    }

    if not test_code or not test_code.strip():
        validation_result["syntax_errors"].append("Test code is empty")
        return validation_result

    # Check for test function presence
    if "def test_" in test_code:
        validation_result["has_test_function"] = True
    else:
        validation_result["syntax_errors"].append("No test function found (should start with 'def test_')")

    # Validate syntax
    try:
        compile(test_code, '<string>', 'exec')
        validation_result["is_valid"] = True
    except SyntaxError as e:
        error_msg = f"Syntax error: {e.msg}"
        if e.lineno:
            error_msg += f" on line {e.lineno}"
        validation_result["syntax_errors"].append(error_msg)
    except Exception as e:
        validation_result["syntax_errors"].append(f"Unexpected compilation error: {str(e)}")

    return validation_result


def generate_test_suggestions(code_snippet: str) -> List[str]:
    """
    Generates high-level suggestions for what types of tests should be written.

    Args:
        code_snippet: The code to analyze for test suggestions

    Returns:
        List of test case suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return ["No code provided for analysis"]

    suggestions = []
    code_lower = code_snippet.lower()

    # Function-based suggestions
    if "def " in code_lower:
        suggestions.extend([
            "Test the happy path with valid inputs",
            "Test edge cases (e.g., empty strings, zero, None)",
            "Test with invalid inputs to verify error handling"
        ])

    # Control flow suggestions
    if "if " in code_lower or "elif " in code_lower:
        suggestions.append("Ensure all conditional branches are tested")

    # Loop suggestions
    if "for " in code_lower or "while " in code_lower:
        suggestions.append("Test loop behavior (e.g., zero, one, and multiple iterations)")

    # Exception handling suggestions
    if "try:" in code_lower and "except" in code_lower:
        suggestions.extend([
            "Verify that expected exceptions are raised correctly",
            "Verify behavior when no exception occurs"
        ])

    # Class-based suggestions
    if "class " in code_lower:
        suggestions.extend([
            "Test object initialization",
            "Test all public methods",
            "Test method interactions and state changes"
        ])

    # Data structure suggestions
    if any(keyword in code_lower for keyword in ["list", "dict", "set", "tuple"]):
        suggestions.append("Test with different data structure sizes and types")

    # Return default suggestions if none found
    if not suggestions:
        suggestions = [
            "Test basic functionality",
            "Test edge cases",
            "Test error conditions"
        ]

    return suggestions


def get_test_coverage_suggestions(code_snippet: str) -> Dict[str, List[str]]:
    """
    Analyze code and provide comprehensive test coverage suggestions.

    Args:
        code_snippet: Code to analyze

    Returns:
        Dictionary categorizing different types of test suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return {"error": ["No code provided for analysis"]}

    suggestions = {
        "unit_tests": generate_test_suggestions(code_snippet),
        "integration_tests": [],
        "edge_cases": [],
        "performance_tests": []
    }

    code_lower = code_snippet.lower()

    # Integration test suggestions
    if any(keyword in code_lower for keyword in ["import", "from", "api", "database", "http"]):
        suggestions["integration_tests"].extend([
            "Test external API interactions",
            "Test database operations if applicable",
            "Test module integration"
        ])

    # Edge case suggestions
    suggestions["edge_cases"].extend([
        "Test with None values",
        "Test with empty collections",
        "Test with maximum/minimum values",
        "Test with malformed input"
    ])

    # Performance test suggestions
    if any(keyword in code_lower for keyword in ["for", "while", "sort", "search"]):
        suggestions["performance_tests"].extend([
            "Test with large datasets",
            "Test execution time constraints",
            "Test memory usage"
        ])

    return suggestions

--- FILE_END: backend/lumiere_core/services/testing.py ---

--- FILE_START: backend/lumiere_core/services/profile_service.py ---
# In backend/lumiere_core/services/profile_service.py
from typing import Dict, Any
from . import github
from . import llm_service

def _format_data_for_llm(profile_data: Dict[str, Any]) -> str:
    """Formats the aggregated GitHub data into a text block for the LLM."""

    user = profile_data['user_profile']
    text = f"""
# GitHub User Profile Analysis for: {user['login']} ({user.get('name', 'N/A')})
Bio: {user.get('bio', 'N/A')}
Followers: {user.get('followers', 0)} | Following: {user.get('following', 0)}
Public Repos: {user.get('public_repos', 0)}

---
## Owned Repositories (Sample)
"""
    if profile_data['repositories']:
        for repo in profile_data['repositories'][:5]:
            text += f"- **{repo['name']}**: {repo.get('language', 'N/A')} | ☆{repo['stargazers_count']} | {repo.get('description', 'No description')}\n"
    else:
        text += "No public repositories found.\n"

    text += "\n---"
    text += "\n## Starred Repositories (Sample)\n"
    if profile_data['starred_repositories']:
        for repo in profile_data['starred_repositories'][:5]:
             text += f"- **{repo['full_name']}**: {repo.get('description', 'No description')}\n"
    else:
        text += "No starred repositories found.\n"

    text += "\n---"
    text += f"\n## Recent Issue/PR Comments & Replies by {user['login']}\n"
    if profile_data['comment_threads']:
        for thread in profile_data['comment_threads']:
            text += f"\nOn repo `{thread['repo_name']}` (Issue/PR #{thread['issue_number']}):\n"
            text += f"  - **Their Comment**: \"{thread['user_comment']['body']}\"\n"
            if thread['replies']:
                for reply in thread['replies']:
                    text += f"    - **Reply from {reply['user']}**: \"{reply['body']}\"\n"
            else:
                text += "    - No replies to this comment found.\n"
    else:
        text += "No recent comments found.\n"

    return text

def generate_profile_review(username: str, model_identifier: str) -> Dict[str, Any]:
    """
    The core logic for the Chronicler Agent.
    Fetches a user's GitHub activity and generates a narrative summary.
    """
    print(f"Initiating Chronicler Agent for user '{username}'")
    print(f"Using model: {model_identifier}")

    print("   -> Step 1: Fetching profile data from GitHub API...")
    user_profile = github.get_user_profile(username)
    if not user_profile:
        raise FileNotFoundError(f"User '{username}' not found on GitHub.")

    repositories = github.get_user_repos(username)
    starred = github.get_user_starred(username)
    comment_threads = github.get_user_comment_threads(username)

    raw_data = {
        "user_profile": user_profile, "repositories": repositories,
        "starred_repositories": starred, "comment_threads": comment_threads,
    }

    print("   -> Step 2: Formatting data and constructing FINAL prompt for LLM...")
    context_string = _format_data_for_llm(raw_data)

    prompt = f"""You are an expert GitHub profile analyst. Your task is to analyze the user '{username}' based ONLY on the provided data.

**CRITICAL INSTRUCTION: Your entire analysis MUST be about the user '{username}'. Do NOT summarize the technical problems in the comments. Instead, use the comments to understand the USER'S BEHAVIOR.**

Generate a "Developer Profile Briefing" in Markdown with these exact sections:

### 1. Identity & Technical Focus
*   Based on their bio, owned repos, and starred repos, what are '{username}'s primary technical interests?
*   What are their main programming languages? (e.g., JavaScript, C++, Python)

### 2. Community Engagement Style
*   Based on their comments, what is '{username}'s role in the community? Are they reporting bugs, asking for help, or providing solutions?
*   Analyze the tone and content of THEIR comments. For example: `The user provides detailed debugging reports ("Debugging Report: itzzzme/anime-api Integration Issues") suggesting a methodical approach to problem-solving.`

### 3. Community Reception
*   Look at the replies to '{username}'s comments. Are others engaging with them? Are they receiving help and feedback?
*   Briefly summarize the nature of the replies they receive (e.g., "The user receives helpful replies from other developers, who offer suggestions and updated decryption keys.").

---
### RAW GITHUB DATA FOR {username}
{context_string}
---

Now, generate the Developer Profile Briefing about the user '{username}'.
"""

    print("   -> Step 3: Sending request to LLM for narrative generation...")
    summary = llm_service.generate_text(prompt, model_identifier=model_identifier)

    final_response = { "profile_summary": summary, "raw_data": raw_data }

    return final_response

--- FILE_END: backend/lumiere_core/services/profile_service.py ---

--- FILE_START: backend/lumiere_core/services/ollama.py ---
# In lumiere_core/services/ollama.py

import ollama
from tqdm import tqdm
from typing import List
import faiss
import numpy as np
import json
from pathlib import Path # <--- ADD THIS

def get_ollama_embeddings(chunks: List[str], model_name: str) -> List[List[float]]:
    """
    Generates embeddings for a list of text chunks using a local Ollama model.

    Args:
        chunks: A list of strings to be embedded.
        model_name: The name of the Ollama model to use (e.g., 'snowflake-arctic-embed').

    Returns:
        A list of embeddings, where each embedding is a list of floats.
    """
    embeddings = []
    # The ollama client automatically connects to http://localhost:11434
    client = ollama.Client()

    # Show a progress bar because this can take time
    for text in tqdm(chunks, desc="Generating Ollama Embeddings"):
        response = client.embeddings(model=model_name, prompt=text)
        embeddings.append(response['embedding'])

    return embeddings

def search_index(
    query_text: str,
    model_name: str,
    repo_id: str, # <--- MODIFIED: Take repo_id directly
    k: int = 10,
    **kwargs # <--- MODIFIED: Accept and ignore old path args for compatibility
) -> List[dict]:
    """
    Searches the Faiss index for the top k most similar chunks to a query for a given repo_id.

    Args:
        query_text: The user's search query.
        model_name: The name of the Ollama model used to create the index.
        repo_id: The unique ID of the repository whose index should be searched.
        k: The number of results to return.

    Returns:
        A list of dictionaries, where each dictionary contains the chunk_id,
        file_path, and the original text of the matching chunk.
    """
    # --- THIS IS THE FIX ---
    # Centralize path construction based on repo_id.
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_dir = backend_dir / "cloned_repositories" / repo_id
    index_path = artifacts_dir / f"{repo_id}_faiss.index"
    map_path = artifacts_dir / f"{repo_id}_id_map.json"

    print(f"Loading index '{index_path}' and map '{map_path}'...")
    # Load the Faiss index
    index = faiss.read_index(str(index_path))

    # Load the ID mapping files
    with open(map_path, 'r', encoding='utf-8') as f:
        id_maps = json.load(f)
    faiss_id_to_chunk_id = id_maps['faiss_id_to_chunk_id']
    chunk_id_to_data = id_maps['chunk_id_to_data']

    print(f"Generating embedding for query: '{query_text}'...")
    # 1. Embed the query using the same Ollama model
    client = ollama.Client()
    response = client.embeddings(model=model_name, prompt=query_text)
    query_vector = np.array([response['embedding']]).astype('float32')

    print(f"Searching index for top {k} results...")
    # 2. Search the Faiss index
    distances, indices = index.search(query_vector, k)

    # 3. Retrieve the results
    results = []
    for i in range(min(k, len(indices[0]))): # Ensure we don't go out of bounds
        faiss_id = indices[0][i]
        chunk_id = faiss_id_to_chunk_id[faiss_id]
        chunk_data = chunk_id_to_data[chunk_id]

        results.append({
            "chunk_id": chunk_id,
            "file_path": chunk_data['file_path'],
            "text": chunk_data['text'],
            "distance": float(distances[0][i])
        })

    return results

--- FILE_END: backend/lumiere_core/services/ollama.py ---

--- FILE_START: backend/lumiere_core/services/ingestion_service.py ---
# backend/lumiere_core/services/ingestion_service.py

import json
import traceback
import os
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from contextlib import contextmanager
from datetime import datetime

from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier
from ingestion.indexing import EmbeddingIndexer
from . import sentinel_service

# BOM parser
from .bom_parser import parse_all_manifests, EnumEncoder

# Configure logging
logger = logging.getLogger(__name__)

class IngestionError(Exception):
    """Custom exception for ingestion-related errors."""
    pass

class RepositoryValidationError(IngestionError):
    """Raised when repository validation fails."""
    pass

def validate_repo_url(repo_url: str) -> bool:
    """
    Validate that the repository URL is a valid GitHub URL.

    Args:
        repo_url: The repository URL to validate

    Returns:
        bool: True if valid, False otherwise

    Raises:
        RepositoryValidationError: If URL is invalid
    """
    if not repo_url or not isinstance(repo_url, str):
        raise RepositoryValidationError("Repository URL must be a non-empty string")

    github_pattern = r'^https://github\.com/[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+/?$'
    if not re.match(github_pattern, repo_url.rstrip('/')):
        raise RepositoryValidationError(f"Invalid GitHub URL format: {repo_url}")

    return True

def generate_repo_id(repo_url: str, max_length: int = 100) -> str:
    """
    Generate a safe repository ID from a GitHub URL with intelligent truncation.

    Args:
        repo_url: The GitHub repository URL
        max_length: Maximum length for the generated ID

    Returns:
        str: Safe repository identifier

    Raises:
        RepositoryValidationError: If URL is invalid
    """
    validate_repo_url(repo_url)

    # Extract the repo path (owner/repo-name)
    repo_path = repo_url.replace("https://github.com/", "").replace("/", "_").rstrip("/")

    # Remove any potentially dangerous characters
    repo_path = re.sub(r'[^a-zA-Z0-9_.-]', '_', repo_path)

    # If it's already within limits, return as-is (backward compatibility)
    if len(repo_path) <= max_length:
        return repo_path

    # Split into owner and repo name for intelligent truncation
    original_path = repo_url.replace("https://github.com/", "").rstrip("/")
    parts = original_path.split("/", 1)

    if len(parts) != 2:
        # Fallback: just truncate the whole thing
        return repo_path[:max_length]

    owner, repo_name = parts
    # Ensure we don't end with an underscore and leave room for separator
    truncated_owner = owner[:40].rstrip('_')
    remaining_length = max_length - len(truncated_owner) - 1  # -1 for separator
    truncated_repo = repo_name[:remaining_length].rstrip('_')

    return f"{truncated_owner}_{truncated_repo}"

def _load_existing_metrics(metrics_path: Path) -> list:
    """
    Load existing metrics from file with error handling.

    Args:
        metrics_path: Path to the metrics file

    Returns:
        list: Existing metrics or empty list if file doesn't exist or is corrupted
    """
    if not metrics_path.exists():
        return []

    try:
        with open(metrics_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"Failed to load existing metrics from {metrics_path}: {e}")
        return []

def _save_metrics(metrics_path: Path, metrics_data: list) -> None:
    """
    Save metrics to file with error handling.

    Args:
        metrics_path: Path to save metrics
        metrics_data: Metrics data to save

    Raises:
        IngestionError: If saving fails
    """
    try:
        # Ensure directory exists
        metrics_path.parent.mkdir(parents=True, exist_ok=True)

        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, indent=2)
    except IOError as e:
        raise IngestionError(f"Failed to save metrics: {e}")

def _save_cortex_file(output_path: Path, project_cortex: Dict[str, Any]) -> None:
    """
    Save project cortex to file with error handling.

    Args:
        output_path: Path to save the cortex file
        project_cortex: The cortex data to save

    Raises:
        IngestionError: If saving fails
    """
    try:
        # Ensure directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(project_cortex, f, indent=2, cls=EnumEncoder)
    except IOError as e:
        raise IngestionError(f"Failed to save cortex file: {e}")

@contextmanager
def _cleanup_on_failure(output_cortex_path: Path):
    """
    Context manager to cleanup cortex file on failure.

    Args:
        output_cortex_path: Path to the cortex file to cleanup on failure
    """
    try:
        yield
    except Exception:
        if output_cortex_path.exists():
            try:
                os.remove(output_cortex_path)
                logger.info(f"Cleaned up cortex file: {output_cortex_path}")
            except OSError as e:
                logger.warning(f"Failed to cleanup cortex file {output_cortex_path}: {e}")
        raise

def _setup_directories(repo_id: str) -> Tuple[Path, Path, Path]:
    """
    Setup required directories for ingestion.

    Args:
        repo_id: Repository identifier

    Returns:
        Tuple of (repo_output_dir, output_cortex_path, metrics_path)
    """
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_base_dir = backend_dir / "cloned_repositories"
    repo_output_dir = artifacts_base_dir / repo_id
    repo_output_dir.mkdir(parents=True, exist_ok=True)

    output_cortex_path = repo_output_dir / f"{repo_id}_cortex.json"
    metrics_path = repo_output_dir / "metrics.json"

    return repo_output_dir, output_cortex_path, metrics_path

def clone_and_embed_repository(
    repo_url: str,
    embedding_model: str = 'snowflake-arctic-embed2:latest',
    skip_indexing: bool = False
) -> Dict[str, Any]:
    """
    Orchestrates the entire ingestion pipeline, including the Sentinel metrics capture.

    Args:
        repo_url: The GitHub repository URL to process
        embedding_model: The embedding model to use for vector indexing
        skip_indexing: If True, skip the vector indexing step

    Returns:
        Dict containing the result status and metadata

    Raises:
        RepositoryValidationError: If repository URL is invalid
        IngestionError: If ingestion process fails
    """
    start_time = datetime.now()

    try:
        # Validate inputs
        validate_repo_url(repo_url)
        if not embedding_model or not isinstance(embedding_model, str):
            raise IngestionError("Embedding model must be a non-empty string")

        repo_id = generate_repo_id(repo_url)
        repo_output_dir, output_cortex_path, metrics_path = _setup_directories(repo_id)

        logger.info(f"Starting ingestion for {repo_id}")
        logger.info(f"Artifacts will be saved to: {repo_output_dir}")

        with _cleanup_on_failure(output_cortex_path):
            project_cortex = None

            # --- Step 1: Crawl & Jsonify ---
            logger.info("[1/4] Cloning repository and generating Project Cortex file...")

            with IntelligentCrawler(repo_url=repo_url) as crawler:
                files_to_process = crawler.get_file_paths()
                if not files_to_process:
                    raise IngestionError("No files found to process in the repository")

                jsonifier = Jsonifier(
                    file_paths=files_to_process,
                    repo_root=crawler.repo_path,
                    repo_id=repo_id
                )
                # The generate_cortex function now returns a complete dictionary, ready for JSON
                project_cortex = jsonifier.generate_cortex()

                if not project_cortex:
                    raise IngestionError("Failed to generate project cortex")

                # --- Step 2: Generate Bill of Materials (BOM) ---
                logger.info("Generating Tech Stack Bill of Materials...")
                try:
                    tech_stack_bom = parse_all_manifests(crawler.repo_path)
                    if tech_stack_bom:
                        project_cortex["tech_stack_bom"] = tech_stack_bom.to_dict()
                        logger.info(f"✓ BOM Generated. Found {len(tech_stack_bom.dependencies['application'])} application dependencies.")
                    else:
                        logger.warning("BOM Generated. Found 0 total dependencies.")
                except Exception as e:
                    logger.warning(f"Failed to generate BOM: {e}")
                    # Continue with ingestion even if BOM fails

                logger.info(f"Project Cortex created successfully: {output_cortex_path}")

                # --- Step 3: Calculate Sentinel Metrics ---
                logger.info("[2/4] Sentinel: Calculating health metrics...")
                graph_data = project_cortex.get("architectural_graph", {})

                try:
                    latest_metrics = sentinel_service.calculate_snapshot_metrics(
                        crawler.repo_path, graph_data
                    )

                    # Add timestamp to metrics
                    latest_metrics['timestamp'] = start_time.isoformat()
                    latest_metrics['repo_id'] = repo_id

                    # Load existing metrics and append the new snapshot
                    historical_metrics = _load_existing_metrics(metrics_path)
                    historical_metrics.append(latest_metrics)

                    _save_metrics(metrics_path, historical_metrics)
                    logger.info("Sentinel: Health metrics saved successfully")

                except Exception as e:
                    logger.warning(f"Failed to calculate or save metrics: {e}")
                    # Continue with ingestion even if metrics fail

            # Save cortex file
            _save_cortex_file(output_cortex_path, project_cortex)

            # --- Step 3: Vector Indexing (Optional) ---
            if not skip_indexing:
                logger.info(f"[3/4] Starting vector indexing with model '{embedding_model}'...")
                try:
                    indexer = EmbeddingIndexer(model_name=embedding_model)
                    indexer.process_cortex(str(output_cortex_path))
                    logger.info("Vector indexing complete")
                except Exception as e:
                    logger.error(f"Vector indexing failed: {e}")
                    # Don't fail the entire process if indexing fails
                    raise IngestionError(f"Vector indexing failed: {e}")
            else:
                logger.info("[3/4] Skipping vector indexing as requested")

            logger.info("[4/4] Ingestion complete")

            processing_time = (datetime.now() - start_time).total_seconds()

            return {
                "status": "success",
                "message": f"Repository '{repo_id}' was successfully cloned, embedded, and indexed.",
                "repo_id": repo_id,
                "original_url": repo_url,
                "processing_time_seconds": processing_time,
                "files_processed": len(files_to_process) if 'files_to_process' in locals() else 0,
                "cortex_path": str(output_cortex_path),
                "metrics_path": str(metrics_path),
                "indexing_skipped": skip_indexing
            }

    except (RepositoryValidationError, IngestionError) as e:
        logger.error(f"Ingestion failed for {repo_url}: {e}")
        return {
            "status": "failed",
            "error": str(e),
            "repo_id": repo_id if 'repo_id' in locals() else None,
            "processing_time_seconds": (datetime.now() - start_time).total_seconds()
        }
    except Exception as e:
        logger.error(f"Unexpected error during ingestion for {repo_url}: {e}")
        logger.error(traceback.format_exc())
        return {
            "status": "failed",
            "error": str(e),
            "details": traceback.format_exc(),
            "repo_id": repo_id if 'repo_id' in locals() else None,
            "processing_time_seconds": (datetime.now() - start_time).total_seconds()
        }

# Backward compatibility alias
def clone_and_embed_repository_legacy(repo_url: str, embedding_model: str = 'snowflake-arctic-embed2:latest') -> Dict[str, Any]:
    """
    Legacy function signature for backward compatibility.
    """
    return clone_and_embed_repository(repo_url, embedding_model, skip_indexing=False)

--- FILE_END: backend/lumiere_core/services/ingestion_service.py ---

--- FILE_START: backend/lumiere_core/services/summoner_service.py ---
# backend/lumiere_core/services/summoner_service.py

import logging
import json
import re
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from collections import defaultdict

from . import llm_service, cortex_service, oracle_service, code_surgery
from .llm_service import TaskType

logger = logging.getLogger(__name__)


class SummonerService:
    """The Summoner - intelligent familiar that materializes entire code patterns."""
    
    def __init__(self):
        self.recipes = {
            "fastapi_endpoint": {
                "name": "FastAPI Endpoint",
                "description": "Create FastAPI endpoint with schemas, handlers, and tests",
                "files": ["schemas", "routes", "tests"],
                "parameters": ["path", "methods", "model_name"]
            },
            "django_rest_viewset": {
                "name": "Django REST ViewSet",
                "description": "Create Django REST framework ViewSet with serializers",
                "files": ["models", "serializers", "views", "urls", "tests"],
                "parameters": ["model_name", "fields"]
            },
            "react_component": {
                "name": "React Component",
                "description": "Create React component with hooks, styles, and tests",
                "files": ["component", "styles", "tests", "types"],
                "parameters": ["component_name", "props"]
            },
            "microservice": {
                "name": "Microservice Structure",
                "description": "Create complete microservice with API, models, and Docker",
                "files": ["api", "models", "config", "docker", "tests"],
                "parameters": ["service_name", "database"]
            },
            "database_model": {
                "name": "Database Model",
                "description": "Create database model with migrations and queries",
                "files": ["model", "migration", "repository", "tests"],
                "parameters": ["model_name", "fields", "relationships"]
            }
        }
        
        self.oracle = oracle_service.OracleService()
    
    def list_recipes(self) -> Dict[str, Dict[str, Any]]:
        """Get available summoning recipes."""
        return self.recipes
    
    def summon_pattern(self, repo_id: str, recipe_name: str, **parameters) -> Dict[str, Any]:
        """
        Summon a complete code pattern based on a recipe.
        
        Args:
            repo_id: Repository identifier
            recipe_name: Name of the recipe to summon
            **parameters: Recipe-specific parameters
            
        Returns:
            Dictionary with summoning result
        """
        try:
            if recipe_name not in self.recipes:
                return {"error": f"Unknown recipe: {recipe_name}. Available: {list(self.recipes.keys())}"}
            
            recipe = self.recipes[recipe_name]
            
            # Step 1: Learn project patterns
            logger.info(f"Learning project patterns for {repo_id}")
            pattern_context = self._learn_project_patterns(repo_id)
            
            if "error" in pattern_context:
                return pattern_context
            
            # Step 2: Generate surgical plan
            logger.info(f"Generating surgical plan for {recipe_name}")
            surgical_plan = self._generate_surgical_plan(recipe, pattern_context, parameters)
            
            if "error" in surgical_plan:
                return surgical_plan
            
            # Step 3: Return plan for approval
            return {
                "recipe_name": recipe_name,
                "recipe_description": recipe["description"],
                "pattern_context": pattern_context,
                "surgical_plan": surgical_plan,
                "ready_to_execute": True
            }
            
        except Exception as e:
            logger.error(f"Error summoning pattern {recipe_name}: {e}")
            return {"error": str(e)}
    
    def execute_summoning(self, repo_id: str, summoning_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the summoning plan using code surgery.
        
        Args:
            repo_id: Repository identifier
            summoning_result: Result from summon_pattern()
            
        Returns:
            Dictionary with execution result
        """
        try:
            if not summoning_result.get("ready_to_execute"):
                return {"error": "Summoning result is not ready for execution"}
            
            surgical_plan = summoning_result["surgical_plan"]
            operations = surgical_plan.get("operations", [])
            
            results = []
            created_files = []
            modified_files = []
            
            for operation in operations:
                op_type = operation.get("type")
                
                if op_type == "CREATE_FILE":
                    result = self._execute_create_file(repo_id, operation)
                    if result.get("success"):
                        created_files.append(operation["file_path"])
                    else:
                        results.append({"operation": operation, "error": result.get("error")})
                        continue
                
                elif op_type == "MODIFY_FILE":
                    result = self._execute_modify_file(repo_id, operation)
                    if result.get("success"):
                        modified_files.append(operation["file_path"])
                    else:
                        results.append({"operation": operation, "error": result.get("error")})
                        continue
                
                elif op_type == "INSERT_CODE":
                    result = self._execute_insert_code(repo_id, operation)
                    if result.get("success"):
                        modified_files.append(operation["file_path"])
                    else:
                        results.append({"operation": operation, "error": result.get("error")})
                        continue
                
                results.append({"operation": operation, "success": True})
            
            # Check if all operations succeeded
            all_success = all(r.get("success", False) for r in results)
            
            return {
                "success": all_success,
                "operations_completed": len([r for r in results if r.get("success")]),
                "total_operations": len(operations),
                "created_files": created_files,
                "modified_files": modified_files,
                "results": results,
                "recipe_name": summoning_result.get("recipe_name"),
                "message": f"Summoning {'completed successfully' if all_success else 'partially completed'}"
            }
            
        except Exception as e:
            logger.error(f"Error executing summoning: {e}")
            return {"error": str(e)}
    
    def _learn_project_patterns(self, repo_id: str) -> Dict[str, Any]:
        """
        Learn the project's architectural patterns and conventions.
        """
        try:
            # Load cortex data
            cortex_data = cortex_service.load_cortex_data(repo_id)
            files_data = cortex_data.get("files", [])
            
            if not files_data:
                return {"error": "No files found in repository cortex data"}
            
            # Analyze different aspects of the project
            patterns = {
                "framework": self._detect_framework(files_data),
                "structure": self._analyze_project_structure(files_data),
                "naming_conventions": self._analyze_naming_conventions(files_data),
                "imports": self._analyze_import_patterns(files_data),
                "testing": self._analyze_testing_patterns(files_data),
                "configuration": self._analyze_config_patterns(files_data)
            }
            
            # Use LLM to enhance pattern analysis
            enhanced_patterns = self._enhance_patterns_with_llm(patterns, files_data[:10])  # Analyze sample files
            
            return {
                "patterns": enhanced_patterns,
                "file_count": len(files_data),
                "analysis_complete": True
            }
            
        except Exception as e:
            logger.error(f"Error learning project patterns: {e}")
            return {"error": str(e)}
    
    def _detect_framework(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Detect the main framework(s) used in the project."""
        framework_indicators = {
            "fastapi": ["from fastapi", "FastAPI", "APIRouter", "@app.route"],
            "django": ["from django", "django.contrib", "models.Model", "views.py"],
            "flask": ["from flask", "Flask", "@app.route", "request."],
            "react": ["import React", "useState", "useEffect", ".jsx", ".tsx"],
            "vue": ["vue", "createApp", ".vue"],
            "angular": ["@angular", "@Component", "angular.json"],
            "express": ["express", "app.get", "app.post", "middleware"],
            "spring": ["@SpringBootApplication", "@RestController", "@Service"],
            "laravel": ["<?php", "Illuminate\\", "artisan"]
        }
        
        detected = defaultdict(int)
        
        for file_data in files_data:
            content = file_data.get("raw_content", "")
            file_path = file_data.get("file_path", "")
            
            for framework, indicators in framework_indicators.items():
                for indicator in indicators:
                    if indicator in content or indicator in file_path:
                        detected[framework] += 1
        
        # Determine primary framework
        if detected:
            primary = max(detected, key=detected.get)
            return {
                "primary": primary,
                "confidence": detected[primary] / len(files_data),
                "all_detected": dict(detected)
            }
        
        return {"primary": "unknown", "confidence": 0, "all_detected": {}}
    
    def _analyze_project_structure(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Analyze the project's directory structure and organization."""
        directories = defaultdict(list)
        file_types = defaultdict(int)
        
        for file_data in files_data:
            file_path = file_data.get("file_path", "")
            path_obj = Path(file_path)
            
            # Track directories
            if len(path_obj.parts) > 1:
                directories[path_obj.parts[0]].append(str(path_obj))
            
            # Track file types
            file_types[path_obj.suffix] += 1
        
        # Common patterns
        common_dirs = {
            "src", "source", "app", "lib", "components", "services", 
            "models", "views", "controllers", "api", "routes",
            "tests", "test", "__tests__", "spec", "schemas",
            "config", "settings", "utils", "helpers", "static"
        }
        
        structure_patterns = {}
        for dir_name in directories:
            if dir_name.lower() in common_dirs:
                structure_patterns[dir_name] = {
                    "file_count": len(directories[dir_name]),
                    "purpose": self._infer_directory_purpose(dir_name, directories[dir_name])
                }
        
        return {
            "directories": dict(directories),
            "file_types": dict(file_types),
            "structure_patterns": structure_patterns,
            "depth": max(len(Path(f.get("file_path", "")).parts) for f in files_data)
        }
    
    def _analyze_naming_conventions(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Analyze naming conventions used in the project."""
        conventions = {
            "file_naming": {"snake_case": 0, "kebab_case": 0, "camelCase": 0, "PascalCase": 0},
            "function_naming": {"snake_case": 0, "camelCase": 0},
            "class_naming": {"PascalCase": 0, "snake_case": 0},
            "variable_naming": {"snake_case": 0, "camelCase": 0}
        }
        
        for file_data in files_data:
            file_path = file_data.get("file_path", "")
            content = file_data.get("raw_content", "")
            
            # Analyze file naming
            file_name = Path(file_path).stem
            if "_" in file_name and file_name.islower():
                conventions["file_naming"]["snake_case"] += 1
            elif "-" in file_name:
                conventions["file_naming"]["kebab_case"] += 1
            elif file_name[0].isupper():
                conventions["file_naming"]["PascalCase"] += 1
            elif file_name[0].islower() and any(c.isupper() for c in file_name):
                conventions["file_naming"]["camelCase"] += 1
            
            # Analyze code naming (simplified)
            # Function definitions
            func_matches = re.findall(r'def\s+([a-zA-Z_]\w*)', content)
            for func_name in func_matches:
                if "_" in func_name:
                    conventions["function_naming"]["snake_case"] += 1
                elif any(c.isupper() for c in func_name[1:]):
                    conventions["function_naming"]["camelCase"] += 1
            
            # Class definitions
            class_matches = re.findall(r'class\s+([a-zA-Z_]\w*)', content)
            for class_name in class_matches:
                if class_name[0].isupper():
                    conventions["class_naming"]["PascalCase"] += 1
                else:
                    conventions["class_naming"]["snake_case"] += 1
        
        # Determine dominant conventions
        dominant = {}
        for category, counts in conventions.items():
            if counts:
                dominant[category] = max(counts, key=counts.get)
            else:
                dominant[category] = "unknown"
        
        return {
            "conventions": conventions,
            "dominant": dominant
        }
    
    def _analyze_import_patterns(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Analyze import and dependency patterns."""
        import_patterns = {
            "relative_imports": 0,
            "absolute_imports": 0,
            "common_modules": defaultdict(int),
            "import_styles": defaultdict(int)
        }
        
        for file_data in files_data:
            content = file_data.get("raw_content", "")
            
            # Python imports
            import_matches = re.findall(r'^(from\s+[.\w]+\s+)?import\s+([.\w, ]+)', content, re.MULTILINE)
            for from_part, import_part in import_matches:
                if from_part and from_part.strip().startswith("from ."):
                    import_patterns["relative_imports"] += 1
                else:
                    import_patterns["absolute_imports"] += 1
                
                # Track common modules
                modules = [m.strip() for m in import_part.split(",")]
                for module in modules:
                    import_patterns["common_modules"][module.split(".")[0]] += 1
            
            # JavaScript/TypeScript imports
            js_imports = re.findall(r'import\s+.*?\s+from\s+[\'"]([^\'\"]+)[\'"]', content)
            for imp in js_imports:
                if imp.startswith("."):
                    import_patterns["relative_imports"] += 1
                else:
                    import_patterns["absolute_imports"] += 1
                    import_patterns["common_modules"][imp.split("/")[0]] += 1
        
        return {
            "patterns": dict(import_patterns),
            "top_modules": dict(sorted(import_patterns["common_modules"].items(), 
                                     key=lambda x: x[1], reverse=True)[:10])
        }
    
    def _analyze_testing_patterns(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Analyze testing patterns and conventions."""
        testing_patterns = {
            "test_frameworks": defaultdict(int),
            "test_file_patterns": [],
            "test_directories": set(),
            "assertion_styles": defaultdict(int)
        }
        
        test_indicators = {
            "pytest": ["import pytest", "def test_", "@pytest"],
            "unittest": ["import unittest", "unittest.TestCase", "def test"],
            "jest": ["describe(", "it(", "test(", "expect("],
            "mocha": ["describe(", "it(", "beforeEach("],
            "jasmine": ["describe(", "it(", "beforeEach(", "jasmine"]
        }
        
        for file_data in files_data:
            file_path = file_data.get("file_path", "")
            content = file_data.get("raw_content", "")
            
            # Check if it's a test file
            if any(pattern in file_path.lower() for pattern in ["test", "spec", "__tests__"]):
                testing_patterns["test_file_patterns"].append(file_path)
                
                # Check directory
                test_dir = Path(file_path).parts[0] if Path(file_path).parts else ""
                if "test" in test_dir.lower():
                    testing_patterns["test_directories"].add(test_dir)
                
                # Detect framework
                for framework, indicators in test_indicators.items():
                    for indicator in indicators:
                        if indicator in content:
                            testing_patterns["test_frameworks"][framework] += 1
        
        return {
            "frameworks": dict(testing_patterns["test_frameworks"]),
            "file_patterns": testing_patterns["test_file_patterns"],
            "directories": list(testing_patterns["test_directories"]),
            "test_file_count": len(testing_patterns["test_file_patterns"])
        }
    
    def _analyze_config_patterns(self, files_data: List[Dict]) -> Dict[str, Any]:
        """Analyze configuration patterns."""
        config_files = []
        config_patterns = {
            "environment_files": [],
            "package_managers": [],
            "build_tools": [],
            "docker": False
        }
        
        config_indicators = {
            "package.json": "npm",
            "yarn.lock": "yarn", 
            "requirements.txt": "pip",
            "Pipfile": "pipenv",
            "poetry.lock": "poetry",
            "Dockerfile": "docker",
            "docker-compose.yml": "docker-compose",
            ".env": "environment",
            "config.py": "python-config",
            "settings.py": "django-settings"
        }
        
        for file_data in files_data:
            file_path = file_data.get("file_path", "")
            file_name = Path(file_path).name
            
            if file_name in config_indicators:
                config_files.append(file_path)
                
                indicator_type = config_indicators[file_name]
                if indicator_type in ["npm", "yarn", "pip", "pipenv", "poetry"]:
                    config_patterns["package_managers"].append(indicator_type)
                elif indicator_type in ["docker", "docker-compose"]:
                    config_patterns["docker"] = True
                elif indicator_type == "environment":
                    config_patterns["environment_files"].append(file_path)
        
        return {
            "config_files": config_files,
            "patterns": config_patterns
        }
    
    def _infer_directory_purpose(self, dir_name: str, files: List[str]) -> str:
        """Infer the purpose of a directory based on its name and contents."""
        purpose_map = {
            "src": "source code",
            "app": "application code", 
            "api": "API endpoints",
            "models": "data models",
            "views": "view components/templates",
            "controllers": "controllers",
            "services": "business logic services",
            "components": "reusable components",
            "utils": "utility functions",
            "helpers": "helper functions",
            "tests": "test files",
            "config": "configuration files",
            "static": "static assets",
            "templates": "template files",
            "migrations": "database migrations"
        }
        
        return purpose_map.get(dir_name.lower(), "unknown purpose")
    
    def _enhance_patterns_with_llm(self, patterns: Dict[str, Any], sample_files: List[Dict]) -> Dict[str, Any]:
        """Use LLM to enhance pattern analysis with deeper insights."""
        try:
            # Prepare sample code for analysis
            code_samples = []
            for file_data in sample_files[:5]:  # Limit to avoid token limits
                file_path = file_data.get("file_path", "")
                content = file_data.get("raw_content", "")[:1000]  # Truncate long files
                
                code_samples.append(f"File: {file_path}\n{content}")
            
            prompt = f"""Analyze these code samples and existing pattern analysis to provide enhanced insights.

Existing Analysis:
{json.dumps(patterns, indent=2)}

Code Samples:
{'---'.join(code_samples)}

Provide enhanced insights in JSON format with these keys:
- architecture_style: (e.g., "MVC", "microservices", "layered")
- dependency_injection: boolean
- async_patterns: boolean  
- error_handling_style: (e.g., "exceptions", "result_types", "callbacks")
- api_style: (e.g., "REST", "GraphQL", "RPC")
- database_access: (e.g., "ORM", "raw_sql", "query_builder")
- recommended_conventions: object with naming, structure recommendations

JSON:"""

            response = llm_service.generate_text(prompt, task_type=TaskType.COMPLEX_REASONING)
            
            try:
                enhanced = json.loads(response.strip())
                patterns.update(enhanced)
            except json.JSONDecodeError:
                logger.warning("Failed to parse LLM enhancement response")
            
            return patterns
            
        except Exception as e:
            logger.warning(f"Failed to enhance patterns with LLM: {e}")
            return patterns
    
    def _generate_surgical_plan(self, recipe: Dict[str, Any], pattern_context: Dict[str, Any], 
                               parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a detailed surgical plan for the summoning."""
        try:
            recipe_name = recipe["name"]
            patterns = pattern_context.get("patterns", {})
            
            # Choose appropriate generator based on recipe
            if recipe_name == "FastAPI Endpoint":
                return self._generate_fastapi_plan(patterns, parameters)
            elif recipe_name == "Django REST ViewSet":
                return self._generate_django_plan(patterns, parameters)
            elif recipe_name == "React Component":
                return self._generate_react_plan(patterns, parameters)
            elif recipe_name == "Microservice Structure":
                return self._generate_microservice_plan(patterns, parameters)
            elif recipe_name == "Database Model":
                return self._generate_database_model_plan(patterns, parameters)
            else:
                return {"error": f"No plan generator for recipe: {recipe_name}"}
                
        except Exception as e:
            logger.error(f"Error generating surgical plan: {e}")
            return {"error": str(e)}
    
    def _generate_fastapi_plan(self, patterns: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan for FastAPI endpoint creation."""
        path = parameters.get("path", "/items")
        methods = parameters.get("methods", "get,post").split(",")
        model_name = parameters.get("model_name", "Item")
        
        # Determine file locations based on project structure
        structure = patterns.get("structure", {})
        directories = structure.get("structure_patterns", {})
        
        # Find appropriate directories
        schema_dir = "schemas" if "schemas" in directories else "app/schemas"
        routes_dir = "routes" if "routes" in directories else "app/routes"
        tests_dir = "tests" if "tests" in directories else "test"
        
        operations = []
        
        # 1. Create schema file
        schema_content = self._generate_fastapi_schema(model_name, patterns)
        operations.append({
            "type": "CREATE_FILE",
            "file_path": f"{schema_dir}/{model_name.lower()}_schemas.py",
            "content": schema_content,
            "description": f"Create Pydantic schemas for {model_name}"
        })
        
        # 2. Create or modify routes file
        route_content = self._generate_fastapi_routes(path, methods, model_name, patterns)
        route_file = f"{routes_dir}/{model_name.lower()}_routes.py"
        
        operations.append({
            "type": "CREATE_FILE",
            "file_path": route_file,
            "content": route_content,
            "description": f"Create API routes for {model_name}"
        })
        
        # 3. Create test file
        test_content = self._generate_fastapi_tests(path, methods, model_name, patterns)
        operations.append({
            "type": "CREATE_FILE", 
            "file_path": f"{tests_dir}/test_{model_name.lower()}_api.py",
            "content": test_content,
            "description": f"Create tests for {model_name} API"
        })
        
        # 4. Update main app file to include new router
        operations.append({
            "type": "INSERT_CODE",
            "file_path": "main.py",
            "target": "# Router includes",
            "content": f"from {routes_dir.replace('/', '.')} import {model_name.lower()}_routes\napp.include_router({model_name.lower()}_routes.router)",
            "description": "Add new router to main app"
        })
        
        return {
            "operations": operations,
            "summary": f"Create FastAPI endpoint for {model_name} at {path}",
            "files_created": len([op for op in operations if op["type"] == "CREATE_FILE"]),
            "files_modified": len([op for op in operations if op["type"] in ["MODIFY_FILE", "INSERT_CODE"]])
        }
    
    def _generate_fastapi_schema(self, model_name: str, patterns: Dict[str, Any]) -> str:
        """Generate FastAPI Pydantic schema content."""
        return f'''from pydantic import BaseModel
from typing import Optional
from datetime import datetime


class {model_name}Base(BaseModel):
    """Base schema for {model_name}."""
    name: str
    description: Optional[str] = None


class {model_name}Create({model_name}Base):
    """Schema for creating a new {model_name}."""
    pass


class {model_name}Update(BaseModel):
    """Schema for updating an existing {model_name}."""
    name: Optional[str] = None
    description: Optional[str] = None


class {model_name}InDB({model_name}Base):
    """Schema for {model_name} as stored in database."""
    id: int
    created_at: datetime
    updated_at: datetime

    class Config:
        orm_mode = True


class {model_name}Response({model_name}InDB):
    """Schema for {model_name} API responses."""
    pass
'''
    
    def _generate_fastapi_routes(self, path: str, methods: List[str], model_name: str, patterns: Dict[str, Any]) -> str:
        """Generate FastAPI routes content."""
        route_methods = []
        
        if "get" in [m.lower() for m in methods]:
            route_methods.append(f'''@router.get("{path}", response_model=List[{model_name}Response])
async def get_{model_name.lower()}s():
    """Get all {model_name.lower()}s."""
    # TODO: Implement database query
    return []


@router.get("{path}/{{item_id}}", response_model={model_name}Response)
async def get_{model_name.lower()}(item_id: int):
    """Get a specific {model_name.lower()} by ID."""
    # TODO: Implement database query
    pass''')
        
        if "post" in [m.lower() for m in methods]:
            route_methods.append(f'''@router.post("{path}", response_model={model_name}Response, status_code=201)
async def create_{model_name.lower()}(item: {model_name}Create):
    """Create a new {model_name.lower()}."""
    # TODO: Implement database creation
    pass''')
        
        if "put" in [m.lower() for m in methods]:
            route_methods.append(f'''@router.put("{path}/{{item_id}}", response_model={model_name}Response)
async def update_{model_name.lower()}(item_id: int, item: {model_name}Update):
    """Update an existing {model_name.lower()}."""
    # TODO: Implement database update
    pass''')
        
        if "delete" in [m.lower() for m in methods]:
            route_methods.append(f'''@router.delete("{path}/{{item_id}}", status_code=204)
async def delete_{model_name.lower()}(item_id: int):
    """Delete a {model_name.lower()}."""
    # TODO: Implement database deletion
    pass''')
        
        return f'''from fastapi import APIRouter, HTTPException, Depends
from typing import List

from .{model_name.lower()}_schemas import (
    {model_name}Create,
    {model_name}Update,
    {model_name}Response
)

router = APIRouter(prefix="{path.rstrip('/')}", tags=["{model_name.lower()}s"])


{chr(10).join(route_methods)}
'''
    
    def _generate_fastapi_tests(self, path: str, methods: List[str], model_name: str, patterns: Dict[str, Any]) -> str:
        """Generate FastAPI test content."""
        test_methods = []
        
        for method in methods:
            method_lower = method.lower()
            if method_lower == "get":
                test_methods.append(f'''def test_get_{model_name.lower()}s(client):
    """Test getting all {model_name.lower()}s."""
    response = client.get("{path}")
    assert response.status_code == 200
    assert isinstance(response.json(), list)


def test_get_{model_name.lower()}_by_id(client):
    """Test getting a {model_name.lower()} by ID."""
    # TODO: Create test data first
    response = client.get("{path}/1")
    # Update assertion based on expected behavior
    assert response.status_code in [200, 404]''')
            
            elif method_lower == "post":
                test_methods.append(f'''def test_create_{model_name.lower()}(client):
    """Test creating a new {model_name.lower()}."""
    test_data = {{
        "name": "Test {model_name}",
        "description": "A test {model_name.lower()}"
    }}
    response = client.post("{path}", json=test_data)
    assert response.status_code == 201
    data = response.json()
    assert data["name"] == test_data["name"]''')
        
        return f'''import pytest
from fastapi.testclient import TestClient


{chr(10).join(test_methods)}
'''
    
    def _generate_django_plan(self, patterns: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan for Django REST ViewSet creation."""
        # Simplified Django plan - would be expanded in full implementation
        return {
            "operations": [
                {
                    "type": "CREATE_FILE",
                    "file_path": "models.py",
                    "content": "# Django model placeholder",
                    "description": "Create Django model"
                }
            ],
            "summary": "Create Django REST ViewSet",
            "files_created": 1,
            "files_modified": 0
        }
    
    def _generate_react_plan(self, patterns: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan for React component creation."""
        # Simplified React plan - would be expanded in full implementation
        return {
            "operations": [
                {
                    "type": "CREATE_FILE",
                    "file_path": "Component.tsx",
                    "content": "// React component placeholder",
                    "description": "Create React component"
                }
            ],
            "summary": "Create React component",
            "files_created": 1,
            "files_modified": 0
        }
    
    def _generate_microservice_plan(self, patterns: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan for microservice structure creation."""
        # Simplified microservice plan - would be expanded in full implementation
        return {
            "operations": [
                {
                    "type": "CREATE_FILE",
                    "file_path": "service.py",
                    "content": "# Microservice placeholder",
                    "description": "Create microservice structure"
                }
            ],
            "summary": "Create microservice structure",
            "files_created": 1,
            "files_modified": 0
        }
    
    def _generate_database_model_plan(self, patterns: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate plan for database model creation."""
        # Simplified database model plan - would be expanded in full implementation
        return {
            "operations": [
                {
                    "type": "CREATE_FILE",
                    "file_path": "models.py",
                    "content": "# Database model placeholder",
                    "description": "Create database model"
                }
            ],
            "summary": "Create database model",
            "files_created": 1,
            "files_modified": 0
        }
    
    def _execute_create_file(self, repo_id: str, operation: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a CREATE_FILE operation."""
        try:
            file_path = operation["file_path"]
            content = operation["content"]
            
            # Use code surgery to create the file
            result = code_surgery.create_file(repo_id, file_path, content)
            
            return {"success": result.get("success", False), "error": result.get("error")}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _execute_modify_file(self, repo_id: str, operation: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a MODIFY_FILE operation."""
        try:
            file_path = operation["file_path"]
            target = operation["target"]
            new_content = operation["content"]
            
            # Use code surgery to modify the file
            result = code_surgery.replace_block(repo_id, file_path, target, new_content)
            
            return {"success": result.get("success", False), "error": result.get("error")}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _execute_insert_code(self, repo_id: str, operation: Dict[str, Any]) -> Dict[str, Any]:
        """Execute an INSERT_CODE operation."""
        try:
            file_path = operation["file_path"]
            target = operation.get("target", "")
            content = operation["content"]
            
            # Use code surgery to insert code
            result = code_surgery.insert_code(repo_id, file_path, target, content)
            
            return {"success": result.get("success", False), "error": result.get("error")}
            
        except Exception as e:
            return {"success": False, "error": str(e)}


# Global instance
_summoner_service = None

def get_summoner_service() -> SummonerService:
    """Get or create the global Summoner service instance."""
    global _summoner_service
    if _summoner_service is None:
        _summoner_service = SummonerService()
    return _summoner_service

# Public API
def list_summoning_recipes() -> Dict[str, Dict[str, Any]]:
    """Get available summoning recipes."""
    service = get_summoner_service()
    return service.list_recipes()

def summon_code_pattern(repo_id: str, recipe_name: str, **parameters) -> Dict[str, Any]:
    """Summon a complete code pattern."""
    service = get_summoner_service()
    return service.summon_pattern(repo_id, recipe_name, **parameters)

def execute_summoning_ritual(repo_id: str, summoning_result: Dict[str, Any]) -> Dict[str, Any]:
    """Execute the summoning ritual to create the code."""
    service = get_summoner_service()
    return service.execute_summoning(repo_id, summoning_result)
--- FILE_END: backend/lumiere_core/services/summoner_service.py ---

--- FILE_START: backend/lumiere_core/wsgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/wsgi.py
"""
WSGI config for backend project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_wsgi_application()

--- FILE_END: backend/lumiere_core/wsgi.py ---

--- FILE_START: backend/.env ---

GEMINI_API_KEY='AIzaSyAVvfR4hTMO15HP0Skk0y3A3Bmsvg9ivmI'

--- FILE_END: backend/.env ---

--- FILE_START: backend/api/migrations/__init__.py ---

--- FILE_END: backend/api/migrations/__init__.py ---

--- FILE_START: backend/api/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/api/models.py ---

--- FILE_START: backend/api/__init__.py ---

--- FILE_END: backend/api/__init__.py ---

--- FILE_START: backend/api/apps.py ---
from django.apps import AppConfig


class ApiConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "api"

--- FILE_END: backend/api/apps.py ---

--- FILE_START: backend/api/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/api/admin.py ---

--- FILE_START: backend/api/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/api/tests.py ---

--- FILE_START: backend/api/urls.py ---
# Enhanced backend/api/urls.py

from django.urls import path
from .views import (
    BriefingView, ScaffoldView, TestGenerationView, RcaView,
    DocstringGenerationView,
    ProfileReviewView, AmbassadorDispatchView, IssueListView,
    StrategistPrioritizeView, FileContentView, DiplomatView,
    CrucibleValidateView, ListModelsView, HealthCheckView,
    IngestRepositoryView, GraphDataView, OracleView,
    AdjudicateView, HarmonizeView, SentinelBriefingView,
    SuggestActionsView,

    # Enhanced BOM Views
    BomDataView, BomDependenciesView, BomServicesView,
    BomSecurityView, BomRegenerateView, BomCompareView,

    # Onboarding Concierge Views
    ExpertiseView, OnboardingGuideView,

    # API Endpoint Inventory View
    ApiEndpointInventoryView,

    # GET and DELETE methods for the /api/v1/repositories/{repo_id}/
    ListRepositoriesView, RepositoryDetailView,
    RepositoryStatusView, SentinelMetricsHistoryView
)

urlpatterns = [
    # --- HEALTH CHECK ENDPOINT ---
    path('health/', HealthCheckView.as_view(), name='health_check'),

    # --- SUGGESTER ENDPOINT ---
    path('suggest-actions/', SuggestActionsView.as_view(), name='suggest_actions'),


    # --- BILL OF MATERIALS (BOM) ENDPOINTS ---
    path('bom/', BomDataView.as_view(), name='bom_data'),
    path('bom/dependencies/', BomDependenciesView.as_view(), name='bom_dependencies'),
    path('bom/services/', BomServicesView.as_view(), name='bom_services'),
    path('bom/security/', BomSecurityView.as_view(), name='bom_security'),
    path('bom/regenerate/', BomRegenerateView.as_view(), name='bom_regenerate'),
    path('bom/compare/', BomCompareView.as_view(), name='bom_compare'),


    # --- MODEL MANAGEMENT ENDPOINT ---
    path('models/list/', ListModelsView.as_view(), name='list_models'),

     # --- INGESTION ENDPOINT ---
    path('ingest/', IngestRepositoryView.as_view(), name='ingest_repository'),

     # --- ORACLE (Q&A) ENDPOINT ---
    path('oracle/ask/', OracleView.as_view(), name='oracle_ask'),

    # --- ONBOARDING CONCIERGE ENDPOINTS ---
    path('expertise/find/', ExpertiseView.as_view(), name='find_experts'),
    path('onboarding/guide/', OnboardingGuideView.as_view(), name='onboarding_guide'),

    # --- REPOSITORY MANAGEMENT ---
    path('repositories/', ListRepositoriesView.as_view(), name='repository-list'),
    path('repositories/<str:repo_id>/', RepositoryDetailView.as_view(), name='repository-detail'),
    path('repositories/<str:repo_id>/status/', RepositoryStatusView.as_view(), name='repository-status'),
    path('repositories/<str:repo_id>/api-endpoints/', ApiEndpointInventoryView.as_view(), name='api-endpoint-inventory'),


    # --- Graph ENDPOINT ---
    path('graph/<str:repo_id>/', GraphDataView.as_view(), name='graph_data'),

    # --- "Triage & Strategy" ENDPOINTS ---
    path('issues/list/', IssueListView.as_view(), name='list_issues'),
    path('strategist/prioritize/', StrategistPrioritizeView.as_view(), name='strategist_prioritize'),
    path('diplomat/find-similar-issues/', DiplomatView.as_view(), name='diplomat_find_similar'),

    # --- "Execution & Validation" ENDPOINTS ---
    path('briefing/', BriefingView.as_view(), name='briefing'),
    path('scaffold/', ScaffoldView.as_view(), name='scaffold'),
    path('crucible/validate/', CrucibleValidateView.as_view(), name='crucible_validate'),
    path('file-content/', FileContentView.as_view(), name='file_content'),
    path('generate-tests/', TestGenerationView.as_view(), name='generate_tests'),
    path('generate-docstring/', DocstringGenerationView.as_view(), name='generate_docstring'),
    path('rca/', RcaView.as_view(), name='rca'),
    path('ambassador/dispatch/', AmbassadorDispatchView.as_view(), name='ambassador_dispatch'),

    # --- "Review" ENDPOINTS ---
    path('profile/review/', ProfileReviewView.as_view(), name='profile_review'),
    path('review/adjudicate/', AdjudicateView.as_view(), name='adjudicate_pr'),
    path('review/harmonize/', HarmonizeView.as_view(), name='harmonize_pr_fix'),

    # --- SENTINEL ENDPOINTS ---
    path('sentinel/briefing/<str:repo_id>/', SentinelBriefingView.as_view(), name='sentinel_briefing'),
    path('sentinel/metrics/<str:repo_id>/', SentinelMetricsHistoryView.as_view(), name='sentinel-metrics-history'),
]

--- FILE_END: backend/api/urls.py ---

--- FILE_START: backend/api/views.py ---
# In backend/api/views.py

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
import os
import re
import traceback
import json
from pathlib import Path
import shutil
from datetime import datetime, timedelta


# --- Correctly import services from lumiere_core ---
from lumiere_core.services import (
    llm_service, github, ambassador, crucible, diplomat,
    documentation, profile_service, rca_service, scaffolding, strategist,
    testing, review_service, ingestion_service, cortex_service, oracle_service,
    suggester_service, expertise_service, onboarding_service,
)
from lumiere_core.services.llm_service import TaskType
from lumiere_core.services.cortex_service import get_bom_data, has_bom_data, get_repository_metadata

# A sensible default model for old workflows, though it's now mostly unused.
DEFAULT_MODEL = "ollama/qwen3:4b"

# --- HELPER CONSTANTS & FUNCTIONS ---
CLONED_REPOS_DIR = Path(__file__).resolve().parent.parent / "cloned_repositories"

def _is_safe_path(base_dir: Path, repo_id: str) -> bool:
    """Checks if the repo_id is a safe path component to prevent traversal."""
    if '..' in repo_id or '/' in repo_id or '\\' in repo_id:
        return False

    # Path().resolve() will resolve symlinks and '..'
    resolved_repo_path = (base_dir / repo_id).resolve()
    resolved_base_dir = base_dir.resolve()

    # Check if the resolved path is a sub-path of the base directory.
    try:
        resolved_repo_path.relative_to(resolved_base_dir)
        return True
    except ValueError:
        # This exception is raised if the path is not a sub-path, which means it's unsafe.
        return False


class HealthCheckView(APIView):
    """A simple view to confirm the server is running."""
    def get(self, request, *args, **kwargs):
        return Response({"status": "ok"}, status=status.HTTP_200_OK)

class ListModelsView(APIView):
    """Returns a list of all available LLM models from configured providers."""
    def get(self, request, *args, **kwargs):
        try:
            models = llm_service.list_available_models()
            return Response(models, status=status.HTTP_200_OK)
        except Exception as e:
            return Response(
                {"error": "Failed to list models.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class StrategistPrioritizeView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # The 'model' parameter is no longer needed. The Task Router handles it.
            result = strategist.analyze_and_prioritize(repo_url)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DiplomatView(APIView):
    def post(self, request, *args, **kwargs):
        issue_title = request.data.get('issue_title')
        issue_body = request.data.get('issue_body')
        if not all([issue_title, issue_body]):
            return Response(
                {"error": "'issue_title' and 'issue_body' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # This service should also be updated to not require model_identifier
            # Assuming diplomat_service is updated internally to use the Task Router
            result = diplomat.find_similar_solved_issues(issue_title, issue_body)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class BriefingView(APIView):
    def post(self, request, *args, **kwargs):
        issue_url = request.data.get('issue_url')
        if not issue_url:
            return Response(
                {"error": "'issue_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Assuming rca_service.generate_briefing is updated to use the Task Router
            result = rca_service.generate_briefing(issue_url)
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class RcaView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        bug_description = request.data.get('bug_description')
        advanced_analysis = request.data.get('advanced_analysis', False)
        confidence_threshold = request.data.get('confidence_threshold', 0.7)

        if not all([repo_url, bug_description]):
            return Response(
                {"error": "'repo_url' and 'bug_description' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Assuming rca_service.perform_rca is updated for the Task Router
            result = rca_service.perform_rca(
                repo_url=repo_url,
                bug_description=bug_description,
                advanced_analysis=advanced_analysis,
                confidence_threshold=confidence_threshold
            )
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class ScaffoldView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        target_files = request.data.get('target_files')
        instruction = request.data.get('instruction')
        rca_report = request.data.get('rca_report')
        refinement_history = request.data.get('refinement_history')

        if not all([repo_id, target_files, instruction, rca_report]):
            return Response(
                {"error": "'repo_id', 'target_files' (list), 'instruction', and 'rca_report' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not isinstance(target_files, list):
            return Response(
                {"error": "'target_files' must be a list of strings."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Scaffolding service uses the Task Router internally now
            result = scaffolding.generate_scaffold(
                repo_id, target_files, instruction, rca_report, refinement_history
            )
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class CrucibleValidateView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        target_file = request.data.get('target_file')
        modified_code = request.data.get('modified_code')
        if not all([repo_url, target_file, modified_code]):
            return Response(
                {"error": "'repo_url', 'target_file', 'modified_code' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            result = crucible.validate_fix(repo_url, target_file, modified_code)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            error_details = traceback.format_exc()
            return Response(
                {
                    "error": "An unexpected internal server error occurred in The Crucible.",
                    "details": str(e),
                    "traceback": error_details
                },
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AmbassadorDispatchView(APIView):
    def post(self, request, *args, **kwargs):
        issue_url = request.data.get('issue_url')
        modified_files = request.data.get('modified_files')

        if not all([issue_url, modified_files]):
            return Response(
                {"error": "'issue_url' and 'modified_files' (dict) are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not isinstance(modified_files, dict):
            return Response(
                {"error": "'modified_files' must be a dictionary."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Ambassador uses the Task Router internally now
            result = ambassador.dispatch_pr(issue_url, modified_files)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class ProfileReviewView(APIView):
    def post(self, request, *args, **kwargs):
        username = request.data.get('username')
        if not username:
            return Response(
                {"error": "'username' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Profile service uses the Task Router internally
            result = profile_service.generate_profile_review(username)
            if "error" in result:
                return Response(result, status=status.HTTP_404_NOT_FOUND)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class IssueListView(APIView):
    def get(self, request, *args, **kwargs):
        repo_url = request.query_params.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            return Response(
                {"error": "Invalid 'repo_url' format."},
                status=status.HTTP_400_BAD_REQUEST
            )
        repo_full_name = f"{match.group(1)}/{match.group(2)}"
        try:
            issues = github.list_open_issues(repo_full_name)
            return Response(issues, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileContentView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        file_path = request.data.get('file_path')
        if not all([repo_id, file_path]):
            return Response(
                {"error": "'repo_id' and 'file_path' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            content = cortex_service.get_file_content(repo_id, file_path)
            if content is None:
                return Response(
                    {"error": f"File '{file_path}' not found for repo '{repo_id}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            return Response({"content": content}, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TestGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        if not all([repo_id, new_code, instruction]):
            return Response(
                {"error": "'repo_id', 'new_code', and 'instruction' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Testing service uses the Task Router internally
            result = testing.generate_tests_for_code(repo_id, new_code, instruction)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DocstringGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        if not all([repo_id, new_code, instruction]):
            return Response(
                {"error": "'repo_id', 'new_code', and 'instruction' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Documentation service uses the Task Router internally
            result = documentation.generate_docstring_for_code(repo_id, new_code, instruction)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class IngestRepositoryView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            result = ingestion_service.clone_and_embed_repository(repo_url)
            if result.get("status") == "failed":
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An unexpected internal server error occurred during ingestion.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

# --- REFACTORED VIEW ---
class GraphDataView(APIView):
    def get(self, request, repo_id, *args, **kwargs):
        if not repo_id:
            return Response(
                {"error": "'repo_id' path parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"error": "Invalid repo_id"}, status=status.HTTP_400_BAD_REQUEST)

        cortex_file = CLONED_REPOS_DIR / repo_id / f"{repo_id}_cortex.json"

        if not cortex_file.exists():
            return Response(
                {"error": f"Cortex file for repo '{repo_id}' not found."},
                status=status.HTTP_404_NOT_FOUND
            )

        try:
            with open(cortex_file, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)

            graph_data = cortex_data.get('architectural_graph')
            if not graph_data:
                new_message = ("Architectural graph is not available. The Cartographer feature currently only supports "
                             "Python projects (.py files). This repository does not appear to contain Python code suitable for graphing.")
                return Response({"message": new_message, "graph": None}, status=status.HTTP_200_OK)

            return Response({"graph": graph_data, "repo_id": repo_id}, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to read or parse graph data.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class OracleView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        question = request.data.get('question')
        if not all([repo_id, question]):
            return Response(
                {"error": "'repo_id' and 'question' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Oracle service uses the Task Router internally
            result = oracle_service.answer_question(repo_id, question)
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Oracle.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AdjudicateView(APIView):
    def post(self, request, *args, **kwargs):
        pr_url = request.data.get('pr_url')
        if not pr_url:
            return Response(
                {"error": "'pr_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Inquire service uses the Task Router internally
            result = review_service.inquire_pr(pr_url)
            if "error" in result:
                # Safe check for ingestion error with proper string handling
                if result.get("error") and "ingested" in str(result.get("error", "")):
                    return Response(result, status=status.HTTP_412_PRECONDITION_FAILED)
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Inquisitor.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class HarmonizeView(APIView):
    def post(self, request, *args, **kwargs):
        pr_url = request.data.get('pr_url')
        review_text = request.data.get('review_text')
        if not all([pr_url, review_text]):
            return Response(
                {"error": "'pr_url' and 'review_text' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Harmonizer uses services that use the Task Router
            result = review_service.harmonize_pr_fix(pr_url, review_text)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Harmonizer.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

# --- REFACTORED VIEW ---
class SentinelBriefingView(APIView):
    def get(self, request, repo_id, *args, **kwargs):
        if not repo_id:
            return Response(
                {"error": "'repo_id' path parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"error": "Invalid repo_id"}, status=status.HTTP_400_BAD_REQUEST)

        metrics_path = CLONED_REPOS_DIR / repo_id / "metrics.json"

        if not metrics_path.exists():
            return Response(
                {"error": f"No metrics file found for repo '{repo_id}'. Please run ingestion/analysis first."},
                status=status.HTTP_404_NOT_FOUND
            )

        try:
            with open(metrics_path, 'r') as f:
                metrics_data = json.load(f)

            if len(metrics_data) < 2:
                return Response({
                    "briefing": "Not enough historical data to generate a trend analysis. At least two data points are needed."
                })

            latest = metrics_data[-1]
            previous = metrics_data[-2]
            trends_str = "Key Trends:\n"

            for key, value in latest.items():
                if isinstance(value, (int, float)) and key in previous:
                    prev_val = previous[key]
                    if prev_val != 0:
                        change = ((value - prev_val) / abs(prev_val)) * 100
                        trends_str += f"- {key.replace('_', ' ').title()}: {value:.2f} ({change:+.1f}% change)\n"

            prompt = f"You are The Sentinel, an AI that monitors software project health. Analyze the following metric trends and provide a concise, human-readable briefing for a developer dashboard. Highlight any significant changes or potential issues.\n\n{trends_str}"
            briefing = llm_service.generate_text(prompt, task_type=TaskType.SIMPLE)

            return Response({"briefing": briefing, "latest_metrics": latest})
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to generate Sentinel briefing.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


# --- NEW REPOSITORY MANAGEMENT VIEWS ---

class ListRepositoriesView(APIView):
    """Lists all previously analyzed repositories that have complete artifacts."""
    def get(self, request, *args, **kwargs):
        analyzed_repos = []
        if not CLONED_REPOS_DIR.is_dir():
            return Response([], status=status.HTTP_200_OK)

        for repo_dir in CLONED_REPOS_DIR.iterdir():
            if repo_dir.is_dir():
                repo_id = repo_dir.name
                cortex_file = repo_dir / f"{repo_id}_cortex.json"
                faiss_file = repo_dir / f"{repo_id}_faiss.index"
                map_file = repo_dir / f"{repo_id}_id_map.json"

                if all([cortex_file.exists(), faiss_file.exists(), map_file.exists()]):
                    try:
                        display_name = repo_id.replace("_", "/", 1)
                        full_url = f"https://github.com/{display_name}"
                        analyzed_repos.append({"repo_id": repo_id, "display_name": display_name, "url": full_url})
                    except Exception:
                        continue
        return Response(sorted(analyzed_repos, key=lambda x: x['repo_id']), status=status.HTTP_200_OK)

class RepositoryDetailView(APIView):
    """Handles retrieval and deletion of a single repository's data."""
    def get(self, request, repo_id, *args, **kwargs):
        """Returns detailed metadata for a single repository."""
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"error": "Invalid repo_id"}, status=status.HTTP_400_BAD_REQUEST)

        try:
            metadata = cortex_service.get_repository_metadata(repo_id)
            if metadata is None:
                return Response({"error": f"Repository '{repo_id}' not found or its cortex file is missing."}, status=status.HTTP_404_NOT_FOUND)
            return Response(metadata, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def delete(self, request, repo_id, *args, **kwargs):
        """Deletes the directory and all contents for a given repository."""
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"error": "Invalid repo_id"}, status=status.HTTP_400_BAD_REQUEST)

        repo_dir = CLONED_REPOS_DIR / repo_id
        if not repo_dir.is_dir():
            return Response(status=status.HTTP_204_NO_CONTENT) # Treat as success if already gone

        try:
            shutil.rmtree(repo_dir)
            return Response(status=status.HTTP_204_NO_CONTENT)
        except Exception as e:
            return Response({"error": f"Failed to delete repository '{repo_id}'.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class RepositoryStatusView(APIView):
    """Checks if a repository has been fully ingested and analyzed."""
    def get(self, request, repo_id, *args, **kwargs):
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"status": "not_found"}, status=status.HTTP_200_OK)

        repo_dir = CLONED_REPOS_DIR / repo_id
        if not repo_dir.is_dir():
            return Response({"status": "not_found"}, status=status.HTTP_200_OK)

        cortex_file = repo_dir / f"{repo_id}_cortex.json"
        faiss_file = repo_dir / f"{repo_id}_faiss.index"
        map_file = repo_dir / f"{repo_id}_id_map.json"

        if all([cortex_file.exists(), faiss_file.exists(), map_file.exists()]):
            return Response({"status": "complete"}, status=status.HTTP_200_OK)
        else:
            return Response({"status": "not_found"}, status=status.HTTP_200_OK)

class SentinelMetricsHistoryView(APIView):
    """Reads and returns the entire metrics.json for a given repository."""
    def get(self, request, repo_id, *args, **kwargs):
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response({"error": "Invalid repo_id"}, status=status.HTTP_400_BAD_REQUEST)

        metrics_file = CLONED_REPOS_DIR / repo_id / "metrics.json"
        if not metrics_file.is_file():
            return Response({"error": f"Metrics file for repo '{repo_id}' not found."}, status=status.HTTP_404_NOT_FOUND)

        try:
            with open(metrics_file, 'r', encoding='utf-8') as f:
                metrics_data = json.load(f)
            return Response(metrics_data, status=status.HTTP_200_OK)
        except json.JSONDecodeError:
            return Response({"error": "Failed to parse metrics.json file."}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        except Exception as e:
            return Response({"error": "An internal server error occurred.", "details": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# --- THE MISSION CONTROLLER ---
class SuggestActionsView(APIView):
    """Takes context from the last action and suggests next steps."""
    def post(self, request, *args, **kwargs):
        last_action = request.data.get("last_action")
        result_data = request.data.get("result_data", {})
        if not last_action:
            return Response(
                {"error": "'last_action' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            suggestions = suggester_service.suggest_next_actions(last_action, result_data)
            return Response(suggestions, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to generate suggestions.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


# --- Bill of Materials (BOM) ---
class BomDataView(APIView):
    """Enhanced BOM view that works with your existing cortex service."""
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        format_type = request.query_params.get('format', 'json')  # json, summary, detailed

        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Use your existing cortex service
            bom_data = get_bom_data(repo_id, format_type)
            if not bom_data:
                # Check if repo exists but just doesn't have BOM
                metadata = get_repository_metadata(repo_id)
                if metadata:
                    return Response(
                        {"error": f"Bill of Materials not found for repo '{repo_id}'. Repository exists but BOM was not generated during ingestion."},
                        status=status.HTTP_404_NOT_FOUND
                    )
                else:
                    return Response(
                        {"error": f"Repository '{repo_id}' not found."},
                        status=status.HTTP_404_NOT_FOUND
                    )
            return Response(bom_data, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred while retrieving BOM data.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class BomDependenciesView(APIView):
    """View for filtered dependency information."""
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        ecosystem = request.query_params.get('ecosystem')
        dependency_type = request.query_params.get('dependency_type')
        outdated_only = request.query_params.get('outdated_only', 'false').lower() == 'true'
        security_risk = request.query_params.get('security_risk')

        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            bom_data = get_bom_data(repo_id, "json")
            if not bom_data:
                return Response(
                    {"error": f"BOM data not found for repository '{repo_id}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            all_dependencies = []
            for dep_type, deps in bom_data.get('dependencies', {}).items():
                for dep in deps:
                    dep['category'] = dep_type
                    all_dependencies.append(dep)
            filtered_deps = all_dependencies
            if ecosystem:
                filtered_deps = [d for d in filtered_deps if d.get('ecosystem') == ecosystem]
            if dependency_type:
                filtered_deps = [d for d in filtered_deps if d.get('category') == dependency_type]
            if security_risk:
                filtered_deps = [d for d in filtered_deps if d.get('security_risk') == security_risk]
            if outdated_only:
                filtered_deps = [d for d in filtered_deps if d.get('deprecated', False)]
            return Response({
                'repo_id': repo_id, 'total_count': len(filtered_deps),
                'filters_applied': {
                    'ecosystem': ecosystem, 'dependency_type': dependency_type,
                    'outdated_only': outdated_only, 'security_risk': security_risk
                },
                'dependencies': filtered_deps,
                'statistics': self._generate_dependency_stats(filtered_deps)
            }, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to retrieve dependencies.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    def _generate_dependency_stats(self, dependencies):
        """Generate statistics for dependencies."""
        ecosystems = {}
        for dep in dependencies:
            ecosystem = dep.get('ecosystem', 'unknown')
            ecosystems[ecosystem] = ecosystems.get(ecosystem, 0) + 1
        return {
            'ecosystems': ecosystems, 'total_count': len(dependencies),
            'deprecated_count': sum(1 for d in dependencies if d.get('deprecated', False))
        }

class BomServicesView(APIView):
    """View for service information."""
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        service_type = request.query_params.get('service_type')
        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            bom_data = get_bom_data(repo_id, "json")
            if not bom_data:
                return Response(
                    {"error": f"BOM data not found for repository '{repo_id}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            services = bom_data.get('services', [])
            if service_type:
                services = [s for s in services if s.get('service_type') == service_type]
            return Response({
                'repo_id': repo_id, 'total_count': len(services),
                'services': services, 'service_types': list(set(s.get('service_type', 'unknown') for s in services)),
                'infrastructure_analysis': bom_data.get('infrastructure', {})
            }, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to retrieve services.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class BomSecurityView(APIView):
    """View for security analysis."""
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        severity = request.query_params.get('severity')
        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            bom_data = get_bom_data(repo_id, "json")
            if not bom_data:
                return Response(
                    {"error": f"BOM data not found for repository '{repo_id}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            security_analysis = bom_data.get('security_analysis', {})
            enhanced_analysis = {
                'summary': security_analysis,
                'vulnerable_dependencies': self._get_vulnerable_dependencies(bom_data, severity),
                'security_recommendations': self._generate_security_recommendations(bom_data),
                'compliance_status': self._check_compliance(bom_data),
                'last_scan': security_analysis.get('last_scan'),
                'repo_id': repo_id
            }
            return Response(enhanced_analysis, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Security analysis failed.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    def _get_vulnerable_dependencies(self, bom_data, severity):
        return []
    def _generate_security_recommendations(self, bom_data):
        return [
            'Enable automated dependency scanning', 'Set up security alerts for new vulnerabilities',
            'Regularly update dependencies', 'Implement security testing in CI/CD pipeline'
        ]
    def _check_compliance(self, bom_data):
        return {
            'compliant': True, 'checks': {
                'outdated_dependencies': 'pass', 'known_vulnerabilities': 'pass',
                'license_compliance': 'pass'
            }
        }

class BomRegenerateView(APIView):
    """View to regenerate BOM for a repository."""
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        force = request.data.get('force', False)
        if not repo_id:
            return Response(
                {"error": "'repo_id' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            metadata = get_repository_metadata(repo_id)
            if not metadata:
                return Response(
                    {"error": f"Repository '{repo_id}' not found."},
                    status=status.HTTP_404_NOT_FOUND
                )
            if has_bom_data(repo_id) and not force:
                existing_bom = get_bom_data(repo_id)
                last_updated = existing_bom.get('summary', {}).get('last_updated')
                if last_updated:
                    try:
                        last_updated_dt = datetime.fromisoformat(last_updated.replace('Z', '+00:00'))
                        if datetime.now().replace(tzinfo=last_updated_dt.tzinfo) - last_updated_dt < timedelta(hours=24):
                            return Response({
                                'message': 'BOM is recent, use force=true to regenerate',
                                'last_updated': last_updated, 'regenerated': False
                            }, status=status.HTTP_200_OK)
                    except:
                        pass
            return Response({
                'message': 'BOM regeneration requires re-ingesting the repository',
                'suggestion': 'Use the /api/ingest/ endpoint to regenerate BOM data',
                'regenerated': False
            }, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "BOM regeneration failed.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class BomCompareView(APIView):
    """View to compare BOMs between repositories."""
    def post(self, request, *args, **kwargs):
        repo_id_1 = request.data.get('repo_id_1')
        repo_id_2 = request.data.get('repo_id_2')
        comparison_type = request.data.get('comparison_type', 'dependencies')
        if not all([repo_id_1, repo_id_2]):
            return Response(
                {"error": "'repo_id_1' and 'repo_id_2' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            bom_1 = get_bom_data(repo_id_1)
            bom_2 = get_bom_data(repo_id_2)
            if not bom_1:
                return Response(
                    {"error": f"BOM data not found for repository '{repo_id_1}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            if not bom_2:
                return Response(
                    {"error": f"BOM data not found for repository '{repo_id_2}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            if comparison_type == "dependencies":
                comparison = self._compare_dependencies(bom_1, bom_2)
            elif comparison_type == "services":
                comparison = self._compare_services(bom_1, bom_2)
            elif comparison_type == "languages":
                comparison = self._compare_languages(bom_1, bom_2)
            else:
                comparison = self._comprehensive_comparison(bom_1, bom_2)
            return Response({
                'repo_1': {'id': repo_id_1}, 'repo_2': {'id': repo_id_2},
                'comparison_type': comparison_type, 'comparison': comparison
            }, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Comparison failed.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    def _compare_dependencies(self, bom_1, bom_2):
        deps_1, deps_2 = set(), set()
        for deps in bom_1.get('dependencies', {}).values():
            for dep in deps:
                deps_1.add(f"{dep.get('name')}@{dep.get('version')}")
        for deps in bom_2.get('dependencies', {}).values():
            for dep in deps:
                deps_2.add(f"{dep.get('name')}@{dep.get('version')}")
        return {
            'common': list(deps_1.intersection(deps_2)),
            'unique_to_repo_1': list(deps_1 - deps_2),
            'unique_to_repo_2': list(deps_2 - deps_1),
            'total_repo_1': len(deps_1), 'total_repo_2': len(deps_2)
        }
    def _compare_services(self, bom_1, bom_2):
        services_1 = {f"{s.get('name')}:{s.get('version')}" for s in bom_1.get('services', [])}
        services_2 = {f"{s.get('name')}:{s.get('version')}" for s in bom_2.get('services', [])}
        return {
            'common': list(services_1.intersection(services_2)),
            'unique_to_repo_1': list(services_1 - services_2),
            'unique_to_repo_2': list(services_2 - services_1)
        }
    def _compare_languages(self, bom_1, bom_2):
        langs_1, langs_2 = set(bom_1.get('languages', {}).keys()), set(bom_2.get('languages', {}).keys())
        return {
            'common': list(langs_1.intersection(langs_2)),
            'unique_to_repo_1': list(langs_1 - langs_2),
            'unique_to_repo_2': list(langs_2 - langs_1)
        }
    def _comprehensive_comparison(self, bom_1, bom_2):
        return {
            'dependencies': self._compare_dependencies(bom_1, bom_2),
            'services': self._compare_services(bom_1, bom_2),
            'languages': self._compare_languages(bom_1, bom_2),
            'summary': {
                'primary_language_1': bom_1.get('summary', {}).get('primary_language'),
                'primary_language_2': bom_2.get('summary', {}).get('primary_language'),
                'dependency_count_1': bom_1.get('summary', {}).get('total_dependencies', 0),
                'dependency_count_2': bom_2.get('summary', {}).get('total_dependencies', 0)
            }
        }


class ExpertiseView(APIView):
    """
    API endpoint for the Expertise Service - finds knowledgeable contributors
    for specific files or modules. Part of the Onboarding Concierge feature.
    """

    def post(self, request):
        """
        Find experts for a given file or module pattern.
        
        Expected payload:
        {
            "repo_id": "string",
            "file_path": "string (optional)",
            "module_pattern": "string (optional)",
            "type": "file" | "module" | "summary"
        }
        """
        try:
            data = request.data
            repo_id = data.get('repo_id')
            file_path = data.get('file_path')
            module_pattern = data.get('module_pattern')
            query_type = data.get('type', 'file')

            if not repo_id:
                return Response(
                    {'error': 'repo_id is required'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Validate repo_id for security
            if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
                return Response(
                    {'error': 'Invalid repo_id'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Initialize expertise service
            expertise_svc = expertise_service.ExpertiseService(CLONED_REPOS_DIR)

            # Handle different query types
            if query_type == 'file':
                if not file_path:
                    return Response(
                        {'error': 'file_path is required for file expertise queries'}, 
                        status=status.HTTP_400_BAD_REQUEST
                    )
                
                experts = expertise_svc.find_experts_for_file(repo_id, file_path)
                
                return Response({
                    'repository': repo_id,
                    'file_path': file_path,
                    'experts': experts,
                    'query_type': 'file',
                    'expert_count': len(experts)
                })

            elif query_type == 'module':
                if not module_pattern:
                    return Response(
                        {'error': 'module_pattern is required for module expertise queries'}, 
                        status=status.HTTP_400_BAD_REQUEST
                    )
                
                experts = expertise_svc.find_experts_for_module(repo_id, module_pattern)
                
                return Response({
                    'repository': repo_id,
                    'module_pattern': module_pattern,
                    'experts': experts,
                    'query_type': 'module',
                    'expert_count': len(experts)
                })

            elif query_type == 'summary':
                summary = expertise_svc.get_repository_experts_summary(repo_id)
                
                return Response({
                    'repository': repo_id,
                    'query_type': 'summary',
                    **summary
                })

            else:
                return Response(
                    {'error': 'Invalid query type. Must be "file", "module", or "summary"'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

        except Exception as e:
            return Response(
                {
                    'error': 'Internal server error occurred while finding experts',
                    'details': str(e),
                    'traceback': traceback.format_exc()
                },
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class OnboardingGuideView(APIView):
    """
    API endpoint for the Onboarding Service - generates personalized onboarding 
    paths for GitHub issues. Part of the Onboarding Concierge feature.
    """

    def post(self, request):
        """
        Generate a personalized onboarding guide for a GitHub issue.
        
        Expected payload:
        {
            "repo_id": "string",
            "issue_number": "integer"
        }
        """
        try:
            data = request.data
            repo_id = data.get('repo_id')
            issue_number = data.get('issue_number')

            if not repo_id:
                return Response(
                    {'error': 'repo_id is required'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            if not issue_number:
                return Response(
                    {'error': 'issue_number is required'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Validate issue_number is an integer
            try:
                issue_number = int(issue_number)
            except (ValueError, TypeError):
                return Response(
                    {'error': 'issue_number must be a valid integer'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Validate repo_id for security
            if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
                return Response(
                    {'error': 'Invalid repo_id'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )

            # Initialize onboarding service
            onboarding_svc = onboarding_service.OnboardingService(CLONED_REPOS_DIR)

            # Generate the onboarding guide
            guide_result = onboarding_svc.generate_onboarding_path(repo_id, issue_number)

            if guide_result.get('generation_successful'):
                return Response({
                    'repository': repo_id,
                    'issue_number': issue_number,
                    'issue_title': guide_result.get('issue_title'),
                    'onboarding_guide': guide_result.get('onboarding_guide'),
                    'learning_path_steps': guide_result.get('learning_path_steps'),
                    'locus_files': guide_result.get('locus_files', []),
                    'detailed_steps': guide_result.get('enriched_steps', []),
                    'generation_timestamp': datetime.now().isoformat()
                })
            else:
                return Response(
                    {
                        'error': 'Failed to generate onboarding guide',
                        'details': guide_result.get('error', 'Unknown error'),
                        'repository': repo_id,
                        'issue_number': issue_number
                    },
                    status=status.HTTP_422_UNPROCESSABLE_ENTITY
                )

        except Exception as e:
            return Response(
                {
                    'error': 'Internal server error occurred while generating onboarding guide',
                    'details': str(e),
                    'traceback': traceback.format_exc()
                },
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class ApiEndpointInventoryView(APIView):
    """
    API endpoint to retrieve all discovered API endpoints from a repository's cortex data.
    This provides a complete inventory of all web API routes discovered during ingestion.
    """

    def get(self, request, repo_id, *args, **kwargs):
        """
        Retrieve all API endpoints for a given repository.
        
        Args:
            repo_id (str): The repository identifier
            
        Returns:
            JSON response containing all discovered API endpoints with metadata
        """
        if not repo_id:
            return Response(
                {"error": "'repo_id' path parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
            
        # Validate repo_id for security
        if not _is_safe_path(CLONED_REPOS_DIR, repo_id):
            return Response(
                {"error": "Invalid repo_id"}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            # Load cortex data using the existing cortex service
            cortex_data = cortex_service.load_cortex_data(repo_id)
            
            if not cortex_data:
                return Response(
                    {"error": f"Cortex data not found for repository '{repo_id}'. Repository may not have been ingested or analysis may have failed."},
                    status=status.HTTP_404_NOT_FOUND
                )
            
            aggregated_endpoints = []
            endpoint_count_by_framework = {}
            total_endpoints = 0
            
            # Process each file in the cortex data
            for file_data in cortex_data.get('files', []):
                file_path = file_data.get('file_path', '')
                api_endpoints = file_data.get('api_endpoints', [])
                
                # Process each endpoint found in this file
                for endpoint in api_endpoints:
                    # Enrich endpoint with file context
                    enriched_endpoint = {
                        **endpoint,  # All original endpoint data
                        'handler_file': file_path,  # Add the file containing this endpoint
                        'file_language': file_data.get('detected_language', 'unknown'),
                        'file_size_kb': file_data.get('file_size_kb', 0)
                    }
                    
                    # Optional: Add architectural graph connections
                    # This could be enhanced to show which functions this endpoint calls
                    if cortex_data.get('architectural_graph'):
                        # Find related functions/classes that this endpoint might call
                        graph_data = cortex_data['architectural_graph']
                        handler_name = endpoint.get('handler_function_name', '')
                        
                        # Look for outbound calls from this handler
                        related_functions = []
                        for node_id, node_data in graph_data.get('nodes', {}).items():
                            if node_data.get('name') == handler_name:
                                # Find outbound edges
                                for edge in graph_data.get('edges', []):
                                    if edge.get('source') == node_id:
                                        target_node = graph_data['nodes'].get(edge.get('target'), {})
                                        related_functions.append(target_node.get('name', ''))
                        
                        if related_functions:
                            enriched_endpoint['related_functions'] = related_functions
                    
                    aggregated_endpoints.append(enriched_endpoint)
                    
                    # Track framework statistics
                    framework = endpoint.get('framework', 'Unknown')
                    endpoint_count_by_framework[framework] = endpoint_count_by_framework.get(framework, 0) + 1
                    total_endpoints += 1
            
            # Generate summary statistics
            methods_distribution = {}
            path_patterns = []
            
            for endpoint in aggregated_endpoints:
                # Count HTTP methods
                for method in endpoint.get('methods', []):
                    methods_distribution[method] = methods_distribution.get(method, 0) + 1
                
                # Collect path patterns for analysis
                path = endpoint.get('path', '')
                if path:
                    path_patterns.append(path)
            
            # Analyze path patterns (simple heuristic for RESTful endpoints)
            rest_score = self._calculate_rest_score(path_patterns)
            
            # Build comprehensive response
            response_data = {
                'repository': repo_id,
                'api_inventory': {
                    'total_endpoints': total_endpoints,
                    'endpoints': aggregated_endpoints,
                    'summary': {
                        'frameworks_detected': list(endpoint_count_by_framework.keys()),
                        'framework_distribution': endpoint_count_by_framework,
                        'http_methods_distribution': methods_distribution,
                        'rest_score': rest_score,
                        'files_with_endpoints': len([f for f in cortex_data.get('files', []) if f.get('api_endpoints')])
                    },
                    'analysis_metadata': {
                        'analyzed_at': cortex_data.get('last_crawled_utc'),
                        'total_files_analyzed': len(cortex_data.get('files', [])),
                        'primary_language': cortex_data.get('polyglot_summary', {}).get('primary_language', 'unknown')
                    }
                }
            }
            
            return Response(response_data, status=status.HTTP_200_OK)
            
        except Exception as e:
            traceback.print_exc()
            return Response(
                {
                    'error': 'Failed to retrieve API endpoint inventory',
                    'details': str(e),
                    'repository': repo_id
                },
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _calculate_rest_score(self, path_patterns):
        """
        Calculate a simple RESTfulness score based on path patterns.
        
        Args:
            path_patterns (List[str]): List of URL paths
            
        Returns:
            float: Score between 0 and 1 indicating RESTfulness
        """
        if not path_patterns:
            return 0.0
        
        rest_indicators = 0
        total_paths = len(path_patterns)
        
        for path in path_patterns:
            # Check for RESTful patterns
            if '/{' in path or '/id' in path or re.search(r'/\d+', path):
                rest_indicators += 1  # Has path parameters
            if path.count('/') >= 2:
                rest_indicators += 0.5  # Has reasonable depth
            if not any(action in path.lower() for action in ['create', 'update', 'delete', 'get', 'list']):
                rest_indicators += 0.5  # Doesn't use action verbs in URL
        
        return min(1.0, rest_indicators / total_paths)

--- FILE_END: backend/api/views.py ---

--- FILE_START: backend/manage.py ---
# In ~/lumiere_semantique/backend/manage.py
#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys


def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lumiere_core.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == "__main__":
    main()

--- FILE_END: backend/manage.py ---

--- FILE_START: README.md ---
# Lumière Sémantique

--- FILE_END: README.md ---

--- FILE_START: .gitignore ---
# In ~/lumiere_semantique/.gitignore

# --- Python / Django ---
__pycache__/
*.pyc

# --- Virtual Environments ---
# This will ignore venv, venv_broken, etc.
venv/
venv_broken/
*.env
.env

# --- OS / IDE Files ---
.DS_Store
.idea/
.vscode/

# --- Database Files ---
db.sqlite3
*.sqlite3-journal

# --- Lumière Sémantique Generated Artifacts ---
# We don't want to commit the large index and JSON files.
# These should be generated by anyone who clones the repo.
*.json
*.index

# --- Django Media/Static Files ---
media/
static/

# --- Build Artifacts ---
build/
dist/
*.egg-info/

--- FILE_END: .gitignore ---

--- FILE_START: manage.py ---
#!/usr/bin/env python
"""
Lumière Sémantique Project - Root Management Utility.

This script acts as a proxy to the real Django manage.py script
located inside the 'backend' directory. This allows you to run
Django commands from the project root.
"""
import os
import sys
from pathlib import Path

def main():
    # 1. Find the project root and the backend directory
    project_root = Path(__file__).resolve().parent
    backend_dir = project_root / 'backend'

    # 2. Add the backend directory to Python's path
    # This is crucial so that Python can find 'lumiere_core.settings'
    sys.path.insert(0, str(backend_dir))

    # 3. Change the current working directory to the backend
    # This ensures that files like 'db.sqlite3' are found correctly
    os.chdir(backend_dir)

    # 4. Set the DJANGO_SETTINGS_MODULE environment variable
    # This is what the original manage.py does.
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lumiere_core.settings')

    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc

    # 5. Execute the command that was passed to this script
    execute_from_command_line(sys.argv)

if __name__ == "__main__":
    main()

--- FILE_END: manage.py ---


--- PROJECT STRUCTURE ---
/Users/latchy/lumiere_semantique
├── .gitignore
├── backend
│   ├── .env
│   ├── api
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   ├── urls.py
│   │   └── views.py
│   ├── build_parsers.py
│   ├── db.sqlite3
│   ├── ingestion
│   │   ├── __init__.py
│   │   ├── admin.py
│   │   ├── apps.py
│   │   ├── crawler.py
│   │   ├── indexing.py
│   │   ├── jsonifier.py
│   │   ├── management
│   │   │   ├── __init__.py
│   │   │   └── commands
│   │   ├── migrations
│   │   │   └── __init__.py
│   │   ├── models.py
│   │   ├── tests.py
│   │   └── views.py
│   ├── lumiere_core
│   │   ├── __init__.py
│   │   ├── .env
│   │   ├── .gitignore
│   │   ├── asgi.py
│   │   ├── services
│   │   │   ├── __init__.py
│   │   │   ├── ambassador.py
│   │   │   ├── cartographer.py
│   │   │   ├── code_surgery.py
│   │   │   ├── cortex_service.py
│   │   │   ├── crucible.py
│   │   │   ├── diff_parser.py
│   │   │   ├── diplomat.py
│   │   │   ├── documentation.py
│   │   │   ├── gemini_service.py
│   │   │   ├── github.py
│   │   │   ├── graph_differ.py
│   │   │   ├── ingestion_service.py
│   │   │   ├── llm_service.py
│   │   │   ├── ollama_service.py
│   │   │   ├── ollama.py
│   │   │   ├── oracle_service.py
│   │   │   ├── profile_service.py
│   │   │   ├── rca_service.py
│   │   │   ├── review_service.py
│   │   │   ├── scaffolding.py
│   │   │   ├── sentinel_service.py
│   │   │   ├── strategist.py
│   │   │   ├── suggester_service.py
│   │   │   ├── testing.py
│   │   │   └── utils.py
│   │   ├── settings.py
│   │   ├── urls.py
│   │   └── wsgi.py
│   ├── manage.py
│   └── requirements.txt
├── crawler.sh
├── llm_project_context.txt
├── lumiere.py
├── manage.py
├── README.md
└── start_server.sh

10 directories, 64 files

--- END PROJECT STRUCTURE ---

--- FILE_START: lumiere.py ---
# In /Users/latchy/lumiere_semantique/lumiere.py

import typer
import sys
sys.path.append('backend')
import requests
import sys
import re
import shlex
import difflib
import traceback
from typing import Optional, List, Dict, Tuple
from collections import defaultdict
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.markdown import Markdown
from rich.text import Text
from rich.status import Status
from rich.live import Live
from rich.align import Align
from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.shortcuts import confirm
from prompt_toolkit.styles import Style
from pathlib import Path
import json
import time
from datetime import datetime
import textwrap
from rich.tree import Tree

# --- Global Objects & Configuration ---
console = Console()
history_path = Path.home() / ".lumiere" / "history.txt"
config_path = Path.home() / ".lumiere" / "config.json"
history_path.parent.mkdir(parents=True, exist_ok=True)

# --- Centralized API URL ---
API_BASE_URL = "http://127.0.0.1:8002/api/v1"

# Create command completers for better UX
main_commands = ['analyze', 'a', 'ask', 'oracle', 'review', 'dashboard', 'd', 'profile', 'p', 'config', 'c', 'help', 'h', 'exit', 'x', 'quit', 'list-repos', 'lr']
analysis_commands = ['list', 'l', 'fix', 'f', 'briefing', 'b', 'rca', 'r', 'details', 'd', 'graph', 'g', 'help', 'h', 'back', 'exit', 'quit']
oracle_commands = ['help', 'h', 'back', 'exit', 'quit']

main_completer = WordCompleter(main_commands, ignore_case=True)
analysis_completer = WordCompleter(analysis_commands, ignore_case=True)
oracle_completer = WordCompleter(oracle_commands, ignore_case=True)

# --- Style for prompt_toolkit prompt to match rich colors ---
prompt_style = Style.from_dict({
    'lumiere': 'bold #00ffff',  # bold cyan
    'provider': 'yellow',
    'separator': 'white'
})

prompt_session = PromptSession(
    history=FileHistory(str(history_path)),
    completer=main_completer,
    style=prompt_style
)

# --- Global CLI State ---
cli_state = {
    "model": None,  # Will be populated from config
    "available_models": [],
    "last_repo_url": None,
    "debug_mode": False,
}

# --- NEW UTILITY FUNCTIONS for managing analyzed repos ---

def check_if_repo_is_analyzed(repo_id: str) -> bool:
    """Checks if a specific repo_id directory has all the necessary analysis files."""
    cloned_repos_dir = Path("backend/cloned_repositories")
    repo_dir = cloned_repos_dir / repo_id
    if not repo_dir.is_dir():
        return False

    cortex_file = repo_dir / f"{repo_id}_cortex.json"
    faiss_file = repo_dir / f"{repo_id}_faiss.index"
    map_file = repo_dir / f"{repo_id}_id_map.json"

    return all([cortex_file.exists(), faiss_file.exists(), map_file.exists()])

def find_analyzed_repos() -> List[Dict[str, str]]:
    """Scans for previously analyzed repositories and validates their artifacts."""
    analyzed_repos = []
    cloned_repos_dir = Path("backend/cloned_repositories")
    if not cloned_repos_dir.is_dir():
        return []

    for repo_dir in cloned_repos_dir.iterdir():
        if repo_dir.is_dir():
            repo_id = repo_dir.name
            if check_if_repo_is_analyzed(repo_id):
                # Attempt to reconstruct a user-friendly name and the full URL
                display_name = repo_id.replace("_", "/", 1)
                full_url = f"https://github.com/{display_name}"
                analyzed_repos.append({"repo_id": repo_id, "display_name": display_name, "url": full_url})

    return sorted(analyzed_repos, key=lambda x: x['repo_id'])


def load_config():
    """Load configuration from file if it exists."""
    try:
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
                cli_state.update(config)
                console.print(f"[dim]✓ Configuration loaded from {config_path}[/dim]")
    except Exception as e:
        console.print(f"[yellow]Warning: Could not load config: {e}[/yellow]")

def save_config():
    """Save current configuration to file."""
    try:
        with open(config_path, 'w') as f:
            json.dump({
                "model": cli_state["model"],
                "last_repo_url": cli_state["last_repo_url"],
                "debug_mode": cli_state["debug_mode"]
            }, f, indent=2)
    except Exception as e:
        console.print(f"[yellow]Warning: Could not save config: {e}[/yellow]")

def validate_github_url(url: str) -> bool:
    """Validate that the URL is a proper GitHub repository URL."""
    github_pattern = r'^https://github\.com/[\w\-\.]+/[\w\-\.]+/?$'
    return bool(re.match(github_pattern, url))

def format_url(url: str) -> str:
    """Normalize GitHub URL format."""
    url = url.strip().rstrip('/')
    if not url.startswith('https://'):
        if url.startswith('github.com/'):
            url = 'https://' + url
        elif '/' in url and not url.startswith('http'):
            url = 'https://github.com/' + url
    return url

def _insert_docstring_into_code(code: str, docstring: str) -> str:
    """Intelligently inserts a docstring into a Python code snippet."""
    # Find the first function or class definition
    match = re.search(r"^(?P<indent>\s*)(def|class)\s+\w+", code, re.MULTILINE)
    if not match:
        return code # Cannot find where to insert, return original

    indentation = match.group('indent')
    insertion_point = match.end()

    # Prepare the docstring with the correct indentation
    indented_docstring = textwrap.indent(f'"""{docstring}"""', indentation + '    ')

    # Insert the docstring
    return f"{code[:insertion_point]}\n{indented_docstring}{code[insertion_point:]}"

# --- Enhanced API Client ---
class LumiereAPIClient:
    def __init__(self, base_url: str = API_BASE_URL, timeout: int = 600):
        self.base_url = base_url
        self.timeout = timeout
        self.session = requests.Session()

    def _request(self, method: str, endpoint: str, **kwargs):
        try:
            if method.upper() in ["POST"]:
                data = kwargs.get("json", {})
                # The Task Router now handles model selection, so we don't add it here.
                # Only check for existence if it's a legacy endpoint that needs it.
                kwargs["json"] = data
            url = f"{self.base_url}/{endpoint}"
            if cli_state["debug_mode"]:
                console.print(f"[dim]DEBUG: {method} {url}[/dim]")
                if "json" in kwargs:
                    console.print(f"[dim]DEBUG: Payload: {kwargs['json']}[/dim]")
            response = self.session.request(method, url, timeout=self.timeout, **kwargs)
            response.raise_for_status()
            # Handle potential empty responses from server
            if response.status_code == 204 or not response.content:
                return {}
            return response.json()
        except requests.exceptions.ConnectionError as e:
            console.print(Panel(f"[bold red]Cannot connect to Lumière backend[/bold red]\n[yellow]Expected URL:[/yellow] {self.base_url}\n[yellow]Error:[/yellow] {str(e)}\n\n[dim]💡 Make sure the backend server is running.[/dim]", title="[red]Connection Error[/red]", border_style="red"))
            return None
        except requests.exceptions.HTTPError as e:
            try:
                error_json = e.response.json()
                error_details = error_json.get('error', str(error_json))
                console.print(Panel(f"[bold red]API Request Failed[/bold red]\n[yellow]URL:[/yellow] {e.request.url}\n[yellow]Status:[/yellow] {e.response.status_code}\n[yellow]Error:[/yellow] {error_details}", title="[red]API Error[/red]", border_style="red"))
                llm_response_for_debug = error_json.get('llm_response')
                if llm_response_for_debug:
                    console.print(Panel(Text(llm_response_for_debug, overflow="fold"), title="[bold yellow]🔍 LLM Raw Response (for debugging)[/bold yellow]", border_style="yellow", expand=False))
            except json.JSONDecodeError:
                console.print(Panel(f"[bold red]HTTP Error {e.response.status_code}[/bold red]\n[yellow]URL:[/yellow] {e.request.url}\n\n[bold]Response Text:[/bold]\n{e.response.text[:500]}...", title="[red]Non-JSON API Error[/red]", border_style="red"))
            return None
        except requests.exceptions.Timeout:
            console.print(Panel(f"The request took longer than {self.timeout} seconds.", title="[red]Timeout Error[/red]"))
            return None
        except requests.exceptions.RequestException as e:
            console.print(Panel(f"[bold red]Request Error[/bold red]\n[yellow]Error:[/yellow] {str(e)}", title="[red]Request Error[/red]"))
            return None

    def health_check(self) -> bool:
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False

    def list_models(self): return self._request("GET", "models/list/")
    def get_analysis(self, repo_url: str): return self._request("POST", "strategist/prioritize/", json={"repo_url": repo_url})
    def get_briefing(self, issue_url: str): return self._request("POST", "briefing/", json={"issue_url": issue_url})
    def get_rca(self, repo_url: str, bug_description: str): return self._request("POST", "rca/", json={"repo_url": repo_url, "bug_description": bug_description})
    def get_profile(self, username: str): return self._request("POST", "profile/review/", json={"username": username})
    def get_graph(self, repo_id: str): return self._request("GET", f"graph/?repo_id={repo_id}")
    def generate_docstring(self, repo_id: str, code: str, instruction: str): return self._request("POST", "generate-docstring/", json={"repo_id": repo_id, "new_code": code, "instruction": instruction})
    def generate_tests(self, repo_id: str, code_to_test: str, instruction: str): return self._request("POST", "generate-tests/", json={"repo_id": repo_id, "new_code": code_to_test, "instruction": instruction})
    def generate_scaffold(self, repo_id: str, target_files: List[str], instruction: str, rca_report: str, refinement_history: Optional[List[Dict]] = None):
        payload = {"repo_id": repo_id, "target_files": target_files, "instruction": instruction, "rca_report": rca_report, "refinement_history": refinement_history or []}
        return self._request("POST", "scaffold/", json=payload)
    def create_pr(self, issue_url: str, modified_files: Dict[str, str]): return self._request("POST", "ambassador/dispatch/", json={"issue_url": issue_url, "modified_files": modified_files})
    def get_diplomat_report(self, issue_title: str, issue_body: str): return self._request("POST", "diplomat/find-similar-issues/", json={"issue_title": issue_title, "issue_body": issue_body})
    def validate_in_crucible(self, repo_url: str, target_file: str, modified_code: str): return self._request("POST", "crucible/validate/", json={"repo_url": repo_url, "target_file": target_file, "modified_code": modified_code})
    def ingest_repository(self, repo_url: str): return self._request("POST", "ingest/", json={"repo_url": repo_url})
    def ask_oracle(self, repo_id: str, question: str): return self._request("POST", "oracle/ask/", json={"repo_id": repo_id, "question": question})
    def adjudicate_pr(self, pr_url: str): return self._request("POST", "review/adjudicate/", json={"pr_url": pr_url})
    def harmonize_fix(self, pr_url: str, review_text: str): return self._request("POST", "review/harmonize/", json={"pr_url": pr_url, "review_text": review_text})
    def get_sentinel_briefing(self, repo_id: str): return self._request("GET", f"sentinel/briefing/?repo_id={repo_id}")
    # --- NEW: Method for the Mission Controller ---
    def get_next_actions(self, last_action: str, result_data: dict): return self._request("POST", "suggest-actions/", json={"last_action": last_action, "result_data": result_data})


def _present_next_actions(api_client, last_action: str, context: dict) -> Tuple[Optional[str], dict]:
    """
    Gets suggestions from the backend and presents a dynamic menu to the user.
    This is the core of the Conversational Mission Controller.

    Args:
        api_client: The instance of LumiereAPIClient.
        last_action: The command that was just executed.
        context: A dictionary containing data from the last action's result.

    Returns:
        A tuple of (command_string, context_dict) for the next action, or (None, {})
    """
    response = api_client.get_next_actions(last_action, context)
    if not response or "suggestions" not in response:
        return None, {}  # No suggestions, fall back to main loop

    suggestions = response["suggestions"]
    recommended_choice = response["recommended_choice"]

    if not suggestions:
        return None, {}

    # Build the rich prompt
    table = Table(title="[bold yellow]🚀 What's Next?[/bold yellow]", show_header=False, box=None, padding=(0, 2))
    choices = []
    choice_map = {}
    for item in suggestions:
        key = item["key"]
        text = item["text"]
        command = item["command"]
        table.add_row(f"([bold cyan]{key}[/bold cyan])", text)
        choices.append(key)
        choice_map[key] = command

    console.print(table)

    try:
        user_choice_key = Prompt.ask("Select an action", choices=choices, default=recommended_choice)
        selected_command = choice_map[user_choice_key]

        if selected_command == "back":
            return "back", {}

        # The context dictionary is passed through to the next command handler.
        return selected_command, context

    except (KeyboardInterrupt, EOFError):
        console.print("\n[yellow]Action cancelled.[/yellow]")
        return "back", {} # Treat cancel as going back to main menu
    except (ValueError, KeyError):
        console.print("[red]Invalid selection.[/red]")
        return "back", {}

# --- NEW: Oracle Session Manager ---
class OracleSession:
    def __init__(self, repo_url: str):
        self.repo_url = repo_url
        self.repo_id = self.repo_url.replace("https://github.com/", "").replace("/", "_")
        self.api = LumiereAPIClient()
        console.print(Panel(
            f"[bold magenta]🔮 Oracle Session Activated[/bold magenta]\n"
            f"[yellow]Repository:[/yellow] {self.repo_id}\n"
            f"[yellow]Model:[/yellow] {cli_state['model']}\n\n"
            "[dim]Ask any architectural question about the codebase. Type 'back' or 'exit' to finish.[/dim]",
            border_style="magenta"
        ))

    def loop(self):
        """Main interactive Q&A loop for The Oracle."""
        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=oracle_completer,
            style=prompt_style
        )

        while True:
            try:
                # Custom prompt for the Oracle
                oracle_prompt_text = [
                    ('class:lumiere', 'Lumière'),
                    ('class:provider', f' (Oracle/{self.repo_id})'),
                    ('class:separator', ' > '),
                ]

                question = prompt_session.prompt(oracle_prompt_text).strip()

                if not question:
                    continue
                if question.lower() in ("q", "quit", "exit", "back"):
                    break
                if question.lower() in ("h", "help"):
                     console.print("\n[dim]Enter your question or type 'back' to exit The Oracle.[/dim]")
                     continue

                with Status("[cyan]The Oracle is consulting the archives...[/cyan]", spinner="dots"):
                    response = self.api.ask_oracle(self.repo_id, question)

                if response and response.get("answer"):
                    console.print(Panel(
                        Markdown(response["answer"]),
                        title="[bold magenta]🔮 The Oracle's Answer[/bold magenta]",
                        border_style="magenta"
                    ))
                elif response and response.get("error"):
                    console.print(Panel(response["error"], title="[yellow]Oracle Warning[/yellow]", border_style="yellow"))
                else:
                    console.print("[red]❌ The Oracle did not provide an answer.[/red]")

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to the main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[magenta]🔮 Oracle session ended.[/magenta]")
        # Restore main completer
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )


# --- MODIFIED: Implemented a two-step provider/model selection process. ---
def handle_model_selection(api: "LumiereAPIClient"):
    """
    Guides the user through a two-step process: first selecting an LLM provider,
    then selecting a model from that provider.
    """
    console.print("\n[bold cyan]🤖 LLM Provider & Model Selection[/bold cyan]")
    with Status("[cyan]Fetching available models from backend...[/cyan]"):
        available_models_data = api.list_models()

    if not available_models_data:
        # Error is printed by the API client
        return

    cli_state["available_models"] = available_models_data

    # Step 1: Group models by provider
    providers = defaultdict(list)
    for model in available_models_data:
        provider = model.get('provider', 'unknown').capitalize()
        providers[provider].append(model)

    if not providers:
        console.print("[red]No providers with available models found.[/red]")
        return

    provider_names = list(providers.keys())

    try:
        # Step 2: Prompt for the provider
        console.print("\n[bold]First, select a provider:[/bold]")
        provider_table = Table(show_header=False, box=None, padding=(0, 2))
        for i, name in enumerate(provider_names, 1):
            provider_table.add_row(f"[cyan]({i})[/cyan]", name)
        console.print(provider_table)

        # Determine default provider choice
        current_provider = ""
        if cli_state.get("model"):
            current_provider = cli_state.get("model").split('/')[0].capitalize()

        default_provider_idx = "1"
        if current_provider in provider_names:
            default_provider_idx = str(provider_names.index(current_provider) + 1)

        provider_choice_str = Prompt.ask(
            "Enter your provider choice",
            choices=[str(i) for i in range(1, len(provider_names) + 1)],
            show_choices=False,
            default=default_provider_idx
        )
        selected_provider_name = provider_names[int(provider_choice_str) - 1]

        # Step 3: Prompt for a model from the selected provider
        models_for_provider = providers[selected_provider_name]

        model_table = Table(title=f"Available Models from [yellow]{selected_provider_name}[/yellow]", border_style="blue")
        model_table.add_column("Choice #", style="dim", justify="center")
        model_table.add_column("Model Name", style="white")
        model_table.add_column("Model ID", style="cyan")

        model_choices_for_prompt = []
        for i, model in enumerate(models_for_provider, 1):
            model_choices_for_prompt.append(str(i))
            model_table.add_row(
                str(i),
                model.get('name', 'N/A'),
                model.get('id', 'N/A')
            )
        console.print(model_table)

        # Find default selection if a model from this provider is already selected
        current_model_index_str = "1"
        current_model_id = cli_state.get("model")
        if current_model_id and current_model_id.startswith(selected_provider_name.lower()):
            for i, model in enumerate(models_for_provider):
                if model['id'] == current_model_id:
                    current_model_index_str = str(i + 1)
                    break

        model_choice_str = Prompt.ask(
            "[bold]Select a model number to use[/bold]",
            choices=model_choices_for_prompt,
            show_choices=False,
            default=current_model_index_str
        )
        selected_model_index = int(model_choice_str) - 1
        selected_model_id = models_for_provider[selected_model_index]['id']

        cli_state["model"] = selected_model_id
        save_config()
        console.print(f"✅ Model set to [bold green]{cli_state['model']}[/bold green]. This will be saved for future sessions.")

    except (ValueError, IndexError):
        console.print("[red]❌ Invalid selection.[/red]")
    except KeyboardInterrupt:
        console.print("\n[yellow]Model selection cancelled.[/yellow]")

# --- MODIFIED: Dynamic prompt text based on selected model/provider ---
def get_prompt_text() -> List[Tuple[str, str]]:
    """
    Builds a prompt_toolkit-compatible formatted text list for the prompt.
    Dynamically displays the provider or model name.
    """
    display_name = "Choose Provider"  # Default text when no model is selected
    model_id = cli_state.get("model")

    if model_id:
        parts = model_id.split('/', 1)
        if len(parts) == 2:
            provider, model_name = parts
            if provider == "ollama":
                display_name = model_name  # Show specific model name for ollama
            else:
                display_name = provider.capitalize()  # Show provider name for others (e.g., Gemini)
        else:
            display_name = model_id  # Fallback for malformed ID

    return [
        ('class:lumiere', 'Lumière'),
        ('class:provider', f' ({display_name})'),
        ('class:separator', ' > '),
    ]

# --- Enhanced Analysis Session Manager ---
class AnalysisSession:
    def __init__(self, repo_url: str):
        self.repo_url = format_url(repo_url)
        if not validate_github_url(self.repo_url):
            raise ValueError(f"Invalid GitHub URL: {self.repo_url}")

        self.repo_id = self.repo_url.replace("https://github.com/", "").replace("/", "_")
        self.issues = []
        # --- State for RCA-to-Fix Pipeline ---
        self.last_rca_report = None
        self.last_rca_issue_num = None
        # ---
        self.api = LumiereAPIClient()
        cli_state["last_repo_url"] = self.repo_url
        save_config()

    def start(self) -> bool:
        """Initialize the analysis session."""
        console.print(Panel(
            f"[bold cyan]🔍 Analysis Session Starting[/bold cyan]\n"
            f"[yellow]Repository:[/yellow] {self.repo_url}\n"
            f"[yellow]Model:[/yellow] {cli_state['model']}",
            border_style="cyan"
        ))

        with Status("[cyan]Checking backend connection...") as status:
            if not self.api.health_check():
                return False # Error already printed by client
            status.update("[green]✓ Backend connection established")
            time.sleep(0.5)

        is_already_analyzed = check_if_repo_is_analyzed(self.repo_id)
        do_embed = False

        if is_already_analyzed:
            console.print(f"\n[bold green]✓ Found existing analyzed repository for '{self.repo_id}'.[/bold green] [dim]Skipping ingestion.[/dim]")
        else:
            try:
                do_embed = Confirm.ask(
                    "\n[bold]Do you want to clone and embed this repo for full analysis (briefing, rca, fix)?[/bold]\n"
                    "[dim](This can take a few minutes for large repos. Choose 'N' for issue listing only.)[/dim]",
                    default=True
                )
            except KeyboardInterrupt:
                console.print("\n[yellow]Analysis cancelled.[/yellow]")
                return False

        if do_embed:
            with Status("[cyan]🚀 Beginning ingestion...[/cyan]", spinner="earth") as status:
                status.update("[cyan]Cloning repository and analyzing files...[/cyan]")
                ingest_result = self.api.ingest_repository(self.repo_url)
                if ingest_result and ingest_result.get("status") == "success":
                    status.update("[green]✓ Repository cloned and embedded successfully.[/green]")
                    time.sleep(1)
                else:
                    error_details = ingest_result.get('error', 'Unknown error during ingestion.') if ingest_result else "No response from server."
                    console.print(f"\n[bold red]❌ Ingestion failed:[/bold red] {error_details}")
                    try:
                        if not Confirm.ask("[yellow]Would you like to continue with limited (issue list only) analysis?[/yellow]", default=True):
                            return False
                    except KeyboardInterrupt:
                        return False

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("[green]🤖 Contacting The Strategist...", total=None)
            strategist_data = self.api.get_analysis(self.repo_url)
            if not strategist_data:
                return False

        self.issues = strategist_data.get("prioritized_issues", [])
        if not self.issues:
            console.print("[yellow]📭 No open issues found in this repository.[/yellow]")
            return False

        console.print(f"✨ The Strategist identified [bold green]{len(self.issues)}[/bold green] open issues for analysis.")
        self.display_issue_table()
        return True

    def display_issue_table(self):
        """Display a formatted table of prioritized issues."""
        table = Table(
            title="[bold blue]🎯 Prioritized Issue Triage[/bold blue]",
            show_header=True,
            header_style="bold magenta",
            border_style="blue"
        )
        table.add_column("Rank", style="dim", justify="center", width=6)
        table.add_column("Score", style="bold", justify="center", width=8)
        table.add_column("Issue #", style="green", justify="center", width=10)
        table.add_column("Title", style="white", no_wrap=False)

        for issue in self.issues:
            score = issue.get('score', 0)
            score_style = "red" if score >= 90 else "yellow" if score >= 70 else "white"
            score_emoji = "🔥" if score >= 90 else "⚡" if score >= 70 else "📝"

            table.add_row(
                f"#{issue['rank']}",
                f"{score_emoji} [{score_style}]{score}[/{score_style}]",
                f"#{issue['number']}",
                issue['title'][:80] + "..." if len(issue['title']) > 80 else issue['title']
            )
        console.print(table)

    def display_graph(self, graph_data: dict, repo_id: str):
        """
        [NEW & IMPROVED] Displays the architectural graph in a summarized, readable format.
        It now groups and counts calls to avoid overwhelming the user.
        """
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph (Summary) ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])

        if not nodes:
            console.print("[yellow]No architectural nodes were mapped for this project.[/yellow]")
            return

        edges_by_source = defaultdict(lambda: {'imports': [], 'calls': defaultdict(int)})
        for edge in edges:
            source_id = edge['source']
            edge_type = edge['type']
            target_id = edge['target']

            if edge_type == 'IMPORTS':
                edges_by_source[source_id]['imports'].append(target_id)
            elif edge_type == 'CALLS':
                edges_by_source[source_id]['calls'][target_id] += 1

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]", guide_style="cyan")
        file_tree_nodes = {}

        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                lang = node_data.get('language', 'unknown')
                icon = "📄"
                if lang == 'python': icon = "🐍"
                if lang == 'javascript': icon = "🟨"

                file_branch = tree.add(f"{icon} [bold green]{node_id}[/bold green] [dim]({lang})[/dim]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_node_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")

                    for method_name in sorted(nodes.get(class_node_id, {}).get('methods', [])):
                        class_branch.add(f"  -  M [dim]{method_name}()[/dim]")

                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"  - F [dim]{func_name}()[/dim]")

        for source_id, relationships in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]

                if relationships['imports']:
                    import_branch = parent_branch.add("📥 [bold]Imports[/bold]")
                    for target in sorted(list(set(relationships['imports']))):
                        import_branch.add(f"[yellow]{target}[/yellow]")

                if relationships['calls']:
                    calls_branch = parent_branch.add("📞 [bold]Calls[/bold]")
                    sorted_calls = sorted(relationships['calls'].items(), key=lambda item: item[1], reverse=True)

                    max_calls_to_show = 15
                    for i, (target, count) in enumerate(sorted_calls):
                        if i >= max_calls_to_show:
                            calls_branch.add(f"[dim]... and {len(sorted_calls) - max_calls_to_show} more.[/dim]")
                            break

                        count_str = f" [dim](x{count})[/dim]" if count > 1 else ""
                        calls_branch.add(f"[magenta]{target}[/magenta]{count_str}")


        console.print(tree)
        console.print("\n[bold magenta]--------------------------------------------------------[/bold magenta]")

    def loop(self):
        """Main interactive loop for the analysis session."""
        display_interactive_help('analyze')

        global prompt_session
        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=analysis_completer,
            style=prompt_style
        )

        while True:
            try:
                prompt_text = get_prompt_text()
                command_str = prompt_session.prompt(prompt_text).strip()

                if not command_str:
                    continue

                if command_str.lower() in ("q", "quit", "exit", "back"):
                    break

                self.handle_analysis_command(command_str)

            except KeyboardInterrupt:
                console.print("\n[yellow]Use 'exit' or 'back' to return to main menu.[/yellow]")
                continue
            except EOFError:
                break

        console.print("[cyan]📊 Analysis session ended.[/cyan]")

        prompt_session = PromptSession(
            history=FileHistory(str(history_path)),
            completer=main_completer,
            style=prompt_style
        )

    def handle_analysis_command(self, command_str: str):
        """Handle commands within the analysis session."""
        try:
            parts = shlex.split(command_str.lower())
        except ValueError:
            console.print("[red]❌ Invalid command syntax.[/red]")
            return

        command = parts[0] if parts else ""
        args = parts[1:] if len(parts) > 1 else []

        if command in ("h", "help"):
            display_interactive_help('analyze')
            return

        if command in ("l", "list"):
            self.display_issue_table()
            return

        if command in ('g', 'graph'):
            self.execute_action(command, {}) # Pass empty dict, issue not needed
            console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
            return

        if command in ('f', 'fix') and self.last_rca_report:
             issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
             if issue:
                 self.execute_action(command, issue)
                 console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")
                 return

        if command not in ('f', 'fix', 'b', 'briefing', 'r', 'rca', 'd', 'details'):
            console.print("[red]❌ Unknown command. Type 'help' for available commands.[/red]")
            return

        issue_num_str = None
        if args and args[0].isdigit():
            issue_num_str = args[0]
        else:
            try:
                prompt_ask_text = f"Which issue # for '[cyan]{command}[/cyan]'?"
                if command in ('f', 'fix') and self.last_rca_report:
                     prompt_ask_text += f"\n[dim](Press Enter to fix issue #{self.last_rca_issue_num} from the last RCA)[/dim]"

                issue_num_str = Prompt.ask(prompt_ask_text).strip()
            except KeyboardInterrupt:
                console.print("\n[yellow]Command cancelled.[/yellow]")
                return

        if not issue_num_str:
            if command in ('f', 'fix') and self.last_rca_report:
                issue = next((iss for iss in self.issues if iss.get('number') == self.last_rca_issue_num), None)
                if issue:
                    self.execute_action(command, issue)
                else:
                    console.print("[red]❌ Could not find issue from last RCA. Please specify an issue number.[/red]")
            else:
                console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        if not issue_num_str.isdigit():
            console.print("[red]❌ Please enter a valid issue number.[/red]")
            return

        target_issue = next((iss for iss in self.issues if iss.get('number') == int(issue_num_str)), None)
        if not target_issue:
            console.print(f"[red]❌ Issue #{issue_num_str} not found in the prioritized list.[/red]")
            console.print("[dim]💡 Use 'list' to see available issues.[/dim]")
            return

        self.execute_action(command, target_issue)
        console.print("\n[dim]💡 Type [bold]list[/bold] to see issues, or [bold]help[/bold] for commands.[/dim]")

    def execute_action(self, command: str, issue: Dict):
        """Execute the specified action on an issue."""
        if command in ("f", "fix"):
            self.handle_fix_dialogue(issue)
            return

        if command in ("r", "rca"):
            self.handle_rca_command(issue)
            return

        if command in ('g', 'graph'):
            with Status("[cyan]🗺️ Contacting Cartographer's Architectural Graph...[/cyan]", spinner="earth") as status:
                graph_data = self.api.get_graph(self.repo_id)
                status.update("[green]✓ Graph retrieved.[/green]")
                time.sleep(0.5)

            if graph_data and graph_data.get("graph"):
                self.display_graph(graph_data["graph"], graph_data["repo_id"])
            elif graph_data and graph_data.get("message"):
                console.print(Panel(graph_data["message"], title="[yellow]Graph Not Available[/yellow]", border_style="yellow"))
            else:
                 console.print("[red]❌ Could not retrieve architectural graph.[/red]")
            return

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            if command in ("b", "briefing"):
                task = progress.add_task(f"[cyan]📋 Getting briefing for issue #{issue['number']}...", total=None)
                briefing_data = self.api.get_briefing(f"{self.repo_url}/issues/{issue['number']}")
                progress.remove_task(task)
                if briefing_data and briefing_data.get("briefing"):
                    console.print(Panel(
                        Markdown(briefing_data["briefing"]),
                        title=f"[bold blue]📋 Issue Briefing #{issue['number']}[/bold blue]",
                        border_style="blue"
                    ))
                else:
                    console.print("[red]❌ Could not retrieve briefing.[/red]")
            elif command in ("d", "details"):
                issue_url = f"{self.repo_url}/issues/{issue['number']}"
                console.print(Panel(
                    f"[bold]Issue #{issue['number']}[/bold]\n"
                    f"[yellow]Title:[/yellow] {issue['title']}\n"
                    f"[yellow]Priority Score:[/yellow] {issue['score']}/100\n"
                    f"[yellow]URL:[/yellow] [link={issue_url}]{issue_url}[/link]\n"
                    f"[yellow]Description:[/yellow] {issue.get('description', 'No description available')[:200]}...",
                    title="[bold green]📝 Issue Details[/bold green]",
                    border_style="green"
                ))

    def handle_rca_command(self, issue: Dict):
        """Handle the new context-aware root cause analysis command."""
        console.print(f"[cyan]🔍 Starting Root Cause Analysis for issue #{issue['number']}[/cyan]")
        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("[cyan]🕵️ Performing multi-file root cause analysis...", total=None)
            rca_data = self.api.get_rca(self.repo_url, issue_desc)
            progress.remove_task(task)

        if rca_data and rca_data.get("analysis"):
            analysis_text = rca_data["analysis"]

            is_error = "Error from Gemini API" in analysis_text or "API Request Failed" in analysis_text

            if is_error:
                self.last_rca_report = None
                self.last_rca_issue_num = None
                console.print(Panel(
                    Markdown(analysis_text),
                    title=f"[bold red]🕵️ Root Cause Analysis - Issue #{issue['number']} (Failed)[/bold red]",
                    border_style="red"
                ))
            else:
                self.last_rca_report = analysis_text
                self.last_rca_issue_num = issue['number']
                console.print(Panel(
                    Markdown(self.last_rca_report),
                    title=f"[bold red]🕵️ Root Cause Analysis - Issue #{issue['number']}[/bold red]",
                    border_style="red"
                ))
                console.print("\n[bold yellow]💡 Pro-tip:[/bold yellow] [dim]You can now type '[/dim][bold]f[/bold][dim]' to start fixing this issue.[/dim]")
        else:
            self.last_rca_report = None
            self.last_rca_issue_num = None
            console.print("[red]❌ Could not perform root cause analysis.[/red]")

    def _display_diff(self, original_code: str, new_code: str, filename: str):
        """Display a formatted diff of code changes for a single file."""
        diff = difflib.unified_diff(
            original_code.splitlines(keepends=True),
            new_code.splitlines(keepends=True),
            fromfile=f'🔴 {filename} (Original)',
            tofile=f'🟢 {filename} (Proposed)'
        )
        diff_panel_content = Text()
        has_changes = False
        for line in diff:
            has_changes = True
            if line.startswith('+++') or line.startswith('---'):
                diff_panel_content.append(line, style="bold blue")
            elif line.startswith('+'):
                diff_panel_content.append(line, style="green")
            elif line.startswith('-'):
                diff_panel_content.append(line, style="red")
            elif line.startswith('@'):
                diff_panel_content.append(line, style="bold yellow")
            else:
                diff_panel_content.append(line, style="dim")

        if not has_changes:
            return

        console.print(Panel(
            diff_panel_content,
            title=f"[bold yellow]📝 Proposed Changes for {filename}[/bold yellow]",
            expand=True,
            border_style="yellow"
        ))

    def _extract_filenames_from_rca(self, rca_report: str) -> List[str]:
        """Extracts filenames from markdown code fences, inline backticks, and lists."""
        pattern = r'(?:\s|-|\*|`)([\w./-]+\.(?:py|js|ts|gs|json|md|html|css|yaml|yml|toml|txt))\b'
        matches = re.findall(pattern, rca_report)

        backtick_pattern = r'`([\w./\\-]+)`'
        matches.extend(re.findall(backtick_pattern, rca_report))

        filenames = sorted(list(set(matches)))
        return [f for f in filenames if '.' in f and f.lower() not in ['true', 'false']]

    def handle_fix_dialogue(self, issue: Dict):
        """Handle the complete fix dialogue, now driven by RCA and with documentation and automated test generation."""
        console.print(Panel(
            f"[bold cyan]🤝 Socratic Dialogue[/bold cyan] starting for:\n"
            f"[bold green]Issue #{issue['number']}: {issue['title']}[/bold green]",
            border_style="cyan"
        ))

        if not self.last_rca_report or self.last_rca_issue_num != issue['number']:
             console.print("[yellow]⚠️  Warning: No Root Cause Analysis has been run for this issue.[/yellow]")
             try:
                 if Confirm.ask("[bold]Would you like to run RCA first to provide context for the fix?[/bold]", default=True):
                     self.handle_rca_command(issue)
                     if not self.last_rca_report:
                         console.print("[red]❌ Cannot proceed with fix without a successful RCA.[/red]")
                         return
                 else:
                     console.print("[red]❌ Fix command cancelled. Please run RCA first.[/red]")
                     return
             except KeyboardInterrupt:
                 console.print("\n[yellow]Fix command cancelled.[/yellow]")
                 return

        issue_desc = f"Title: {issue.get('title', '')}\n\nDescription: {issue.get('description', '')}"

        target_files = self._extract_filenames_from_rca(self.last_rca_report)
        if not target_files:
            console.print("[red]❌ Could not automatically determine target files from the RCA report.[/red]")
            return

        console.print(f"[dim]✓ Identified suspect files from RCA: {', '.join(target_files)}[/dim]")

        refinement_history = []
        iteration_count = 0
        max_iterations = 5
        modified_files = {}
        original_contents = {}
        is_documented = False
        are_tests_generated = False

        while iteration_count < max_iterations:
            iteration_count += 1

            if not modified_files or refinement_history:
                console.print(f"\n[dim]🔄 Iteration {iteration_count}/{max_iterations}[/dim]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task(f"[cyan]⚡ Generating multi-file code fix...", total=None)
                    fix_data = self.api.generate_scaffold(self.repo_id, target_files, issue_desc, self.last_rca_report, refinement_history)
                    progress.remove_task(task)

                if not fix_data:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    return

                if "modified_files" not in fix_data or not fix_data["modified_files"]:
                    console.print("[red]❌ Failed to generate fix or no files were modified.[/red]")
                    if fix_data and fix_data.get("llm_response"): console.print(Panel(Text(fix_data["llm_response"], overflow="fold"), title="[yellow]🔍 LLM Raw Response (for debugging)[/yellow]", border_style="yellow"))
                    return

                modified_files = fix_data["modified_files"]
                original_contents = fix_data["original_contents"]
                is_documented = False
                are_tests_generated = False

            console.rule("[bold]📝 Review Proposed Changes[/bold]")
            for filename, new_code in modified_files.items():
                if original_contents.get(filename, "") != new_code:
                    self._display_diff(original_contents.get(filename, ""), new_code, filename)

            all_validations_passed = True
            files_to_validate = [
                f for f in modified_files.keys()
                if not f.lower().endswith(('.md', '.txt', '.json', '.toml', '.yaml', '.yml', '.ron'))
            ]

            if not files_to_validate:
                 console.print(Panel("✅ [bold green]No runnable code files to validate. Skipping Crucible.[/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))
            else:
                for filename in files_to_validate:
                    new_code = modified_files[filename]
                    with Progress(SpinnerColumn(), TextColumn("[bold cyan][progress.description]{task.description}"), transient=True) as progress:
                        task = progress.add_task(f"🔥 Entering The Crucible for {filename}...", total=None)
                        validation_result = self.api.validate_in_crucible(self.repo_url, filename, new_code)
                        progress.remove_task(task)

                    if not validation_result or validation_result.get("status") != "passed":
                        all_validations_passed = False
                        console.print(Panel(f"❌ [bold red]Validation Failed for {filename}[/bold red]\n[bold]Test Results:[/bold]\n{validation_result.get('logs', 'No logs') if validation_result else 'Crucible service error'}", title=f"[red]🔥 Crucible Report: {filename}[/red]", border_style="red"))
                        break

            if all_validations_passed and files_to_validate:
                 console.print(Panel("✅ [bold green]All tests passed for all modified files![/bold green]", title="[green]🔥 Crucible Report[/green]", border_style="green"))

            while True:
                try:
                    action_choices = ['r', 'c']
                    prompt_text = ""

                    if all_validations_passed:
                        action_choices.append('a')
                        prompt_text += "\n[bold]✅ All tests passed! Choose action:[/bold]\n[bold green](a)[/bold green] Approve & create PR\n"
                        if not is_documented and any(f.endswith((".py", ".js", ".ts")) for f in modified_files.keys()):
                           action_choices.append('d')
                           prompt_text += "[bold blue](d)[/bold blue] Document the changes\n"
                        if not are_tests_generated:
                            action_choices.append('t')
                            prompt_text += "[bold yellow](t)[/bold yellow] Generate tests for the fix\n"
                    else:
                        prompt_text += "\n[bold red]❌ Validation failed. Choose action:[/bold red]\n"

                    prompt_text += "[bold yellow](r)[/bold yellow] Refine with feedback\n"
                    prompt_text += "[bold red](c)[/bold red] Cancel"

                    default_choice = 'a' if all_validations_passed else 'r'
                    choice = Prompt.ask(prompt_text, choices=action_choices, default=default_choice).lower()

                except KeyboardInterrupt:
                    choice = 'c'

                if choice == 'd':
                    if is_documented:
                        console.print("[yellow]Code is already documented.[/yellow]")
                        continue
                    if not all_validations_passed:
                        console.print("[red]Cannot document code that has failed validation.[/red]")
                        continue

                    documented_files = {}
                    with Status("[bold blue]✒️  Calling The Chronicler agent to document changes...[/bold blue]") as status:
                        for filename, code in modified_files.items():
                            if not any(filename.endswith(ext) for ext in [".py", ".js", ".ts"]): continue
                            status.update(f"[bold blue]✒️  Documenting {filename}...[/bold blue]")
                            doc_result = self.api.generate_docstring(self.repo_id, code, issue_desc)
                            if doc_result and doc_result.get("docstring"):
                                documented_code = _insert_docstring_into_code(code, doc_result["docstring"])
                                documented_files[filename] = documented_code
                            else:
                                documented_files[filename] = code
                    modified_files.update(documented_files)
                    is_documented = True
                    console.print("[green]✓ Documentation complete.[/green]")
                    console.rule("[bold]📝 Review Updated Changes with Documentation[/bold]")
                    for filename, new_code in modified_files.items():
                         if original_contents.get(filename, "") != new_code:
                            self._display_diff(original_contents.get(filename, ""), new_code, filename)
                    continue

                if choice == 't':
                    if are_tests_generated:
                        console.print("[yellow]Tests have already been generated for this fix.[/yellow]")
                        continue
                    if not all_validations_passed:
                        console.print("[red]Cannot generate tests for code that has failed validation.[/red]")
                        continue

                    generated_test_files = {}
                    with Status("[bold yellow]🔬 Calling Test Generation Agent...[/bold yellow]") as status:
                        for filename, code in modified_files.items():
                            if any(filename.endswith(ext) for ext in [".py", ".js", ".ts"]) and 'test' not in filename:
                                status.update(f"[bold yellow]🔬 Generating tests for {filename}...[/bold yellow]")
                                test_result = self.api.generate_tests(self.repo_id, code, issue_desc)

                                if test_result and test_result.get("generated_tests"):
                                    test_file_path = f"tests/test_{Path(filename).stem}.py"
                                    generated_test_files[test_file_path] = test_result["generated_tests"]
                                    console.print(f"  [green]✓[/green] Generated test file: [cyan]{test_file_path}[/cyan]")
                                else:
                                    console.print(f"  [red]✗[/red] Failed to generate tests for [cyan]{filename}[/cyan].")

                    if generated_test_files:
                        modified_files.update(generated_test_files)
                        are_tests_generated = True
                        console.print("\n[green]✓ Test generation complete.[/green]")
                        console.rule("[bold]📝 Review New Test Files[/bold]")
                        for filename, new_code in generated_test_files.items():
                            self._display_diff("", new_code, filename)
                    else:
                        console.print("[yellow]No new tests were generated.[/yellow]")

                    continue

                if choice == 'c': break
                if choice == 'a' and all_validations_passed: break
                if choice == 'r': break

            if choice == 'c':
                console.print("[yellow]🛑 Operation cancelled.[/yellow]")
                break

            if choice == 'r':
                if iteration_count >= max_iterations:
                    console.print(f"[yellow]⚠️ Maximum iterations ({max_iterations}) reached.[/yellow]")
                    break
                try:
                    feedback = Prompt.ask("\n[bold]💭 Your feedback for improvement[/bold]")
                    if not feedback.strip():
                        console.print("[yellow]⚠️ Empty feedback, skipping refinement.[/yellow]")
                        continue
                    refinement_history.append({"feedback": feedback, "code_generated": modified_files})
                    modified_files.clear()
                    continue
                except KeyboardInterrupt:
                    break

            if choice == 'a' and all_validations_passed:
                with Progress(SpinnerColumn(),TextColumn("[progress.description]{task.description}"),transient=True) as progress:
                    task = progress.add_task("[cyan]🚀 Dispatching Ambassador for multi-file PR...", total=None)
                    pr_data = self.api.create_pr(f"{self.repo_url}/issues/{issue['number']}", modified_files)
                    progress.remove_task(task)
                if pr_data and pr_data.get("pull_request_url"):
                    console.print(Panel(f"✅ [bold green]Success![/bold green]\nPull request created: [link={pr_data['pull_request_url']}]{pr_data['pull_request_url']}[/link]", title="[green]🚀 Mission Complete[/green]", border_style="green"))
                else:
                    console.print("[red]❌ Failed to create pull request.[/red]")
                break

        self.last_rca_report = None
        self.last_rca_issue_num = None


# --- Utility Function for Help Display ---
def display_interactive_help(context: str = 'main'):
    """Display help instructions based on the current CLI context."""
    title = f"🆘 Lumière Help — {context.capitalize()} Context"
    help_table = Table(title=f"[bold magenta]{title}[/bold magenta]", border_style="magenta")
    help_table.add_column("Command", style="bold cyan")
    help_table.add_column("Description", style="white")

    if context == 'main':
        help_table.add_row("analyze / a", "Ingest or re-ingest a repo for analysis")
        help_table.add_row("ask / oracle", "Ask architectural questions about a repo")
        help_table.add_row("review", "Perform an AI-powered review of a Pull Request")
        help_table.add_row("dashboard / d", "View the project health dashboard")
        help_table.add_row("profile / p", "Get GitHub user profile analysis")

        # --- DYNAMIC HELP TEXT ---
        if cli_state.get("model"):
            help_table.add_row("config / c", "Change LLM model or view settings")
        else:
            help_table.add_row("config / c", "Choose LLM provider & model")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("exit / quit", "Exit the application")
    elif context == 'analyze':
        help_table.add_row("list / l", "Show prioritized issues")
        help_table.add_row("graph / g", "Display the repository's architectural graph")
        help_table.add_row("briefing / b", "Show issue briefing")
        help_table.add_row("details / d", "Show issue metadata")
        help_table.add_row("rca / r", "Root cause analysis")
        help_table.add_row("fix / f", "Launch fix dialogue")
        help_table.add_row("help / h", "Show this help menu")
        help_table.add_row("back / exit / quit", "Return to main menu")

    console.print(help_table)

# --- Main Entry Point ---
app = typer.Typer()

@app.command()
def run():
    """Launch Lumière interactive shell."""
    api_client = LumiereAPIClient()

    health_status = api_client._request("GET", "health/")
    if health_status is None:
        console.print("[bold red]Lumière CLI cannot start without a backend connection.[/bold red]")
        sys.exit(1)

    load_config()

    welcome_text = (f"[bold cyan]✨ Welcome to Lumière Sémantique ✨[/bold cyan]\n"
                    f"The Conversational Mission Controller is active.\n\n"
                    f"[dim]Backend Status: [green]Online[/green] at [underline]{API_BASE_URL}[/underline][/dim]")
    console.print(Panel(welcome_text, border_style="cyan"))
    display_interactive_help('main')

    next_command = None
    context = {}

    while True:
        try:
            if next_command is None:
                prompt_text = get_prompt_text()
                command = prompt_session.prompt(prompt_text).strip()
            else:
                command = next_command
                console.print(f"\n[dim]Executing suggested action: [bold]{command}[/bold]...[/dim]")

            next_command = None

            if not command:
                continue

            if command.lower() in ("exit", "quit", "x"):
                console.print("[dim]👋 Goodbye![/dim]")
                break

            elif command.lower() == "back":
                console.print()
                continue

            elif command.lower() in ("help", "h"):
                display_interactive_help('main')
                continue

            elif command.lower() in ("config", "c"):
                console.print(Panel(
                    f"[bold]Current Settings[/bold]\n"
                    f"  [cyan]LLM Model:[/cyan] [yellow]{cli_state.get('model', 'Not set')}[/yellow]\n"
                    f"  [cyan]Last Repo:[/cyan] {cli_state.get('last_repo_url', 'Not set')}\n"
                    f"  [cyan]Debug Mode:[/cyan] {'On' if cli_state.get('debug_mode') else 'Off'}",
                    title="⚙️ Configuration", border_style="magenta"
                ))
                handle_model_selection(api_client)
                continue

            elif command.lower() in ("profile", "p"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue
                username = Prompt.ask("Enter GitHub username")
                if not username.strip(): continue
                with Status("[cyan]Generating profile analysis...[/cyan]"):
                    profile = api_client.get_profile(username)
                if profile and profile.get("profile_summary"):
                    console.print(Panel(Markdown(profile["profile_summary"]), title=f"👤 Profile Analysis for {username}"))
                else: console.print("[red]❌ Could not retrieve profile.[/red]")
                continue

            elif command.lower() in ("ask", "oracle"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_url = context.get("repo_url") or context.get("pr_url", "").split("/pull/")[0]
                if not repo_url:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to ask questions about.", title="[magenta]🔮 The Oracle[/magenta]", border_style="magenta"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_url = analyzed_repos[int(choice) - 1]['url']
                    except (ValueError, IndexError, KeyboardInterrupt): continue

                oracle_session = OracleSession(repo_url)
                oracle_session.loop()
                context = {}
                continue

            elif command.lower() in ("review",):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                pr_url = Prompt.ask("Enter the full GitHub Pull Request URL to review").strip()
                if not pr_url or "github.com" not in pr_url or "/pull/" not in pr_url:
                    console.print("[red]❌ Invalid Pull Request URL.[/red]")
                    continue

                result_data = None
                with Status("[cyan]The Inquisitor is reviewing the PR...[/cyan]", spinner="earth"):
                    result_data = api_client.adjudicate_pr(pr_url)

                if result_data and result_data.get("review"):
                    console.print(Panel(Markdown(result_data["review"]), title=f"⚖️ Inquisitor's Review", border_style="blue"))
                    context = {"pr_url": pr_url, "repo_url": pr_url.split("/pull/")[0], **result_data}
                    next_command, context = _present_next_actions(api_client, "review", context)
                elif result_data and result_data.get("error"):
                    console.print(Panel(f"[bold red]Review Failed:[/bold red]\n{result_data['error']}", title="[red]Inquisitor Error[/red]"))
                else: console.print("[red]❌ The Inquisitor did not provide a review.[/red]")
                continue

            elif command.lower() == "harmonize":
                 pr_url = context.get("pr_url")
                 review_text = context.get("review")
                 if not pr_url or not review_text:
                     console.print("[red]Harmonize command requires context from a review. Please run 'review' first.[/red]")
                     continue

                 with Status("[cyan]The Harmonizer is composing a fix...[/cyan]"):
                     fix_data = api_client.harmonize_fix(pr_url, review_text)

                 if fix_data and "modified_files" in fix_data:
                     console.rule("[bold]📝 Review Harmonizer's Proposed Changes[/bold]")
                     dummy_session = AnalysisSession(pr_url.split('/pull/')[0])
                     for filename, new_code in fix_data["modified_files"].items():
                         original_code = fix_data.get("original_contents", {}).get(filename, "")
                         if original_code != new_code:
                             dummy_session._display_diff(original_code, new_code, filename)
                     console.print(Panel("✅ [bold green]Harmonizer's patch generated.[/bold green]", border_style="green"))
                 else:
                     console.print(Panel(f"[red]Harmonizer failed to generate a fix: {fix_data.get('error', 'Unknown error')}[/red]", border_style="red"))

                 context = {}
                 continue

            elif command.lower() in ("dashboard", "d"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_id = context.get("repo_id")
                if not repo_id:
                    analyzed_repos = find_analyzed_repos()
                    if not analyzed_repos:
                        console.print("[yellow]No analyzed repositories found. Use 'analyze' to ingest a repo first.[/yellow]")
                        continue
                    console.print(Panel("Select a repository to view its health dashboard.", title="[cyan]🔭 The Sentinel[/cyan]", border_style="cyan"))
                    table = Table(show_header=False, box=None, padding=(0, 2))
                    for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                    console.print(table)
                    try:
                        choice = Prompt.ask("Enter choice", choices=[str(i) for i in range(1, len(analyzed_repos) + 1)], show_choices=False, default='1')
                        repo_id = analyzed_repos[int(choice) - 1]['repo_id']
                    except (ValueError, IndexError, KeyboardInterrupt): continue

                with Status("[cyan]The Sentinel is gathering intelligence...[/cyan]"):
                    response = api_client.get_sentinel_briefing(repo_id)

                if response and response.get("briefing"):
                    console.print(Panel(Markdown(response["briefing"]), title=f"[cyan]🔭 Sentinel Health Briefing for {repo_id}[/cyan]", border_style="cyan"))
                    context = {"repo_id": repo_id, "repo_url": f"https://github.com/{repo_id.replace('_', '/')}", **response}
                    next_command, context = _present_next_actions(api_client, "dashboard", context)
                elif response and response.get("error"):
                    console.print(Panel(response['error'], title="[red]Sentinel Error[/red]"))
                else: console.print("[red]❌ The Sentinel did not provide a briefing.[/red]")
                continue

            elif command.lower() in ("analyze", "a"):
                if not cli_state.get("model"):
                    console.print("[bold red]Please select a model first using the 'config' command.[/bold red]")
                    continue

                repo_url_to_analyze = context.get("repo_url")
                if not repo_url_to_analyze:
                    analyzed_repos = find_analyzed_repos()
                    if analyzed_repos:
                        console.print(Panel("Select a repository to analyze or ingest a new one.", title="[cyan]Select Repository for Analysis[/cyan]", border_style="cyan"))
                        table = Table(show_header=False, box=None, padding=(0, 2))
                        for i, repo in enumerate(analyzed_repos, 1): table.add_row(f"([bold cyan]{i}[/bold cyan])", repo['display_name'])
                        table.add_row("([bold yellow]N[/bold yellow])", "Analyze a new repository")
                        console.print(table)
                        choices = [str(i) for i in range(1, len(analyzed_repos) + 1)] + ['n', 'N']
                        try:
                            choice = Prompt.ask("Enter choice", choices=choices, show_choices=False, default='1').lower()
                            if choice == 'n': repo_url_to_analyze = Prompt.ask("Enter GitHub repository URL").strip()
                            else: repo_url_to_analyze = analyzed_repos[int(choice) - 1]['url']
                        except(ValueError, IndexError, KeyboardInterrupt): continue
                    else:
                        repo_url_to_analyze = Prompt.ask("Enter GitHub repository URL").strip()

                if not repo_url_to_analyze: continue

                try:
                    session = AnalysisSession(repo_url_to_analyze)
                    if session.start():
                        session.loop()
                except ValueError as e: console.print(f"[red]{e}[/red]")
                continue

            else:
                console.print("[red]❌ Unknown command. Type 'help' for options.[/red]")

        except KeyboardInterrupt:
            console.print("\n[dim]💤 Interrupted. Type 'exit' to quit.[/dim]")
            next_command = None
            continue
        except EOFError:
            break

if __name__ == "__main__":
    app()

--- FILE_END: lumiere.py ---

--- FILE_START: start_server.sh ---
#!/bin/bash
echo "Starting Lumière Sémantique development server on http://127.0.0.1:8002/"
python manage.py runserver 8002

--- FILE_END: start_server.sh ---

--- FILE_START: backend/ingestion/migrations/__init__.py ---

--- FILE_END: backend/ingestion/migrations/__init__.py ---

--- FILE_START: backend/ingestion/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/ingestion/models.py ---

--- FILE_START: backend/ingestion/management/__init__.py ---

--- FILE_END: backend/ingestion/management/__init__.py ---

--- FILE_START: backend/ingestion/management/commands/__init__.py ---

--- FILE_END: backend/ingestion/management/commands/__init__.py ---

--- FILE_START: backend/ingestion/management/commands/generate_briefing.py ---
# In ingestion/management/commands/generate_briefing.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index
from backend.lumiere_core.services.ollama_service import generate_text

class Command(BaseCommand):
    help = 'Generates a "Pre-flight Briefing" for a given query using a RAG pipeline.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The user query or GitHub issue description.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')
        # --- CHANGE 1: Add an argument for the generation model ---
        parser.add_argument('--generation_model', type=str, default='qwen3:4b', help='The Ollama model to use for text generation.')
        parser.add_argument('--k', type=int, default=7, help='Number of context chunks to retrieve.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        embedding_model = options['embedding_model']
        generation_model = options['generation_model'] # <-- Get the new option
        k = options['k']

        self.stdout.write(self.style.NOTICE(f"Step 1: Retrieving context for query: '{query}'..."))

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        try:
            context_chunks = search_index(
                query_text=query,
                model_name=embedding_model, # Use the embedding model here
                index_path=index_path,
                map_path=map_path,
                k=k
            )
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Failed to retrieve context: {e}"))
            return

        self.stdout.write(self.style.SUCCESS(f"✓ Retrieved {len(context_chunks)} context chunks."))

        context_string = ""
        for i, chunk in enumerate(context_chunks):
            context_string += f"--- Context Chunk {i+1} from file '{chunk['file_path']}' ---\n"
            context_string += chunk['text']
            context_string += "\n\n"

        prompt = f"""
        You are Lumière Sémantique, an expert AI programming assistant acting as a Principal Engineer.
        Your mission is to provide a "Pre-flight Briefing" for a developer about to work on a task.
        Analyze the user's query and the provided context from the codebase to generate your report.

        The report must be clear, concise, and structured in Markdown. It must include the following sections:
        1.  **Task Summary:** Briefly rephrase the user's request.
        2.  **Core Analysis:** Based on the provided context, explain how the system currently works in relation to the query. Synthesize information from the different context chunks.
        3.  **Key Files & Code:** Point out the most important files or functions from the context that the developer should focus on.
        4.  **Suggested Approach or Potential Challenges:** Offer a high-level plan or mention any potential issues you foresee.

        --- PROVIDED CONTEXT FROM THE CODEBASE ---
        {context_string}
        --- END OF CONTEXT ---

        USER'S QUERY: "{query}"

        Now, generate the Pre-flight Briefing.
        """

        self.stdout.write(self.style.NOTICE(f"\nStep 2: Sending context and query to the LLM ('{generation_model}') for generation..."))

        # --- CHANGE 2: Pass the generation model name to the function ---
        final_report = generate_text(prompt, model_name=generation_model)

        self.stdout.write(self.style.SUCCESS("\n--- LUMIÈRE SÉMANTIQUE: PRE-FLIGHT BRIEFING ---"))
        self.stdout.write(final_report)

--- FILE_END: backend/ingestion/management/commands/generate_briefing.py ---

--- FILE_START: backend/ingestion/management/commands/run_indexer.py ---
# In ingestion/management/commands/run_indexer.py

from django.core.management.base import BaseCommand
from ingestion.indexing import EmbeddingIndexer
import os

class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and creates a Faiss index from its text chunks using Ollama.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')
        parser.add_argument(
            '--model',
            type=str,
            default='snowflake-arctic-embed2:latest', # <-- Defaults to your preferred model
            help='The name of the Ollama embedding model to use.'
        )

    def handle(self, *args, **options):
        cortex_file_path = options['cortex_file']
        model_name = options['model']

        if not os.path.exists(cortex_file_path):
            self.stdout.write(self.style.ERROR(f"Error: File not found at '{cortex_file_path}'"))
            return

        self.stdout.write(self.style.NOTICE(f"Starting Ollama indexing for {cortex_file_path} using model '{model_name}'..."))

        try:
            # Pass the model name to the indexer
            indexer = EmbeddingIndexer(model_name=model_name)
            indexer.process_cortex(cortex_file_path)
            self.stdout.write(self.style.SUCCESS('✓ Ollama indexing process completed successfully.'))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'An unexpected error occurred during indexing: {e}'))

--- FILE_END: backend/ingestion/management/commands/run_indexer.py ---

--- FILE_START: backend/ingestion/management/commands/search.py ---
# In ingestion/management/commands/search.py

from django.core.management.base import BaseCommand
from lumiere_core.services.ollama import search_index # <-- Import our new function

class Command(BaseCommand):
    help = 'Searches a Faiss index for a given query string.'

    def add_arguments(self, parser):
        parser.add_argument('repo_id', type=str, help="The ID of the repo (e.g., 'pallets_flask').")
        parser.add_argument('query', type=str, help='The search query string.')
        parser.add_argument('--model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use.')
        parser.add_argument('--k', type=int, default=5, help='The number of results to return.')

    def handle(self, *args, **options):
        repo_id = options['repo_id']
        query = options['query']
        model = options['model']
        k = options['k']

        index_path = f"{repo_id}_faiss.index"
        map_path = f"{repo_id}_id_map.json"

        self.stdout.write(self.style.NOTICE(f"Searching for '{query}'..."))

        try:
            results = search_index(
                query_text=query,
                model_name=model,
                index_path=index_path,
                map_path=map_path,
                k=k
            )

            self.stdout.write(self.style.SUCCESS(f"\n--- Top {len(results)} search results ---"))
            for i, res in enumerate(results):
                self.stdout.write(self.style.HTTP_INFO(f"\n{i+1}. File: {res['file_path']} (Distance: {res['distance']:.4f})"))
                self.stdout.write(f"Chunk ID: {res['chunk_id']}")
                self.stdout.write("---")
                # Print the first few lines of the text chunk
                content_preview = "\n".join(res['text'].splitlines()[:5])
                self.stdout.write(content_preview)
                self.stdout.write("...")

        except FileNotFoundError:
            self.stdout.write(self.style.ERROR(f"Could not find index files for '{repo_id}'. Please run the indexer first."))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"An error occurred: {e}"))

--- FILE_END: backend/ingestion/management/commands/search.py ---

--- FILE_START: backend/ingestion/management/commands/inspect_graph.py ---
# backend/ingestion/management/commands/inspect_graph.py

import json
from pathlib import Path
from collections import defaultdict
from django.core.management.base import BaseCommand
from rich.console import Console
from rich.tree import Tree
from rich.panel import Panel


class Command(BaseCommand):
    help = 'Loads a Project Cortex JSON file and displays its architectural graph in a human-readable format.'

    def add_arguments(self, parser):
        parser.add_argument('cortex_file', type=str, help='The path to the Project Cortex JSON file.')

    def handle(self, *args, **options):
        console = Console()
        cortex_file_path = Path(options['cortex_file'])

        if not cortex_file_path.exists():
            console.print(f"[bold red]Error: File not found at '{cortex_file_path}'[/bold red]")
            return

        data = self._load_json(console, cortex_file_path)
        if data is None:
            return

        graph_data = data.get('architectural_graph')
        if not graph_data:
            self._print_graph_not_found(console)
            return

        self._display_graph(console, data['repo_id'], graph_data)

    def _load_json(self, console: Console, path: Path):
        try:
            console.print(f"🔎 Reading Cortex file: [cyan]{path}[/cyan]")
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            console.print(f"[bold red]Error: Invalid JSON in file '{path}'[/bold red]")
            return None

    def _print_graph_not_found(self, console: Console):
        console.print(
            Panel(
                "This Project Cortex file was generated before 'The Cartographer' was implemented or the project contains no Python files.\nNo architectural graph is available to display.",
                title="[yellow]Architectural Graph Not Found[/yellow]",
                border_style="yellow",
                expand=False
            )
        )

    def _display_graph(self, console: Console, repo_id: str, graph_data: dict):
        console.print("\n[bold magenta]--- 🗺️ Cartographer's Architectural Graph ---[/bold magenta]")

        nodes = graph_data.get('nodes', {})
        edges = graph_data.get('edges', [])
        edges_by_source = defaultdict(list)
        for edge in edges:
            edges_by_source[edge['source']].append(edge)

        tree = Tree(f"[bold blue]Project: {repo_id}[/bold blue]")
        file_tree_nodes = {}

        # First pass: Build file, class, and function structure
        for node_id, node_data in sorted(nodes.items()):
            if node_data.get('type') == 'file':
                file_branch = tree.add(f"📄 [bold green]{node_id}[/bold green]")
                file_tree_nodes[node_id] = file_branch

                for class_name in sorted(node_data.get('classes', [])):
                    class_id = f"{node_id}::{class_name}"
                    class_branch = file_branch.add(f"📦 [cyan]class[/cyan] {class_name}")
                    for method_name in sorted(nodes.get(class_id, {}).get('methods', [])):
                        class_branch.add(f"  - 🐍 [dim]def[/dim] {method_name}()")

                for func_name in sorted(node_data.get('functions', [])):
                    file_branch.add(f"🐍 [dim]def[/dim] {func_name}()")

        # Second pass: Add import/call edges
        for source_id, edge_list in edges_by_source.items():
            if source_id in file_tree_nodes:
                parent_branch = file_tree_nodes[source_id]
                for edge in edge_list:
                    target = edge.get('target', 'Unknown')
                    edge_type = edge.get('type', 'RELATES_TO')
                    if edge_type == 'IMPORTS':
                        parent_branch.add(f"📥 [dim]imports[/dim] [yellow]{target}[/yellow]")
                    elif edge_type == 'CALLS':
                        parent_branch.add(f"📞 [dim]calls[/dim] [magenta]{target}[/magenta]")

        console.print(tree)
        console.print("\n[bold magenta]------------------------------------[/bold magenta]")

--- FILE_END: backend/ingestion/management/commands/inspect_graph.py ---

--- FILE_START: backend/ingestion/management/commands/ingest_repo.py ---
# backend/ingestion/management/commands/ingest_repo.py

from django.core.management.base import BaseCommand
from rich.console import Console
from rich.panel import Panel
import traceback

from lumiere_core.services import ingestion_service

class Command(BaseCommand):
    help = 'Runs the full ingestion pipeline (clone, embed, index) for a single repository URL.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The full URL of the GitHub repository to ingest.')
        parser.add_argument('--embedding_model', type=str, default='snowflake-arctic-embed2:latest', help='The Ollama model to use for embeddings.')

    def handle(self, *args, **options):
        console = Console()
        repo_url = options['repo_url']
        embedding_model = options['embedding_model']

        console.print(
            Panel(
                f"[bold]Starting full ingestion for:[/] [cyan]{repo_url}[/cyan]\n"
                f"[bold]Using embedding model:[/] [yellow]{embedding_model}[/yellow]",
                title="🚀 Lumière Ingestion Service",
                border_style="blue"
            )
        )

        try:
            result = ingestion_service.clone_and_embed_repository(
                repo_url=repo_url,
                embedding_model=embedding_model
            )

            if result.get("status") == "success":
                console.print(
                    Panel(
                        f"[bold green]✓ Success![/bold green]\n{result.get('message', 'Ingestion complete.')}",
                        title="✅ Mission Complete",
                        border_style="green"
                    )
                )
                repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

                # --- THE FIX: Update the output path to match the new structure ---
                cortex_file_path = f"backend/cloned_repositories/{repo_id}/{repo_id}_cortex.json"
                console.print(f"\n[dim]To inspect the graph, run:[/dim]\n[bold cyan]python backend/manage.py inspect_graph {cortex_file_path}[/bold cyan]")

            else:
                error_details = result.get('details', result.get('error', 'An unknown error occurred.'))
                console.print(
                    Panel(
                        f"[bold red]✗ Ingestion Failed[/bold red]\n\n[yellow]Reason:[/yellow] {error_details}",
                        title="🚨 Error",
                        border_style="red"
                    )
                )

        except Exception as e:
            console.print(
                Panel(
                    f"[bold red]An unexpected critical error occurred:[/bold red]\n\n{traceback.format_exc()}",
                    title="💥 Critical Failure",
                    border_style="red"
                )
            )

--- FILE_END: backend/ingestion/management/commands/ingest_repo.py ---

--- FILE_START: backend/ingestion/management/commands/run_crawler.py ---
# In backend/ingestion/management/commands/run_crawler.py

import json
import traceback
from django.core.management.base import BaseCommand
from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier

class Command(BaseCommand):
    help = 'Clones a Git repository, creates the Project Cortex JSON, and saves it.'

    def add_arguments(self, parser):
        parser.add_argument('repo_url', type=str, help='The URL of the Git repository to clone.')

    def handle(self, *args, **options):
        repo_url = options['repo_url']
        # Generate the repo_id just like the API does.
        repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")

        self.stdout.write(self.style.NOTICE(f'Starting process for {repo_id} ({repo_url})...'))

        try:
            # --- FIX: Use the IntelligentCrawler as a context manager ---
            # The `with` statement correctly handles the setup (cloning) and
            # teardown (cleanup) of the temporary repository directory.
            with IntelligentCrawler(repo_url=repo_url) as crawler:
                # The cloning is now handled automatically when the 'with' block is entered.
                # We simply need to get the list of files to process.
                files_to_process = crawler.get_file_paths()

                if files_to_process:
                    self.stdout.write(self.style.SUCCESS(f'\nFound {len(files_to_process)} files. Starting JSON-ification...'))

                    # We now correctly pass the crawler's repo_path attribute.
                    jsonifier = Jsonifier(
                        file_paths=files_to_process,
                        repo_root=crawler.repo_path,
                        repo_id=repo_id
                    )
                    project_cortex = jsonifier.generate_cortex()

                    output_filename = f"{repo_id}_cortex.json"
                    with open(output_filename, 'w', encoding='utf-8') as f:
                        json.dump(project_cortex, f, indent=2)

                    self.stdout.write(self.style.SUCCESS(f'✓ Project Cortex created successfully: {output_filename}'))
                    self.stdout.write(self.style.NOTICE(f"\nNext Step: Run the indexer command:"))
                    self.stdout.write(self.style.SUCCESS(f"python manage.py run_indexer {output_filename}"))


                else:
                    self.stdout.write(self.style.WARNING('No files found to process or an error occurred.'))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f'\nAn unexpected error occurred: {e}'))
            self.stdout.write(self.style.ERROR('--- Full Traceback ---'))
            traceback.print_exc()
            self.stdout.write(self.style.ERROR('--- End Traceback ---'))
        # NOTE: No explicit crawler.cleanup() is needed here because the
        # `with` statement guarantees cleanup even if errors occur.

--- FILE_END: backend/ingestion/management/commands/run_crawler.py ---

--- FILE_START: backend/ingestion/__init__.py ---

--- FILE_END: backend/ingestion/__init__.py ---

--- FILE_START: backend/ingestion/apps.py ---
from django.apps import AppConfig


class IngestionConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "ingestion"

--- FILE_END: backend/ingestion/apps.py ---

--- FILE_START: backend/ingestion/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/ingestion/admin.py ---

--- FILE_START: backend/ingestion/jsonifier.py ---
import json
import pathlib
import datetime
import ast
import re
from typing import List, Dict, TypedDict, Optional, Any, Tuple
from tqdm import tqdm

from lumiere_core.services import cartographer


class TextChunk(TypedDict):
    chunk_id: str
    chunk_text: str
    token_count: int
    chunk_type: str
    language: Optional[str]
    start_line: Optional[int]
    end_line: Optional[int]


class FileCortex(TypedDict):
    file_path: str
    file_size_kb: float
    raw_content: str
    code_smells: List[str]
    ast_summary: str
    text_chunks: List[TextChunk]
    detected_language: str
    framework_hints: List[str]


class ProjectCortex(TypedDict):
    repo_id: str
    last_crawled_utc: str
    project_health_score: float
    project_structure_tree: str
    github_metadata: Dict
    files: List[FileCortex]
    architectural_graph: Optional[Dict[str, Any]]
    language_statistics: Dict[str, Any]
    polyglot_summary: Dict[str, Any]


class PolyglotChunker:
    """
    Universal code chunker that can intelligently parse and chunk code from multiple languages.
    """

    # Language-specific patterns for different constructs
    LANGUAGE_PATTERNS = {
        'python': {
            'function': r'^\s*def\s+(\w+)\s*\(',
            'class': r'^\s*class\s+(\w+)\s*[\(:]',
            'import': r'^\s*(?:from\s+\S+\s+)?import\s+',
            'comment': r'^\s*#',
            'docstring': r'^\s*["\']{{3}',
        },
        'javascript': {
            'function': r'^\s*(?:function\s+(\w+)|(?:const|let|var)\s+(\w+)\s*=\s*(?:function|\(.*\)\s*=>))',
            'class': r'^\s*class\s+(\w+)',
            'import': r'^\s*(?:import|export)',
            'comment': r'^\s*//',
            'block_comment': r'/\*.*?\*/',
        },
        'typescript': {
            'function': r'^\s*(?:function\s+(\w+)|(?:const|let|var)\s+(\w+)\s*=\s*(?:function|\(.*\)\s*=>))',
            'class': r'^\s*(?:export\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:export\s+)?interface\s+(\w+)',
            'type': r'^\s*(?:export\s+)?type\s+(\w+)',
            'import': r'^\s*(?:import|export)',
            'comment': r'^\s*//',
        },
        'java': {
            'function': r'^\s*(?:public|private|protected)?\s*(?:static\s+)?(?:\w+\s+)*(\w+)\s*\(',
            'class': r'^\s*(?:public\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:public\s+)?interface\s+(\w+)',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        },
        'csharp': {
            'function': r'^\s*(?:public|private|protected|internal)?\s*(?:static\s+)?(?:\w+\s+)*(\w+)\s*\(',
            'class': r'^\s*(?:public\s+)?class\s+(\w+)',
            'interface': r'^\s*(?:public\s+)?interface\s+(\w+)',
            'using': r'^\s*using\s+',
            'comment': r'^\s*//',
        },
        'go': {
            'function': r'^\s*func\s+(?:\(.*\)\s+)?(\w+)\s*\(',
            'struct': r'^\s*type\s+(\w+)\s+struct',
            'interface': r'^\s*type\s+(\w+)\s+interface',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        },
        'rust': {
            'function': r'^\s*(?:pub\s+)?fn\s+(\w+)\s*\(',
            'struct': r'^\s*(?:pub\s+)?struct\s+(\w+)',
            'enum': r'^\s*(?:pub\s+)?enum\s+(\w+)',
            'trait': r'^\s*(?:pub\s+)?trait\s+(\w+)',
            'impl': r'^\s*impl\s+(?:<.*>\s+)?(\w+)',
            'use': r'^\s*use\s+',
            'comment': r'^\s*//',
        },
        'ruby': {
            'function': r'^\s*def\s+(\w+)',
            'class': r'^\s*class\s+(\w+)',
            'module': r'^\s*module\s+(\w+)',
            'require': r'^\s*require',
            'comment': r'^\s*#',
        },
        'php': {
            'function': r'^\s*(?:public|private|protected)?\s*function\s+(\w+)\s*\(',
            'class': r'^\s*class\s+(\w+)',
            'interface': r'^\s*interface\s+(\w+)',
            'namespace': r'^\s*namespace\s+',
            'use': r'^\s*use\s+',
            'comment': r'^\s*//',
        },
        'cpp': {
            'function': r'^\s*(?:\w+\s+)*(\w+)\s*\([^)]*\)\s*{',
            'class': r'^\s*class\s+(\w+)',
            'struct': r'^\s*struct\s+(\w+)',
            'namespace': r'^\s*namespace\s+(\w+)',
            'include': r'^\s*#include',
            'comment': r'^\s*//',
        },
        'swift': {
            'function': r'^\s*(?:public|private|internal)?\s*func\s+(\w+)\s*\(',
            'class': r'^\s*(?:public|private|internal)?\s*class\s+(\w+)',
            'struct': r'^\s*(?:public|private|internal)?\s*struct\s+(\w+)',
            'protocol': r'^\s*(?:public|private|internal)?\s*protocol\s+(\w+)',
            'import': r'^\s*import\s+',
            'comment': r'^\s*//',
        }
    }

    @staticmethod
    def detect_language_from_extension(file_path: str) -> str:
        """Detect programming language from file extension."""
        ext = pathlib.Path(file_path).suffix.lower()

        language_map = {
            '.py': 'python', '.pyx': 'python', '.pyi': 'python',
            '.js': 'javascript', '.mjs': 'javascript', '.cjs': 'javascript',
            '.jsx': 'javascript', '.gs': 'javascript',
            '.ts': 'typescript', '.tsx': 'typescript',
            '.java': 'java',
            '.cs': 'csharp',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.cpp': 'cpp', '.cxx': 'cpp', '.cc': 'cpp', '.c': 'cpp',
            '.h': 'cpp', '.hpp': 'cpp',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala',
            '.hs': 'haskell',
            '.ml': 'ocaml',
            '.ex': 'elixir',
            '.erl': 'erlang',
            '.clj': 'clojure',
            '.lua': 'lua',
            '.pl': 'perl',
            '.r': 'r',
            '.jl': 'julia',
            '.dart': 'dart',
            '.zig': 'zig',
            '.nim': 'nim',
            '.crystal': 'crystal',
        }

        return language_map.get(ext, 'unknown')

    @staticmethod
    def detect_framework_hints(content: str, language: str, file_path: str) -> List[str]:
        """Detect framework or library usage from content."""
        hints = []
        content_lower = content.lower()

        # Framework detection patterns
        framework_patterns = {
            'react': ['import react', 'from \'react\'', 'from "react"', 'usestate', 'useeffect'],
            'vue': ['vue.component', 'new vue', '@vue/', 'vue-'],
            'angular': ['@angular/', '@component', '@injectable', 'ngmodule'],
            'express': ['express()', 'app.get(', 'app.post(', 'require(\'express\')'],
            'fastapi': ['from fastapi', 'fastapi()', '@app.get', '@app.post'],
            'django': ['from django', 'django.', 'models.model', 'django.conf'],
            'flask': ['from flask', 'flask()', '@app.route'],
            'spring': ['@controller', '@service', '@repository', '@autowired'],
            'laravel': ['illuminate\\', 'artisan', 'eloquent'],
            'rails': ['activerecord', 'actioncontroller', 'rails.application'],
            'jquery': ['$(', 'jquery', '.ready('],
            'bootstrap': ['bootstrap', 'btn-', 'col-', 'row'],
            'tailwind': ['tailwind', 'tw-', 'bg-', 'text-'],
            'material-ui': ['@mui/', '@material-ui/', 'makeStyles'],
            'styled-components': ['styled-components', 'styled.'],
            'redux': ['redux', 'createstore', 'useselector', 'usedispatch'],
            'tensorflow': ['tensorflow', 'tf.', 'keras'],
            'pytorch': ['torch', 'pytorch', 'nn.module'],
            'numpy': ['import numpy', 'np.'],
            'pandas': ['import pandas', 'pd.'],
            'unittest': ['import unittest', 'testcase'],
            'pytest': ['import pytest', '@pytest.'],
            'jest': ['describe(', 'it(', 'expect('],
        }

        for framework, patterns in framework_patterns.items():
            if any(pattern in content_lower for pattern in patterns):
                hints.append(framework)

        return hints

    @classmethod
    def chunk_by_language(cls, content: str, language: str, file_path: str) -> List[Dict[str, Any]]:
        """Intelligently chunk content based on detected programming language."""
        if language == 'python':
            return cls._chunk_python(content)
        elif language in ['javascript', 'typescript']:
            return cls._chunk_javascript_typescript(content, language)
        elif language in ['java', 'csharp', 'cpp', 'swift']:
            return cls._chunk_c_style(content, language)
        elif language == 'go':
            return cls._chunk_go(content)
        elif language == 'rust':
            return cls._chunk_rust(content)
        elif language == 'ruby':
            return cls._chunk_ruby(content)
        elif language == 'php':
            return cls._chunk_php(content)
        else:
            return cls._chunk_generic(content, language)

    @classmethod
    def _chunk_python(cls, content: str) -> List[Dict[str, Any]]:
        """Enhanced Python chunking using AST when possible."""
        chunks = []

        try:
            tree = ast.parse(content)
            lines = content.splitlines()

            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "function_definition",
                            "name": node.name,
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })
                elif isinstance(node, ast.ClassDef):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "class_definition",
                            "name": node.name,
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    chunk_text = ast.get_source_segment(content, node)
                    if chunk_text:
                        chunks.append({
                            "text": chunk_text,
                            "type": "import_statement",
                            "start_line": node.lineno,
                            "end_line": node.end_lineno
                        })

            # Fill gaps with generic chunks
            if chunks:
                chunks = cls._fill_gaps_with_generic_chunks(content, chunks)

        except SyntaxError:
            # Fallback to pattern-based chunking
            chunks = cls._chunk_by_patterns(content, 'python')

        return chunks or cls._chunk_generic(content, 'python')

    @classmethod
    def _chunk_javascript_typescript(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Chunk JavaScript/TypeScript using pattern matching."""
        return cls._chunk_by_patterns(content, language)

    @classmethod
    def _chunk_c_style(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Chunk C-style languages (Java, C#, C++, Swift)."""
        return cls._chunk_by_patterns(content, language)

    @classmethod
    def _chunk_go(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Go code."""
        return cls._chunk_by_patterns(content, 'go')

    @classmethod
    def _chunk_rust(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Rust code."""
        return cls._chunk_by_patterns(content, 'rust')

    @classmethod
    def _chunk_ruby(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk Ruby code."""
        return cls._chunk_by_patterns(content, 'ruby')

    @classmethod
    def _chunk_php(cls, content: str) -> List[Dict[str, Any]]:
        """Chunk PHP code."""
        return cls._chunk_by_patterns(content, 'php')

    @classmethod
    def _chunk_by_patterns(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Generic pattern-based chunking for various languages."""
        if language not in cls.LANGUAGE_PATTERNS:
            return cls._chunk_generic(content, language)

        patterns = cls.LANGUAGE_PATTERNS[language]
        lines = content.splitlines()
        chunks = []
        current_chunk = []
        current_type = "code_block"
        current_name = None
        start_line = 1

        for i, line in enumerate(lines, 1):
            chunk_detected = False

            # Check for different construct patterns
            for construct_type, pattern in patterns.items():
                if construct_type in ['comment', 'block_comment']:
                    continue

                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    # Save previous chunk if it has meaningful content
                    if current_chunk and "\n".join(current_chunk).strip():
                        chunks.append({
                            "text": "\n".join(current_chunk),
                            "type": current_type,
                            "name": current_name,
                            "start_line": start_line,
                            "end_line": i - 1
                        })

                    # Start new chunk
                    current_chunk = [line]
                    current_type = f"{construct_type}_definition"
                    current_name = match.group(1) if match.groups() else None
                    start_line = i
                    chunk_detected = True
                    break

            if not chunk_detected:
                current_chunk.append(line)

            # For languages with braces, try to detect end of blocks
            if language in ['javascript', 'typescript', 'java', 'csharp', 'cpp', 'swift', 'go', 'rust']:
                if line.strip() == '}' and current_type.endswith('_definition'):
                    # End of current block
                    chunks.append({
                        "text": "\n".join(current_chunk),
                        "type": current_type,
                        "name": current_name,
                        "start_line": start_line,
                        "end_line": i
                    })
                    current_chunk = []
                    current_type = "code_block"
                    current_name = None
                    start_line = i + 1

        # Add remaining content as final chunk if it has meaningful content
        if current_chunk and "\n".join(current_chunk).strip():
            chunks.append({
                "text": "\n".join(current_chunk),
                "type": current_type,
                "name": current_name,
                "start_line": start_line,
                "end_line": len(lines)
            })

        return chunks

    @classmethod
    def _chunk_generic(cls, content: str, language: str) -> List[Dict[str, Any]]:
        """Fallback generic chunking for unsupported languages or when parsing fails."""
        chunks = []

        # Split by logical paragraphs (double newlines)
        paragraphs = content.split('\n\n')
        current_line = 1

        for paragraph in paragraphs:
            if paragraph.strip():
                line_count = paragraph.count('\n') + 1
                chunks.append({
                    "text": paragraph,
                    "type": "paragraph",
                    "start_line": current_line,
                    "end_line": current_line + line_count - 1
                })
                current_line += line_count + 1  # +1 for the empty line
            else:
                current_line += 1

        # If no paragraphs found, split by logical line groups
        if not chunks:
            lines = content.splitlines()
            chunk_size = min(50, max(10, len(lines) // 10))  # Adaptive chunk size

            for i in range(0, len(lines), chunk_size):
                chunk_lines = lines[i:i + chunk_size]
                chunks.append({
                    "text": "\n".join(chunk_lines),
                    "type": "line_group",
                    "start_line": i + 1,
                    "end_line": i + len(chunk_lines)
                })

        return chunks

    @classmethod
    def _fill_gaps_with_generic_chunks(cls, content: str, existing_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Fill gaps between structured chunks with generic content chunks."""
        if not existing_chunks:
            return existing_chunks

        lines = content.splitlines()
        all_chunks = []
        last_end = 0

        # Sort chunks by start line
        existing_chunks.sort(key=lambda x: x.get('start_line', 0))

        for chunk in existing_chunks:
            start_line = chunk.get('start_line', 1) - 1  # Convert to 0-based

            # Add gap content if exists
            if start_line > last_end:
                gap_content = "\n".join(lines[last_end:start_line])
                if gap_content.strip():
                    all_chunks.append({
                        "text": gap_content,
                        "type": "code_block",
                        "start_line": last_end + 1,
                        "end_line": start_line
                    })

            all_chunks.append(chunk)
            last_end = chunk.get('end_line', start_line + 1)

        # Add remaining content
        if last_end < len(lines):
            remaining_content = "\n".join(lines[last_end:])
            if remaining_content.strip():
                all_chunks.append({
                    "text": remaining_content,
                    "type": "code_block",
                    "start_line": last_end + 1,
                    "end_line": len(lines)
                })

        return all_chunks


class Jsonifier:
    """
    Enhanced Jsonifier with comprehensive polyglot support.
    Reads a list of files, intelligently chunks their content based on language,
    runs the Cartographer, and builds the Project Cortex JSON object.
    """

    def __init__(self, file_paths: List[pathlib.Path], repo_root: pathlib.Path, repo_id: str):
        self.file_paths = file_paths
        self.repo_root = repo_root
        self.repo_id = repo_id
        self.chunker = PolyglotChunker()

    def _read_file_content(self, file_path: pathlib.Path) -> str:
        """Read file content with multiple encoding fallbacks."""
        encodings = ['utf-8', 'utf-16', 'latin-1', 'cp1252']

        for encoding in encodings:
            try:
                return file_path.read_text(encoding=encoding)
            except UnicodeDecodeError:
                continue
            except Exception:
                break

        # Final fallback - read as binary and decode with errors='replace'
        try:
            return file_path.read_text(encoding='utf-8', errors='replace')
        except Exception:
            return ""

    def _analyze_code_quality(self, content: str, language: str, file_path: str) -> List[str]:
        """Basic code quality analysis to detect potential issues."""
        smells = []
        lines = content.splitlines()

        # Generic code smells
        if len(lines) > 1000:
            smells.append("very_large_file")

        if len(content) > 100000:  # 100KB
            smells.append("large_file_size")

        # Language-specific analysis
        if language == 'python':
            # Python-specific smells
            if 'import *' in content:
                smells.append("wildcard_import")
            if content.count('except:') > content.count('except '):
                smells.append("bare_except")
            if len([line for line in lines if len(line.strip()) > 100]) > len(lines) * 0.1:
                smells.append("long_lines")

        elif language in ['javascript', 'typescript']:
            # JS/TS-specific smells
            if 'eval(' in content:
                smells.append("eval_usage")
            if content.count('var ') > content.count('let ') + content.count('const '):
                smells.append("var_over_let_const")

        elif language == 'java':
            # Java-specific smells
            if content.count('public class') > 1:
                smells.append("multiple_public_classes")
            if 'System.out.print' in content:
                smells.append("system_out_usage")

        # Security-related patterns
        security_patterns = [
            'password', 'secret', 'api_key', 'private_key', 'token',
            'TODO', 'FIXME', 'HACK', 'XXX'
        ]

        for pattern in security_patterns:
            if pattern.lower() in content.lower():
                smells.append(f"potential_{pattern.lower()}_exposure")

        return smells

    def _create_ast_summary(self, chunks: List[Dict[str, Any]], language: str) -> str:
        """Create a summary of the AST/structure information."""
        summary = {
            "language": language,
            "total_chunks": len(chunks),
            "chunk_types": {},
            "named_constructs": []
        }

        for chunk in chunks:
            chunk_type = chunk.get("type", "unknown")
            summary["chunk_types"][chunk_type] = summary["chunk_types"].get(chunk_type, 0) + 1

            if chunk.get("name"):
                summary["named_constructs"].append({
                    "name": chunk["name"],
                    "type": chunk_type,
                    "line": chunk.get("start_line")
                })

        return json.dumps(summary, indent=2)

    def generate_cortex(self) -> ProjectCortex:
        """
        Enhanced cortex generation with comprehensive polyglot support.

        This method now acts as a sophisticated pre-processor for the Polyglot Cartographer:
        - Detects language for each file automatically
        - Generates intelligent chunks based on language-specific patterns
        - Provides framework detection and code quality analysis
        - Supports 50+ programming languages and frameworks
        - Maintains backward compatibility with existing systems
        """
        print("🔬 Starting enhanced polyglot analysis...")

        all_files_cortex: List[FileCortex] = []
        language_stats = {}

        # Enhanced data collection for the Polyglot Cartographer
        files_for_cartographer: Dict[str, Dict[str, Any]] = {}

        for file_path in tqdm(self.file_paths, desc="Analyzing files", unit="file"):

            content = self._read_file_content(file_path)
            relative_path_str = str(file_path.relative_to(self.repo_root))

            # Enhanced language detection
            detected_language = self.chunker.detect_language_from_extension(relative_path_str)

            # Framework detection
            framework_hints = self.chunker.detect_framework_hints(content, detected_language, relative_path_str)

            # Language-aware intelligent chunking
            raw_chunks = self.chunker.chunk_by_language(content, detected_language, relative_path_str)

            # Code quality analysis
            code_smells = self._analyze_code_quality(content, detected_language, relative_path_str)

            # Prepare data for Cartographer based on language
            if detected_language == 'python':
                try:
                    tree = ast.parse(content)
                    files_for_cartographer[relative_path_str] = {
                        "language": "python",
                        "ast": tree,
                        "content": content
                    }
                except SyntaxError:
                    files_for_cartographer[relative_path_str] = {
                        "language": "python",
                        "content": content,
                        "parse_error": True
                    }

            elif detected_language in ['javascript', 'typescript']:
                files_for_cartographer[relative_path_str] = {
                    "language": detected_language,
                    "content": content,
                    "framework_hints": framework_hints
                }

            elif detected_language in ['java', 'csharp', 'go', 'rust', 'swift', 'kotlin']:
                files_for_cartographer[relative_path_str] = {
                    "language": detected_language,
                    "content": content,
                    "chunks": raw_chunks
                }

            # Convert chunks to TextChunk format for RAG/Indexing
            text_chunks: List[TextChunk] = []
            for i, chunk_data in enumerate(raw_chunks):
                chunk_id = f"{self.repo_id}_{relative_path_str}_{i}"
                chunk_text = chunk_data["text"]

                text_chunks.append({
                    "chunk_id": chunk_id,
                    "chunk_text": chunk_text,
                    "token_count": len(chunk_text.split()),
                    "chunk_type": chunk_data.get("type", "unknown"),
                    "language": detected_language,
                    "start_line": chunk_data.get("start_line"),
                    "end_line": chunk_data.get("end_line")
                })

            # Update language statistics
            if detected_language not in language_stats:
                language_stats[detected_language] = {
                    "file_count": 0,
                    "total_lines": 0,
                    "total_chunks": 0,
                    "frameworks": set()
                }

            language_stats[detected_language]["file_count"] += 1
            language_stats[detected_language]["total_lines"] += len(content.splitlines())
            language_stats[detected_language]["total_chunks"] += len(text_chunks)
            language_stats[detected_language]["frameworks"].update(framework_hints)

            # Create enhanced FileCortex
            file_cortex: FileCortex = {
                "file_path": relative_path_str,
                "file_size_kb": round(file_path.stat().st_size / 1024, 2),
                "raw_content": content,
                "code_smells": code_smells,
                "ast_summary": self._create_ast_summary(raw_chunks, detected_language),
                "text_chunks": text_chunks,
                "detected_language": detected_language,
                "framework_hints": framework_hints
            }

            all_files_cortex.append(file_cortex)

        # Convert sets to lists for JSON serialization
        for lang_data in language_stats.values():
            lang_data["frameworks"] = list(lang_data["frameworks"])

        print("🗺️  Calling Polyglot Cartographer...")

        # Call the enhanced Polyglot Cartographer
        architectural_graph = None
        if files_for_cartographer:
            try:
                architectural_graph = cartographer.generate_graph(files_for_cartographer)
            except Exception as e:
                print(f"  ⚠️  Cartographer warning: {e}")
                architectural_graph = {"error": str(e), "supported_files": len(files_for_cartographer)}

        # Create polyglot summary
        polyglot_summary = {
            "total_languages": len(language_stats),
            "primary_language": max(language_stats.items(), key=lambda x: x[1]["file_count"])[0] if language_stats else "unknown",
            "language_distribution": {lang: data["file_count"] for lang, data in language_stats.items()},
            "detected_frameworks": list(set().union(*[data["frameworks"] for data in language_stats.values()])),
            "total_files_analyzed": len(all_files_cortex),
            "total_chunks_generated": sum(len(f["text_chunks"]) for f in all_files_cortex),
            "analysis_timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
        }

        print(f"✅ Analysis complete! Detected {len(language_stats)} languages across {len(all_files_cortex)} files")

        # Assemble the enhanced Project Cortex object
        return {
            "repo_id": self.repo_id,
            "last_crawled_utc": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "project_health_score": self._calculate_health_score(all_files_cortex, language_stats),
            "project_structure_tree": self._generate_structure_tree(),
            "github_metadata": {},
            "files": all_files_cortex,
            "architectural_graph": architectural_graph,
            "language_statistics": language_stats,
            "polyglot_summary": polyglot_summary,
        }

    def _calculate_health_score(self, files: List[FileCortex], language_stats: Dict) -> float:
        """Calculate a basic project health score based on various metrics."""
        if not files:
            return 0.0

        total_score = 0.0
        factors = 0

        # Factor 1: Code smell density (lower is better)
        total_smells = sum(len(f["code_smells"]) for f in files)
        smell_density = total_smells / len(files)
        smell_score = max(0.0, 1.0 - (smell_density / 10))  # Normalize to 0-1
        total_score += smell_score
        factors += 1

        # Factor 2: Language diversity (moderate diversity is good)
        language_count = len(language_stats)
        if language_count == 1:
            diversity_score = 0.8  # Single language is good
        elif language_count <= 5:
            diversity_score = 1.0  # Moderate diversity is excellent
        else:
            diversity_score = max(0.5, 1.0 - (language_count - 5) * 0.1)  # Too many languages might indicate complexity
        total_score += diversity_score
        factors += 1

        # Factor 3: Documentation presence
        doc_files = [f for f in files if any(keyword in f["file_path"].lower()
                     for keyword in ["readme", "doc", "changelog", "contributing"])]
        doc_score = min(1.0, len(doc_files) / 3)  # Up to 3 doc files gives full score
        total_score += doc_score
        factors += 1

        # Factor 4: Test presence
        test_files = [f for f in files if any(keyword in f["file_path"].lower()
                      for keyword in ["test", "spec", "__test__", ".test.", ".spec."])]
        test_ratio = len(test_files) / len(files)
        test_score = min(1.0, test_ratio * 5)  # 20% test files gives full score
        total_score += test_score
        factors += 1

        return round(total_score / factors, 2)

    def _generate_structure_tree(self) -> str:
        """Generate a simple project structure tree."""
        # This is a simplified version - could be enhanced to show actual directory structure
        paths = [str(fp.relative_to(self.repo_root)) for fp in self.file_paths]
        paths.sort()

        tree_lines = []
        for path in paths[:20]:  # Limit to first 20 files for brevity
            depth = path.count('/')
            indent = "  " * depth
            filename = path.split('/')[-1]
            tree_lines.append(f"{indent}├── {filename}")

        if len(paths) > 20:
            tree_lines.append(f"  ... and {len(paths) - 20} more files")

        return "\n".join(tree_lines)

--- FILE_END: backend/ingestion/jsonifier.py ---

--- FILE_START: backend/ingestion/indexing.py ---
# backend/ingestion/indexing.py

import json
import numpy as np
import faiss
from pathlib import Path
from lumiere_core.services.ollama import get_ollama_embeddings
from tqdm import tqdm

class EmbeddingIndexer:
    """
    Loads a Project Cortex JSON, generates embeddings via Ollama,
    and saves the Faiss index and the ID-to-chunk mapping into the SAME
    directory as the source cortex file.
    """
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.dimension = None

    def _extract_repo_id_from_cortex(self, project_cortex: dict, cortex_path_obj: Path) -> str:
        """
        Extract repo_id from cortex with fallback strategies for backward compatibility.

        Args:
            project_cortex: The loaded cortex JSON data
            cortex_path_obj: Path object of the cortex file

        Returns:
            A valid repo_id string
        """
        # Primary: Try to get repo_id from the cortex data
        if 'repo_id' in project_cortex and project_cortex['repo_id']:
            return project_cortex['repo_id']

        # Fallback 1: Extract from cortex filename (assumes format: {repo_id}_cortex.json)
        cortex_filename = cortex_path_obj.stem  # Gets filename without extension
        if cortex_filename.endswith('_cortex'):
            potential_repo_id = cortex_filename[:-7]  # Remove '_cortex' suffix
            if potential_repo_id:
                print(f"Warning: repo_id not found in cortex data, using filename-derived ID: {potential_repo_id}")
                return potential_repo_id

        # Fallback 2: Use the parent directory name
        parent_dir_name = cortex_path_obj.parent.name
        if parent_dir_name and parent_dir_name != 'cloned_repositories':
            print(f"Warning: repo_id not found, using parent directory name: {parent_dir_name}")
            return parent_dir_name

        # Fallback 3: Generate from cortex filename
        fallback_id = cortex_filename.replace('_cortex', '') if '_cortex' in cortex_filename else cortex_filename
        print(f"Warning: Using fallback repo_id derived from filename: {fallback_id}")
        return fallback_id or 'unknown_repo'

    def process_cortex(self, cortex_file_path: str):
        """
        Main method to load cortex, create embeddings, and build the index.
        """
        cortex_path_obj = Path(cortex_file_path)

        # Validate that the cortex file exists
        if not cortex_path_obj.exists():
            raise FileNotFoundError(f"Cortex file not found: {cortex_path_obj}")

        print(f"Loading Project Cortex from: {cortex_path_obj}")

        # --- THIS IS THE KEY FIX ---
        # Derive the output directory from the location of the cortex file.
        output_dir = cortex_path_obj.parent

        try:
            with open(cortex_path_obj, 'r', encoding='utf-8') as f:
                project_cortex = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in cortex file {cortex_path_obj}: {e}")
        except Exception as e:
            raise IOError(f"Error reading cortex file {cortex_path_obj}: {e}")

        # 1. Collect all text chunks and their IDs
        all_chunks_text = []
        all_chunk_ids = []
        id_to_chunk_map = {}

        files_data = project_cortex.get('files', [])
        if not files_data:
            print("Warning: No 'files' array found in cortex data.")
            return

        for file_data in files_data:
            if not isinstance(file_data, dict):
                print(f"Warning: Skipping invalid file data entry: {file_data}")
                continue

            file_path = file_data.get('file_path', 'unknown_file')
            text_chunks = file_data.get('text_chunks', [])

            for chunk in text_chunks:
                if not isinstance(chunk, dict):
                    print(f"Warning: Skipping invalid chunk in {file_path}: {chunk}")
                    continue

                chunk_text = chunk.get('chunk_text', '')
                chunk_id = chunk.get('chunk_id')

                if not chunk_text or not chunk_id:
                    print(f"Warning: Skipping chunk with missing text or ID in {file_path}")
                    continue

                all_chunks_text.append(chunk_text)
                all_chunk_ids.append(chunk_id)
                id_to_chunk_map[chunk_id] = {
                    "text": chunk_text,
                    "file_path": file_path
                }

        if not all_chunks_text:
            print("No valid text chunks found. Exiting.")
            return

        print(f"Found {len(all_chunks_text)} text chunks to embed using Ollama model '{self.model_name}'.")

        # 2. Generate embeddings using our Ollama service
        try:
            embeddings_list = get_ollama_embeddings(all_chunks_text, model_name=self.model_name)
        except Exception as e:
            print(f"Error generating embeddings: {e}")
            return

        if not embeddings_list:
            print("Embedding generation failed. Exiting.")
            return

        # Validate embeddings
        if len(embeddings_list) != len(all_chunks_text):
            print(f"Error: Mismatch between chunks ({len(all_chunks_text)}) and embeddings ({len(embeddings_list)})")
            return

        self.dimension = len(embeddings_list[0])
        print(f"Ollama model '{self.model_name}' produced embeddings with dimension: {self.dimension}")

        embeddings = np.array(embeddings_list).astype('float32')

        # 3. Create and populate the Faiss index
        print("Creating Faiss index...")
        try:
            index = faiss.IndexFlatL2(self.dimension)
            index.add(embeddings)
            print(f"Faiss index created. Total vectors in index: {index.ntotal}")
        except Exception as e:
            print(f"Error creating Faiss index: {e}")
            return

        # 4. Save the artifacts
        # Use the enhanced repo_id extraction method
        repo_id = self._extract_repo_id_from_cortex(project_cortex, cortex_path_obj)

        # Use the 'output_dir' to save files in the correct location
        index_filename = output_dir / f"{repo_id}_faiss.index"
        map_filename = output_dir / f"{repo_id}_id_map.json"

        print(f"Saving Faiss index to: {index_filename}")
        try:
            faiss.write_index(index, str(index_filename))
        except Exception as e:
            print(f"Error saving Faiss index: {e}")
            return

        print(f"Saving ID-to-Chunk mapping to: {map_filename}")
        save_data = {
            "faiss_id_to_chunk_id": all_chunk_ids,
            "chunk_id_to_data": id_to_chunk_map
        }

        try:
            with open(map_filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2)
        except Exception as e:
            print(f"Error saving ID mapping: {e}")
            return

        print("Indexing complete.")

--- FILE_END: backend/ingestion/indexing.py ---

--- FILE_START: backend/ingestion/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/ingestion/tests.py ---

--- FILE_START: backend/ingestion/views.py ---
from django.shortcuts import render

# Create your views here.

--- FILE_END: backend/ingestion/views.py ---

--- FILE_START: backend/ingestion/crawler.py ---
# In ingestion/crawler.py
import subprocess
import tempfile
import pathlib
from typing import List, Optional, Union, Dict, Set
import logging
import os
from functools import lru_cache

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(name)s: %(message)s')
logger = logging.getLogger(__name__)

# --- THE FIX: Silence noisy loggers ---
# Set the log level for httpx (used by ollama) and faiss to WARNING to reduce noise.
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("faiss").setLevel(logging.WARNING)

class IntelligentCrawler:
    """
    Clones a Git repository and performs file operations safely.
    Includes path-finding, git blame, and git diff capabilities.
    Enhanced with comprehensive polyglot language support.
    """

    # Class-level constants for better maintainability
    DEFAULT_EXCLUDED_DIRS = {
        '.git', '__pycache__', 'venv', 'node_modules', '.vscode', '.idea',
        'dist', 'build', '.pytest_cache', '.mypy_cache', '.tox', 'target',
        'bin', 'obj', 'out', '.gradle', '.mvn', 'vendor', 'deps', '_build',
        '.stack-work', '.cabal-sandbox', 'elm-stuff', '.pub-cache', '.dart_tool'
    }

    # Comprehensive language support - organized by ecosystem
    DEFAULT_INCLUDED_EXTENSIONS = [
        # === WEB TECHNOLOGIES ===
        # Frontend JavaScript/TypeScript
        '*.js', '*.mjs', '*.cjs', '*.jsx', '*.ts', '*.tsx', '*.vue', '*.svelte',
        '*.astro', '*.lit', '*.stencil', '*.qwik',
        # Web markup and styling
        '*.html', '*.htm', '*.xhtml', '*.xml', '*.css', '*.scss', '*.sass',
        '*.less', '*.styl', '*.stylus', '*.postcss',
        # Web frameworks and configs
        '*.ejs', '*.pug', '*.handlebars', '*.hbs', '*.mustache', '*.twig',
        '*.blade.php', '*.erb', '*.haml', '*.slim',

        # === BACKEND LANGUAGES ===
        # Python ecosystem
        '*.py', '*.pyx', '*.pyi', '*.pyw', '*.py3',
        # JavaScript/Node.js
        '*.gs',  # Google Apps Script
        # Java ecosystem
        '*.java', '*.kt', '*.kts', '*.scala', '*.groovy', '*.clj', '*.cljs', '*.cljc',
        # .NET ecosystem
        '*.cs', '*.vb', '*.fs', '*.fsx', '*.csx',
        # Systems programming
        '*.c', '*.cpp', '*.cxx', '*.cc', '*.c++', '*.h', '*.hpp', '*.hxx', '*.h++',
        '*.rs', '*.go', '*.zig', '*.odin', '*.v', '*.nim', '*.crystal',
        # Apple ecosystem
        '*.swift', '*.m', '*.mm',
        # Other compiled languages
        '*.d', '*.ada', '*.adb', '*.ads',

        # === SCRIPTING LANGUAGES ===
        '*.rb', '*.rake', '*.gemspec', '*.rbw',  # Ruby
        '*.php', '*.phtml', '*.php3', '*.php4', '*.php5', '*.php7', '*.php8',  # PHP
        '*.pl', '*.pm', '*.t', '*.pod',  # Perl
        '*.lua', '*.luac',  # Lua
        '*.tcl', '*.tk',  # Tcl/Tk
        '*.ps1', '*.psm1', '*.psd1',  # PowerShell

        # === SHELL SCRIPTING ===
        '*.sh', '*.bash', '*.zsh', '*.fish', '*.csh', '*.tcsh', '*.ksh',
        '*.bat', '*.cmd', '*.command',

        # === FUNCTIONAL LANGUAGES ===
        '*.hs', '*.lhs',  # Haskell
        '*.ml', '*.mli', '*.mll', '*.mly',  # OCaml
        '*.elm',  # Elm
        '*.ex', '*.exs',  # Elixir
        '*.erl', '*.hrl',  # Erlang
        '*.lisp', '*.lsp', '*.cl', '*.el',  # Lisp family
        '*.scm', '*.ss', '*.rkt',  # Scheme/Racket
        '*.f', '*.for', '*.f90', '*.f95', '*.f03', '*.f08',  # Fortran

        # === DATA SCIENCE & MATH ===
        '*.r', '*.R', '*.rmd', '*.rnw',  # R
        '*.jl',  # Julia
        '*.m', '*.mat',  # MATLAB/Octave
        '*.nb', '*.wl',  # Mathematica
        '*.sas', '*.stata', '*.do',  # Statistics
        '*.ipynb',  # Jupyter notebooks

        # === MOBILE DEVELOPMENT ===
        '*.dart',  # Dart/Flutter
        '*.kt', '*.kts',  # Kotlin (Android)
        '*.java',  # Java (Android)

        # === GAME DEVELOPMENT ===
        '*.cs',  # Unity C#
        '*.gd', '*.tres', '*.tscn',  # Godot
        '*.lua',  # Love2D, World of Warcraft addons
        '*.as', '*.mxml',  # ActionScript/Flex
        '*.hlsl', '*.glsl', '*.cg', '*.shader',  # Shaders

        # === DATABASE ===
        '*.sql', '*.sqlite', '*.db', '*.mysql', '*.pgsql', '*.plsql',
        '*.cypher', '*.cql', '*.sparql', '*.graphql', '*.gql',

        # === CONFIGURATION FORMATS ===
        '*.json', '*.json5', '*.jsonc', '*.jsonl', '*.ndjson',
        '*.toml', '*.yaml', '*.yml', '*.ini', '*.cfg', '*.conf', '*.config',
        '*.properties', '*.env', '*.dotenv', '*.editorconfig',
        '*.hcl', '*.tf', '*.tfvars',  # Terraform
        '*.dhall', '*.nix',  # Nix
        '*.hocon',  # HOCON (Typesafe Config)

        # === MARKUP & DOCUMENTATION ===
        '*.md', '*.markdown', '*.mdown', '*.mkd', '*.mdx',
        '*.rst', '*.adoc', '*.asciidoc', '*.txt', '*.text',
        '*.tex', '*.latex', '*.ltx', '*.cls', '*.sty',
        '*.org', '*.wiki', '*.textile',

        # === DATA FORMATS ===
        '*.csv', '*.tsv', '*.psv', '*.ssv',
        '*.parquet', '*.avro', '*.orc', '*.arrow',
        '*.proto', '*.protobuf',  # Protocol Buffers
        '*.thrift',  # Apache Thrift
        '*.capnp',  # Cap'n Proto

        # === SCIENTIFIC COMPUTING ===
        '*.cu', '*.cuh',  # CUDA
        '*.opencl', '*.cl',  # OpenCL
        '*.sage', '*.magma',  # Mathematical software

        # === EMERGING LANGUAGES ===
        '*.move',  # Move (Diem/Aptos)
        '*.sol',  # Solidity (Ethereum)
        '*.cairo',  # Cairo (StarkNet)
        '*.fe',  # Fe (Ethereum)
        '*.gleam',  # Gleam
        '*.roc',  # Roc
        '*.grain',  # Grain
        '*.red',  # Red
        '*.io',  # Io
        '*.pony',  # Pony
        '*.chapel', '*.chpl',  # Chapel

        # === DOMAIN-SPECIFIC LANGUAGES ===
        '*.vhdl', '*.vhd',  # VHDL
        '*.v', '*.sv',  # Verilog/SystemVerilog
        '*.asl', '*.dsl',  # Domain-specific languages
        '*.feature',  # Gherkin/Cucumber
        '*.story',  # JBehave
        '*.bdd',  # Behavior-driven development

        # === LEGACY & SPECIALTY ===
        '*.pas', '*.pp',  # Pascal
        '*.cob', '*.cbl', '*.cpy',  # COBOL
        '*.for', '*.f77',  # FORTRAN 77
        '*.asm', '*.s', '*.S',  # Assembly
        '*.awk',  # AWK
        '*.sed',  # Sed scripts
        '*.regex', '*.re',  # Regex files

        # === AUTOMATION & TESTING ===
        '*.robot',  # Robot Framework
        '*.feature',  # Cucumber/Gherkin
        '*.spec', '*.test',  # Test specifications
        '*.e2e', '*.integration',  # Test files

        # === BUILD & INFRASTRUCTURE ===
        '*.bazel', '*.bzl',  # Bazel
        '*.buck',  # Buck
        '*.ninja',  # Ninja
        '*.gyp', '*.gypi',  # GYP

        # === VIRTUALIZATION & CONTAINERS ===
        '*.dockerfile',  # Dockerfile variants
        '*.containerfile',
        '*.vagrantfile',
    ]

    DEFAULT_SPECIAL_FILES = [
        # === BUILD SYSTEMS ===
        'Dockerfile', 'Containerfile', 'docker-compose.yml', 'docker-compose.yaml',
        'Makefile', 'makefile', 'GNUmakefile', 'Makefile.am', 'Makefile.in',
        'CMakeLists.txt', 'cmake.txt', 'meson.build', 'meson_options.txt',
        'SConstruct', 'SConscript', 'wscript', 'waf',
        'BUILD', 'BUILD.bazel', 'WORKSPACE', 'WORKSPACE.bazel',
        'buck', 'BUCK', 'TARGETS',
        'ninja.build', 'build.ninja',

        # === JAVASCRIPT/NODE.JS ECOSYSTEM ===
        'package.json', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
        'bower.json', 'component.json', 'npm-shrinkwrap.json',
        'webpack.config.js', 'webpack.config.ts', 'webpack.common.js',
        'vite.config.js', 'vite.config.ts', 'rollup.config.js', 'rollup.config.ts',
        'parcel.config.js', 'snowpack.config.js', 'esbuild.config.js',
        'tsconfig.json', 'jsconfig.json', 'tsconfig.build.json',
        'babel.config.js', 'babel.config.json', '.babelrc', '.babelrc.js',
        'postcss.config.js', 'tailwind.config.js', 'tailwind.config.ts',
        '.eslintrc.js', '.eslintrc.json', '.eslintrc.yml', '.eslintrc.yaml',
        '.prettierrc', '.prettierrc.js', '.prettierrc.json', '.prettierignore',
        'jest.config.js', 'jest.config.ts', 'vitest.config.js', 'vitest.config.ts',
        'playwright.config.js', 'playwright.config.ts', 'cypress.config.js',

        # === PYTHON ECOSYSTEM ===
        'requirements.txt', 'requirements-dev.txt', 'requirements-test.txt',
        'Pipfile', 'Pipfile.lock', 'poetry.lock', 'pdm.lock',
        'pyproject.toml', 'setup.py', 'setup.cfg', 'manifest.in',
        'tox.ini', 'pytest.ini', 'conftest.py', '.coveragerc', 'coverage.ini',
        'mypy.ini', '.mypy.ini', 'pyrightconfig.json',
        'flake8.cfg', '.flake8', 'pylintrc', '.pylintrc',
        'black.toml', 'isort.cfg', '.isort.cfg', 'bandit.yaml',
        'environment.yml', 'conda.yml', 'environment.yaml',

        # === RUST ECOSYSTEM ===
        'Cargo.toml', 'Cargo.lock', 'rust-toolchain', 'rust-toolchain.toml',
        'clippy.toml', 'rustfmt.toml', '.rustfmt.toml',

        # === GO ECOSYSTEM ===
        'go.mod', 'go.sum', 'go.work', 'go.work.sum',

        # === JAVA ECOSYSTEM ===
        'pom.xml', 'build.gradle', 'build.gradle.kts', 'settings.gradle',
        'gradle.properties', 'gradlew', 'gradlew.bat',
        'build.xml', 'ivy.xml', 'build.sbt', 'project.clj',

        # === .NET ECOSYSTEM ===
        '*.csproj', '*.vbproj', '*.fsproj', '*.sln', '*.proj',
        'packages.config', 'nuget.config', 'global.json',
        'Directory.Build.props', 'Directory.Build.targets',

        # === RUBY ECOSYSTEM ===
        'Gemfile', 'Gemfile.lock', 'Rakefile', '.ruby-version',
        'config.ru', '.rspec', '.rubocop.yml',

        # === PHP ECOSYSTEM ===
        'composer.json', 'composer.lock', 'phpunit.xml', 'phpunit.xml.dist',
        '.php_cs', '.php_cs.dist', 'phpstan.neon', 'psalm.xml',

        # === INFRASTRUCTURE AS CODE ===
        'terraform.tf', 'main.tf', 'variables.tf', 'outputs.tf',
        'terraform.tfvars', 'terraform.tfvars.json',
        'ansible.cfg', 'playbook.yml', 'hosts', 'inventory',
        'Vagrantfile', 'Berksfile', 'Policyfile.rb',
        'docker-stack.yml', 'docker-swarm.yml',

        # === KUBERNETES ===
        'kustomization.yaml', 'kustomization.yml',
        'deployment.yaml', 'service.yaml', 'ingress.yaml',
        'configmap.yaml', 'secret.yaml', 'namespace.yaml',

        # === CI/CD ===
        '.gitlab-ci.yml', '.gitlab-ci.yaml',
        'Jenkinsfile', 'jenkins.yml', 'jenkins.yaml',
        '.circleci/config.yml', '.circle/config.yml',
        '.travis.yml', 'appveyor.yml', '.appveyor.yml',
        'azure-pipelines.yml', 'azure-pipelines.yaml',
        'bitbucket-pipelines.yml', 'drone.yml', '.drone.yml',
        'wercker.yml', 'shippable.yml', 'codefresh.yml',

        # === GITHUB ACTIONS ===
        '.github/workflows/*.yml', '.github/workflows/*.yaml',
        '.github/dependabot.yml', '.github/renovate.json',

        # === DOCUMENTATION ===
        'README', 'README.txt', 'README.md', 'README.rst',
        'CHANGELOG', 'CHANGELOG.md', 'CHANGELOG.txt', 'CHANGES',
        'CONTRIBUTING.md', 'CONTRIBUTING.rst', 'CONTRIBUTING.txt',
        'CODE_OF_CONDUCT.md', 'SECURITY.md', 'SUPPORT.md',
        'LICENSE', 'LICENSE.txt', 'LICENSE.md', 'COPYING',
        'AUTHORS', 'AUTHORS.txt', 'AUTHORS.md', 'MAINTAINERS',
        'NOTICE', 'ACKNOWLEDGMENTS', 'CREDITS',

        # === VERSION CONTROL ===
        '.gitignore', '.gitattributes', '.gitmodules', '.gitmessage',
        '.hgignore', '.svnignore', '.bzrignore',

        # === EDITOR CONFIGURATIONS ===
        '.editorconfig', '.dir-locals.el', '.projectile',
        '.vimrc', '.nvimrc', '.emacs', '.spacemacs',

        # === MOBILE DEVELOPMENT ===
        'pubspec.yaml', 'pubspec.lock',  # Dart/Flutter
        'android_app.yaml', 'ios_app.yaml',
        'Info.plist', 'AndroidManifest.xml',
        'build.gradle', 'proguard-rules.pro',

        # === GAME DEVELOPMENT ===
        'project.godot', 'export_presets.cfg',  # Godot
        'game.project', 'main.tscn',
        'love.exe', 'main.lua',  # Love2D

        # === DATA SCIENCE ===
        'requirements-dev.txt', 'environment.yml',
        'notebook.ipynb', '*.ipynb',
        '.RData', '.Rprofile', 'DESCRIPTION',

        # === DATABASE ===
        'schema.sql', 'migrations.sql', 'seeds.sql',
        'alembic.ini', 'flyway.conf',
        '.sequelizerc', 'knexfile.js',

        # === WEB FRAMEWORKS ===
        'next.config.js', 'nuxt.config.js', 'svelte.config.js',
        'gatsby-config.js', 'gridsome.config.js',
        'quasar.conf.js', 'vue.config.js', 'angular.json',
        'ember-cli-build.js', '.ember-cli',

        # === TESTING ===
        'karma.conf.js', 'protractor.conf.js', 'wdio.conf.js',
        'codecept.conf.js', 'nightwatch.conf.js',
        'testcafe.json', 'cucumber.js',

        # === MONITORING & OBSERVABILITY ===
        'prometheus.yml', 'grafana.json', 'jaeger.yml',
        'newrelic.yml', 'datadog.yaml', 'sentry.properties',

        # === MISC CONFIGURATION ===
        'nodemon.json', 'browserslist', '.nvmrc', '.node-version',
        'lerna.json', 'rush.json', 'nx.json', 'workspace.json',
        'bit.json', '.bitmap', 'now.json', 'vercel.json',
        'netlify.toml', '_redirects', '_headers',
        'firebase.json', '.firebaserc', 'app.yaml', 'cron.yaml',
    ]

    def __init__(self, repo_url: str, shallow: bool = False, depth: Optional[int] = None):
        """
        Initializes the crawler with the repository URL.

        Args:
            repo_url: The Git repository URL to clone
            shallow: Whether to perform a shallow clone (faster, less history)
            depth: Depth for shallow clone (only used if shallow=True)
        """
        self.repo_url = repo_url
        self.shallow = shallow
        self.depth = depth or 1 if shallow else None
        self.temp_dir_handle = tempfile.TemporaryDirectory()
        self.repo_path = pathlib.Path(self.temp_dir_handle.name)
        self._file_paths_cache: Optional[List[pathlib.Path]] = None
        self._current_ref: Optional[str] = None

    def __enter__(self):
        """
        Enters the context manager, cloning the repository.
        """
        logger.info(f"Cloning {self.repo_url} into {self.repo_path}")
        self._clone_repo()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Exits the context manager, cleaning up resources.
        """
        logger.info("Cleaning up resources")
        self.cleanup()

    def _run_git_command(self, cmd: List[str], check: bool = True, capture_output: bool = True) -> subprocess.CompletedProcess:
        """
        Helper method to run git commands with consistent error handling.

        Args:
            cmd: Git command as list of strings
            check: Whether to raise CalledProcessError on non-zero exit
            capture_output: Whether to capture stdout/stderr

        Returns:
            CompletedProcess result
        """
        try:
            result = subprocess.run(
                cmd,
                cwd=self.repo_path,
                check=check,
                capture_output=capture_output,
                text=True,
                timeout=300  # 5 minute timeout for git operations
            )
            return result
        except subprocess.TimeoutExpired:
            logger.error(f"Git command timed out: {' '.join(cmd)}")
            raise
        except subprocess.CalledProcessError as e:
            logger.error(f"Git command failed: {' '.join(cmd)}, Error: {e.stderr}")
            raise

    def _clone_repo(self):
        """
        Clones the git repository with optimized settings.
        """
        try:
            clone_cmd = ['git', 'clone']

            if self.shallow:
                clone_cmd.extend(['--depth', str(self.depth)])
                clone_cmd.append('--single-branch')

            clone_cmd.extend([self.repo_url, str(self.repo_path)])

            self._run_git_command(clone_cmd)

            if not self.shallow:
                # Fetch all tags and remote branches for full clones
                self._run_git_command(['git', 'fetch', 'origin', '--tags'])
                self._run_git_command(['git', 'remote', 'update'])

            # Get current branch/ref
            result = self._run_git_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
            self._current_ref = result.stdout.strip()

            logger.info(f"Repository cloned successfully on branch: {self._current_ref}")

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to clone repository: {e.stderr}")
            raise

    def get_current_ref(self) -> str:
        """
        Gets the currently checked out reference.

        Returns:
            Current branch/tag/commit hash
        """
        if self._current_ref is None:
            result = self._run_git_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
            self._current_ref = result.stdout.strip()
        return self._current_ref

    def list_branches(self, remote: bool = True) -> List[str]:
        """
        Lists available branches.

        Args:
            remote: Whether to include remote branches

        Returns:
            List of branch names
        """
        cmd = ['git', 'branch']
        if remote:
            cmd.append('-a')

        result = self._run_git_command(cmd)
        branches = []
        for line in result.stdout.strip().split('\n'):
            line = line.strip()
            if line.startswith('*'):
                line = line[1:].strip()
            if line and not line.startswith('('):
                branches.append(line)

        return branches

    def list_tags(self) -> List[str]:
        """
        Lists available tags.

        Returns:
            List of tag names
        """
        result = self._run_git_command(['git', 'tag', '-l'])
        return [tag.strip() for tag in result.stdout.strip().split('\n') if tag.strip()]

    def get_blame_for_file(self, target_file: str, line_range: Optional[tuple] = None) -> str:
        """
        Runs `git blame` on a specific file in the repo.

        Args:
            target_file: Path to the file relative to repo root
            line_range: Optional tuple of (start_line, end_line) for partial blame

        Returns:
            Git blame output or error message
        """
        file_full_path = self.repo_path / target_file
        if not file_full_path.exists():
            error_msg = f"Error from crawler: File '{target_file}' does not exist in the repository."
            logger.warning(error_msg)
            return error_msg

        try:
            logger.info(f"Running 'git blame' on {file_full_path}")

            cmd = ['git', 'blame', '--show-email']
            if line_range:
                start, end = line_range
                cmd.extend(['-L', f'{start},{end}'])
            cmd.append(str(file_full_path))

            result = self._run_git_command(cmd)
            return result.stdout

        except subprocess.CalledProcessError as e:
            error_message = f"Error running 'git blame' on '{target_file}': {e.stderr}"
            logger.error(error_message)
            return f"Error from crawler: {error_message}"

    def get_diff_for_branch(self, ref_name: str, base_ref: str = 'main', stat_only: bool = False) -> str:
        """
        Gets the `git diff` between two refs (branch, tag, or commit).

        Args:
            ref_name: The reference to compare
            base_ref: The base reference to compare against
            stat_only: Whether to return only diff statistics

        Returns:
            Git diff output or error message
        """
        try:
            logger.info(f"Calculating diff for '{ref_name}' against base '{base_ref}'")

            cmd = ['git', 'diff']
            if stat_only:
                cmd.append('--stat')
            cmd.append(f'{base_ref}...{ref_name}')

            logger.debug(f"Running command: {' '.join(cmd)}")
            result = self._run_git_command(cmd)
            return result.stdout

        except subprocess.CalledProcessError:
            # Try with 'master' if 'main' fails
            if base_ref == 'main':
                logger.info("Diff against 'main' failed, trying 'master' as base")
                return self.get_diff_for_branch(ref_name, 'master', stat_only)

            error_message = f"Error running 'git diff' between '{base_ref}' and '{ref_name}'"
            logger.error(error_message)
            return f"Error from crawler: {error_message}"

    def checkout_ref(self, ref_name: str, create_branch: bool = False) -> bool:
        """
        Checks out a specific git reference (branch, tag, or commit hash).

        Args:
            ref_name: The name of the reference to check out
            create_branch: Whether to create a new branch if it doesn't exist

        Returns:
            True if checkout was successful, False otherwise
        """
        logger.info(f"Attempting to checkout ref '{ref_name}'")
        try:
            # Invalidate caches since the files will change
            self._file_paths_cache = None

            cmd = ['git', 'checkout']
            if create_branch:
                cmd.extend(['-b', ref_name])
            else:
                cmd.append(ref_name)

            self._run_git_command(cmd)
            self._current_ref = ref_name
            logger.info(f"Successfully checked out '{ref_name}'")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to checkout '{ref_name}': {e.stderr}")
            return False

    def get_file_content(self, file_path: str, encoding: str = 'utf-8') -> Optional[str]:
        """
        Reads the content of a file in the repository.

        Args:
            file_path: Path to the file relative to repo root
            encoding: File encoding (default: utf-8)

        Returns:
            File content or None if file doesn't exist/can't be read
        """
        full_path = self.repo_path / file_path
        try:
            return full_path.read_text(encoding=encoding)
        except (FileNotFoundError, UnicodeDecodeError, PermissionError) as e:
            logger.warning(f"Could not read file '{file_path}': {e}")
            return None

    def find_file_path(self, target_filename: str, case_sensitive: bool = True) -> Union[str, Dict, None]:
        """
        Searches the repository for a file by its name.

        Args:
            target_filename: Name of the file to search for
            case_sensitive: Whether to perform case-sensitive search

        Returns:
            File path, conflict dict with options, or None if not found
        """
        logger.info(f"Searching for file matching '{target_filename}'")
        all_files = self.get_file_paths()

        possible_matches = []
        search_name = target_filename if case_sensitive else target_filename.lower()

        for file_path in all_files:
            relative_path = file_path.relative_to(self.repo_path)
            file_name = relative_path.name if case_sensitive else relative_path.name.lower()
            path_str = str(relative_path) if case_sensitive else str(relative_path).lower()

            if file_name == search_name or path_str.endswith('/' + search_name):
                possible_matches.append(relative_path)

        if not possible_matches:
            logger.info(f"No match found for '{target_filename}'")
            return None

        if len(possible_matches) == 1:
            match = str(possible_matches[0])
            logger.info(f"Found unique match: {match}")
            return match

        logger.info(f"Found multiple matches, checking for root-level file")
        root_matches = [p for p in possible_matches if len(p.parts) == 1]

        if len(root_matches) == 1:
            match = str(root_matches[0])
            logger.info(f"Prioritized unique root match: {match}")
            return match

        logger.warning(f"Ambiguous path detected for '{target_filename}'")
        return {
            "error": "ambiguous_path",
            "message": f"Multiple files found matching '{target_filename}'. Please specify one.",
            "options": [str(p) for p in possible_matches]
        }

    def get_file_paths(self, custom_extensions: Optional[List[str]] = None,
                      custom_excluded_dirs: Optional[Set[str]] = None,
                      include_hidden: bool = False) -> List[pathlib.Path]:
        """
        Scans the cloned repo and returns a list of relevant files.
        Caches the result for performance.

        Args:
            custom_extensions: Override default file extensions to include
            custom_excluded_dirs: Override default directories to exclude
            include_hidden: Whether to include hidden files/directories

        Returns:
            List of file paths
        """
        # Use custom parameters if provided, otherwise use defaults
        if custom_extensions is not None or custom_excluded_dirs is not None or include_hidden:
            # Don't use cache for custom parameters
            return self._scan_files(custom_extensions, custom_excluded_dirs, include_hidden)

        if self._file_paths_cache is not None:
            return self._file_paths_cache

        logger.info("Scanning for relevant files...")
        self._file_paths_cache = self._scan_files()
        logger.info(f"Found and cached {len(self._file_paths_cache)} files to process")
        return self._file_paths_cache

    def _scan_files(self, custom_extensions: Optional[List[str]] = None,
                   custom_excluded_dirs: Optional[Set[str]] = None,
                   include_hidden: bool = False) -> List[pathlib.Path]:
        """
        Internal method to scan files with given parameters.
        """
        extensions = custom_extensions if custom_extensions is not None else (
            self.DEFAULT_INCLUDED_EXTENSIONS + self.DEFAULT_SPECIAL_FILES
        )
        excluded_dirs = custom_excluded_dirs if custom_excluded_dirs is not None else self.DEFAULT_EXCLUDED_DIRS

        files_to_process = []

        for file_path in self.repo_path.rglob('*'):
            relative_path = file_path.relative_to(self.repo_path)

            # Skip hidden files/directories unless explicitly included
            if not include_hidden and any(part.startswith('.') for part in relative_path.parts):
                # Allow certain dotfiles that are in our special files list
                if file_path.name not in self.DEFAULT_SPECIAL_FILES:
                    continue

            # Skip excluded directories
            if any(part in excluded_dirs for part in relative_path.parts):
                continue

            if file_path.is_file():
                # Check against extensions (patterns) and special filenames
                if any(file_path.match(ext) for ext in extensions):
                    files_to_process.append(file_path)

        return files_to_process

    def get_language_statistics(self) -> Dict[str, Dict[str, Union[int, List[str]]]]:
        """
        Enhanced method to get comprehensive language statistics.

        Returns:
            Dictionary with language statistics including file counts and examples
        """
        files = self.get_file_paths()

        # Comprehensive language mapping
        LANGUAGE_MAP = {
            # Web Technologies
            '.js': 'JavaScript', '.mjs': 'JavaScript', '.cjs': 'JavaScript',
            '.jsx': 'JavaScript (React)', '.ts': 'TypeScript', '.tsx': 'TypeScript (React)',
            '.vue': 'Vue.js', '.svelte': 'Svelte', '.astro': 'Astro',
            '.html': 'HTML', '.htm': 'HTML', '.css': 'CSS', '.scss': 'SASS',
            '.sass': 'SASS', '.less': 'LESS', '.styl': 'Stylus',

            # Backend Languages
            '.py': 'Python', '.pyx': 'Cython', '.pyi': 'Python Interface',
            '.java': 'Java', '.kt': 'Kotlin', '.scala': 'Scala', '.groovy': 'Groovy',
            '.cs': 'C#', '.vb': 'Visual Basic .NET', '.fs': 'F#',
            '.c': 'C', '.cpp': 'C++', '.cxx': 'C++', '.cc': 'C++', '.h': 'C/C++ Header',
            '.rs': 'Rust', '.go': 'Go', '.swift': 'Swift',
            '.rb': 'Ruby', '.php': 'PHP', '.pl': 'Perl', '.lua': 'Lua',

            # Functional Languages
            '.hs': 'Haskell', '.ml': 'OCaml', '.elm': 'Elm', '.ex': 'Elixir',
            '.erl': 'Erlang', '.clj': 'Clojure', '.scm': 'Scheme', '.lisp': 'Common Lisp',

            # Data Science
            '.r': 'R', '.jl': 'Julia', '.m': 'MATLAB', '.ipynb': 'Jupyter Notebook',

            # Mobile
            '.dart': 'Dart', '.gs': 'Google Apps Script',

            # Systems
            '.zig': 'Zig', '.nim': 'Nim', '.crystal': 'Crystal', '.d': 'D',

            # Markup and Config
            '.md': 'Markdown', '.rst': 'reStructuredText', '.tex': 'LaTeX',
            '.json': 'JSON', '.yaml': 'YAML', '.yml': 'YAML', '.toml': 'TOML',
            '.xml': 'XML', '.ini': 'INI', '.cfg': 'Config',

            # Database
            '.sql': 'SQL', '.graphql': 'GraphQL', '.gql': 'GraphQL',

            # Shell
            '.sh': 'Shell Script', '.bash': 'Bash', '.zsh': 'Zsh', '.fish': 'Fish',
            '.ps1': 'PowerShell', '.bat': 'Batch', '.cmd': 'Command Script',

            # Infrastructure
            '.tf': 'Terraform', '.hcl': 'HCL', '.dockerfile': 'Dockerfile',

            # Game Development
            '.gd': 'GDScript', '.hlsl': 'HLSL', '.glsl': 'GLSL', '.shader': 'Shader',

            # Emerging/Blockchain
            '.sol': 'Solidity', '.move': 'Move', '.cairo': 'Cairo',

            # Legacy
            '.pas': 'Pascal', '.cob': 'COBOL', '.for': 'FORTRAN', '.asm': 'Assembly',

            # Special cases
            'no_extension': 'No Extension'
        }

        language_stats = {}

        for file_path in files:
            ext = file_path.suffix.lower() or 'no_extension'
            language = LANGUAGE_MAP.get(ext, f'Unknown ({ext})')

            if language not in language_stats:
                language_stats[language] = {
                    'count': 0,
                    'extensions': set(),
                    'examples': []
                }

            language_stats[language]['count'] += 1
            language_stats[language]['extensions'].add(ext)

            # Add up to 3 example files
            if len(language_stats[language]['examples']) < 3:
                relative_path = str(file_path.relative_to(self.repo_path))
                language_stats[language]['examples'].append(relative_path)

        # Convert sets to lists for JSON serialization
        for lang_data in language_stats.values():
            lang_data['extensions'] = list(lang_data['extensions'])

        return language_stats

    def get_repo_stats(self) -> Dict[str, Union[int, str, List[str], Dict]]:
        """
        Gets comprehensive statistics about the repository including language breakdown.

        Returns:
            Dictionary with enhanced repo statistics
        """
        try:
            files = self.get_file_paths()
            language_stats = self.get_language_statistics()

            # Count files by extension (legacy compatibility)
            extensions = {}
            for file_path in files:
                ext = file_path.suffix.lower() or 'no_extension'
                extensions[ext] = extensions.get(ext, 0) + 1

            # Get commit count (if not shallow)
            commit_count = "N/A (shallow clone)"
            if not self.shallow:
                try:
                    result = self._run_git_command(['git', 'rev-list', '--count', 'HEAD'])
                    commit_count = int(result.stdout.strip())
                except:
                    pass

            # Detect primary language
            if language_stats:
                primary_language = max(language_stats.items(), key=lambda x: x[1]['count'])
                primary_lang_name = primary_language[0]
                primary_lang_count = primary_language[1]['count']
            else:
                primary_lang_name = "Unknown"
                primary_lang_count = 0

            return {
                'total_files': len(files),
                'primary_language': primary_lang_name,
                'primary_language_files': primary_lang_count,
                'language_breakdown': language_stats,
                'file_extensions': extensions,  # Legacy compatibility
                'current_ref': self.get_current_ref(),
                'commit_count': commit_count,
                'branches': self.list_branches() if not self.shallow else "N/A (shallow clone)",
                'tags': self.list_tags() if not self.shallow else "N/A (shallow clone)",
                'polyglot_support': True,
                'supported_languages': len(language_stats)
            }
        except Exception as e:
            logger.error(f"Error getting repo stats: {e}")
            return {'error': str(e)}

    def cleanup(self):
        """
        Removes the temporary directory and all its contents.
        """
        try:
            self.temp_dir_handle.cleanup()
            logger.info(f"Cleaned up temporary directory: {self.repo_path}")
        except Exception as e:
            logger.warning(f"Error during cleanup: {e}")

    # Enhanced utility methods
    def find_files_by_language(self, language: str) -> List[pathlib.Path]:
        """
        Find files by programming language name.

        Args:
            language: Language name (e.g., 'Python', 'JavaScript', 'Rust')

        Returns:
            List of file paths for that language
        """
        language_stats = self.get_language_statistics()
        matching_files = []

        if language in language_stats:
            extensions = language_stats[language]['extensions']
            all_files = self.get_file_paths()

            for file_path in all_files:
                ext = file_path.suffix.lower() or 'no_extension'
                if ext in extensions:
                    matching_files.append(file_path)

        return matching_files

    def get_project_type_hints(self) -> Dict[str, Union[str, List[str]]]:
        """
        Analyze the repository to determine likely project types and frameworks.

        Returns:
            Dictionary with project type information
        """
        files = self.get_file_paths()
        file_names = {f.name for f in files}
        language_stats = self.get_language_statistics()

        project_hints = {
            'primary_language': None,
            'frameworks': [],
            'build_systems': [],
            'package_managers': [],
            'project_types': []
        }

        # Determine primary language
        if language_stats:
            primary_lang = max(language_stats.items(), key=lambda x: x[1]['count'])
            project_hints['primary_language'] = primary_lang[0]

        # Framework detection
        framework_indicators = {
            'React': ['package.json', '.jsx', '.tsx'] + [f for f in file_names if 'react' in f.lower()],
            'Vue.js': ['vue.config.js', '.vue'] + [f for f in file_names if 'vue' in f.lower()],
            'Angular': ['angular.json', '.component.ts'] + [f for f in file_names if 'angular' in f.lower()],
            'Next.js': ['next.config.js'] + [f for f in file_names if 'next' in f.lower()],
            'Svelte': ['svelte.config.js', '.svelte'],
            'Django': ['manage.py', 'wsgi.py'] + [f for f in file_names if 'django' in f.lower()],
            'Flask': [f for f in file_names if 'flask' in f.lower()],
            'FastAPI': [f for f in file_names if 'fastapi' in f.lower()],
            'Spring': ['pom.xml'] + [f for f in file_names if 'spring' in f.lower()],
            'Express': [f for f in file_names if 'express' in f.lower()],
            'Ruby on Rails': ['Gemfile', 'config.ru'] + [f for f in file_names if 'rails' in f.lower()],
            'Laravel': ['composer.json'] + [f for f in file_names if 'laravel' in f.lower()],
            'Unity': [f for f in file_names if f.endswith('.unity')],
            'Godot': ['project.godot', '.gd'],
            'Flutter': ['pubspec.yaml', '.dart'],
            'React Native': ['metro.config.js'] + [f for f in file_names if 'react-native' in f.lower()],
        }

        for framework, indicators in framework_indicators.items():
            if any(indicator in file_names or any(f.endswith(indicator) for f in file_names) for indicator in indicators):
                project_hints['frameworks'].append(framework)

        # Build system detection
        build_systems = {
            'Make': ['Makefile', 'makefile', 'GNUmakefile'],
            'CMake': ['CMakeLists.txt'],
            'Gradle': ['build.gradle', 'gradlew'],
            'Maven': ['pom.xml'],
            'Cargo': ['Cargo.toml'],
            'npm': ['package.json'],
            'Webpack': ['webpack.config.js'],
            'Vite': ['vite.config.js'],
            'Bazel': ['WORKSPACE', 'BUILD'],
        }

        for build_system, files_list in build_systems.items():
            if any(f in file_names for f in files_list):
                project_hints['build_systems'].append(build_system)

        # Package manager detection
        package_managers = {
            'npm': ['package-lock.json'],
            'yarn': ['yarn.lock'],
            'pnpm': ['pnpm-lock.yaml'],
            'pip': ['requirements.txt'],
            'poetry': ['poetry.lock'],
            'pipenv': ['Pipfile'],
            'conda': ['environment.yml'],
            'cargo': ['Cargo.lock'],
            'composer': ['composer.lock'],
            'bundler': ['Gemfile.lock'],
        }

        for pm, files_list in package_managers.items():
            if any(f in file_names for f in files_list):
                project_hints['package_managers'].append(pm)

        # Project type classification
        if 'package.json' in file_names:
            project_hints['project_types'].append('Node.js Application')
        if any(f.endswith('.py') for f in file_names):
            project_hints['project_types'].append('Python Application')
        if 'Cargo.toml' in file_names:
            project_hints['project_types'].append('Rust Application')
        if any(f.endswith('.java') for f in file_names):
            project_hints['project_types'].append('Java Application')
        if 'go.mod' in file_names:
            project_hints['project_types'].append('Go Application')
        if any(f.endswith('.cs') for f in file_names):
            project_hints['project_types'].append('.NET Application')
        if 'Dockerfile' in file_names:
            project_hints['project_types'].append('Containerized Application')
        if any(f.endswith('.tf') for f in file_names):
            project_hints['project_types'].append('Infrastructure as Code')

        return project_hints

    # Backward compatibility aliases
    def find_files_by_extension(self, extension: str) -> List[pathlib.Path]:
        """
        Backward compatibility method to find files by extension.
        """
        all_files = self.get_file_paths()
        return [f for f in all_files if f.suffix.lower() == extension.lower()]

--- FILE_END: backend/ingestion/crawler.py ---

--- FILE_START: backend/build_parsers.py ---
#!/usr/bin/env python3
"""
Enhanced Tree-sitter Parser Builder for Lumiere Core
Builds parsers for all supported languages in your polyglot application.
Now handles grammars that require generation step.
"""

import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

try:
    from tree_sitter import Language
except ImportError:
    print("✗ 'tree_sitter' library not found. Install with:")
    print("  pip install tree-sitter==0.20.1")
    sys.exit(1)

# --- Enhanced Configuration ---
GRAMMARS_DIR = Path("lumiere_core/services/grammars")
BUILD_DIR = Path("lumiere_core/services/build")
LIBRARY_PATH = BUILD_DIR / "my-languages.so"

# Comprehensive language mappings based on your polyglot config
LANGUAGE_GRAMMARS = {
    # Web Technologies
    'javascript': {
        'url': 'https://github.com/tree-sitter/tree-sitter-javascript',
        'extensions': ['.js', '.jsx', '.mjs'],
        'priority': 'high'
    },
    # 'typescript': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-typescript',
    #     'extensions': ['.ts', '.tsx'],
    #     'priority': 'high',
    #     'subdirs': ['typescript', 'tsx']  # TypeScript repo has multiple grammars
    # },
    'html': {
        'url': 'https://github.com/tree-sitter/tree-sitter-html',
        'extensions': ['.html', '.htm'],
        'priority': 'medium'
    },
    'css': {
        'url': 'https://github.com/tree-sitter/tree-sitter-css',
        'extensions': ['.css', '.scss', '.sass'],
        'priority': 'medium'
    },
    # 'vue': {
    #     'url': 'https://github.com/ikatyang/tree-sitter-vue',
    #     'extensions': ['.vue'],
    #     'priority': 'medium'
    # },

    # Backend Languages
    'python': {
        'url': 'https://github.com/tree-sitter/tree-sitter-python',
        'extensions': ['.py', '.pyx', '.pyi'],
        'priority': 'high'
    },
    'java': {
        'url': 'https://github.com/tree-sitter/tree-sitter-java',
        'extensions': ['.java'],
        'priority': 'high'
    },
    'c_sharp': {
        'url': 'https://github.com/tree-sitter/tree-sitter-c-sharp',
        'extensions': ['.cs'],
        'priority': 'high'
    },
    'go': {
        'url': 'https://github.com/tree-sitter/tree-sitter-go',
        'extensions': ['.go'],
        'priority': 'high'
    },
    'rust': {
        'url': 'https://github.com/tree-sitter/tree-sitter-rust',
        'extensions': ['.rs'],
        'priority': 'high',
        'requires_generate': True  
    },
    'swift': {
        'url': 'https://github.com/tree-sitter/tree-sitter-swift',
        'extensions': ['.swift'],
        'priority': 'medium',
        'requires_generate': True
    },

    # Functional Languages
    'haskell': {
        'url': 'https://github.com/tree-sitter/tree-sitter-haskell',
        'extensions': ['.hs', '.lhs'],
        'priority': 'medium'
    },
    'elixir': {
        'url': 'https://github.com/elixir-lang/tree-sitter-elixir',
        'extensions': ['.ex', '.exs'],
        'priority': 'medium',
        'requires_generate': True
    },
    'clojure': {
        'url': 'https://github.com/sogaiu/tree-sitter-clojure',
        'extensions': ['.clj', '.cljs', '.cljc'],
        'priority': 'medium'
    },

    # Systems Languages
    'c': {
        'url': 'https://github.com/tree-sitter/tree-sitter-c',
        'extensions': ['.c', '.h'],
        'priority': 'high'
    },
    # 'cpp': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-cpp',
    #     'extensions': ['.cpp', '.cxx', '.cc', '.hpp', '.hxx'],
    #     'priority': 'high'
    # },

    # Scripting Languages
    'ruby': {
        'url': 'https://github.com/tree-sitter/tree-sitter-ruby',
        'extensions': ['.rb', '.rake'],
        'priority': 'medium'
    },
    # 'php': {
    #     'url': 'https://github.com/tree-sitter/tree-sitter-php',
    #     'extensions': ['.php'],
    #     'priority': 'medium',
    #     'requires_generate': True  # This grammar needs generation step
    # },

    # Mobile/Cross-platform
    'kotlin': {
        'url': 'https://github.com/fwcd/tree-sitter-kotlin',
        'extensions': ['.kt', '.kts'],
        'priority': 'medium',
        'requires_generate': True
    },
    'dart': {
        'url': 'https://github.com/UserNobody14/tree-sitter-dart',
        'extensions': ['.dart'],
        'priority': 'medium',
        'requires_generate': True
    },

    # Data Science
    'r': {
        'url': 'https://github.com/r-lib/tree-sitter-r',
        'extensions': ['.r', '.R'],
        'priority': 'low'
    },
    'julia': {
        'url': 'https://github.com/tree-sitter/tree-sitter-julia',
        'extensions': ['.jl'],
        'priority': 'low'
    },

    # Additional useful languages
    'bash': {
        'url': 'https://github.com/tree-sitter/tree-sitter-bash',
        'extensions': ['.sh', '.bash'],
        'priority': 'medium'
    },
    'json': {
        'url': 'https://github.com/tree-sitter/tree-sitter-json',
        'extensions': ['.json'],
        'priority': 'high'
    },
    # 'yaml': {
    #     'url': 'https://github.com/ikatyang/tree-sitter-yaml',
    #     'extensions': ['.yml', '.yaml'],
    #     'priority': 'medium'
    # },
    'toml': {
        'url': 'https://github.com/ikatyang/tree-sitter-toml',
        'extensions': ['.toml'],
        'priority': 'low'
    },
    # 'sql': {
    #     'url': 'https://github.com/derekstride/tree-sitter-sql',
    #     'extensions': ['.sql'],
    #     'priority': 'medium',
    #     'requires_generate': True
    # }
}

# Languages known to require generation or have build issues
REQUIRES_GENERATION = {'php', 'haskell', 'ocaml', 'agda', 'sql', 'swift', 'kotlin', 'dart', 'elixir', 'vue'}

class ParserBuilder:
    def __init__(self, languages_subset: Optional[List[str]] = None,
                 priority_filter: Optional[str] = None):
        self.languages_subset = languages_subset
        self.priority_filter = priority_filter
        self.failed_languages = []
        self.successful_languages = []
        self.skipped_languages = []

    def check_prerequisites(self) -> bool:
        """Check if all required tools are available."""
        logger.info("Checking prerequisites...")

        # Check if we're in the right directory
        if not Path("manage.py").exists() or not Path("lumiere_core").is_dir():
            logger.error("This script must be run from the 'backend' directory")
            logger.error("Please run: cd backend && python build_parsers.py")
            return False

        # Check Git
        try:
            subprocess.run(["git", "--version"], check=True,
                         capture_output=True, text=True)
            logger.info("✓ Git is available")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("✗ Git is required but not found in PATH")
            return False

        # Check for C/C++ compiler
        compilers = ['gcc', 'clang', 'cl']  # Windows, Unix
        compiler_found = False
        for compiler in compilers:
            try:
                subprocess.run([compiler, '--version'], check=True,
                             capture_output=True, text=True)
                logger.info(f"✓ C/C++ compiler found: {compiler}")
                compiler_found = True
                break
            except (subprocess.CalledProcessError, FileNotFoundError):
                continue

        if not compiler_found:
            logger.error("✗ No C/C++ compiler found")
            logger.error("Install build tools:")
            logger.error("  macOS: xcode-select --install")
            logger.error("  Ubuntu/Debian: sudo apt install build-essential")
            logger.error("  Fedora/CentOS: sudo dnf groupinstall 'Development Tools'")
            return False

        # Check Node.js/npm (needed for grammar generation)
        try:
            subprocess.run(["node", "--version"], check=True,
                         capture_output=True, text=True)
            subprocess.run(["npm", "--version"], check=True,
                         capture_output=True, text=True)
            logger.info("✓ Node.js and npm are available")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.warning("⚠ Node.js/npm not found - some grammars may fail to build")
            logger.warning("  Install from: https://nodejs.org/")

        return True

    def get_languages_to_build(self) -> Dict[str, dict]:
        """Get the filtered list of languages to build."""
        languages = LANGUAGE_GRAMMARS.copy()

        # Filter by subset if specified
        if self.languages_subset:
            languages = {k: v for k, v in languages.items()
                        if k in self.languages_subset}

        # Filter by priority if specified
        if self.priority_filter:
            languages = {k: v for k, v in languages.items()
                        if v.get('priority', 'medium') == self.priority_filter}

        return languages

    def clone_or_update_grammar(self, name: str, config: dict) -> bool:
        """Clone or update a single grammar repository."""
        lang_path = GRAMMARS_DIR / name
        url = config['url']

        try:
            if lang_path.is_dir():
                logger.info(f"Updating {name}...")
                result = subprocess.run(
                    ["git", "pull", "--quiet"],
                    cwd=lang_path,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                if result.returncode != 0:
                    logger.warning(f"Failed to update {name}: {result.stderr}")
                    return False
            else:
                logger.info(f"Cloning {name}...")
                result = subprocess.run(
                    ["git", "clone", "--depth", "1", "--quiet", url, str(lang_path)],
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                if result.returncode != 0:
                    logger.error(f"Failed to clone {name}: {result.stderr}")
                    return False

            return True

        except subprocess.TimeoutExpired:
            logger.error(f"Timeout while processing {name}")
            return False
        except Exception as e:
            logger.error(f"Error processing {name}: {e}")
            return False

    def generate_parser_if_needed(self, name: str, lang_path: Path) -> bool:
        """Generate parser.c if the grammar requires it."""
        # Check all common source locations for an existing parser file
        src_locations = ["src", "php/src", "php_only/src", "."]
        for loc in src_locations:
            if list((lang_path / loc).glob("parser.c")):
                # Found it, no need to generate
                return True

        # If not found, and it's a language that requires generation, then run the build steps.
        if name in REQUIRES_GENERATION or LANGUAGE_GRAMMARS[name].get('requires_generate'):
            logger.info(f"Generating parser for {name}...")

            if not (lang_path / "package.json").exists():
                logger.warning(f"Generation required for {name} but no package.json found. Skipping generation.")
                return False

            try:
                # Use npm ci for faster, more reliable installs if a lock file exists
                install_command = ["npm", "ci"] if (lang_path / "package-lock.json").exists() else ["npm", "install"]

                logger.info(f"Installing dependencies for {name} using '{' '.join(install_command)}'...")
                # We need to run with shell=True on Windows for npm.cmd, and it's generally safer for npm scripts.
                result = subprocess.run(
                    install_command,
                    cwd=lang_path,
                    check=True,
                    capture_output=True,
                    text=True,
                    timeout=300, # Increased timeout for slow installs
                    shell=sys.platform == 'win32'
                )
                if result.stdout: logger.debug(f"npm install stdout for {name}: {result.stdout}")
                if result.stderr: logger.warning(f"npm install stderr for {name}: {result.stderr}")

                # After installation, re-check for the generated parser file. Some packages generate on install.
                for loc in src_locations:
                    if list((lang_path / loc).glob("parser.c")):
                        logger.info(f"✓ Parser for {name} was generated during npm install.")
                        return True

                # If it wasn't generated during install, try the `generate` command explicitly.
                logger.info(f"Attempting explicit 'tree-sitter generate' for {name}...")
                subprocess.run(
                    ["npx", "tree-sitter", "generate"],
                    cwd=lang_path,
                    check=True,
                    capture_output=True,
                    text=True,
                    timeout=60,
                    shell=sys.platform == 'win32'
                )

                # Final check
                for loc in src_locations:
                    if list((lang_path / loc).glob("parser.c")):
                        logger.info(f"✓ Successfully generated parser for {name}")
                        return True

                logger.error(f"Failed to generate parser.c for {name} after all steps.")
                return False

            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to generate parser for {name}. Command: '{e.cmd}'. Stderr: {e.stderr}. Stdout: {e.stdout}")
                return False
            except subprocess.TimeoutExpired:
                logger.error(f"Timeout generating parser for {name}")
                return False

        # Grammar doesn't require generation, but we didn't find the file.
        logger.warning(f"No parser.c found for {name}, and it is not marked for generation.")
        return False

    def clone_grammars_parallel(self, languages: Dict[str, dict]) -> List[str]:
        """Clone/update all grammars in parallel for speed."""
        logger.info(f"Processing {len(languages)} language grammars...")
        GRAMMARS_DIR.mkdir(parents=True, exist_ok=True)

        successful_langs = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_lang = {
                executor.submit(self.clone_or_update_grammar, name, config): name
                for name, config in languages.items()
            }

            for future in as_completed(future_to_lang):
                lang_name = future_to_lang[future]
                try:
                    if future.result():
                        # Check if parser generation is needed
                        lang_path = GRAMMARS_DIR / lang_name
                        if self.generate_parser_if_needed(lang_name, lang_path):
                            successful_langs.append(lang_name)
                        else:
                            logger.warning(f"Skipping {lang_name} - parser generation failed")
                            self.skipped_languages.append(lang_name)
                    else:
                        self.failed_languages.append(lang_name)
                except Exception as e:
                    logger.error(f"Unexpected error with {lang_name}: {e}")
                    self.failed_languages.append(lang_name)

        return successful_langs

    def build_library(self, successful_languages: List[str]) -> bool:
        """Build the shared library from successfully cloned grammars."""
        if not successful_languages:
            logger.error("No languages available to build")
            return False

        logger.info(f"Building library with {len(successful_languages)} languages...")
        BUILD_DIR.mkdir(parents=True, exist_ok=True)

        # Prepare grammar paths, handling special cases
        grammar_paths = []
        for lang in successful_languages:
            lang_path = GRAMMARS_DIR / lang
            config = LANGUAGE_GRAMMARS[lang]

            # Handle languages with subdirectories (like TypeScript)
            if 'subdirs' in config:
                for subdir in config['subdirs']:
                    subdir_path = lang_path / subdir
                    if subdir_path.exists():
                        grammar_paths.append(str(subdir_path))
            else:
                grammar_paths.append(str(lang_path))

        # Remove any paths that don't exist
        grammar_paths = [p for p in grammar_paths if Path(p).exists()]

        if not grammar_paths:
            logger.error("No valid grammar paths found")
            return False

        # Try to build with all grammars first
        logger.info(f"Compiling {len(grammar_paths)} grammars into {LIBRARY_PATH}")

        try:
            Language.build_library(str(LIBRARY_PATH), grammar_paths)
            logger.info("✓ Successfully built language library")

            # Verify the library was created and has reasonable size
            if LIBRARY_PATH.exists():
                size_mb = LIBRARY_PATH.stat().st_size / (1024 * 1024)
                logger.info(f"Library size: {size_mb:.1f} MB")
                return True
            else:
                logger.error("Library file was not created")
                return False

        except Exception as e:
            logger.error(f"Failed to build library: {e}")

            # If it's a C++ compilation error and Vue is in the mix, try without Vue
            if "scanner.cc" in str(e) and any("vue" in p for p in grammar_paths):
                logger.warning("Retrying build without Vue grammar due to C++ compilation issue...")

                # Remove Vue from successful languages and grammar paths
                if 'vue' in successful_languages:
                    successful_languages.remove('vue')
                    self.skipped_languages.append('vue')

                grammar_paths = [p for p in grammar_paths if "vue" not in p]

                if grammar_paths:
                    try:
                        Language.build_library(str(LIBRARY_PATH), grammar_paths)
                        logger.info("✓ Successfully built language library (without Vue)")

                        if LIBRARY_PATH.exists():
                            size_mb = LIBRARY_PATH.stat().st_size / (1024 * 1024)
                            logger.info(f"Library size: {size_mb:.1f} MB")
                            return True
                    except Exception as e2:
                        logger.error(f"Second build attempt failed: {e2}")
                        return False

            logger.error("This usually indicates a missing C/C++ compiler or development headers")
            return False

    def generate_language_mapping(self, successful_languages: List[str]) -> None:
        """Generate a language mapping file for the application."""
        mapping_file = BUILD_DIR / "language_mapping.json"

        mapping = {}
        for lang in successful_languages:
            config = LANGUAGE_GRAMMARS[lang]
            for ext in config.get('extensions', []):
                mapping[ext] = lang

        try:
            with open(mapping_file, 'w') as f:
                json.dump(mapping, f, indent=2, sort_keys=True)
            logger.info(f"Generated language mapping: {mapping_file}")
        except Exception as e:
            logger.warning(f"Could not generate language mapping: {e}")

    def print_summary(self, successful_languages: List[str]) -> None:
        """Print build summary."""
        logger.info("=" * 60)
        logger.info("BUILD SUMMARY")
        logger.info("=" * 60)

        if successful_languages:
            logger.info(f"✓ Successfully built {len(successful_languages)} languages:")
            for lang in sorted(successful_languages):
                extensions = LANGUAGE_GRAMMARS[lang].get('extensions', [])
                logger.info(f"  {lang:<12} -> {', '.join(extensions)}")

        if self.skipped_languages:
            logger.warning(f"⚠ Skipped {len(self.skipped_languages)} languages (generation failed):")
            for lang in sorted(self.skipped_languages):
                logger.warning(f"  {lang}")

        if self.failed_languages:
            logger.warning(f"✗ Failed to build {len(self.failed_languages)} languages:")
            for lang in sorted(self.failed_languages):
                logger.warning(f"  {lang}")

        logger.info("=" * 60)
        logger.info("Ready to start your server with: ./run_server.sh")

    def build(self) -> bool:
        """Main build process."""
        if not self.check_prerequisites():
            return False

        languages = self.get_languages_to_build()
        if not languages:
            logger.error("No languages selected for building")
            return False

        successful_languages = self.clone_grammars_parallel(languages)

        if not successful_languages:
            logger.error("No languages were successfully downloaded")
            return False

        if not self.build_library(successful_languages):
            return False

        self.generate_language_mapping(successful_languages)
        self.print_summary(successful_languages)

        return True

def main():
    """Main entry point with CLI argument support."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Build Tree-sitter parsers for Lumiere Core"
    )
    parser.add_argument(
        '--languages',
        nargs='+',
        help='Specific languages to build (default: all)'
    )
    parser.add_argument(
        '--priority',
        choices=['high', 'medium', 'low'],
        help='Build only languages with specific priority'
    )
    parser.add_argument(
        '--list-languages',
        action='store_true',
        help='List all available languages and exit'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Clean existing grammars and build directories'
    )
    parser.add_argument(
        '--skip-problematic',
        action='store_true',
        help='Skip languages that commonly fail (PHP, Haskell, etc.)'
    )

    args = parser.parse_args()

    if args.list_languages:
        print("Available languages:")
        for name, config in sorted(LANGUAGE_GRAMMARS.items()):
            priority = config.get('priority', 'medium')
            extensions = ', '.join(config.get('extensions', []))
            needs_gen = " [requires generation]" if config.get('requires_generate') or name in REQUIRES_GENERATION else ""
            print(f"  {name:<12} [{priority:<6}] -> {extensions}{needs_gen}")
        return

    if args.clean:
        logger.info("Cleaning existing directories...")
        for path in [GRAMMARS_DIR, BUILD_DIR]:
            if path.exists():
                shutil.rmtree(path)
                logger.info(f"Removed {path}")

    # Handle skip-problematic flag
    languages_to_skip = set()
    if args.skip_problematic:
        languages_to_skip = REQUIRES_GENERATION.copy()
        logger.info(f"Skipping problematic languages: {', '.join(sorted(languages_to_skip))}")

    # Filter out skipped languages
    if languages_to_skip and not args.languages:
        for lang in languages_to_skip:
            if lang in LANGUAGE_GRAMMARS:
                del LANGUAGE_GRAMMARS[lang]

    builder = ParserBuilder(
        languages_subset=args.languages,
        priority_filter=args.priority
    )

    success = builder.build()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

--- FILE_END: backend/build_parsers.py ---

--- FILE_START: backend/requirements.txt ---
# In backend/requirements.txt

# Django Core
Django
djangorestframework

# LLM & Vector Database
ollama
faiss-cpu
numpy
tqdm
networkx

# Web Scraping (Legacy, but keep for now)
requests
beautifulsoup4

# GitHub API Client
PyGithub

# --- loading .env files ---
python-dotenv

# --- The Conductor CLI ---
typer[all]
rich
prompt-toolkit

# --- The Crucible service ---
docker

# --- Google Gemini API ---
google-generativeai

#  --- The Polyglot Cartographer ---
tree_sitter==0.20.1

--- FILE_END: backend/requirements.txt ---

--- FILE_START: backend/lumiere_core/asgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/asgi.py
"""
ASGI config for backend project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_asgi_application()

--- FILE_END: backend/lumiere_core/asgi.py ---

--- FILE_START: backend/lumiere_core/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/__init__.py

--- FILE_END: backend/lumiere_core/__init__.py ---

--- FILE_START: backend/lumiere_core/.gitignore ---
# In ~/lumiere_semantique/backend/lumiere_core/.gitignore
# Python
__pycache__/
*.pyc

# Virtual Environment
venv/

# Django
db.sqlite3
*.log

# Environment variables
.env

--- FILE_END: backend/lumiere_core/.gitignore ---

--- FILE_START: backend/lumiere_core/.env ---
ANTHROPIC_API_KEY="sk-ant-your-api-key-here"

GITHUB_ACCESS_TOKEN="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"

# The GitHub username the agent will use to fork repos and create PRs.
GITHUB_FORK_USERNAME="LatchyCat"

--- FILE_END: backend/lumiere_core/.env ---

--- FILE_START: backend/lumiere_core/settings.py ---
# In ~/lumiere_semantique/backend/lumiere_core/settings.py
"""
Django settings for lumiere_core project.

Generated by 'django-admin startproject' using Django 5.2.3.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.2/ref/settings/
"""

from pathlib import Path
from dotenv import load_dotenv

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent

load_dotenv(BASE_DIR / '.env')

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = "django-insecure-7f#&l-vg3lb9%s5lkx!352hf2^&!w%ro6wa97*kqm@8+d94*67"

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",

    # --- Third-party apps ---
    'rest_framework',

    # --- Our local apps ---
    'ingestion',
    'api',
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
]

# This should already be correct from our previous fixes.
ROOT_URLCONF = 'lumiere_core.urls'

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

# This should also be correct, but ensure it points to 'lumiere_core'.
WSGI_APPLICATION = "lumiere_core.wsgi.application"


# Database
# https://docs.djangoproject.com/en/5.2/ref/settings/#databases

DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.sqlite3",
        "NAME": BASE_DIR / "db.sqlite3",
    }
}


# Password validation
# https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        "NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",
    },
]


# Internationalization
# https://docs.djangoproject.com/en/5.2/topics/i18n/

LANGUAGE_CODE = "en-us"

TIME_ZONE = "UTC"

USE_I18N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/5.2/howto/static-files/

STATIC_URL = "static/"

# Default primary key field type
# https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"

# --- Add this section for Django REST Framework ---
# This allows DRF to have sensible default settings.
REST_FRAMEWORK = {
    'DEFAULT_RENDERER_CLASSES': [
        'rest_framework.renderers.JSONRenderer',
    ],
    # Use BrowsableAPIRenderer only during development for easier debugging
    'DEFAULT_PARSER_CLASSES': [
        'rest_framework.parsers.JSONParser',
    ]
}

--- FILE_END: backend/lumiere_core/settings.py ---

--- FILE_START: backend/lumiere_core/urls.py ---
# In ~/lumiere_semantique/backend/lumiere_core/urls.py
# In lumiere_core/urls.py

from django.contrib import admin
from django.urls import path, include # <-- Make sure 'include' is imported

urlpatterns = [
    path('admin/', admin.site.urls),

    # This line tells Django that any URL starting with 'api/v1/'
    # should be handled by the URL patterns defined in our 'api.urls' file.
    path('api/v1/', include('api.urls')),
]

--- FILE_END: backend/lumiere_core/urls.py ---

--- FILE_START: backend/lumiere_core/services/diff_parser.py ---
# backend/lumiere_core/services/diff_parser.py

import re
import logging
from typing import Dict, List, Set, Tuple, Optional, Union
from dataclasses import dataclass
from pathlib import Path

# Set up logging
logger = logging.getLogger(__name__)

@dataclass
class DiffStats:
    """Statistics about a parsed diff."""
    total_files_changed: int
    total_lines_added: int
    total_lines_removed: int
    affected_nodes: int
    files_with_unknown_nodes: List[str]

@dataclass
class FileChange:
    """Represents changes to a single file."""
    file_path: str
    lines_added: int
    lines_removed: int
    changed_lines: Set[int]
    is_new_file: bool = False
    is_deleted_file: bool = False

class DiffParseError(Exception):
    """Custom exception for diff parsing errors."""
    pass

def get_changed_files_from_diff(diff_text: str) -> List[str]:
    """
    Extracts a list of file paths that were changed in the diff.

    Args:
        diff_text: The full text of a git diff

    Returns:
        Sorted list of unique file paths that were modified

    Raises:
        DiffParseError: If the diff text is malformed or empty
    """
    if not diff_text or not diff_text.strip():
        logger.warning("Empty or whitespace-only diff text provided")
        return []

    try:
        # Enhanced pattern to handle various diff formats
        pattern = re.compile(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', re.MULTILINE)
        matches = pattern.findall(diff_text)

        # Filter out /dev/null (used for new/deleted files) and empty matches
        file_paths = [
            match.strip() for match in matches
            if match.strip() and match.strip() != '/dev/null'
        ]

        if not file_paths:
            logger.info("No file changes detected in diff")

        return sorted(list(set(file_paths)))

    except re.error as e:
        raise DiffParseError(f"Regex compilation error: {e}") from e
    except Exception as e:
        raise DiffParseError(f"Unexpected error parsing diff: {e}") from e

def get_changed_files_with_stats(diff_text: str) -> List[FileChange]:
    """
    Enhanced version that returns detailed information about each changed file.

    Args:
        diff_text: The full text of a git diff

    Returns:
        List of FileChange objects with detailed statistics
    """
    if not diff_text or not diff_text.strip():
        return []

    file_changes = {}

    # Split diff by file sections
    file_sections = re.split(r'^diff --git', diff_text, flags=re.MULTILINE)

    for section in file_sections:
        if not section.strip():
            continue

        # Extract file path
        file_match = re.search(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', section, re.MULTILINE)
        if not file_match:
            continue

        file_path = file_match.group(1).strip()
        if file_path == '/dev/null':
            continue

        # Check if it's a new or deleted file
        is_new_file = '/dev/null' in re.search(r'^---\s(.+?)(?:\s|$)', section, re.MULTILINE).group(1) if re.search(r'^---\s(.+?)(?:\s|$)', section, re.MULTILINE) else False
        is_deleted_file = file_path == '/dev/null' or '--- a/' + file_path in section and '+++ /dev/null' in section

        # Count line changes and extract changed line numbers
        lines_added = len(re.findall(r'^\+(?!\+)', section, re.MULTILINE))
        lines_removed = len(re.findall(r'^-(?!-)', section, re.MULTILINE))

        # Extract changed line numbers
        changed_lines = set()
        hunk_headers = re.findall(r'^@@\s-\d+(?:,\d+)?\s\+(\d+)(?:,(\d+))?\s@@', section, re.MULTILINE)

        for start_line_str, length_str in hunk_headers:
            start_line = int(start_line_str)
            length = int(length_str) if length_str else 1
            changed_lines.update(range(start_line, start_line + length))

        file_changes[file_path] = FileChange(
            file_path=file_path,
            lines_added=lines_added,
            lines_removed=lines_removed,
            changed_lines=changed_lines,
            is_new_file=is_new_file,
            is_deleted_file=is_deleted_file
        )

    return sorted(file_changes.values(), key=lambda x: x.file_path)

def parse_diff_to_nodes(diff_text: str, file_to_node_map: Dict[str, List[Dict]]) -> List[str]:
    """
    Parses a git diff and, using a map of files to their nodes (functions, classes),
    identifies which specific nodes were modified.

    Args:
        diff_text: The full text of a `git diff`.
        file_to_node_map: A dictionary mapping file paths to a list of their contained
                          nodes, where each node has a 'name', 'start_line', and 'end_line'.
                          e.g., {'src/main.py': [{'id': '...', 'start_line': 10, 'end_line': 25}]}

    Returns:
        A list of unique node IDs that were affected by the changes in the diff.

    Raises:
        DiffParseError: If the diff text is malformed
        ValueError: If file_to_node_map has invalid structure
    """
    if not diff_text or not diff_text.strip():
        logger.warning("Empty diff text provided to parse_diff_to_nodes")
        return []

    if not isinstance(file_to_node_map, dict):
        raise ValueError("file_to_node_map must be a dictionary")

    # Validate node map structure
    _validate_node_map(file_to_node_map)

    affected_node_ids: Set[str] = set()

    try:
        # Split the diff by file sections - more robust approach
        file_sections = re.split(r'^diff --git', diff_text, flags=re.MULTILINE)

        for section in file_sections:
            if not section.strip():
                continue

            # Extract file path with better error handling
            file_path_match = re.search(r'^\+\+\+\s(?:b/)?(.+?)(?:\s|$)', section, re.MULTILINE)
            if not file_path_match:
                continue

            current_file = file_path_match.group(1).strip()

            # Skip /dev/null (new/deleted files)
            if current_file == '/dev/null':
                continue

            # If we don't have a map for this file, log it and continue
            if current_file not in file_to_node_map:
                logger.debug(f"No node mapping found for file: {current_file}")
                continue

            # Extract changed lines more efficiently
            changed_lines = _extract_changed_lines(section)

            if not changed_lines:
                continue

            # Find affected nodes
            affected_nodes = _find_affected_nodes(
                file_to_node_map[current_file],
                changed_lines
            )
            affected_node_ids.update(affected_nodes)

        result = sorted(list(affected_node_ids))
        logger.info(f"Found {len(result)} affected nodes from diff")
        return result

    except re.error as e:
        raise DiffParseError(f"Regex error while parsing diff: {e}") from e
    except Exception as e:
        raise DiffParseError(f"Unexpected error parsing diff to nodes: {e}") from e

def parse_diff_to_nodes_with_stats(
    diff_text: str,
    file_to_node_map: Dict[str, List[Dict]]
) -> Tuple[List[str], DiffStats]:
    """
    Enhanced version that returns both affected nodes and detailed statistics.

    Args:
        diff_text: The full text of a git diff
        file_to_node_map: Dictionary mapping file paths to their nodes

    Returns:
        Tuple of (affected_node_ids, diff_stats)
    """
    if not diff_text or not diff_text.strip():
        return [], DiffStats(0, 0, 0, 0, [])

    file_changes = get_changed_files_with_stats(diff_text)
    affected_node_ids = parse_diff_to_nodes(diff_text, file_to_node_map)

    files_with_unknown_nodes = [
        fc.file_path for fc in file_changes
        if fc.file_path not in file_to_node_map
    ]

    total_lines_added = sum(fc.lines_added for fc in file_changes)
    total_lines_removed = sum(fc.lines_removed for fc in file_changes)

    stats = DiffStats(
        total_files_changed=len(file_changes),
        total_lines_added=total_lines_added,
        total_lines_removed=total_lines_removed,
        affected_nodes=len(affected_node_ids),
        files_with_unknown_nodes=files_with_unknown_nodes
    )

    return affected_node_ids, stats

def filter_nodes_by_change_type(
    diff_text: str,
    file_to_node_map: Dict[str, List[Dict]],
    change_types: Set[str] = {'added', 'modified', 'deleted'}
) -> Dict[str, List[str]]:
    """
    Categorizes affected nodes by the type of change.

    Args:
        diff_text: The full text of a git diff
        file_to_node_map: Dictionary mapping file paths to their nodes
        change_types: Set of change types to include ('added', 'modified', 'deleted')

    Returns:
        Dictionary mapping change types to lists of affected node IDs
    """
    result = {change_type: [] for change_type in change_types}

    if not diff_text or not diff_text.strip():
        return result

    file_changes = get_changed_files_with_stats(diff_text)

    for file_change in file_changes:
        if file_change.file_path not in file_to_node_map:
            continue

        nodes = file_to_node_map[file_change.file_path]

        if file_change.is_new_file and 'added' in change_types:
            # All nodes in new files are considered added
            result['added'].extend(node['id'] for node in nodes)
        elif file_change.is_deleted_file and 'deleted' in change_types:
            # All nodes in deleted files are considered deleted
            result['deleted'].extend(node['id'] for node in nodes)
        elif 'modified' in change_types:
            # Find nodes that intersect with changed lines
            affected_nodes = _find_affected_nodes(nodes, file_change.changed_lines)
            result['modified'].extend(affected_nodes)

    # Remove duplicates and sort
    for change_type in result:
        result[change_type] = sorted(list(set(result[change_type])))

    return result

def _validate_node_map(file_to_node_map: Dict[str, List[Dict]]) -> None:
    """Validates the structure of the file_to_node_map."""
    for file_path, nodes in file_to_node_map.items():
        if not isinstance(nodes, list):
            raise ValueError(f"Nodes for file {file_path} must be a list")

        for i, node in enumerate(nodes):
            if not isinstance(node, dict):
                raise ValueError(f"Node {i} in file {file_path} must be a dictionary")

            required_keys = {'id', 'start_line', 'end_line'}
            if not required_keys.issubset(node.keys()):
                missing = required_keys - node.keys()
                raise ValueError(f"Node {i} in file {file_path} missing keys: {missing}")

            if not isinstance(node['start_line'], int) or not isinstance(node['end_line'], int):
                raise ValueError(f"Node {i} in file {file_path} line numbers must be integers")

            if node['start_line'] > node['end_line']:
                raise ValueError(f"Node {i} in file {file_path} has start_line > end_line")

def _extract_changed_lines(file_diff_section: str) -> Set[int]:
    """Extracts the set of changed line numbers from a file's diff section."""
    changed_lines: Set[int] = set()

    # Find all hunk headers like '@@ -15,7 +15,9 @@'
    hunk_headers = re.findall(
        r'^@@\s-\d+(?:,\d+)?\s\+(\d+)(?:,(\d+))?\s@@',
        file_diff_section,
        re.MULTILINE
    )

    for start_line_str, length_str in hunk_headers:
        start_line = int(start_line_str)
        # Length defaults to 1 if not specified
        length = int(length_str) if length_str else 1

        # Add all lines in this hunk to the changed lines set
        changed_lines.update(range(start_line, start_line + length))

    return changed_lines

def _find_affected_nodes(nodes: List[Dict], changed_lines: Set[int]) -> List[str]:
    """Finds nodes that are affected by the given changed lines."""
    affected_node_ids = []

    for node in nodes:
        node_line_range = range(node['start_line'], node['end_line'] + 1)

        # Check if any changed line falls within this node's range
        if any(line in node_line_range for line in changed_lines):
            affected_node_ids.append(node['id'])

    return affected_node_ids

# Backward compatibility aliases
def get_files_from_diff(diff_text: str) -> List[str]:
    """Deprecated: Use get_changed_files_from_diff instead."""
    logger.warning("get_files_from_diff is deprecated, use get_changed_files_from_diff")
    return get_changed_files_from_diff(diff_text)

# Utility functions for common use cases
def is_valid_diff(diff_text: str) -> bool:
    """
    Checks if the provided text appears to be a valid git diff.

    Args:
        diff_text: Text to validate

    Returns:
        True if the text appears to be a valid diff, False otherwise
    """
    if not diff_text or not diff_text.strip():
        return False

    # Look for common diff indicators
    diff_indicators = [
        r'^diff --git',
        r'^---\s',
        r'^\+\+\+\s',
        r'^@@.*@@'
    ]

    return any(re.search(pattern, diff_text, re.MULTILINE) for pattern in diff_indicators)

def get_diff_summary(diff_text: str) -> str:
    """
    Returns a human-readable summary of the diff.

    Args:
        diff_text: The full text of a git diff

    Returns:
        A summary string describing the changes
    """
    if not is_valid_diff(diff_text):
        return "Invalid or empty diff"

    file_changes = get_changed_files_with_stats(diff_text)

    if not file_changes:
        return "No file changes detected"

    total_files = len(file_changes)
    total_added = sum(fc.lines_added for fc in file_changes)
    total_removed = sum(fc.lines_removed for fc in file_changes)
    new_files = sum(1 for fc in file_changes if fc.is_new_file)
    deleted_files = sum(1 for fc in file_changes if fc.is_deleted_file)

    parts = [f"{total_files} file{'s' if total_files != 1 else ''} changed"]

    if total_added > 0:
        parts.append(f"{total_added} insertion{'s' if total_added != 1 else ''}")

    if total_removed > 0:
        parts.append(f"{total_removed} deletion{'s' if total_removed != 1 else ''}")

    if new_files > 0:
        parts.append(f"{new_files} new file{'s' if new_files != 1 else ''}")

    if deleted_files > 0:
        parts.append(f"{deleted_files} deleted file{'s' if deleted_files != 1 else ''}")

    return ", ".join(parts)

--- FILE_END: backend/lumiere_core/services/diff_parser.py ---

--- FILE_START: backend/lumiere_core/services/documentation.py ---
# backend/lumiere_core/services/documentation.py
from typing import Dict
# --- All services should import the master llm_service ---
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output


def generate_docstring_for_code(repo_id: str, new_code: str, instruction: str, model_identifier: str) -> Dict[str, str]:
    """
    The core logic for the Chronicler Agent (Documentation).
    It finds existing docstring patterns in the repo and uses them as a style guide
    to generate a new docstring for the provided code.

    Args:
        repo_id: Identifier for the repository
        new_code: The code that needs documentation
        instruction: Instructions for docstring generation
        model_identifier: The model to use for generation

    Returns:
        Dict containing the generated docstring
    """
    print(f"✒️  Initiating Chronicler Agent for repo '{repo_id}'")

    # Step 1: Find Existing Documentation Patterns with RAG
    print("   -> Step 1: Finding existing docstring patterns with RAG...")
    search_query = f"Example docstrings in Python code for a function about: {instruction}"

    try:
        # --- CORRECTED CALL: Pass repo_id directly ---
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=5
        )
    except Exception as e:
        print(f"   -> Warning: RAG search failed for Chronicler: {e}. Proceeding without context examples.")
        context_chunks = []

    doc_context_string = ""
    found_files = set()

    for chunk in context_chunks:
        text = chunk.get('text', '').strip()
        file_path = chunk.get('file_path', '')
        if text.startswith(('def ', 'class ')) and file_path not in found_files:
            doc_context_string += f"--- Example from file \"{file_path}\" ---\n{text}\n\n"
            found_files.add(file_path)

    if not doc_context_string:
        doc_context_string = "No specific docstring styles found. Please generate a standard Google-style docstring."
        print("   -> Warning: No existing docstring examples found via RAG.")
    else:
        print(f"   -> Found docstring patterns from files: {list(found_files)}")

    # Step 2: Construct the Docstring Generation Prompt
    print("   -> Step 2: Constructing docstring generation prompt...")
    prompt = f"""You are an expert technical writer specializing in Python documentation.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING DOCSTRING EXAMPLES"** to learn the project's documentation style (e.g., Google, reStructuredText, numpy). Pay attention to sections like `Args:`, `Returns:`, `Raises:`.
2. **Analyze the "CODE TO BE DOCUMENTED"** to understand its parameters, logic, and what it returns.
3. **Write a complete and professional docstring** for the provided code. It is CRITICAL that you exactly match the style of the examples.
4. **Output ONLY the docstring itself.** Do not include the function definition or any other text, just the \"\"\"...\"\"\" block.

---
### EXISTING DOCSTRING EXAMPLES
{doc_context_string}

---
### CODE TO BE DOCUMENTED
```python
{new_code}
```

Now, generate ONLY the docstring for the code above."""

    # Step 3: Generate and Clean the Docstring
    print(f"   -> Step 3: Sending request to model '{model_identifier}'...")
    raw_docstring = llm_service.generate_text(prompt, model_identifier=model_identifier)

    # Step 4: Clean the docstring output
    print("   -> Step 4: Cleaning and finalizing the docstring...")
    final_docstring = clean_llm_code_output(raw_docstring)

    # Remove surrounding triple quotes if present
    if final_docstring.startswith('"""') and final_docstring.endswith('"""'):
        final_docstring = final_docstring[3:-3].strip()

    return {"docstring": final_docstring}

--- FILE_END: backend/lumiere_core/services/documentation.py ---

--- FILE_START: backend/lumiere_core/services/review_service.py ---
# backend/lumiere_core/services/review_service.py

import logging
import json
from pathlib import Path

from . import (
    github, llm_service, graph_differ, oracle_service, cartographer,
    diff_parser, scaffolding
)
from .llm_service import TaskType
from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier

logger = logging.getLogger(__name__)

def _generate_graph_for_ref(crawler: IntelligentCrawler) -> dict:
    """Helper function to generate a full architectural graph for the current state of the crawler."""
    files_to_process = crawler.get_file_paths()
    if not files_to_process:
        return {}
    jsonifier = Jsonifier(
        file_paths=files_to_process,
        repo_root=crawler.repo_path,
        repo_id="dynamic_analysis"
    )
    project_cortex = jsonifier.generate_cortex()
    return project_cortex.get("architectural_graph", {})


def inquire_pr(pr_url: str) -> dict:
    """
    Main orchestration logic for "The Inquisitor" dynamic code review agent.
    The model_identifier is no longer needed.
    """
    logger.info(f"--- INQUISITOR AGENT ACTIVATED for PR: {pr_url} ---")

    # 1. On-Demand Workspace Creation & PR Detail Fetching
    try:
        pr_details = github.scrape_github_issue(pr_url)
        if not pr_details:
            return {"error": "Failed to scrape PR details."}
        repo_url = pr_details['repo_url']
        parsed_url = github._parse_github_issue_url(pr_url)
        if not parsed_url:
            return {"error": "Could not parse PR URL."}
        owner, repo_name, pr_number = parsed_url
        gh_repo = github.g.get_repo(f"{owner}/{repo_name}")
        pr_obj = gh_repo.get_pull(pr_number)
        base_ref = pr_obj.base.ref
        head_ref = pr_obj.head.ref
        logger.info(f"Analyzing PR #{pr_number}: merging '{head_ref}' into '{base_ref}'")
    except Exception as e:
        logger.error(f"Failed to get PR details from GitHub API: {e}", exc_info=True)
        return {"error": f"Failed to get PR details from GitHub API: {e}"}

    with IntelligentCrawler(repo_url=repo_url) as crawler:
        # 2. Analyze Base Branch
        logger.info(f"[1/3] Checking out base branch '{base_ref}' and generating graph...")
        if not crawler.checkout_ref(base_ref):
            return {"error": f"Could not checkout base branch '{base_ref}'."}
        base_graph_data = _generate_graph_for_ref(crawler)
        base_graph = oracle_service._build_knowledge_graph(base_graph_data)

        # 3. Analyze Head Branch
        head_sha = pr_obj.head.sha
        logger.info(f"[2/3] Checking out head commit '{head_sha}' and generating graph...")

        # --- FIX: Fetch PR commits from origin to handle forks ---
        try:
            # This fetches the specific commits associated with the PR's head.
            # Using the private method is acceptable here to avoid major refactoring.
            crawler._run_git_command(['git', 'fetch', 'origin', f'pull/{pr_number}/head'], check=True)
            logger.info(f"Successfully fetched commits for PR #{pr_number}")
        except Exception as e:
            logger.warning(f"Could not fetch PR head directly: {e}. The commit might already exist locally.")
            pass # We can still try to checkout the sha

        # Now, checkout the specific commit hash, which is more reliable than branch names.
        if not crawler.checkout_ref(head_sha):
            return {"error": f"Could not checkout head commit SHA '{head_sha}'. The branch might have been deleted or force-pushed."}

        head_graph_data = _generate_graph_for_ref(crawler)
        head_graph = oracle_service._build_knowledge_graph(head_graph_data)

        # 4. Perform Architectural Graph Differencing
        logger.info("[3/3] Comparing architectural graphs...")
        architectural_delta = graph_differ.compare_graphs(base_graph, head_graph)
        delta_str = json.dumps(architectural_delta, indent=2)
        # --- FIX: Use the reliable SHA for the diff calculation ---
        text_diff = crawler.get_diff_for_branch(ref_name=head_sha, base_ref=base_ref)
        if "Error from crawler" in text_diff:
            return {"error": f"Failed to get git diff. {text_diff}"}

        # 5. Synthesize the Inquisitive Review
        prompt_parts = [
            "You are The Inquisitor, an AI agent expert in software architecture. Your task is to review a Pull Request by analyzing how it changes the project's structure.",
            f"\n\n**Pull Request Details:**\n- Title: \"{pr_details.get('title', '')}\"\n- Description: \"{pr_details.get('description', '')}\"",
            "\n\n**ARCHITECTURAL DELTA (Summary of Structural Changes):**\nThis report shows what was added to or removed from the codebase's architecture.",
            f"```json\n{delta_str}\n```",
            f"\n\n**CODE CHANGES (Text Diff):**\n```diff\n{text_diff[:4000]}\n```",
            "\n\n---\n\n**YOUR TASK:**\nBased on the **Architectural Delta**, write a code review.",
            "- Focus on the *structural implications* of the changes.",
            "- Highlight any new dependencies, removed functions that might be used elsewhere, or significant changes in how components connect.",
            "- Use the text diff for specific line context.",
            "- Start with a summary of the most important architectural changes.",
            "- Provide clear, constructive feedback."
        ]
        synthesis_prompt = "\n".join(prompt_parts)

        # --- THE CHANGE IS HERE ---
        review_text = llm_service.generate_text(
            synthesis_prompt,
            task_type=TaskType.COMPLEX_REASONING
        )
        logger.info("--- INQUISITOR AGENT MISSION COMPLETE ---")

        return {"review": review_text, "metadata": {"delta": architectural_delta}}

def harmonize_pr_fix(pr_url: str, review_text: str) -> dict:
    """
    Orchestrates the fix-generation process based on a review from the Inquisitor.
    """
    logger.info(f"--- HARMONIZER AGENT ACTIVATED for PR: {pr_url} ---")

    # 1. Get PR details
    try:
        pr_details = github.scrape_github_issue(pr_url)
        if not pr_details:
            return {"error": "Failed to scrape PR details."}
        repo_url = pr_details['repo_url']
        parsed_url = github._parse_github_issue_url(pr_url)
        owner, repo_name, pr_number = parsed_url
        gh_repo = github.g.get_repo(f"{owner}/{repo_name}")
        pr_obj = gh_repo.get_pull(pr_number)
        base_ref = pr_obj.base.ref
        head_ref = pr_obj.head.ref
        repo_id = repo_url.replace("https://github.com/", "").replace("/", "_")
    except Exception as e:
        logger.error(f"Failed to get PR details for Harmonizer: {e}", exc_info=True)
        return {"error": "Failed to get PR details for Harmonizer.", "details": str(e)}

    # 2. Get working copy of the code and diff
    with IntelligentCrawler(repo_url=repo_url) as crawler:
        if not crawler.checkout_ref(head_ref):
            return {"error": f"Could not checkout head branch '{head_ref}' for fixing."}
        all_files_in_pr = {str(p.relative_to(crawler.repo_path)): p.read_text(encoding='utf-8', errors='ignore') for p in crawler.get_file_paths()}
        text_diff = crawler.get_diff_for_branch(ref_name=head_ref, base_ref=base_ref)
        if "Error from crawler" in text_diff:
            return {"error": "Failed to get git diff for Harmonizer.", "details": text_diff}
        target_files = diff_parser.get_changed_files_from_diff(text_diff)
        if not target_files:
            return {"error": "Harmonizer could not determine which files to fix from the PR diff."}
        original_contents_for_fix = {fp: all_files_in_pr.get(fp, "") for fp in target_files}

    # 3. Call the Scaffolder
    logger.info(f"Harmonizer is calling the Scaffolder with {len(target_files)} target files.")

    # Scaffolder no longer needs a model passed in; it will use the Task Router.
    scaffold_result = scaffolding.generate_scaffold(
        repo_id=repo_id,
        target_files=target_files,
        instruction=f"Fix the issues identified in the following code review for PR titled '{pr_details.get('title')}'.",
        rca_report=review_text,
        refinement_history=[]
    )

    if "error" in scaffold_result:
        return scaffold_result

    scaffold_result["original_contents"] = original_contents_for_fix
    logger.info("--- HARMONIZER MISSION COMPLETE ---")
    return scaffold_result

--- FILE_END: backend/lumiere_core/services/review_service.py ---

--- FILE_START: backend/lumiere_core/services/sentinel_service.py ---
# backend/lumiere_core/services/sentinel_service.py

import os
import ast
import logging
import networkx as nx
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)

# --- Constants ---
TEST_KEYWORDS = ("test", "spec")
IMPL_PREFIXES = ("src/", "lib/")
IMPL_SUFFIXES = (".py", ".js", ".rs", ".go")


# --- Metric Calculation Helpers ---

def _calculate_code_to_test_ratio(files_data: List[Dict[str, str]]) -> float:
    """Calculate the ratio of test files to implementation files."""
    test_files = 0
    impl_files = 0

    for file_info in files_data:
        path = file_info.get("file_path", "").lower()
        if any(keyword in path for keyword in TEST_KEYWORDS):
            test_files += 1
        elif path.startswith(IMPL_PREFIXES) or path.endswith(IMPL_SUFFIXES):
            impl_files += 1

    if impl_files == 0:
        return 0.0
    return round(test_files / impl_files, 3)


def _has_docstring(node: ast.AST) -> bool:
    """Check if the first statement of a node is a docstring."""
    if hasattr(node, "body") and node.body:
        first_stmt = node.body[0]
        return isinstance(first_stmt, ast.Expr) and isinstance(getattr(first_stmt, "value", None), ast.Str)
    return False


def _calculate_documentation_coverage(graph: nx.DiGraph) -> float:
    """Calculate the percentage of functions, methods, and classes with a docstring."""
    documented = 0
    total = 0

    for _, data in graph.nodes(data=True):
        if data.get("type") in ("function", "method", "class"):
            total += 1
            raw = data.get("raw_content", "")
            if isinstance(raw, str):
                try:
                    parsed = ast.parse(raw)
                    if parsed.body and _has_docstring(parsed.body[0]):
                        documented += 1
                except (SyntaxError, IndexError, TypeError) as e:
                    logger.debug(f"Docstring parse error: {e}")

    if total == 0:
        return 0.0
    return round((documented / total) * 100, 2)


def _build_graph(graph_data: Dict[str, Any]) -> nx.DiGraph:
    """Builds a NetworkX directed graph from graph_data dictionary."""
    graph = nx.DiGraph()
    nodes = graph_data.get("nodes", {})
    edges = graph_data.get("edges", [])

    graph.add_nodes_from(nodes.keys())
    graph.add_edges_from((e["source"], e["target"]) for e in edges if "source" in e and "target" in e)

    return graph


# --- Main Service Function ---

def calculate_snapshot_metrics(repo_path: Path, graph_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calculates a snapshot of project health metrics at a given point in time.

    Args:
        repo_path: Root path of the repository.
        graph_data: Architectural graph data from the cartographer.

    Returns:
        A dictionary with calculated health metrics.
    """
    logger.info(f"Sentinel: Calculating snapshot metrics for {repo_path}")
    metrics: Dict[str, Any] = {}

    # --- Architectural Metrics ---
    graph = _build_graph(graph_data) if graph_data else nx.DiGraph()

    metrics["total_nodes"] = graph.number_of_nodes()
    metrics["total_edges"] = graph.number_of_edges()
    metrics["coupling_factor"] = (
        round(metrics["total_edges"] / metrics["total_nodes"], 3)
        if metrics["total_nodes"] > 0 else 0
    )
    metrics["average_cyclomatic_complexity"] = 5.0  # Placeholder

    # --- Code Quality Metrics ---
    all_files_info = [
        {"file_path": str(Path(root) / file)}
        for root, _, files in os.walk(repo_path)
        for file in files
    ]

    metrics["code_to_test_ratio"] = _calculate_code_to_test_ratio(all_files_info)
    metrics["documentation_coverage"] = _calculate_documentation_coverage(graph)
    metrics["timestamp"] = datetime.utcnow().isoformat()

    logger.info(f"Sentinel: Metrics calculated: {metrics}")
    return metrics

--- FILE_END: backend/lumiere_core/services/sentinel_service.py ---

--- FILE_START: backend/lumiere_core/services/rca_service.py ---
# backend/lumiere_core/services/rca_service.py

import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum

from . import llm_service, github
from .ollama import search_index

# Configure logging
logger = logging.getLogger(__name__)

class ComplexityLevel(Enum):
    """Enumeration for complexity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"

class FileCategory(Enum):
    """Enumeration for file categories."""
    BACKEND = "backend"
    FRONTEND = "frontend"
    STYLING = "styling"
    CONFIG = "config"
    DOCS = "docs"
    DATABASE = "database"
    INFRASTRUCTURE = "infrastructure"
    BUILD = "build"
    TESTING = "testing"
    OTHER = "other"

@dataclass
class FileRelationships:
    """Data class for file relationship analysis results."""
    total_files: int
    file_types: Dict[str, List[str]]
    type_distribution: Dict[str, int]
    cross_layer_issue: bool
    complexity_indicator: ComplexityLevel
    primary_category: Optional[str] = None
    secondary_categories: List[str] = None

@dataclass
class AnalysisMetadata:
    """Enhanced metadata for analysis results."""
    total_context_chunks: int
    search_time_ms: Optional[float] = None
    analysis_time_ms: Optional[float] = None
    confidence_score: Optional[float] = None
    architectural_context_available: bool = False

def _get_repo_id_from_url(repo_url: str) -> str:
    """
    Helper to derive a filesystem-safe repo_id from a URL.

    Args:
        repo_url: The GitHub repository URL

    Returns:
        Filesystem-safe repository identifier

    Raises:
        ValueError: If the URL format is invalid
    """
    if not repo_url or not isinstance(repo_url, str):
        raise ValueError("Repository URL must be a non-empty string")

    if not repo_url.startswith("https://github.com/"):
        raise ValueError("Repository URL must be a valid GitHub URL")

    try:
        return repo_url.replace("https://github.com/", "").replace("/", "_")
    except Exception as e:
        raise ValueError(f"Failed to parse repository URL: {e}")

def _classify_file_by_extension(file_path: str) -> FileCategory:
    """
    Classify files into broad categories for better analysis context.

    Args:
        file_path: Path to the file

    Returns:
        FileCategory enum value
    """
    if not file_path:
        return FileCategory.OTHER

    # Enhanced extension mapping with more comprehensive coverage
    extension_map = {
        # Backend languages
        '.py': FileCategory.BACKEND, '.rb': FileCategory.BACKEND,
        '.java': FileCategory.BACKEND, '.go': FileCategory.BACKEND,
        '.php': FileCategory.BACKEND, '.cs': FileCategory.BACKEND,
        '.cpp': FileCategory.BACKEND, '.c': FileCategory.BACKEND,
        '.rs': FileCategory.BACKEND, '.kt': FileCategory.BACKEND,

        # Frontend
        '.js': FileCategory.FRONTEND, '.ts': FileCategory.FRONTEND,
        '.jsx': FileCategory.FRONTEND, '.tsx': FileCategory.FRONTEND,
        '.html': FileCategory.FRONTEND, '.htm': FileCategory.FRONTEND,
        '.vue': FileCategory.FRONTEND, '.svelte': FileCategory.FRONTEND,

        # Styling
        '.css': FileCategory.STYLING, '.scss': FileCategory.STYLING,
        '.sass': FileCategory.STYLING, '.less': FileCategory.STYLING,
        '.styl': FileCategory.STYLING,

        # Configuration
        '.json': FileCategory.CONFIG, '.yaml': FileCategory.CONFIG,
        '.yml': FileCategory.CONFIG, '.toml': FileCategory.CONFIG,
        '.ini': FileCategory.CONFIG, '.conf': FileCategory.CONFIG,
        '.env': FileCategory.CONFIG, '.properties': FileCategory.CONFIG,

        # Documentation
        '.md': FileCategory.DOCS, '.rst': FileCategory.DOCS,
        '.txt': FileCategory.DOCS, '.adoc': FileCategory.DOCS,

        # Database
        '.sql': FileCategory.DATABASE, '.graphql': FileCategory.DATABASE,
        '.gql': FileCategory.DATABASE,

        # Infrastructure
        '.tf': FileCategory.INFRASTRUCTURE, '.hcl': FileCategory.INFRASTRUCTURE,
        '.sh': FileCategory.INFRASTRUCTURE, '.bash': FileCategory.INFRASTRUCTURE,
        '.ps1': FileCategory.INFRASTRUCTURE, '.bat': FileCategory.INFRASTRUCTURE,
    }

    # Special filename mappings
    filename_map = {
        'dockerfile': FileCategory.INFRASTRUCTURE,
        'dockerfile.dev': FileCategory.INFRASTRUCTURE,
        'dockerfile.prod': FileCategory.INFRASTRUCTURE,
        'requirements.txt': FileCategory.BUILD,
        'package.json': FileCategory.BUILD,
        'package-lock.json': FileCategory.BUILD,
        'yarn.lock': FileCategory.BUILD,
        'pipfile': FileCategory.BUILD,
        'pipfile.lock': FileCategory.BUILD,
        'gemfile': FileCategory.BUILD,
        'gemfile.lock': FileCategory.BUILD,
        'composer.json': FileCategory.BUILD,
        'composer.lock': FileCategory.BUILD,
        'pom.xml': FileCategory.BUILD,
        'build.gradle': FileCategory.BUILD,
        'cargo.toml': FileCategory.BUILD,
        'cargo.lock': FileCategory.BUILD,
        'makefile': FileCategory.BUILD,
        'cmake.txt': FileCategory.BUILD,
    }

    file_path_lower = file_path.lower()

    # Check for test files first (highest priority)
    test_patterns = ['.test.', '.spec.', '_test.', '/test/', '/tests/', '__test__', '__tests__']
    if any(pattern in file_path_lower for pattern in test_patterns):
        return FileCategory.TESTING

    # Check filename mappings
    filename = Path(file_path_lower).name
    if filename in filename_map:
        return filename_map[filename]

    # Check extension mappings
    suffix = Path(file_path_lower).suffix
    return extension_map.get(suffix, FileCategory.OTHER)

def _determine_complexity_level(file_count: int, type_count: int, cross_layer: bool) -> ComplexityLevel:
    """
    Determine complexity level based on multiple factors.

    Args:
        file_count: Number of files involved
        type_count: Number of different file types
        cross_layer: Whether the issue crosses multiple layers

    Returns:
        ComplexityLevel enum value
    """
    if cross_layer and file_count > 15:
        return ComplexityLevel.VERY_HIGH
    elif file_count > 12 or (cross_layer and type_count > 4):
        return ComplexityLevel.HIGH
    elif file_count > 6 or type_count > 3:
        return ComplexityLevel.MEDIUM
    else:
        return ComplexityLevel.LOW

def _analyze_file_relationships(context_chunks: List[Dict]) -> FileRelationships:
    """
    Analyze relationships between files found in RAG results to provide better context.

    Args:
        context_chunks: List of context chunks from RAG search

    Returns:
        FileRelationships object with comprehensive analysis
    """
    if not context_chunks:
        return FileRelationships(
            total_files=0,
            file_types={},
            type_distribution={},
            cross_layer_issue=False,
            complexity_indicator=ComplexityLevel.LOW,
            primary_category=None,
            secondary_categories=[]
        )

    file_types = defaultdict(list)
    file_paths = set()
    category_counts = defaultdict(int)

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        if file_path:
            file_paths.add(file_path)
            file_category = _classify_file_by_extension(file_path)
            category_str = file_category.value
            file_types[category_str].append(file_path)
            category_counts[category_str] += 1

    # Determine primary and secondary categories
    sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
    primary_category = sorted_categories[0][0] if sorted_categories else None
    secondary_categories = [cat for cat, count in sorted_categories[1:4] if count > 1]

    total_files = len(file_paths)
    type_count = len(file_types)
    cross_layer_issue = type_count > 2
    complexity = _determine_complexity_level(total_files, type_count, cross_layer_issue)

    return FileRelationships(
        total_files=total_files,
        file_types=dict(file_types),
        type_distribution={k: len(set(v)) for k, v in file_types.items()},
        cross_layer_issue=cross_layer_issue,
        complexity_indicator=complexity,
        primary_category=primary_category,
        secondary_categories=secondary_categories
    )

def _load_architectural_context(repo_id: str) -> Tuple[str, bool]:
    """
    Load architectural context from cortex file.

    Args:
        repo_id: Repository identifier

    Returns:
        Tuple of (architectural_context_string, context_available_bool)
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_path.exists():
            logger.debug(f"Cortex file not found at {cortex_path}")
            return "No architectural context available for this analysis.", False

        with open(cortex_path, 'r', encoding='utf-8') as f:
            cortex_data = json.load(f)

        graph_data = cortex_data.get('architectural_graph')
        if not graph_data:
            return "Architectural context file found but contains no graph data.", False

        nodes = graph_data.get('nodes', {})  # Default to empty dict
        edges = graph_data.get('edges', [])

        # Correctly get the first 5 nodes from the dictionary's values
        nodes_list = list(nodes.values())

        context = f"""Architectural graph available with {len(nodes)} components and {len(edges)} connections.
Key components identified: {', '.join([node.get('name', 'Unknown') for node in nodes_list[:5]])}
This provides insights into system architecture and component relationships."""

        logger.info(f"Loaded architectural context for {repo_id}: {len(nodes)} nodes, {len(edges)} edges")
        return context, True

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse cortex JSON for {repo_id}: {e}")
        return "Architectural context file found but contains invalid JSON.", False
    except Exception as e:
        logger.error(f"Failed to load architectural context for {repo_id}: {e}")
        return "Error loading architectural context.", False

def _format_context_for_prompt(context_chunks: List[Dict], relationships: FileRelationships) -> str:
    """
    Format context chunks for LLM prompt with improved organization.

    Args:
        context_chunks: List of context chunks
        relationships: File relationship analysis

    Returns:
        Formatted context string
    """
    if not context_chunks:
        return "No relevant code context found."

    # Group chunks by file category for better organization
    categorized_chunks = defaultdict(list)
    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        category = _classify_file_by_extension(file_path).value
        categorized_chunks[category].append(chunk)

    context_parts = []

    # Present primary category first
    if relationships.primary_category and relationships.primary_category in categorized_chunks:
        chunks = categorized_chunks[relationships.primary_category]
        context_parts.append(f"=== PRIMARY CATEGORY: {relationships.primary_category.upper()} FILES ===")
        for chunk in chunks:
            context_parts.append(
                f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
            )

    # Then secondary categories
    for category in relationships.secondary_categories:
        if category in categorized_chunks:
            chunks = categorized_chunks[category]
            context_parts.append(f"=== SECONDARY CATEGORY: {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    # Finally, remaining categories
    for category, chunks in categorized_chunks.items():
        if category != relationships.primary_category and category not in relationships.secondary_categories:
            context_parts.append(f"=== {category.upper()} FILES ===")
            for chunk in chunks:
                context_parts.append(
                    f"--- Context from `{chunk['file_path']}` ---\n```\n{chunk['text']}\n```\n"
                )

    return "\n\n".join(context_parts)

def generate_briefing(issue_url: str, model_identifier: str) -> Dict[str, Any]:
    """
    Generates a 'Pre-flight Briefing' for a GitHub issue using RAG.

    Args:
        issue_url: GitHub issue URL
        model_identifier: LLM model identifier

    Returns:
        Dictionary containing briefing and metadata
    """
    logger.info(f"Generating briefing for issue: {issue_url}")

    try:
        # Validate inputs
        if not issue_url or not isinstance(issue_url, str):
            return {"error": "Invalid issue URL provided"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        # Scrape GitHub issue
        issue_data = github.scrape_github_issue(issue_url)
        if not issue_data:
            return {"error": "Could not retrieve issue details from GitHub. Please check the URL and try again."}

        repo_id = _get_repo_id_from_url(issue_data['repo_url'])
        query = issue_data['full_text_query']

        if not query:
            return {"error": "No query text found in issue data"}

        # Perform RAG search
        try:
            context_chunks = search_index(
                query_text=query,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=7
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed for {repo_id}: {e}")
            return {"error": f"Failed to retrieve context from vector index: {str(e)}"}

        # Analyze relationships and format context
        relationships = _analyze_file_relationships(context_chunks)
        context_string = _format_context_for_prompt(context_chunks, relationships)

        # Enhanced prompt with better structure
        prompt = f"""You are Lumière Sémantique, an expert AI programming assistant.
Your mission is to provide a comprehensive "Pre-flight Briefing" for a developer about to work on this GitHub issue.

**CODEBASE ANALYSIS SUMMARY:**
- Files involved: {relationships.total_files} files across {len(relationships.file_types)} different categories
- Primary category: {relationships.primary_category or 'Unknown'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

**INSTRUCTIONS:**
Analyze the GitHub issue and the provided codebase context to generate a comprehensive briefing report.

The report must be clear, well-structured, and formatted in Markdown. Include these sections:

1. **🎯 Task Summary**
   - Concise rephrasing of the issue request
   - Key objectives and expected outcomes

2. **🏗️ Codebase Architecture**
   - Relevant system architecture and component relationships
   - How the affected components interact

3. **🔍 Current System Analysis**
   - How the system currently handles the functionality in question
   - Existing patterns and conventions

4. **📁 Key Files & Components**
   - Most important files and functions from the context
   - Their roles and relationships

5. **🚀 Suggested Approach**
   - High-level implementation strategy
   - Recommended order of operations
   - Potential challenges and considerations

6. **⚠️ Important Notes**
   - Dependencies and side effects to consider
   - Testing recommendations

--- CODEBASE CONTEXT ---
{context_string}
--- END CONTEXT ---

**GITHUB ISSUE DETAILS:**
{query}

Generate the comprehensive Pre-flight Briefing now:
"""

        # Generate briefing
        briefing_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare metadata
        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        metadata = {
            "file_relationships": relationships_dict,
            "issue_url": issue_url,
            "repo_id": repo_id,
            "context_chunks_count": len(context_chunks),
            "model_used": model_identifier
        }

        logger.info(f"Successfully generated briefing for {issue_url}")
        return {"briefing": briefing_report, "metadata": metadata}

    except ValueError as e:
        logger.error(f"Validation error in generate_briefing: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in generate_briefing: {e}")
        return {"error": f"An unexpected error occurred: {str(e)}"}

def perform_rca(
    repo_url: str,
    bug_description: str,
    model_identifier: str,
    advanced_analysis: bool = False,
    confidence_threshold: float = 0.7
) -> Dict[str, Any]:
    """
    Performs a multi-file, context-aware Root Cause Analysis using RAG.

    Args:
        repo_url: GitHub repository URL
        bug_description: Description of the bug to analyze
        model_identifier: LLM model identifier
        advanced_analysis: Whether to perform advanced analysis with more context
        confidence_threshold: Minimum confidence threshold for results

    Returns:
        Dictionary containing analysis results and metadata
    """
    logger.info(f"Performing Multi-file RCA for bug: '{bug_description[:100]}...'")

    try:
        # Validate inputs
        if not repo_url or not isinstance(repo_url, str):
            return {"error": "Invalid repository URL provided"}

        if not bug_description or not isinstance(bug_description, str):
            return {"error": "Bug description is required"}

        if not model_identifier:
            return {"error": "Model identifier is required"}

        if not 0.0 <= confidence_threshold <= 1.0:
            return {"error": "Confidence threshold must be between 0.0 and 1.0"}

        repo_id = _get_repo_id_from_url(repo_url)

        # Load architectural context
        architectural_context, arch_available = _load_architectural_context(repo_id)

        # Perform RAG search with enhanced parameters
        try:
            logger.debug("Searching for relevant code chunks...")
            initial_k = 25 if advanced_analysis else 15

            context_chunks = search_index(
                query_text=bug_description,
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=initial_k
            )
        except FileNotFoundError:
            return {
                "error": f"Vector index for repository '{repo_id}' not found. "
                        "Please ensure the repository has been ingested with clone/embed enabled."
            }
        except Exception as e:
            logger.error(f"RAG search failed during RCA for {repo_id}: {e}")
            return {"error": f"RAG search failed during RCA: {str(e)}"}

        if not context_chunks:
            return {
                "analysis": "Could not find any relevant code context for the bug description. "
                           "Unable to perform RCA. Please try rephrasing the bug description or "
                           "ensure the repository has been properly indexed."
            }

        logger.debug("Analyzing file relationships and filtering context...")
        relationships = _analyze_file_relationships(context_chunks)

        # Format context with improved organization
        logger.debug("Synthesizing context from suspect files...")
        formatted_context = _format_context_for_prompt(context_chunks, relationships)

        # Generate complexity guidance
        complexity_guidance = ""
        if relationships.cross_layer_issue:
            complexity_guidance = """
**COMPLEXITY ALERT:** This issue spans multiple system layers. Pay special attention to:
- Interface boundaries and data contracts
- State management across components
- Error propagation paths
- Dependency chains and side effects"""

        if relationships.complexity_indicator in [ComplexityLevel.HIGH, ComplexityLevel.VERY_HIGH]:
            complexity_guidance += """
- Consider breaking down the analysis into smaller, focused areas
- Look for common patterns or shared dependencies
- Pay attention to configuration and environment differences"""

        # Enhanced RCA prompt
        prompt = f"""You are a world-class debugging expert performing a comprehensive Root Cause Analysis (RCA).

**BUG DESCRIPTION:**
{bug_description}

**SYSTEM CONTEXT:**
{architectural_context}

**CODEBASE ANALYSIS:**
- Relevant files: {relationships.total_files} files analyzed
- Primary category: {relationships.primary_category or 'Mixed'}
- File categories: {', '.join(relationships.file_types.keys())}
- Complexity level: {relationships.complexity_indicator.value}
- Cross-layer issue: {'Yes' if relationships.cross_layer_issue else 'No'}

{complexity_guidance}

**ANALYSIS INSTRUCTIONS:**
You must analyze ALL provided context systematically. Use the code evidence to build a comprehensive understanding of the bug's root cause.

Your analysis must follow this structure:

## 🎯 Executive Summary
A clear, single-sentence explanation of the root cause.

## 🏗️ System Overview
Brief explanation of how the affected components are designed to interact and what the expected behavior should be.

## 🔍 Root Cause Analysis
Detailed breakdown of:
- What is happening vs. what should happen
- The specific mechanism causing the failure
- Why this particular scenario triggers the bug

## 📋 Evidence & Reasoning
Cite specific files, functions, and code snippets that support your analysis:
- Direct evidence from the code
- Logical connections between components
- Data flow analysis

## 💥 Impact Assessment
Explain the cascading effects:
- What breaks when this bug occurs
- Which users/systems are affected
- Performance or security implications

## 🛠️ Recommended Fix Strategy
High-level approach including:
- Which specific files need modification
- Order of operations for the fix
- Testing strategy to verify the fix
- Potential risks and mitigation strategies

## ⚠️ Prevention Recommendations
Suggestions to prevent similar issues in the future.

**RELEVANT CODE CONTEXT:**
{formatted_context}

Now generate your comprehensive Root Cause Analysis:
"""

        logger.debug("Generating comprehensive RCA report...")
        analysis_report = llm_service.generate_text(prompt, model_identifier)

        # Prepare enhanced metadata
        metadata = AnalysisMetadata(
            total_context_chunks=len(context_chunks),
            architectural_context_available=arch_available,
            confidence_score=None  # Could be implemented based on context quality
        )

        relationships_dict = relationships.__dict__
        relationships_dict['complexity_indicator'] = relationships.complexity_indicator.value

        result = {
            "analysis": analysis_report,
            "metadata": {
                **relationships_dict,
                **metadata.__dict__,
                "repo_id": repo_id,
                "advanced_analysis": advanced_analysis,
                "confidence_threshold": confidence_threshold
            }
        }

        logger.info(f"Successfully completed RCA for {repo_id}")
        return result

    except ValueError as e:
        logger.error(f"Validation error in perform_rca: {e}")
        return {"error": f"Input validation failed: {str(e)}"}
    except Exception as e:
        logger.error(f"Unexpected error in perform_rca: {e}")
        return {"error": f"An unexpected error occurred during RCA: {str(e)}"}

def get_analysis_health_check(repo_id: str) -> Dict[str, Any]:
    """
    Perform a health check for RCA analysis capabilities.

    Args:
        repo_id: Repository identifier

    Returns:
        Dictionary containing health check results
    """
    try:
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        health_status = {
            "repo_id": repo_id,
            "vector_index_available": False,
            "architectural_context_available": False,
            "cortex_file_exists": cortex_path.exists(),
            "recommendations": []
        }

        # Check vector index (this would need to be implemented based on your vector storage)
        try:
            # Placeholder for vector index check
            test_chunks = search_index(
                query_text="test query",
                model_name='snowflake-arctic-embed2:latest',
                repo_id=repo_id,
                k=1
            )
            health_status["vector_index_available"] = len(test_chunks) > 0
        except FileNotFoundError:
            health_status["recommendations"].append(
                "Vector index not found. Please re-ingest the repository with embedding enabled."
            )
        except Exception as e:
            health_status["recommendations"].append(f"Vector index check failed: {str(e)}")

        # Check architectural context
        if cortex_path.exists():
            try:
                with open(cortex_path, 'r', encoding='utf-8') as f:
                    cortex_data = json.load(f)
                graph_data = cortex_data.get('architectural_graph')
                health_status["architectural_context_available"] = bool(graph_data)

                if not graph_data:
                    health_status["recommendations"].append(
                        "Cortex file exists but contains no architectural graph data."
                    )
            except Exception as e:
                health_status["recommendations"].append(f"Failed to parse cortex file: {str(e)}")
        else:
            health_status["recommendations"].append(
                "No architectural context available. Consider running architectural analysis."
            )

        # Overall health assessment
        if health_status["vector_index_available"] and health_status["architectural_context_available"]:
            health_status["overall_status"] = "excellent"
        elif health_status["vector_index_available"]:
            health_status["overall_status"] = "good"
        else:
            health_status["overall_status"] = "poor"

        return health_status

    except Exception as e:
        logger.error(f"Health check failed for {repo_id}: {e}")
        return {
            "repo_id": repo_id,
            "overall_status": "error",
            "error": str(e)
        }

--- FILE_END: backend/lumiere_core/services/rca_service.py ---

--- FILE_START: backend/lumiere_core/services/suggester_service.py ---
# backend/lumiere_core/services/suggester_service.py

import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

def suggest_next_actions(last_action: str, result_data: Dict = None) -> Dict:
    """
    Generates a list of suggested next actions based on the last command's result.

    Args:
        last_action: The command that was just executed (e.g., "review", "analyze").
        result_data: The JSON data returned by the last command.

    Returns:
        A dictionary containing a list of suggestions and a recommended choice.
    """
    if result_data is None:
        result_data = {}

    suggestions = []
    recommended_choice = None

    if last_action == "review":
        suggestions.append({
            "key": "1",
            "text": "🎵 Harmonize: Attempt an automated fix based on this review.",
            "command": "harmonize"
        })
        suggestions.append({
            "key": "2",
            "text": "🔮 Oracle: Ask a follow-up question about the PR.",
            "command": "ask"
        })
        recommended_choice = "1"

    elif last_action == "analyze":
        suggestions.append({
            "key": "1",
            "text": "🎯 List Issues: View the prioritized list of issues for this repo.",
            "command": "list"
        })
        suggestions.append({
            "key": "2",
            "text": "🗺️ Graph: Visualize the repository's architecture.",
            "command": "graph"
        })
        suggestions.append({
            "key": "3",
            "text": "🔮 Oracle: Ask a high-level question about the codebase.",
            "command": "ask"
        })
        recommended_choice = "1"

    elif last_action == "dashboard":
        # Check if the briefing text suggests negative trends
        briefing_text = result_data.get("briefing", "").lower()
        if "increase" in briefing_text or "jumped" in briefing_text or "dropped" in briefing_text:
            suggestions.append({
                "key": "1",
                "text": "🔍 Re-analyze: Run a fresh analysis to get the latest graph and issues.",
                "command": "analyze"
            })
            recommended_choice = "1"

    # Always add a way to go back
    back_key = str(len(suggestions) + 1)
    suggestions.append({
        "key": back_key,
        "text": "↩️ Main Menu: Return to the main command prompt.",
        "command": "back"
    })

    # If no specific recommendation, recommend going back.
    if not recommended_choice:
        recommended_choice = back_key

    return {
        "suggestions": suggestions,
        "recommended_choice": recommended_choice
    }

--- FILE_END: backend/lumiere_core/services/suggester_service.py ---

--- FILE_START: backend/lumiere_core/services/cartographer.py ---
# backend/lumiere_core/services/cartographer.py

import ast
import logging
import json
from collections import defaultdict
from typing import Dict, Any, List, Optional, Union
from pathlib import Path


# --- Tree-sitter imports ---
try:
    from tree_sitter import Language, Parser, Node
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False
    logging.warning("Tree-sitter not available. JavaScript analysis will be limited.")

# Configure logging
logger = logging.getLogger(__name__)


class TreeSitterConfig:
    """Centralized Tree-sitter configuration and initialization."""

    def __init__(self):
        # --- FIX: Calculate the absolute path to the library ---
        # This makes the path resilient to where the script is run from.
        # It finds the directory of this file (services/) and then navigates to the build directory.
        current_file_dir = Path(__file__).resolve().parent
        self.library_path = current_file_dir / 'build' / 'my-languages.so'

        self.js_language: Optional[Language] = None
        self.js_parser: Optional[Parser] = None
        self.is_ready = False

        if TREE_SITTER_AVAILABLE:
            self._initialize()

    def _initialize(self) -> None:
        """Initialize Tree-sitter parser for JavaScript."""
        try:
            from tree_sitter import Language, Parser, Node
            TREE_SITTER_AVAILABLE = True
        except ImportError:
            TREE_SITTER_AVAILABLE = False
            logging.warning("Tree-sitter not available. JavaScript analysis will be limited.")

        # Configure logging
        logger = logging.getLogger(__name__)


# Global Tree-sitter configuration
ts_config = TreeSitterConfig()


class PythonCartographerVisitor(ast.NodeVisitor):
    """Enhanced AST visitor for Python with better error handling and organization."""

    def __init__(self):
        self.imports: List[Dict[str, Any]] = []
        self.function_calls: List[Dict[str, Any]] = []
        self.class_defs: List[Dict[str, Any]] = []
        self.function_defs: List[Dict[str, Any]] = []
        self.current_context_stack: List[str] = []  # Track nested contexts

    @property
    def current_class(self) -> Optional[str]:
        """Get the current class context."""
        for context in reversed(self.current_context_stack):
            if context.startswith('class:'):
                return context[6:]  # Remove 'class:' prefix
        return None

    def visit_Import(self, node: ast.Import) -> None:
        """Process direct imports."""
        for alias in node.names:
            self.imports.append({
                'type': 'direct',
                'name': alias.name,
                'alias': alias.asname,
                'line': node.lineno
            })
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        """Process from imports."""
        module = node.module or '.'
        for alias in node.names:
            self.imports.append({
                'type': 'from',
                'module': module,
                'name': alias.name,
                'alias': alias.asname,
                'line': node.lineno
            })
        self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        """Process class definitions with context tracking."""
        self.current_context_stack.append(f'class:{node.name}')

        self.class_defs.append({
            'name': node.name,
            'inherits_from': [safe_unparse(base) for base in node.bases],
            'line': node.lineno,
            'decorators': [safe_unparse(dec) for dec in node.decorator_list]
        })

        self.generic_visit(node)
        self.current_context_stack.pop()

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """Process function definitions."""
        self.function_defs.append({
            'name': node.name,
            'class_context': self.current_class,
            'line': node.lineno,
            'args': [arg.arg for arg in node.args.args],
            'decorators': [safe_unparse(dec) for dec in node.decorator_list]
        })
        self.generic_visit(node)

    def visit_Call(self, node: ast.Call) -> None:
        """Process function calls."""
        call_name = safe_unparse(node.func)
        self.function_calls.append({
            'name': call_name,
            'class_context': self.current_class,
            'line': node.lineno
        })
        self.generic_visit(node)


def safe_unparse(node: ast.AST) -> str:
    """Enhanced version of ast.unparse with better error handling."""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        try:
            return f"{safe_unparse(node.value)}.{node.attr}"
        except Exception:
            return f"<complex>.{node.attr}"
    elif isinstance(node, ast.Constant):
        return str(node.value)

    try:
        return ast.unparse(node)
    except Exception as e:
        logger.debug(f"Failed to unparse AST node: {e}")
        return f"<unparseable:{type(node).__name__}>"


class JavaScriptMapper:
    """Handles JavaScript AST mapping with Tree-sitter."""

    QUERY_PATTERNS = {
        "requires": '(call_expression function: (identifier) @func (#eq? @func "require") arguments: (arguments (string (string_fragment) @module)))',
        "imports": '(import_statement source: (string (string_fragment) @module))',
        "functions": '(function_declaration name: (identifier) @name)',
        "arrow_functions": '(variable_declarator id: (identifier) @name value: (arrow_function))',
        "classes": '(class_declaration name: (identifier) @name)',
        "calls": '(call_expression function: [ (identifier) @name (member_expression property: (property_identifier) @name) ] )',
        "exports": '(export_statement declaration: (function_declaration name: (identifier) @name))'
    }

    @staticmethod
    def map_ast(file_path: str, content: str, nodes: defaultdict, edges: List[Dict]) -> None:
        """Maps JavaScript content to graph nodes and edges using Tree-sitter."""
        if not ts_config.is_ready:
            logger.warning(f"Skipping JavaScript analysis for {file_path} - parser not ready")
            return

        try:
            tree = ts_config.js_parser.parse(bytes(content, "utf8"))
            root_node = tree.root_node

            # Initialize file node
            nodes[file_path].update({
                "type": "file",
                "language": "javascript",
                "classes": [],
                "functions": [],
                "exports": []
            })

            JavaScriptMapper._process_queries(file_path, root_node, nodes, edges)

        except Exception as e:
            logger.error(f"Failed to parse JavaScript file {file_path}: {e}")

    @staticmethod
    def _process_queries(file_path: str, root_node: Node, nodes: defaultdict, edges: List[Dict]) -> None:
        """Process Tree-sitter queries for JavaScript analysis."""
        for query_name, pattern in JavaScriptMapper.QUERY_PATTERNS.items():
            try:
                query = ts_config.js_language.query(pattern)
                captures = query.captures(root_node)

                for node, name in captures:
                    text = node.text.decode('utf8')
                    JavaScriptMapper._handle_capture(query_name, name, text, file_path, nodes, edges)

            except Exception as e:
                logger.warning(f"Tree-sitter query '{query_name}' failed for {file_path}: {e}")

    @staticmethod
    def _handle_capture(query_name: str, capture_name: str, text: str,
                       file_path: str, nodes: defaultdict, edges: List[Dict]) -> None:
        """Handle individual Tree-sitter captures."""
        if capture_name == 'module':
            edges.append({"source": file_path, "target": text, "type": "IMPORTS"})
        elif capture_name == 'name':
            if query_name in ['functions', 'arrow_functions']:
                nodes[file_path]['functions'].append(text)
            elif query_name == 'classes':
                nodes[file_path]['classes'].append(text)
            elif query_name == 'calls':
                edges.append({"source": file_path, "target": text, "type": "CALLS"})
            elif query_name == 'exports':
                nodes[file_path]['exports'].append(text)


def map_python_ast(file_path: str, tree: ast.AST, nodes: defaultdict, edges: List[Dict]) -> None:
    """Enhanced Python AST mapping with better organization."""
    try:
        visitor = PythonCartographerVisitor()
        visitor.visit(tree)

        # Initialize file node
        nodes[file_path].update({
            "type": "file",
            "language": "python",
            "classes": [],
            "functions": [],
            "imports": len(visitor.imports)
        })

        # Process classes
        for class_def in visitor.class_defs:
            class_name = class_def['name']
            class_node_id = f"{file_path}::{class_name}"

            nodes[file_path]['classes'].append(class_name)
            nodes[class_node_id].update({
                "type": "class",
                "name": class_name,
                "file": file_path,
                "methods": [],
                "line": class_def['line'],
                "decorators": class_def['decorators']
            })

            # Add inheritance edges
            for base in class_def['inherits_from']:
                if base != '<unparseable:Name>':  # Skip unparseable bases
                    edges.append({"source": class_node_id, "target": base, "type": "INHERITS_FROM"})

        # Process functions and methods
        for func_def in visitor.function_defs:
            func_name = func_def['name']
            if func_def['class_context']:
                parent_class_id = f"{file_path}::{func_def['class_context']}"
                if parent_class_id in nodes:
                    nodes[parent_class_id]['methods'].append(func_name)
            else:
                nodes[file_path]['functions'].append(func_name)

        # Process imports
        for imp in visitor.imports:
            target = imp.get('module', imp['name'])
            edges.append({
                "source": file_path,
                "target": target,
                "type": "IMPORTS",
                "import_type": imp['type']
            })

        # Process function calls
        for call in visitor.function_calls:
            source_id = file_path
            if call['class_context']:
                source_id = f"{file_path}::{call['class_context']}"

            if source_id in nodes:
                edges.append({
                    "source": source_id,
                    "target": call['name'],
                    "type": "CALLS"
                })

    except Exception as e:
        logger.error(f"Failed to map Python AST for {file_path}: {e}")


def generate_graph(analyzed_files: Dict[str, Any]) -> Dict[str, Any]:
    """Enhanced orchestration of multi-language architectural analysis."""
    logger.info("--- ENHANCED POLYGLOT CARTOGRAPHER AGENT ACTIVATED ---")
    logger.info(f"   -> Mapping architecture for {len(analyzed_files)} files...")

    nodes = defaultdict(dict)
    edges = []
    languages_found = set()
    processing_stats = {"success": 0, "errors": 0}

    for file_path, analysis_data in analyzed_files.items():
        try:
            lang = analysis_data.get('language')
            if not lang:
                logger.warning(f"No language specified for {file_path}")
                continue

            languages_found.add(lang)
            logger.debug(f"      - Analyzing: {file_path} ({lang})")

            if lang == 'python' and 'ast' in analysis_data:
                map_python_ast(file_path, analysis_data['ast'], nodes, edges)
                processing_stats["success"] += 1
            elif lang == 'javascript' and 'content' in analysis_data:
                JavaScriptMapper.map_ast(file_path, analysis_data['content'], nodes, edges)
                processing_stats["success"] += 1
            else:
                logger.warning(f"Unsupported language or missing data for {file_path} ({lang})")

        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            processing_stats["errors"] += 1

    # Generate summary
    total_nodes = len(nodes)
    total_edges = len(edges)

    logger.info("✓ Enhanced Polyglot Cartographer mapping complete.")
    logger.info(f"   -> Generated {total_nodes} nodes and {total_edges} edges")
    logger.info(f"   -> Analyzed languages: {', '.join(filter(None, languages_found))}")
    logger.info(f"   -> Processing stats: {processing_stats['success']} successful, {processing_stats['errors']} errors")

    return {
        "nodes": dict(nodes),
        "edges": edges,
        "metadata": {
            "languages": list(languages_found),
            "stats": processing_stats,
            "tree_sitter_available": TREE_SITTER_AVAILABLE,
            "javascript_parser_ready": ts_config.is_ready
        }
    }


# Backward compatibility aliases
_PythonCartographerVisitor = PythonCartographerVisitor
_safe_unparse = safe_unparse
_map_python_ast = map_python_ast
_map_javascript_ast = JavaScriptMapper.map_ast

# Legacy global variables for backward compatibility
LANGUAGE_LIBRARY_PATH = ts_config.library_path
JS_LANGUAGE = ts_config.js_language
js_parser = ts_config.js_parser
JAVASCRIPT_PARSER_READY = ts_config.is_ready

--- FILE_END: backend/lumiere_core/services/cartographer.py ---

--- FILE_START: backend/lumiere_core/services/cortex_service.py ---
# backend/lumiere_core/services/cortex_service.py

import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)

# Constants
CORTEX_FILENAME_TEMPLATE = "{repo_id}_cortex.json"
CLONED_REPOS_DIR = "cloned_repositories"


class CortexFileNotFound(Exception):
    """Raised when the Cortex file is missing for a given repository."""


class CortexFileMalformed(Exception):
    """Raised when the Cortex file is unreadable or not valid JSON."""


def _get_cortex_path(repo_id: str) -> Path:
    """
    Constructs the full path to a repository's Cortex file.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Path object pointing to the expected Cortex file location.
    """
    base_dir = Path(__file__).resolve().parent.parent.parent
    return base_dir / CLONED_REPOS_DIR / repo_id / CORTEX_FILENAME_TEMPLATE.format(repo_id=repo_id)


def load_cortex_data(repo_id: str) -> Dict[str, Any]:
    """
    Loads and parses the cortex JSON file for a given repository.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        Parsed JSON data as a dictionary.

    Raises:
        CortexFileNotFound: If the cortex file does not exist.
        CortexFileMalformed: If the file is not valid JSON.
    """
    cortex_path = _get_cortex_path(repo_id)

    if not cortex_path.exists():
        logger.error(f"Cortex file not found at: {cortex_path}")
        raise CortexFileNotFound(f"Cortex file not found for repo: {repo_id}")

    try:
        with cortex_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.exception(f"Failed to parse cortex file: {cortex_path}")
        raise CortexFileMalformed(f"Failed to load or parse cortex file: {e}") from e


def get_file_content(repo_id: str, file_path: str) -> Optional[str]:
    """
    Retrieves the raw content of a specific file from the repository's Cortex data.

    Args:
        repo_id: The unique ID of the repository.
        file_path: The relative path of the file within the repo.

    Returns:
        The raw file content as a string, or None if the file isn't found.
    """
    try:
        cortex_data = load_cortex_data(repo_id)
        for file_entry in cortex_data.get("files", []):
            if file_entry.get("file_path") == file_path:
                return file_entry.get("raw_content")
    except (CortexFileNotFound, CortexFileMalformed):
        return None

    return None


def get_node_line_map(repo_id: str) -> Dict[str, List[Dict]]:
    """
    Creates a map from file paths to a list of their contained nodes with line numbers.
    This is a critical helper for the Adjudicator's diff parser.

    Args:
        repo_id: The unique ID of the repository.

    Returns:
        A dictionary mapping filenames to lists of node info.
        e.g. {'src/main.py': [{'id': '...', 'start_line': 10, 'end_line': 25}]}
    """
    node_map = {}
    try:
        cortex_data = load_cortex_data(repo_id)
        graph = cortex_data.get("architectural_graph", {})
        nodes = graph.get("nodes", {})

        for node_id, node_data in nodes.items():
            # We are interested in nodes that have line numbers and a file path.
            # This typically means functions, classes, and methods.
            start_line = node_data.get("start_line")
            end_line = node_data.get("end_line")
            # The 'file' attribute is present on class/function/method nodes.
            file_path = node_data.get("file")

            if start_line and end_line and file_path:
                if file_path not in node_map:
                    node_map[file_path] = []

                node_map[file_path].append({
                    "id": node_id,
                    "start_line": start_line,
                    "end_line": end_line,
                })
    except (CortexFileNotFound, CortexFileMalformed) as e:
        logger.error(f"Could not generate node line map for {repo_id}: {e}")
        return {}

    return node_map

--- FILE_END: backend/lumiere_core/services/cortex_service.py ---

--- FILE_START: backend/lumiere_core/services/__init__.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/__init__.py

--- FILE_END: backend/lumiere_core/services/__init__.py ---

--- FILE_START: backend/lumiere_core/services/code_surgery.py ---
# backend/lumiere_core/services/code_surgery.py

import re
import ast
from typing import Dict, Tuple, Optional, List, Set, Any

# ==============================================================================
# SECTION 1: CORE EXECUTOR / ROUTER
# ==============================================================================

def execute_surgical_plan(
    original_contents: Dict[str, str],
    plan: List[Dict[str, Any]]
) -> Tuple[Dict[str, str], Dict[str, Any]]:
    """
    Main entry point for the Code Surgery agent.
    Iterates through a surgical plan and applies the specified operations.
    """
    modified_contents = original_contents.copy()
    report = {
        "operations_attempted": len(plan),
        "operations_succeeded": 0,
        "operations_failed": 0,
        "errors": []
    }

    for op in plan:
        operation_type = op.get("operation")
        file_path = op.get("file_path")

        try:
            if operation_type == "CREATE_FILE":
                modified_contents[file_path] = _handle_create_file(op)

            elif operation_type == "REPLACE_BLOCK":
                current_content = modified_contents.get(file_path, "")
                modified_contents[file_path] = _handle_replace_block(current_content, op)

            elif operation_type == "ADD_FIELD_TO_STRUCT":
                current_content = modified_contents.get(file_path, "")
                modified_contents[file_path] = _handle_add_field_to_struct(current_content, op)

            else:
                raise ValueError(f"Unknown operation type: '{operation_type}'")

            print(f"  ✓ Operation '{operation_type}' on '{file_path}' succeeded.")
            report["operations_succeeded"] += 1

        except Exception as e:
            error_msg = f"Operation '{operation_type}' on '{file_path}' failed: {e}"
            print(f"  ❌ {error_msg}")
            report["errors"].append(error_msg)
            report["operations_failed"] += 1

    return modified_contents, report


# ==============================================================================
# SECTION 2: OPERATION HANDLERS
# ==============================================================================

def _handle_create_file(operation: Dict[str, Any]) -> str:
    """Handles the CREATE_FILE operation. Returns the new file content."""
    return operation.get("content", "")

def _handle_replace_block(original_content: str, operation: Dict[str, Any]) -> str:
    """Handles the REPLACE_BLOCK operation for functions, methods, or classes."""
    target_id = operation.get("target_identifier")
    new_content = operation.get("content", "")
    file_path = operation.get("file_path", "")

    language = 'python' if file_path.endswith('.py') else 'rust' if file_path.endswith('.rs') else 'markdown' if file_path.endswith('.md') else 'javascript'

    boundaries = _find_block_boundaries(original_content, target_id, language)
    if not boundaries:
        raise ValueError(f"Could not find function/block '{target_id}' to replace.")

    start_pos, end_pos = boundaries
    # Ensure a newline after the replacement, unless it's the end of the file
    new_code_with_newline = new_content + "\n" if end_pos < len(original_content) else new_content
    return original_content[:start_pos] + new_code_with_newline + original_content[end_pos:]

def _handle_add_field_to_struct(original_content: str, operation: Dict[str, Any]) -> str:
    """Handles the ADD_FIELD_TO_STRUCT operation, specifically for Rust/C-like languages."""
    target_id = operation.get("target_identifier")
    new_field_line = operation.get("content", "")

    pattern = re.compile(
        rf"(struct\s+{re.escape(target_id)}\s*{{)(.*?)(\}})",
        re.DOTALL | re.MULTILINE
    )
    match = pattern.search(original_content)

    if not match:
        raise ValueError(f"Could not find struct definition for '{target_id}'.")

    struct_header, struct_body, struct_footer = match.groups()
    lines = struct_body.strip().split('\n')
    indentation = "    "
    if lines and lines[-1].strip():
        last_line = lines[-1]
        indentation = " " * (len(last_line) - len(last_line.lstrip()))

    new_body = struct_body.rstrip() + "\n" + indentation + new_field_line.strip() + "\n"
    new_struct_code = struct_header + new_body + struct_footer
    return original_content.replace(match.group(0), new_struct_code)


# ==============================================================================
# SECTION 3: THE FINAL, UPGRADED "FINDER"
# ==============================================================================

def _find_block_boundaries(content: str, block_name: str, language: str) -> Optional[Tuple[int, int]]:
    """
    Find the start and end text positions of a function/method/block in the content.
    --- THIS IS THE ENHANCED VERSION ---
    """
    if language == 'markdown':
        # ... (markdown logic is fine, no changes needed here) ...
        # For brevity, I'm omitting the markdown part. The code is in your file.
        heading_level = block_name.count('#')
        safe_block_name = re.escape(block_name.replace('#', '').strip())
        pattern = re.compile(rf"^(#{'{'}{heading_level}{'}'}\s*{safe_block_name}.*?)$", re.MULTILINE | re.IGNORECASE)
        match = pattern.search(content)

        if not match:
            return None

        start_pos = match.start()
        next_heading_pattern = re.compile(rf"^(#{'{1,'}{heading_level}{'}'}\s+.*)$", re.MULTILINE)
        next_match = next_heading_pattern.search(content, pos=match.end())

        end_pos = next_match.start() if next_match else len(content)
        return (start_pos, end_pos)

    # --- Start of significant changes ---
    if language == 'python':
        try:
            tree = ast.parse(content)
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)) and node.name == block_name:
                    # AST parsing logic remains the same
                    start_pos = -1
                    # This logic correctly finds the start and end using AST, which is reliable.
                    # No changes needed here.
                    if hasattr(node, 'decorator_list') and node.decorator_list:
                        # Find the start of the first decorator
                        start_pos = content.find(ast.get_source_segment(content, node.decorator_list[0]))
                    else:
                        # No decorators, find the start of the node itself
                        start_pos = content.find(ast.get_source_segment(content, node))

                    if start_pos != -1:
                        end_pos = start_pos + len(ast.get_source_segment(content, node))
                        return (start_pos, end_pos)
            return None # Explicitly return None if not found in AST walk
        except (SyntaxError, ValueError):
            # Fallback to regex if AST parsing fails
            pass

    # Generic Regex-based search for Rust, JS, etc.
    # Escape the block name to be safe in regex, but keep ` ` as a flexible spacer
    flexible_name = re.escape(block_name).replace(r'\ ', r'\s+')

    # NEW, more robust regex patterns for Rust `impl` blocks
    patterns = [
        # Rust `impl Trait for Struct` (e.g., `impl Default for ConfigFile`)
        rf"^(?:pub(?:\(.*\))?\s+)?impl\s+{flexible_name}\s*{{",
        # Standard impl block (e.g., `impl ConfigFile`)
        rf"^(?:pub(?:\(.*\))?\s+)?impl(?:<[^>]*>)?\s+{flexible_name}\s*{{",
        # Functions
        rf"^(?:pub(?:\(.*\))?\s+)?(?:async\s+)?fn\s+{flexible_name}\s*\(",
        # Structs
        rf"^(?:pub(?:\(.*\))?\s+)?struct\s+{flexible_name}\s*{{",
        # JS/TS/Python Fallbacks
        rf"^(?:export\s+)?(?:async\s+)?function\s+{flexible_name}\s*\(",
        rf"^\s*@?.*\s*def\s+{flexible_name}\s*\(",
        rf"^\s*class\s+{flexible_name}",
    ]

    for pattern_str in patterns:
        pattern = re.compile(pattern_str, re.MULTILINE)
        match = pattern.search(content)
        if match:
            start_pos = match.start()
            # Find the end of the block by matching braces `{}`
            if '{' in content[start_pos:match.end()]:
                pos = content.find('{', start_pos)
                brace_count = 1
                while pos < len(content) - 1:
                    pos += 1
                    if content[pos] == '{':
                        brace_count += 1
                    elif content[pos] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            return (start_pos, pos + 1)
            # Find the end of Python block by indentation
            elif ':' in content[start_pos:match.end()]:
                lines = content[start_pos:].splitlines()
                if not lines: return None
                base_indent = len(lines[0]) - len(lines[0].lstrip())
                end_line_index = 0
                for i, line in enumerate(lines[1:]):
                    if line.strip() and (len(line) - len(line.lstrip()) <= base_indent):
                        end_line_index = i
                        break
                else:
                    end_line_index = len(lines) -1

                end_pos = start_pos + len("\n".join(lines[:end_line_index + 1]))
                return (start_pos, end_pos)

    return None


def get_relevant_code_from_cortex(content: str, rca_report: str, file_path: str) -> str:
    """
    Extracts relevant code sections from a file's content based on an RCA report.
    """
    if not content or not rca_report:
        return content

    keywords = _extract_keywords_from_rca(rca_report)
    if not keywords:
        return content

    if not file_path.endswith('.py'):
        relevant_lines = []
        for line in content.splitlines():
            if any(kw in line for kw in keywords):
                relevant_lines.append(line)
        return "\n".join(relevant_lines) if relevant_lines else content

    try:
        tree = ast.parse(content)
    except SyntaxError:
        return content

    relevant_nodes = []
    import_nodes = [node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))]

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            if node.name in keywords:
                relevant_nodes.append(node)
                if isinstance(node, ast.ClassDef):
                    for sub_node in node.body:
                        if isinstance(sub_node, (ast.FunctionDef, ast.AsyncFunctionDef)) and sub_node.name in keywords and sub_node not in relevant_nodes:
                            relevant_nodes.append(sub_node)

    if not relevant_nodes:
        return content

    code_parts = [ast.get_source_segment(content, n) for n in import_nodes if n]
    if code_parts and relevant_nodes:
        code_parts.append("\n... # (Code imports)\n")

    added_sources = set()
    for node in sorted(relevant_nodes, key=lambda n: n.lineno):
        source_segment = ast.get_source_segment(content, node)
        if source_segment and source_segment not in added_sources:
            code_parts.append(source_segment)
            added_sources.add(source_segment)

    separator = "\n\n... # (Code omitted for brevity)\n\n"
    final_code = separator.join(part for part in code_parts if part)

    print(f"  🧠 Compressed code for '{file_path}'. Original: {len(content)} chars, Compressed: {len(final_code)} chars.")
    return final_code


def _extract_keywords_from_rca(rca_report: str) -> Set[str]:
    """Extracts potential function, class, and variable names from the RCA report."""
    keywords = set()
    keywords.update(re.findall(r'`([^`]+)`', rca_report))
    keywords.update(re.findall(r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', rca_report))
    keywords.update(re.findall(r'\b([A-Z][a-z]+(?:[A-Z][a-z]+)*|[a-z]+(?:_[a-z]+)+)\b', rca_report))

    cleaned_keywords = set()
    for kw in keywords:
        cleaned = kw.split('.')[-1].split('::')[-1]
        if len(cleaned) > 2:
            cleaned_keywords.add(cleaned)

    return cleaned_keywords

--- FILE_END: backend/lumiere_core/services/code_surgery.py ---

--- FILE_START: backend/lumiere_core/services/ambassador.py ---
# In backend/lumiere_core/services/ambassador.py

import os
import re
import time
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dotenv import load_dotenv

from github import Github, GithubException

from .github import scrape_github_issue, _parse_github_issue_url

from . import llm_service
from .llm_service import TaskType
from ingestion.crawler import IntelligentCrawler

# --- Configuration ---
load_dotenv(dotenv_path=Path(__file__).resolve().parent.parent / '.env')
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
GITHUB_USERNAME = os.getenv("GITHUB_FORK_USERNAME")

if not GITHUB_TOKEN or not GITHUB_USERNAME:
    raise ValueError("GITHUB_ACCESS_TOKEN and GITHUB_FORK_USERNAME must be set in the .env file.")

g = Github(GITHUB_TOKEN)
user = g.get_user(GITHUB_USERNAME)

def _sanitize_branch_name(text: str) -> str:
    """Creates a URL- and git-safe branch name from a string."""
    text = text.lower()
    text = re.sub(r'[\s/]+', '-', text)
    text = re.sub(r'[^a-z0-9-]', '', text)
    text = text.strip('-')
    return text[:60]

def _validate_file_changes(modified_files: Dict[str, str]) -> List[str]:
    """Validates the file changes dictionary and returns any validation errors."""
    errors = []

    if not modified_files:
        errors.append("No files provided for modification")
        return errors

    for file_path, content in modified_files.items():
        if not isinstance(file_path, str) or not file_path.strip():
            errors.append(f"Invalid file path: {repr(file_path)}")

        if not isinstance(content, str):
            errors.append(f"Invalid content type for {file_path}: expected string, got {type(content)}")

        # Check for potentially dangerous paths
        if file_path.startswith('/') or '..' in file_path:
            errors.append(f"Potentially unsafe file path: {file_path}")

    return errors

def _write_files_safely(repo_path: Path, modified_files: Dict[str, str]) -> List[str]:
    """
    Safely writes files to the repository with proper error handling.
    Returns a list of successfully written files.
    """
    successfully_written = []

    for file_path, new_content in modified_files.items():
        try:
            full_target_path = repo_path / file_path

            # Ensure parent directories exist
            full_target_path.parent.mkdir(parents=True, exist_ok=True)

            # Create backup if file exists
            backup_path = None
            if full_target_path.exists():
                backup_path = full_target_path.with_suffix(full_target_path.suffix + '.bak')
                full_target_path.rename(backup_path)

            try:
                # Write new content with explicit encoding
                full_target_path.write_text(new_content, encoding='utf-8')

                # Stage the file
                subprocess.run(
                    ['git', 'add', str(full_target_path)],
                    cwd=repo_path,
                    capture_output=True,
                    text=True,
                    check=True
                )

                successfully_written.append(file_path)
                print(f"✓ Staged changes for: {file_path}")

                # Remove backup if successful
                if backup_path and backup_path.exists():
                    backup_path.unlink()

            except Exception as e:
                # Restore backup if write failed
                if backup_path and backup_path.exists():
                    if full_target_path.exists():
                        full_target_path.unlink()
                    backup_path.rename(full_target_path)
                raise e

        except Exception as e:
            print(f"⚠ Failed to write and stage {file_path}: {e}")
            # Continue with other files rather than failing completely
            continue

    return successfully_written

def _run_git_command(command: List[str], repo_path: Path, description: str) -> bool:
    """
    Runs a git command with proper error handling and logging.
    Returns True if successful, False otherwise.
    """
    try:
        subprocess.run(
            command,
            cwd=repo_path,
            capture_output=True,
            text=True,
            check=True
        )
        print(f"✓ {description}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"✗ {description} failed: {e.stderr.strip()}")
        return False

def _generate_pr_body(issue_data: Dict[str, Any], issue_number: int, modified_files: List[str]) -> str:
    """Generates a comprehensive PR body with file change summary."""
    pr_body = f"""
This pull request was automatically generated and approved by the user via the Lumière Sémantique 'Socratic Dialogue' interface to address Issue #{issue_number}.

## Issue Summary
> {issue_data.get('body', 'No description provided.')[:500]}{'...' if len(issue_data.get('body', '')) > 500 else ''}

## Changes in this PR
This PR modifies **{len(modified_files)}** file(s) to resolve the issue:

"""

    for file_path in sorted(modified_files):
        pr_body += f"- `{file_path}`\n"

    pr_body += "\n---\n*This fix was validated by The Crucible against the project's existing test suite.*"

    return pr_body

def dispatch_pr(
    issue_url: str,
    modified_files: Dict[str, str],
    custom_commit_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Orchestrates the git operations and PR creation for a multi-file change set.
    """
    print("--- AMBASSADOR AGENT ACTIVATED (MULTI-FILE MODE) ---")
    validation_errors = _validate_file_changes(modified_files)
    if validation_errors:
        return {"error": f"Validation failed: {'; '.join(validation_errors)}"}
    print("\n[Step 1/4] Gathering Intel...")
    try:
        issue_data = scrape_github_issue(issue_url)
        if not issue_data: raise ValueError("Failed to scrape issue data.")
        parsed_url = _parse_github_issue_url(issue_url)
        if not parsed_url: raise ValueError("Could not parse issue URL.")
        owner, repo_name, issue_number = parsed_url
        repo_full_name = f"{owner}/{repo_name}"
    except Exception as e:
        return {"error": f"Intel gathering failed: {e}"}
    print("\n[Step 2/4] Preparing Repository...")
    try:
        upstream_repo = g.get_repo(repo_full_name)
        fork_name = f"{GITHUB_USERNAME}/{repo_name}"
        try:
            working_repo = g.get_repo(fork_name)
            print(f"✓ Using existing fork: {fork_name}")
        except GithubException:
            print("Creating fork...")
            working_repo = upstream_repo.create_fork()
            time.sleep(15)
        with IntelligentCrawler(repo_url=working_repo.clone_url) as crawler:
            repo_path = crawler.repo_path
            branch_name = f"lumiere-fix/{issue_number}-{_sanitize_branch_name(issue_data['title'])}"
            default_branch = upstream_repo.default_branch
            print(f"\n[Step 3/4] Applying Fix on new branch '{branch_name}'...")
            if not _run_git_command(['git', 'checkout', default_branch], repo_path, f"Switched to {default_branch}"):
                return {"error": "Failed to checkout default branch"}
            if not _run_git_command(['git', 'pull', upstream_repo.clone_url, default_branch], repo_path, f"Pulled latest from upstream {default_branch}"):
                 return {"error": "Failed to pull latest upstream changes"}
            if not _run_git_command(['git', 'checkout', '-b', branch_name], repo_path, f"Created branch {branch_name}"):
                _run_git_command(['git', 'checkout', branch_name], repo_path, f"Switched to existing branch {branch_name}")
            successfully_written = _write_files_safely(repo_path, modified_files)
            if not successfully_written:
                return {"error": "No files were successfully written"}
            if custom_commit_message:
                commit_message = custom_commit_message
            else:
                commit_prompt = f"Based on the issue title '{issue_data['title']}' and modified files: {', '.join(successfully_written)}, write a concise one-line Conventional Commits message. Output ONLY the message line."

                # --- THE CHANGE IS HERE ---
                commit_message = llm_service.generate_text(
                    commit_prompt,
                    task_type=TaskType.SIMPLE
                ).strip()

            if not _run_git_command(['git', 'commit', '-m', commit_message], repo_path, "Committed changes"):
                return {"error": "Failed to commit changes. Nothing to commit or git error."}
            if not _run_git_command(['git', 'push', '--set-upstream', 'origin', branch_name, '--force'], repo_path, "Pushed branch"):
                return {"error": "Failed to push branch"}
            print("\n[Step 4/4] Creating Pull Request...")
            pr_title = f"fix: {issue_data['title']} (resolves #{issue_number})"
            pr_body = _generate_pr_body(issue_data, issue_number, successfully_written)
            head_branch = f"{working_repo.owner.login}:{branch_name}"
            pull_request = upstream_repo.create_pull(
                title=pr_title, body=pr_body, head=head_branch, base=default_branch
            )
            print(f"✓ Pull Request created: {pull_request.html_url}")
            return {"status": "success", "pull_request_url": pull_request.html_url}
    except Exception as e:
        error_details = str(e)
        if isinstance(e, GithubException): error_details = e.data.get('message', str(e))
        return {"error": f"Operation failed: {error_details}"}

--- FILE_END: backend/lumiere_core/services/ambassador.py ---

--- FILE_START: backend/lumiere_core/services/scaffolding.py ---
# backend/lumiere_core/services/scaffolding.py

import json
import re
import traceback
from pathlib import Path
from typing import Dict, Optional, List, Any, Tuple
from enum import Enum

from . import llm_service
from .utils import clean_llm_code_output
from . import code_surgery

# Enhanced language configuration with comprehensive polyglot support
class TaskType(Enum):
    CODE_GENERATION = "code_generation"
    COMPLEX_REASONING = "complex_reasoning"
    ANALYSIS = "analysis"

POLYGLOT_LANGUAGE_CONFIG = {
    # Web Technologies
    '.js': {
        'name': 'JavaScript',
        'family': 'web',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['dynamic', 'interpreted', 'event-driven'],
        'frameworks': ['React', 'Vue', 'Angular', 'Express', 'Node.js'],
        'common_patterns': ['async/await', 'promises', 'closures', 'prototypes']
    },
    '.ts': {
        'name': 'TypeScript',
        'family': 'web',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'object-oriented'],
        'frameworks': ['Angular', 'React', 'Vue', 'NestJS'],
        'common_patterns': ['interfaces', 'generics', 'decorators', 'modules']
    },
    '.jsx': {
        'name': 'JavaScript (React)',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'jsx',
        'features': ['virtual-dom', 'component-lifecycle', 'hooks'],
        'frameworks': ['React', 'Next.js', 'Gatsby'],
        'common_patterns': ['JSX', 'hooks', 'state management', 'props']
    },
    '.tsx': {
        'name': 'TypeScript (React)',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'jsx',
        'features': ['static-typing', 'virtual-dom', 'component-lifecycle'],
        'frameworks': ['React', 'Next.js'],
        'common_patterns': ['typed props', 'interfaces', 'generic components']
    },
    '.vue': {
        'name': 'Vue.js',
        'family': 'web',
        'paradigm': 'component-based',
        'syntax_style': 'template-based',
        'features': ['reactive', 'template-driven', 'single-file-components'],
        'frameworks': ['Vue', 'Nuxt.js', 'Quasar'],
        'common_patterns': ['template', 'script', 'style', 'composition API']
    },

    # Backend Languages
    '.py': {
        'name': 'Python',
        'family': 'general-purpose',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'indented',
        'features': ['dynamic', 'interpreted', 'duck-typing', 'comprehensive-stdlib'],
        'frameworks': ['Django', 'Flask', 'FastAPI', 'Pyramid'],
        'common_patterns': ['list comprehensions', 'decorators', 'context managers', 'generators']
    },
    '.java': {
        'name': 'Java',
        'family': 'enterprise',
        'paradigm': 'object-oriented',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'garbage-collected', 'platform-independent'],
        'frameworks': ['Spring', 'Spring Boot', 'Hibernate', 'Struts'],
        'common_patterns': ['dependency injection', 'annotations', 'interfaces', 'inheritance']
    },
    '.cs': {
        'name': 'C#',
        'family': 'enterprise',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'garbage-collected', 'linq'],
        'frameworks': ['ASP.NET', '.NET Core', 'Entity Framework', 'Blazor'],
        'common_patterns': ['properties', 'events', 'delegates', 'async/await']
    },
    '.go': {
        'name': 'Go',
        'family': 'systems',
        'paradigm': 'procedural',
        'syntax_style': 'c-like',
        'features': ['static-typing', 'compiled', 'concurrent', 'simple'],
        'frameworks': ['Gin', 'Echo', 'Fiber', 'Buffalo'],
        'common_patterns': ['goroutines', 'channels', 'interfaces', 'composition']
    },
    '.rs': {
        'name': 'Rust',
        'family': 'systems',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['memory-safe', 'zero-cost-abstractions', 'ownership', 'pattern-matching'],
        'frameworks': ['Actix', 'Rocket', 'Warp', 'Axum'],
        'common_patterns': ['ownership', 'borrowing', 'traits', 'match expressions']
    },
    '.swift': {
        'name': 'Swift',
        'family': 'mobile',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'modern',
        'features': ['type-safe', 'memory-safe', 'performant', 'expressive'],
        'frameworks': ['SwiftUI', 'UIKit', 'Vapor', 'Perfect'],
        'common_patterns': ['optionals', 'closures', 'protocols', 'extensions']
    },

    # Functional Languages
    '.hs': {
        'name': 'Haskell',
        'family': 'functional',
        'paradigm': 'purely-functional',
        'syntax_style': 'mathematical',
        'features': ['lazy-evaluation', 'immutable', 'type-inference', 'monads'],
        'frameworks': ['Yesod', 'Snap', 'Happstack'],
        'common_patterns': ['monads', 'functors', 'type classes', 'pattern matching']
    },
    '.ex': {
        'name': 'Elixir',
        'family': 'functional',
        'paradigm': 'functional',
        'syntax_style': 'ruby-like',
        'features': ['actor-model', 'fault-tolerant', 'concurrent', 'distributed'],
        'frameworks': ['Phoenix', 'Nerves', 'Broadway'],
        'common_patterns': ['pattern matching', 'pipe operator', 'GenServer', 'supervision trees']
    },
    '.clj': {
        'name': 'Clojure',
        'family': 'functional',
        'paradigm': 'functional',
        'syntax_style': 'lisp',
        'features': ['immutable', 'jvm-hosted', 'concurrent', 'homoiconic'],
        'frameworks': ['Ring', 'Compojure', 'Luminus'],
        'common_patterns': ['s-expressions', 'persistent data structures', 'multimethods', 'macros']
    },

    # Systems Languages
    '.c': {
        'name': 'C',
        'family': 'systems',
        'paradigm': 'procedural',
        'syntax_style': 'c-like',
        'features': ['low-level', 'manual-memory', 'portable', 'efficient'],
        'frameworks': ['glib', 'SDL', 'OpenGL'],
        'common_patterns': ['pointers', 'manual memory management', 'header files', 'macros']
    },
    '.cpp': {
        'name': 'C++',
        'family': 'systems',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'c-like',
        'features': ['object-oriented', 'template-metaprogramming', 'raii', 'zero-overhead'],
        'frameworks': ['Qt', 'Boost', 'POCO', 'FLTK'],
        'common_patterns': ['classes', 'templates', 'RAII', 'smart pointers']
    },

    # Scripting Languages
    '.rb': {
        'name': 'Ruby',
        'family': 'scripting',
        'paradigm': 'object-oriented',
        'syntax_style': 'natural',
        'features': ['dynamic', 'expressive', 'metaprogramming', 'blocks'],
        'frameworks': ['Rails', 'Sinatra', 'Hanami'],
        'common_patterns': ['blocks', 'mixins', 'metaprogramming', 'duck typing']
    },
    '.php': {
        'name': 'PHP',
        'family': 'web',
        'paradigm': 'imperative',
        'syntax_style': 'c-like',
        'features': ['web-focused', 'dynamic', 'interpreted', 'embedded'],
        'frameworks': ['Laravel', 'Symfony', 'CodeIgniter', 'Zend'],
        'common_patterns': ['superglobals', 'include/require', 'associative arrays', 'traits']
    },

    # Mobile/Cross-platform
    '.dart': {
        'name': 'Dart',
        'family': 'mobile',
        'paradigm': 'object-oriented',
        'syntax_style': 'c-like',
        'features': ['widget-based', 'hot-reload', 'ahead-of-time', 'just-in-time'],
        'frameworks': ['Flutter', 'AngularDart'],
        'common_patterns': ['widgets', 'futures', 'streams', 'isolates']
    },
    '.kt': {
        'name': 'Kotlin',
        'family': 'mobile',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'modern',
        'features': ['null-safe', 'interoperable', 'concise', 'expressive'],
        'frameworks': ['Android SDK', 'Ktor', 'Spring'],
        'common_patterns': ['null safety', 'data classes', 'extension functions', 'coroutines']
    },

    # Data Science
    '.r': {
        'name': 'R',
        'family': 'statistical',
        'paradigm': 'functional',
        'syntax_style': 'domain-specific',
        'features': ['statistical', 'vectorized', 'data-analysis', 'visualization'],
        'frameworks': ['Shiny', 'ggplot2', 'dplyr', 'tidyverse'],
        'common_patterns': ['data frames', 'vectorization', 'pipes', 'factors']
    },
    '.jl': {
        'name': 'Julia',
        'family': 'scientific',
        'paradigm': 'multi-paradigm',
        'syntax_style': 'mathematical',
        'features': ['high-performance', 'scientific', 'multiple-dispatch', 'metaprogramming'],
        'frameworks': ['Genie.jl', 'Flux.jl', 'DifferentialEquations.jl'],
        'common_patterns': ['multiple dispatch', 'macros', 'broadcasting', 'type system']
    }
}

def _get_enhanced_language_config(file_path: str) -> Dict[str, Any]:
    """Get comprehensive language configuration including paradigm and features."""
    ext = Path(file_path).suffix.lower()
    config = POLYGLOT_LANGUAGE_CONFIG.get(ext, {
        'name': 'Unknown',
        'family': 'unknown',
        'paradigm': 'unknown',
        'syntax_style': 'unknown',
        'features': [],
        'frameworks': [],
        'common_patterns': []
    })

    # Add file extension for reference
    config['extension'] = ext
    return config

def _detect_polyglot_context(target_files: List[str], cortex_data: Dict) -> Dict[str, Any]:
    """
    Enhanced context detection for polyglot projects.
    Analyzes multiple languages and provides comprehensive project context.
    """
    if not target_files:
        return _get_enhanced_language_config('')

    language_analysis = {}
    framework_hints = set()
    project_patterns = set()

    # Analyze each target file
    for file_path in target_files:
        config = _get_enhanced_language_config(file_path)
        language_name = config['name']

        if language_name not in language_analysis:
            language_analysis[language_name] = {
                'files': [],
                'config': config,
                'weight': 0
            }

        language_analysis[language_name]['files'].append(file_path)
        language_analysis[language_name]['weight'] += 1

        # Collect framework hints from cortex if available
        if cortex_data and 'files' in cortex_data:
            for file_cortex in cortex_data['files']:
                if file_cortex['file_path'] == file_path:
                    if 'framework_hints' in file_cortex:
                        framework_hints.update(file_cortex['framework_hints'])
                    break

        # Add common patterns
        project_patterns.update(config['common_patterns'])

    # Determine primary language by weight and importance
    if language_analysis:
        # Weight by file count and language importance
        language_priority = {
            'Python': 10, 'JavaScript': 9, 'TypeScript': 9, 'Java': 8, 'C#': 8,
            'Go': 7, 'Rust': 7, 'Swift': 6, 'C++': 6, 'Ruby': 5, 'PHP': 5,
            'Dart': 4, 'Kotlin': 4, 'Haskell': 3, 'Elixir': 3, 'Clojure': 3
        }

        weighted_scores = {}
        for lang_name, data in language_analysis.items():
            file_weight = data['weight']
            priority_weight = language_priority.get(lang_name, 1)
            weighted_scores[lang_name] = file_weight * priority_weight

        primary_language = max(weighted_scores, key=weighted_scores.get)
        primary_config = language_analysis[primary_language]['config']
    else:
        primary_language = 'Unknown'
        primary_config = _get_enhanced_language_config('')

    return {
        'primary_language': primary_language,
        'primary_config': primary_config,
        'all_languages': language_analysis,
        'detected_frameworks': list(framework_hints),
        'project_patterns': list(project_patterns),
        'is_polyglot': len(language_analysis) > 1,
        'polyglot_complexity': len(language_analysis)
    }

def _extract_json_from_llm(raw_text: str) -> Optional[str]:
    """Enhanced JSON extraction with better error handling."""
    # Try multiple patterns for JSON extraction
    patterns = [
        r'```(?:json)?\s*(\[.*?\])\s*```',  # Standard markdown code block
        r'```(?:json)?\s*(\{.*?\})\s*```',  # Object in code block
        r'(\[(?:[^[\]]|(?1))*\])',          # Recursive bracket matching
        r'(\{(?:[^{}]|(?1))*\})'            # Recursive brace matching
    ]

    for pattern in patterns:
        match = re.search(pattern, raw_text, re.DOTALL)
        if match:
            return match.group(1)

    # Fallback: try to find JSON-like structures
    try:
        start = raw_text.index('[')
        end = raw_text.rindex(']') + 1
        return raw_text[start:end]
    except ValueError:
        try:
            start = raw_text.index('{')
            end = raw_text.rindex('}') + 1
            return raw_text[start:end]
        except ValueError:
            return None

def _validate_and_parse_surgical_plan(json_str: str, language_context: Dict[str, Any]) -> Tuple[Optional[List[Dict]], str]:
    """Enhanced validation with language-specific checks."""
    if not json_str:
        return None, "AI response was empty or did not contain a JSON object."

    try:
        plan = json.loads(json_str)
    except json.JSONDecodeError as e:
        return None, f"AI response was not valid JSON. Parser error: {e}"

    if not isinstance(plan, list):
        return None, f"AI response was not a list of operations. Found type: {type(plan).__name__}."

    # Enhanced operation validation
    valid_operations = {
        "REPLACE_BLOCK": {
            "required": ["file_path", "target_identifier", "content"],
            "description": "Replace an entire function, method, class, or code block"
        },
        "ADD_FIELD_TO_STRUCT": {
            "required": ["file_path", "target_identifier", "content"],
            "description": "Add a new field to a struct (Rust/C/Go)"
        },
        "CREATE_FILE": {
            "required": ["file_path", "content"],
            "description": "Create a new file"
        },
        "INSERT_CODE_AT": {
            "required": ["file_path", "line_number", "content"],
            "description": "Insert code at a specific line number"
        },
        "ADD_IMPORT": {
            "required": ["file_path", "import_statement"],
            "description": "Add an import/require/using statement"
        },
        "REPLACE_FUNCTION": {
            "required": ["file_path", "function_name", "content"],
            "description": "Replace a specific function"
        },
        "ADD_METHOD_TO_CLASS": {
            "required": ["file_path", "class_name", "method_content"],
            "description": "Add a method to an existing class"
        }
    }

    for i, op in enumerate(plan):
        op_num = i + 1
        if not isinstance(op, dict):
            return None, f"Operation #{op_num} is not a valid object."

        operation_type = op.get("operation")
        if not operation_type:
            return None, f"Operation #{op_num} is missing the required 'operation' field."

        if operation_type not in valid_operations:
            return None, f"Operation #{op_num} has an unknown operation type: '{operation_type}'. Valid operations: {list(valid_operations.keys())}"

        required_fields = valid_operations[operation_type]["required"]
        missing = [field for field in required_fields if field not in op]
        if missing:
            return None, f"Operation #{op_num} ('{operation_type}') is missing required fields: {', '.join(missing)}."

        # Language-specific validation
        file_path = op.get("file_path", "")
        if file_path:
            file_config = _get_enhanced_language_config(file_path)
            language_name = file_config['name']

            # Validate operation compatibility with language
            if operation_type == "ADD_FIELD_TO_STRUCT":
                if language_name not in ['Rust', 'C', 'C++', 'Go']:
                    return None, f"Operation #{op_num}: ADD_FIELD_TO_STRUCT is not applicable to {language_name}. Use ADD_METHOD_TO_CLASS for object-oriented languages."

    return plan, ""

def _scout_expand_scope(
    original_contents: Dict[str, str],
    surgical_plan: List[Dict],
    full_file_map: Dict[str, str]
) -> Dict[str, str]:
    """
    Enhanced Scout Service with polyglot awareness.
    Ensures the file scope matches the AI's plan by loading any missing files.
    """
    print("🛰️  Activating Enhanced Scout: Verifying and expanding polyglot file scope...")

    plan_files = {op['file_path'] for op in surgical_plan if 'file_path' in op}

    updated_contents = original_contents.copy()
    expanded_files_loaded = 0
    language_stats = {}

    for file_path in plan_files:
        if file_path not in updated_contents:
            updated_contents[file_path] = full_file_map.get(file_path, "")
            print(f"  → Scout expanded scope to include: {file_path}")
            expanded_files_loaded += 1

            # Track language diversity
            config = _get_enhanced_language_config(file_path)
            language_name = config['name']
            language_stats[language_name] = language_stats.get(language_name, 0) + 1

    if expanded_files_loaded > 0:
        print(f"✓ Scout successfully expanded scope with {expanded_files_loaded} new file(s).")
        if language_stats:
            print(f"  → Languages in expanded scope: {', '.join(language_stats.keys())}")
    else:
        print("✓ File scope is consistent with the AI's plan.")

    return updated_contents

def _generate_language_aware_prompt(
    target_files: List[str],
    instruction: str,
    rca_report: str,
    language_context: Dict[str, Any],
    file_content_section: str,
    refinement_context: str = ""
) -> str:
    """
    Generate a sophisticated, language-aware prompt for the AI.
    """
    primary_language = language_context['primary_language']
    primary_config = language_context['primary_config']
    is_polyglot = language_context['is_polyglot']

    # Build language expertise section
    language_expertise = f"""You are an expert software architect specializing in {primary_language}"""

    if is_polyglot:
        all_languages = list(language_context['all_languages'].keys())
        language_expertise += f" and polyglot development with {', '.join(all_languages)}"

    language_expertise += "."

    # Add language-specific context
    language_context_section = f"""
### LANGUAGE CONTEXT ###
Primary Language: {primary_language}
- Paradigm: {primary_config.get('paradigm', 'unknown')}
- Syntax Style: {primary_config.get('syntax_style', 'unknown')}
- Key Features: {', '.join(primary_config.get('features', []))}
- Common Patterns: {', '.join(primary_config.get('common_patterns', []))}
"""

    if primary_config.get('frameworks'):
        language_context_section += f"- Popular Frameworks: {', '.join(primary_config['frameworks'])}\n"

    if is_polyglot:
        language_context_section += f"""
This is a polyglot project with {language_context['polyglot_complexity']} languages:
"""
        for lang_name, lang_data in language_context['all_languages'].items():
            files = lang_data['files']
            language_context_section += f"- {lang_name}: {len(files)} file(s) - {', '.join(files)}\n"

    if language_context['detected_frameworks']:
        language_context_section += f"Detected Frameworks: {', '.join(language_context['detected_frameworks'])}\n"

    # Enhanced operation examples based on language
    operation_examples = _get_language_specific_operation_examples(primary_language, primary_config)

    surgical_prompt = f"""{language_expertise} Your task is to generate a precise surgical plan to fix a bug in this {'polyglot' if is_polyglot else primary_language} project.

{language_context_section}

<Goal>{instruction}</Goal>
<RCA_Report>{rca_report}</RCA_Report>
{refinement_context}
{file_content_section}

### YOUR TASK ###
Based on all the provided information, create a step-by-step surgical plan as a JSON array that follows {primary_language} best practices{' and handles the polyglot nature of this project' if is_polyglot else ''}.

### AVAILABLE OPERATIONS ###
You can use the following operations in your plan:

1. `"operation": "CREATE_FILE"`: Creates a new file.
   - Required fields: `file_path`, `content`.
   - Use for: New modules, configuration files, or missing dependencies.

2. `"operation": "REPLACE_BLOCK"`: Replaces an entire function, method, class, or other code block.
   - Required fields: `file_path`, `target_identifier` (the unique name/signature), `content`.
   - Use for: Complete rewrites of functions, classes, or major code blocks.

3. `"operation": "ADD_FIELD_TO_STRUCT"`: (For Rust/C/Go) Adds a new field to a struct.
   - Required fields: `file_path`, `target_identifier` (struct name), `content`.
   - Use for: Adding new data fields to existing structures.

4. `"operation": "INSERT_CODE_AT"`: Inserts code at a specific line number.
   - Required fields: `file_path`, `line_number`, `content`.
   - Use for: Adding code at precise locations.

5. `"operation": "ADD_IMPORT"`: Adds an import/require/using statement.
   - Required fields: `file_path`, `import_statement`.
   - Use for: Adding dependencies or modules.

6. `"operation": "REPLACE_FUNCTION"`: Replaces a specific function.
   - Required fields: `file_path`, `function_name`, `content`.
   - Use for: Function-specific changes.

7. `"operation": "ADD_METHOD_TO_CLASS"`: Adds a method to an existing class.
   - Required fields: `file_path`, `class_name`, `method_content`.
   - Use for: Extending classes with new functionality.

{operation_examples}

### IMPORTANT GUIDELINES ###
- Follow {primary_language} naming conventions and style guidelines
- Ensure type safety and error handling appropriate for {primary_language}
- Use idiomatic {primary_language} patterns: {', '.join(primary_config.get('common_patterns', [])[:3])}
{f'- Consider cross-language compatibility in this polyglot project' if is_polyglot else ''}
- Maintain consistency with existing code architecture
- Include proper documentation/comments in the target language style

### RESPONSE FORMAT ###
- You MUST respond with ONLY a valid JSON array `[...]`.
- Do not include any explanations, markdown fences, or other text.
- If you need to modify a file not in the provided context, add an operation for that file.

Generate the {primary_language} surgical plan now."""

    return surgical_prompt

def _get_language_specific_operation_examples(language: str, config: Dict[str, Any]) -> str:
    """Generate language-specific operation examples."""
    examples = f"\n### {language.upper()} SPECIFIC EXAMPLES ###\n"

    if language == "Python":
        examples += """
<Python_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/main.py",
    "import_statement": "from typing import Optional, Dict"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/utils.py",
    "function_name": "process_data",
    "content": "def process_data(data: Dict[str, Any]) -> Optional[str]:\\n    \\"\\"\\"Process data with type safety.\\"\\"\\"\\n    if not data:\\n        return None\\n    return str(data.get('result', ''))"
  }
]
```
</Python_Examples>"""

    elif language in ["JavaScript", "TypeScript"]:
        examples += """
<JavaScript_TypeScript_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/utils.js",
    "import_statement": "import { validateInput } from './validators';"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/api.js",
    "function_name": "fetchData",
    "content": "async function fetchData(url) {\\n  try {\\n    const response = await fetch(url);\\n    if (!response.ok) throw new Error('Network error');\\n    return await response.json();\\n  } catch (error) {\\n    console.error('Fetch failed:', error);\\n    throw error;\\n  }\\n}"
  }
]
```
</JavaScript_TypeScript_Examples>"""

    elif language == "Java":
        examples += """
<Java_Examples>
```json
[
  {
    "operation": "ADD_IMPORT",
    "file_path": "src/main/java/Main.java",
    "import_statement": "import java.util.Optional;"
  },
  {
    "operation": "ADD_METHOD_TO_CLASS",
    "file_path": "src/main/java/UserService.java",
    "class_name": "UserService",
    "method_content": "    public Optional<User> findUserById(Long id) {\\n        if (id == null || id <= 0) {\\n            return Optional.empty();\\n        }\\n        return userRepository.findById(id);\\n    }"
  }
]
```
</Java_Examples>"""

    elif language == "Go":
        examples += """
<Go_Examples>
```json
[
  {
    "operation": "ADD_FIELD_TO_STRUCT",
    "file_path": "internal/models/user.go",
    "target_identifier": "User",
    "content": "    Email    string `json:\\"email\\" validate:\\"required,email\\"`"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "internal/handlers/user.go",
    "function_name": "CreateUser",
    "content": "func CreateUser(w http.ResponseWriter, r *http.Request) {\\n    var user User\\n    if err := json.NewDecoder(r.Body).Decode(&user); err != nil {\\n        http.Error(w, \\"Invalid JSON\\", http.StatusBadRequest)\\n        return\\n    }\\n    // Process user creation\\n    w.WriteHeader(http.StatusCreated)\\n}"
  }
]
```
</Go_Examples>"""

    elif language == "Rust":
        examples += """
<Rust_Examples>
```json
[
  {
    "operation": "ADD_FIELD_TO_STRUCT",
    "file_path": "src/models/user.rs",
    "target_identifier": "User",
    "content": "    pub email: Option<String>,"
  },
  {
    "operation": "REPLACE_FUNCTION",
    "file_path": "src/lib.rs",
    "function_name": "process_data",
    "content": "pub fn process_data(input: &str) -> Result<String, Box<dyn std::error::Error>> {\\n    if input.is_empty() {\\n        return Err(\\"Input cannot be empty\\".into());\\n    }\\n    Ok(input.to_uppercase())\\n}"
  }
]
```
</Rust_Examples>"""

    else:
        # Generic example for other languages
        examples += f"""
<{language}_Examples>
```json
[
  {{
    "operation": "REPLACE_BLOCK",
    "file_path": "src/main{config.get('extension', '')}",
    "target_identifier": "main_function",
    "content": "// Enhanced main function with proper error handling\\n// TODO: Implement based on {language} best practices"
  }}
]
```
</{language}_Examples>"""

    return examples

def generate_scaffold(
    repo_id: str,
    target_files: List[str],
    instruction: str,
    rca_report: str,
    refinement_history: Optional[List[Dict[str, str]]] = None
) -> Dict[str, Any]:
    """
    Enhanced Code Scaffolding with comprehensive polyglot support and dynamic scope expansion.

    This version provides:
    - Multi-language project detection and analysis
    - Language-aware prompt generation
    - Framework-specific code generation hints
    - Enhanced operation validation
    - Polyglot project complexity handling
    """
    print(f"🔧 Initiating Enhanced Polyglot Surgical Scaffolding for {target_files} in repo '{repo_id}'")

    try:
        # Load and validate cortex data
        backend_dir = Path(__file__).resolve().parent.parent.parent
        cortex_path = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"
        if not cortex_path.exists():
            return {"error": "Cortex file not found", "details": f"Expected path: {cortex_path}"}

        with open(cortex_path, 'r', encoding='utf-8') as f:
            cortex_data = json.load(f)

        file_map = {file['file_path']: file['raw_content'] for file in cortex_data.get('files', [])}
        original_contents = {fp: file_map.get(fp, "") for fp in target_files}

        # Enhanced polyglot context detection
        language_context = _detect_polyglot_context(target_files, cortex_data)

        print(f"  🌐 Detected project context:")
        print(f"     • Primary language: {language_context['primary_language']}")
        print(f"     • Project type: {'Polyglot' if language_context['is_polyglot'] else 'Monoglot'}")
        if language_context['is_polyglot']:
            print(f"     • Languages involved: {list(language_context['all_languages'].keys())}")
        if language_context['detected_frameworks']:
            print(f"     • Frameworks detected: {', '.join(language_context['detected_frameworks'])}")

        # --- STEP 1: GENERATE THE ENHANCED SURGICAL PLAN ---
        file_content_prompt_section = "\n\n### RELEVANT EXISTING CODE\n"
        for path, content in original_contents.items():
            if content:
                # Enhanced compression with language awareness
                compressed_content = code_surgery.get_relevant_code_from_cortex(content, rca_report, path)
                language_config = _get_enhanced_language_config(path)
                file_content_prompt_section += f"<file path=\"{path}\" language=\"{language_config['name']}\">\n{compressed_content}\n</file>\n\n"

        refinement_context = ""
        if refinement_history:
            refinement_context = "\n\n### PREVIOUS REFINEMENT ATTEMPTS\n"
            for i, refinement in enumerate(refinement_history[-2:]):
                feedback = refinement.get("feedback", "No feedback provided.")
                refinement_context += f"Attempt {i+1} Feedback: {feedback}\n"
            refinement_context += "\nPlease learn from the previous feedback and generate a better plan that follows language-specific best practices.\n"

        # Generate sophisticated language-aware prompt
        surgical_prompt = _generate_language_aware_prompt(
            target_files, instruction, rca_report, language_context,
            file_content_prompt_section, refinement_context
        )

        max_retries = 3
        surgical_plan = None
        last_llm_response = ""

        for attempt in range(max_retries):
            print(f"  🤖 Attempt {attempt + 1}: Calling LLM for {language_context['primary_language']} surgical plan...")

            # Enhanced LLM call with appropriate task type
            task_type = TaskType.CODE_GENERATION
            if language_context['is_polyglot'] or language_context['polyglot_complexity'] > 2:
                task_type = TaskType.COMPLEX_REASONING

            llm_response = llm_service.generate_text(
                surgical_prompt,
                task_type=task_type
            )
            last_llm_response = llm_response

            if not llm_response or not llm_response.strip():
                print("  ⚠️ Empty response, retrying...")
                continue

            # Enhanced JSON extraction and validation
            json_str = _extract_json_from_llm(llm_response)
            if json_str:
                plan, error_msg = _validate_and_parse_surgical_plan(json_str, language_context)
                if plan:
                    surgical_plan = plan
                    print(f"  ✓ Successfully parsed {language_context['primary_language']} surgical plan with {len(plan)} operations.")

                    # Log plan summary
                    operation_summary = {}
                    for op in plan:
                        op_type = op.get('operation', 'unknown')
                        operation_summary[op_type] = operation_summary.get(op_type, 0) + 1
                    print(f"    → Operations: {', '.join(f'{count}x {op}' for op, count in operation_summary.items())}")
                    break
                else:
                    print(f"  ❌ Blueprint Rejected (Attempt {attempt+1}/{max_retries}): {error_msg}")
                    print(f"  📝 Faulty JSON received: {json_str[:250]}...")
            else:
                print(f"  ❌ Could not extract JSON from LLM response (Attempt {attempt+1}/{max_retries}).")

        if not surgical_plan:
            return {
                "error": f"Failed to generate a valid {language_context['primary_language']} surgical plan after {max_retries} attempts.",
                "details": "The AI's final proposed plan was malformed or incomplete. This may be due to the complexity of the polyglot project or language-specific constraints.",
                "llm_response": last_llm_response,
                "language_context": language_context,
            }

        # Enhanced Scout with polyglot awareness
        final_contents_for_surgery = _scout_expand_scope(
            original_contents,
            surgical_plan,
            file_map
        )

        # Execute surgical plan with enhanced error handling
        print("🔬 Executing enhanced surgical plan...")
        modified_files, surgery_report = code_surgery.execute_surgical_plan(
            final_contents_for_surgery,
            surgical_plan
        )

        if surgery_report.get("errors"):
            return {
                "error": "Enhanced Code Surgery failed to apply the plan.",
                "details": surgery_report["errors"],
                "language_context": language_context,
                "partial_results": surgery_report.get("successes", [])
            }

        # Enhanced success reporting
        print("🎉 Enhanced polyglot surgical scaffolding completed successfully!")
        print(f"  ✓ Modified {len(modified_files)} files")
        print(f"  ✓ Applied {len(surgical_plan)} operations")
        print(f"  ✓ Primary language: {language_context['primary_language']}")

        if language_context['is_polyglot']:
            print(f"  ✓ Handled polyglot complexity: {language_context['polyglot_complexity']} languages")

        return {
            "modified_files": modified_files,
            "original_contents": final_contents_for_surgery,
            "plan_executed": surgical_plan,
            "surgery_report": surgery_report,
            "language_context": language_context,
            "polyglot_summary": {
                "primary_language": language_context['primary_language'],
                "is_polyglot": language_context['is_polyglot'],
                "languages_involved": list(language_context['all_languages'].keys()),
                "frameworks_detected": language_context['detected_frameworks'],
                "complexity_score": language_context['polyglot_complexity']
            }
        }

    except Exception as e:
        print(f"❌ Critical error in enhanced polyglot scaffolding: {str(e)}")
        return {
            "error": "A critical error occurred in the enhanced polyglot scaffolding service.",
            "details": str(e),
            "traceback": traceback.format_exc(),
        }

# --- Enhanced utility functions for polyglot support ---

def analyze_code_complexity(target_files: List[str], cortex_data: Dict) -> Dict[str, Any]:
    """
    Analyze the complexity of a polyglot codebase for better scaffolding decisions.
    """
    if not cortex_data or 'files' not in cortex_data:
        return {"error": "Invalid cortex data"}

    complexity_analysis = {
        "total_files": len(target_files),
        "languages": {},
        "frameworks": set(),
        "complexity_indicators": {},
        "recommendations": []
    }

    for file_path in target_files:
        config = _get_enhanced_language_config(file_path)
        language = config['name']

        if language not in complexity_analysis["languages"]:
            complexity_analysis["languages"][language] = {
                "file_count": 0,
                "total_lines": 0,
                "paradigm": config.get('paradigm', 'unknown'),
                "features": config.get('features', [])
            }

        complexity_analysis["languages"][language]["file_count"] += 1

        # Find corresponding file in cortex
        for file_cortex in cortex_data['files']:
            if file_cortex['file_path'] == file_path:
                lines = len(file_cortex['raw_content'].splitlines())
                complexity_analysis["languages"][language]["total_lines"] += lines

                if 'framework_hints' in file_cortex:
                    complexity_analysis["frameworks"].update(file_cortex['framework_hints'])
                break

    # Convert set to list for JSON serialization
    complexity_analysis["frameworks"] = list(complexity_analysis["frameworks"])

    # Generate complexity indicators
    language_count = len(complexity_analysis["languages"])
    if language_count > 1:
        complexity_analysis["complexity_indicators"]["polyglot"] = True
        complexity_analysis["complexity_indicators"]["language_diversity"] = language_count

    total_lines = sum(lang["total_lines"] for lang in complexity_analysis["languages"].values())
    complexity_analysis["complexity_indicators"]["total_lines"] = total_lines

    if total_lines > 10000:
        complexity_analysis["complexity_indicators"]["large_codebase"] = True

    # Generate recommendations
    if language_count > 3:
        complexity_analysis["recommendations"].append(
            "High language diversity detected. Consider breaking down changes into language-specific phases."
        )

    if len(complexity_analysis["frameworks"]) > 2:
        complexity_analysis["recommendations"].append(
            "Multiple frameworks detected. Ensure cross-framework compatibility."
        )

    return complexity_analysis

def get_language_specific_best_practices(language: str) -> List[str]:
    """
    Return language-specific best practices for code generation.
    """
    practices = {
        "Python": [
            "Follow PEP 8 style guidelines",
            "Use type hints for better code documentation",
            "Implement proper exception handling",
            "Use list comprehensions and generator expressions where appropriate",
            "Follow the principle of 'Pythonic' code"
        ],
        "JavaScript": [
            "Use const and let instead of var",
            "Implement proper error handling with try-catch",
            "Use async/await for asynchronous operations",
            "Follow ESLint recommendations",
            "Use destructuring and modern ES6+ features"
        ],
        "TypeScript": [
            "Leverage strong typing features",
            "Use interfaces and type definitions",
            "Implement proper generic types",
            "Use strict mode configuration",
            "Follow Angular/React specific patterns if applicable"
        ],
        "Java": [
            "Follow Java naming conventions",
            "Use dependency injection patterns",
            "Implement proper exception handling",
            "Use Optional for nullable values",
            "Follow SOLID principles"
        ],
        "Go": [
            "Follow Go formatting standards (gofmt)",
            "Use goroutines and channels for concurrency",
            "Implement proper error handling",
            "Keep interfaces small and focused",
            "Use composition over inheritance"
        ],
        "Rust": [
            "Leverage ownership and borrowing system",
            "Use Result and Option types for error handling",
            "Implement proper trait patterns",
            "Use pattern matching effectively",
            "Follow Rust naming conventions"
        ]
    }

    return practices.get(language, [
        "Follow language-specific style guidelines",
        "Implement proper error handling",
        "Use idiomatic patterns for the language",
        "Ensure code readability and maintainability"
    ])

def detect_framework_patterns(file_content: str, language: str) -> List[str]:
    """
    Detect framework-specific patterns in code for better scaffolding.
    """
    patterns = []
    content_lower = file_content.lower()

    framework_signatures = {
        "react": ["usestate", "useeffect", "jsx", "react.component"],
        "vue": ["vue.component", "template>", "@click", "v-if"],
        "angular": ["@component", "@injectable", "ngmodule", "ngoninit"],
        "express": ["app.get", "app.post", "express()", "router."],
        "flask": ["@app.route", "flask()", "request."],
        "django": ["models.model", "views.view", "urls.py", "django."],
        "spring": ["@controller", "@service", "@autowired", "@requestmapping"],
        "laravel": ["route::", "eloquent", "blade.php", "artisan"]
    }

    for framework, signatures in framework_signatures.items():
        if any(sig in content_lower for sig in signatures):
            patterns.append(framework)

    return patterns

# --- Backward Compatibility ---
def generate_scaffold_legacy(repo_id: str, target_files: List[str], instruction: str, rca_report: str) -> Dict[str, Any]:
    """Legacy interface for backward compatibility."""
    return generate_scaffold(repo_id, target_files, instruction, rca_report)

--- FILE_END: backend/lumiere_core/services/scaffolding.py ---

--- FILE_START: backend/lumiere_core/services/diplomat.py ---
# In backend/lumiere_core/services/diplomat.py

import os
import re
import requests
from typing import Dict, Any, List
from github import Github, GithubException, Issue

from . import llm_service
from .utils import clean_llm_code_output

# --- Configuration ---
GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    raise ValueError("GITHUB_ACCESS_TOKEN must be set in the .env file.")

g = Github(GITHUB_TOKEN)

def _get_pr_for_issue(issue: Issue) -> Dict[str, Any] | None:
    """Finds the Pull Request that closed a given issue."""
    try:
        for event in issue.get_timeline():
            if event.event == "closed" and event.source and event.source.issue:
                pr = event.source.issue
                return {
                    "url": pr.html_url,
                    "title": pr.title,
                    "diff_url": pr.diff_url,
                }
    except GithubException as e:
        print(f"   -> API error while fetching timeline for {issue.html_url}: {e}")
    return None

def find_similar_solved_issues(issue_title: str, issue_body: str, model_identifier: str) -> Dict[str, Any]:
    """
    The main logic for The Diplomat agent.
    Searches GitHub for similar, solved issues and synthesizes the findings.
    """
    print("--- DIPLOMAT AGENT ACTIVATED ---")
    print(f"Using model: {model_identifier}")

    print("\n[Step 1/3] Generating a targeted search query from issue details...")
    query_generation_prompt = f"""
You are an expert GitHub search querycrafter. Based on the following issue title and body, generate a concise, powerful search query for finding similar issues.
Focus on extracting key library names, error messages, and critical function names.
For example, for a "TypeError" in "requests", the query might be: `requests "TypeError: timeout value must be a float"`.

ISSUE TITLE: {issue_title}
ISSUE BODY:
{issue_body}

Now, provide ONLY the search query string. Do not include any of your own commentary or XML tags.
"""
    raw_query = llm_service.generate_text(query_generation_prompt, model_identifier)
    search_query = clean_llm_code_output(raw_query).replace('"', '')
    print(f"✓ Generated Search Query: '{search_query}'")

    print("\n[Step 2/3] Searching GitHub for similar, solved issues...")
    qualified_query = f'{search_query} is:issue is:closed stars:>100 in:body'

    try:
        issues = g.search_issues(query=qualified_query, order="desc")
        print(f"✓ Found {issues.totalCount} potential matches. Analyzing the top 5...")

        # --- FIX: Check if there are any results before trying to iterate ---
        if issues.totalCount == 0:
            return {
                "summary": "The Diplomat was unable to find relevant, solved issues on GitHub for this specific problem.",
                "evidence": []
            }

        evidence = []
        # We can now safely iterate over the slice
        for issue in issues[:5]:
            print(f"   -> Analyzing: {issue.html_url}")
            closing_pr = _get_pr_for_issue(issue)
            if closing_pr:
                evidence.append({
                    "issue_title": issue.title,
                    "issue_url": issue.html_url,
                    "repo_name": issue.repository.full_name,
                    "solution_url": closing_pr['url'],
                    "diff_url": closing_pr['diff_url'],
                })

        if not evidence:
            return {
                "summary": "The Diplomat found some potentially related issues, but none had a clear linked Pull Request to analyze for a solution.",
                "evidence": []
            }

    except GithubException as e:
        return {"error": f"An error occurred while searching GitHub: {e.data.get('message', str(e))}"}

    print("\n[Step 3/3] Synthesizing findings into an intelligence briefing...")
    evidence_str = ""
    for item in evidence:
        evidence_str += f"- Issue in **{item['repo_name']}**: \"{item['issue_title']}\"\n"
        evidence_str += f"  - Issue Link: {item['issue_url']}\n"
        evidence_str += f"  - Solved by PR: {item['solution_url']}\n"

    synthesis_prompt = f"""
You are "The Diplomat," an AI agent for Lumière Sémantique.
You have found several solved issues on GitHub that are similar to the user's current problem.
Your mission is to write a concise intelligence briefing summarizing your findings. Do NOT tell the user how to fix their code. Instead, highlight the PATTERNS you found in the solutions.

Example summary format:
"This appears to be a known configuration issue. I found similar reports in `psf/requests` and `org/project` that were solved by changing a specific parameter. This strengthens the case for a configuration-based fix."

Here is the evidence you collected:
{evidence_str}

Now, generate the "Diplomat Intelligence Briefing" in Markdown.
"""
    summary = llm_service.generate_text(synthesis_prompt, model_identifier)

    print("--- DIPLOMAT AGENT MISSION COMPLETE ---")
    return {"summary": summary, "evidence": evidence}

--- FILE_END: backend/lumiere_core/services/diplomat.py ---

--- FILE_START: backend/lumiere_core/services/llm_service.py ---
# backend/lumiere_core/services/llm_service.py

from typing import List, Dict, Any

from . import ollama_service
from . import gemini_service

# --- The Lumière Task Router Configuration ---
# This is our Python equivalent of the config.json from claude-code-router.
# We will start with models we know work in our system.
TASK_ROUTER_CONFIG = {
    # For simple, non-critical tasks that require instruction following (like JSON output).
    "ROUTER_TASK_SIMPLE": "gemini/gemini-1.5-flash-latest",

    # For complex reasoning, code generation, and architectural analysis.
    "ROUTER_TASK_COMPLEX_REASONING": "gemini/gemini-1.5-flash-latest",

    # For tasks requiring a huge context window, like summarizing a whole repo.
    "ROUTER_TASK_LONG_CONTEXT": "gemini/gemini-1.5-pro-latest",

    # For code-specific generation tasks.
    "ROUTER_TASK_CODE_GENERATION": "gemini/gemini-1.5-flash-latest",
}


class TaskType:
    """Defines the types of tasks our system can perform."""
    SIMPLE = "ROUTER_TASK_SIMPLE"
    COMPLEX_REASONING = "ROUTER_TASK_COMPLEX_REASONING"
    LONG_CONTEXT = "ROUTER_TASK_LONG_CONTEXT"
    CODE_GENERATION = "ROUTER_TASK_CODE_GENERATION"


def generate_text(prompt: str, task_type: str = TaskType.SIMPLE) -> str:
    """
    The new intelligent entry point. It routes the prompt to the best model for the job.

    Args:
        prompt: The text prompt for the model.
        task_type: The type of task, used to select the right model from the router config.

    Returns:
        The generated text from the chosen model.
    """
    # 1. Select the model based on the task type
    model_identifier = TASK_ROUTER_CONFIG.get(task_type)
    if not model_identifier:
        # Fallback to a simple model if the task type is unknown
        model_identifier = TASK_ROUTER_CONFIG[TaskType.SIMPLE]
        print(f"Warning: Unknown task_type '{task_type}'. Defaulting to {model_identifier}.")

    print(f"Task Router: Routing task '{task_type}' to model '{model_identifier}'")

    # 2. Split the identifier to find the provider
    parts = model_identifier.split('/', 1)
    if len(parts) != 2:
        return f"Error: Invalid model identifier format '{model_identifier}'."

    provider, model_name = parts

    # 3. Call the appropriate provider service
    if provider == "ollama":
        return ollama_service.generate_text(prompt, model_name)
    elif provider == "gemini":
        return gemini_service.generate_text(prompt, model_name)
    else:
        return f"Error: Unknown LLM provider '{provider}'."

def list_available_models() -> List[Dict[str, Any]]:
    """
    Aggregates available models from all configured providers. (Unchanged)
    """
    all_models = []
    all_models.extend(ollama_service.list_models())
    all_models.extend(gemini_service.list_models())
    return all_models

--- FILE_END: backend/lumiere_core/services/llm_service.py ---

--- FILE_START: backend/lumiere_core/services/gemini_service.py ---
# In backend/lumiere_core/services/gemini_service.py

import os
import google.generativeai as genai
from typing import List, Dict

# Configure the Gemini client from environment variables
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

def is_configured() -> bool:
    """Check if the Gemini service is ready to be used."""
    return GEMINI_API_KEY is not None

def generate_text(prompt: str, model_name: str) -> str:
    """
    Sends a prompt to the Google Gemini API.

    Args:
        prompt: The full prompt to send.
        model_name: The specific Gemini model to use (e.g., 'gemini-1.5-pro-latest').
    """
    if not is_configured():
        return "Error: GEMINI_API_KEY is not configured in the environment."

    print(f"Sending prompt to Google Gemini model: '{model_name}'...")
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"An error occurred while communicating with the Gemini API: {e}")
        return f"Error from Gemini API: {e}"

def list_models() -> List[Dict[str, str]]:
    """Lists available Gemini models that support text generation."""
    if not is_configured():
        return []

    print("Fetching available Google Gemini models...")
    available = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                model_id = f"gemini/{m.name.replace('models/', '')}"
                available.append({
                    "id": model_id,
                    "provider": "gemini",
                    "name": m.display_name
                })
        return available
    except Exception as e:
        print(f"Could not fetch Gemini models: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/gemini_service.py ---

--- FILE_START: backend/lumiere_core/services/utils.py ---
# In ~/lumiere_semantique/backend/lumiere_core/services/utils.py
# In lumiere_core/services/utils.py
import re

def clean_llm_code_output(raw_code: str) -> str:
    """
    [Robustness] Removes Markdown code fences and extraneous whitespace from LLM output.

    This function uses a regular expression to find and remove
    common Markdown code block fences (like ```python or ```) from the start
    and end of the string. It also strips any leading or trailing whitespace.
    This is a shared utility to ensure all code-generating agents produce
    clean, machine-readable output.
    """
    # This regex matches an optional language specifier (like 'python', 'toml')
    # and the code fences themselves at the start and end of the string.
    code_fence_pattern = r"^\s*```[a-zA-Z]*\n?|```\s*$"
    cleaned_code = re.sub(code_fence_pattern, '', raw_code)
    return cleaned_code.strip()

--- FILE_END: backend/lumiere_core/services/utils.py ---

--- FILE_START: backend/lumiere_core/services/oracle_service.py ---
# backend/lumiere_core/services/oracle_service.py

import logging
import json
import networkx as nx
from typing import Dict, Any, List, Optional, Tuple, Set, Union
from collections import defaultdict, Counter
import re
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import hashlib
import time
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from functools import lru_cache
import threading
from queue import Queue, Empty
import asyncio
from contextlib import contextmanager

from . import llm_service, cortex_service, ollama
from .llm_service import TaskType
from .utils import clean_llm_code_output

logger = logging.getLogger(__name__)

# --- Exceptions ---
class OracleServiceError(Exception):
    """Base exception for Oracle service errors."""
    pass

class GraphBuildingError(OracleServiceError):
    """Raised when graph building fails."""
    pass

class EntityExtractionError(OracleServiceError):
    """Raised when entity extraction fails."""
    pass

class GraphAnalysisError(OracleServiceError):
    """Raised when graph analysis fails."""
    pass

class RAGSearchError(OracleServiceError):
    """Raised when RAG search fails."""
    pass

# --- Enhanced Configuration ---
class SearchStrategy(Enum):
    """Search strategies for different query types."""
    SIMPLE = "simple"
    EXPANDED = "expanded"
    MULTI_HOP = "multi_hop"
    HYBRID = "hybrid"
    GRAPH_GUIDED = "graph_guided"

@dataclass
class RAGConfig:
    """Enhanced configuration for RAG search behavior."""
    # Search parameters
    default_k: int = 20
    max_k: int = 50
    min_k: int = 5

    # Reranking thresholds
    similarity_threshold: float = 0.7
    diversity_threshold: float = 0.3
    relevance_decay: float = 0.95

    # Query expansion
    max_expansions: int = 5
    max_synonyms: int = 3
    expansion_depth: int = 2

    # Multi-hop settings
    max_hops: int = 3
    hop_decay: float = 0.8
    min_hop_relevance: float = 0.5

    # Parallel execution
    max_workers: int = 5
    timeout_seconds: int = 30

    # Caching
    cache_ttl: int = 3600  # 1 hour
    max_cache_size: int = 1000

    # Advanced features
    use_semantic_clustering: bool = True
    use_entity_linking: bool = True
    use_code_understanding: bool = True
    enable_self_reflection: bool = True

# Global config instance
rag_config = RAGConfig()

# --- Cache Management ---
class SearchCache:
    """Thread-safe cache for search results."""
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self._cache = {}
        self._timestamps = {}
        self._lock = threading.RLock()
        self._max_size = max_size
        self._ttl = ttl

    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < self._ttl:
                    return self._cache[key]
                else:
                    del self._cache[key]
                    del self._timestamps[key]
            return None

    def put(self, key: str, value: Any):
        with self._lock:
            if len(self._cache) >= self._max_size:
                # Remove oldest entry
                oldest_key = min(self._timestamps, key=self._timestamps.get)
                del self._cache[oldest_key]
                del self._timestamps[oldest_key]

            self._cache[key] = value
            self._timestamps[key] = time.time()

    def clear(self):
        with self._lock:
            self._cache.clear()
            self._timestamps.clear()

# Global cache instance
search_cache = SearchCache(rag_config.max_cache_size, rag_config.cache_ttl)

# --- Advanced Entity Processing ---
@dataclass
class Entity:
    """Represents an extracted entity with metadata."""
    text: str
    type: str
    confidence: float = 1.0
    context: str = ""
    aliases: List[str] = field(default_factory=list)
    related: List[str] = field(default_factory=list)

class EntityExtractor:
    """Advanced entity extraction with multiple strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self._pattern_cache = {}
        self._init_patterns()

    def _init_patterns(self):
        """Initialize regex patterns for code entity extraction."""
        self._pattern_cache = {
            'import': re.compile(r'(?:from|import)\s+([a-zA-Z_][\w\.]*)', re.MULTILINE),
            'class': re.compile(r'class\s+([A-Z]\w*)', re.MULTILINE),
            'function': re.compile(r'def\s+([a-z_]\w*)', re.MULTILINE),
            'variable': re.compile(r'([a-z_]\w*)\s*=', re.MULTILINE),
            'file_path': re.compile(r'[\'"]([^\'"\s]+\.[a-z]+)[\'"]', re.MULTILINE),
            'url': re.compile(r'https?://[^\s]+', re.MULTILINE),
            'api_endpoint': re.compile(r'[\'"][/][\w/\-{}]+[\'"]', re.MULTILINE),
        }

    def extract(self, text: str, use_llm: bool = True) -> Dict[str, List[Entity]]:
        """Extract entities using both pattern matching and LLM."""
        entities = defaultdict(list)

        # Pattern-based extraction
        if self.config.use_code_understanding:
            entities.update(self._extract_with_patterns(text))

        # LLM-based extraction
        if use_llm:
            try:
                llm_entities = self._extract_with_llm(text)
                self._merge_entities(entities, llm_entities)
            except Exception as e:
                logger.warning(f"LLM entity extraction failed: {e}")

        # Entity linking and expansion
        if self.config.use_entity_linking:
            entities = self._link_entities(entities)

        return dict(entities)

    def _extract_with_patterns(self, text: str) -> Dict[str, List[Entity]]:
        """Extract entities using regex patterns."""
        entities = defaultdict(list)

        for pattern_name, pattern in self._pattern_cache.items():
            matches = pattern.findall(text)
            entity_type = self._pattern_to_entity_type(pattern_name)

            for match in matches:
                if isinstance(match, str) and match.strip():
                    entity = Entity(
                        text=match.strip(),
                        type=entity_type,
                        confidence=0.8,
                        context=self._extract_context(text, match)
                    )
                    entities[entity_type].append(entity)

        return entities

    def _extract_with_llm(self, text: str) -> Dict[str, List[Entity]]:
        """Extract entities using LLM with structured output."""
        prompt = f"""Extract software development entities from this text.
Return a JSON object with these keys: functions, classes, files, technologies, patterns, concepts.
Each value should be a list of objects with: text, confidence (0-1), and context.

Text: "{text[:1000]}"

JSON Output:"""

        try:
            response = llm_service.generate_text(prompt, task_type=TaskType.SIMPLE)
            cleaned = clean_llm_code_output(response)
            data = json.loads(cleaned)

            entities = defaultdict(list)
            for entity_type, entity_list in data.items():
                if isinstance(entity_list, list):
                    for item in entity_list:
                        if isinstance(item, dict) and 'text' in item:
                            entity = Entity(
                                text=item['text'],
                                type=entity_type,
                                confidence=item.get('confidence', 0.9),
                                context=item.get('context', '')
                            )
                            entities[entity_type].append(entity)

            return entities
        except Exception as e:
            logger.error(f"Failed to parse LLM entity response: {e}")
            return {}

    def _merge_entities(self, base: Dict[str, List[Entity]],
                       new: Dict[str, List[Entity]]) -> None:
        """Merge entity lists, avoiding duplicates."""
        for entity_type, entity_list in new.items():
            existing_texts = {e.text.lower() for e in base.get(entity_type, [])}

            for entity in entity_list:
                if entity.text.lower() not in existing_texts:
                    base[entity_type].append(entity)
                    existing_texts.add(entity.text.lower())

    def _link_entities(self, entities: Dict[str, List[Entity]]) -> Dict[str, List[Entity]]:
        """Link related entities and find aliases."""
        # Build entity graph
        entity_graph = defaultdict(set)

        for entity_type, entity_list in entities.items():
            for entity in entity_list:
                # Find potential aliases (e.g., MyClass vs my_class)
                aliases = self._find_aliases(entity.text, entities)
                entity.aliases = aliases

                # Link related entities
                for alias in aliases:
                    entity_graph[entity.text].add(alias)

        return entities

    def _find_aliases(self, text: str, all_entities: Dict[str, List[Entity]]) -> List[str]:
        """Find potential aliases for an entity."""
        aliases = []
        text_lower = text.lower()
        text_snake = self._to_snake_case(text)
        text_camel = self._to_camel_case(text)

        for entity_list in all_entities.values():
            for entity in entity_list:
                if entity.text != text:
                    other_lower = entity.text.lower()
                    if (other_lower == text_lower or
                        other_lower == text_snake or
                        other_lower == text_camel):
                        aliases.append(entity.text)

        return aliases

    def _extract_context(self, text: str, match: str, window: int = 50) -> str:
        """Extract context around a match."""
        index = text.find(match)
        if index == -1:
            return ""

        start = max(0, index - window)
        end = min(len(text), index + len(match) + window)
        return text[start:end].strip()

    def _pattern_to_entity_type(self, pattern_name: str) -> str:
        """Map pattern names to entity types."""
        mapping = {
            'import': 'technologies',
            'class': 'classes',
            'function': 'functions',
            'variable': 'variables',
            'file_path': 'files',
            'url': 'urls',
            'api_endpoint': 'endpoints'
        }
        return mapping.get(pattern_name, 'general_terms')

    def _to_snake_case(self, text: str) -> str:
        """Convert to snake_case."""
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def _to_camel_case(self, text: str) -> str:
        """Convert to CamelCase."""
        components = text.split('_')
        return ''.join(x.title() for x in components)

# --- Query Processing and Expansion ---
class QueryProcessor:
    """Advanced query processing with multiple expansion strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self._synonym_cache = {}

    def process(self, query: str, entities: Dict[str, List[Entity]],
                strategy: SearchStrategy = SearchStrategy.EXPANDED) -> List[str]:
        """Process and expand query based on strategy."""
        if strategy == SearchStrategy.SIMPLE:
            return [query]
        elif strategy == SearchStrategy.EXPANDED:
            return self._expand_query(query, entities)
        elif strategy == SearchStrategy.MULTI_HOP:
            return self._generate_hop_queries(query, entities)
        elif strategy == SearchStrategy.HYBRID:
            return self._hybrid_expansion(query, entities)
        else:
            return [query]

    def _expand_query(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Expand query with entity variations and synonyms."""
        expansions = [query]

        # Add entity-based expansions
        for entity_type, entity_list in entities.items():
            for entity in entity_list[:2]:  # Limit to avoid explosion
                # Add direct entity queries
                expansions.append(f"{entity.text} implementation")
                expansions.append(f"usage of {entity.text}")

                # Add alias queries
                for alias in entity.aliases[:2]:
                    expansions.append(f"{alias} {entity_type[:-1]}")

        # Add pattern-based expansions
        patterns = self._extract_query_patterns(query)
        for pattern in patterns:
            expansions.extend(self._expand_pattern(pattern, entities))

        # Deduplicate and limit
        seen = set()
        unique = []
        for exp in expansions:
            if exp not in seen and len(exp) > 3:
                seen.add(exp)
                unique.append(exp)

        return unique[:self.config.max_expansions]

    def _generate_hop_queries(self, query: str, entities: Dict[str, List[Entity]],
                             hop: int = 0) -> List[str]:
        """Generate queries for multi-hop search."""
        if hop == 0:
            return [query]

        hop_queries = []

        # Focus on different aspects based on hop
        if hop == 1:
            # Look for implementations
            for entity in self._get_top_entities(entities, 3):
                hop_queries.append(f"implementation of {entity.text}")
                hop_queries.append(f"{entity.text} definition")
        elif hop == 2:
            # Look for usages and connections
            for entity in self._get_top_entities(entities, 2):
                hop_queries.append(f"calls to {entity.text}")
                hop_queries.append(f"{entity.text} dependencies")
        else:
            # Look for related concepts
            for entity in self._get_top_entities(entities, 2):
                hop_queries.append(f"related to {entity.text}")

        return hop_queries[:self.config.max_expansions]

    def _hybrid_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Combine multiple expansion strategies."""
        expansions = []

        # Simple expansions
        expansions.extend(self._expand_query(query, entities))

        # Semantic expansions
        if self.config.use_semantic_clustering:
            expansions.extend(self._semantic_expansion(query, entities))

        # Code-aware expansions
        if self.config.use_code_understanding:
            expansions.extend(self._code_aware_expansion(query, entities))

        # Deduplicate and rank
        ranked = self._rank_expansions(expansions, query)
        return ranked[:self.config.max_expansions * 2]

    def _semantic_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Generate semantically related queries."""
        expansions = []

        # Extract key concepts
        concepts = self._extract_concepts(query)

        for concept in concepts:
            # Add conceptual queries
            expansions.append(f"examples of {concept}")
            expansions.append(f"{concept} pattern")
            expansions.append(f"best practices for {concept}")

        return expansions

    def _code_aware_expansion(self, query: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Generate code-aware query expansions."""
        expansions = []

        # Look for code patterns in query
        if "api" in query.lower():
            expansions.extend(["API routes", "endpoint definitions", "REST endpoints"])
        if "database" in query.lower():
            expansions.extend(["schema", "models", "queries"])
        if "test" in query.lower():
            expansions.extend(["unit tests", "test cases", "test fixtures"])

        return expansions

    def _extract_query_patterns(self, query: str) -> List[str]:
        """Extract patterns from query."""
        patterns = []

        # Question patterns
        question_words = ["what", "how", "where", "when", "why", "which"]
        for word in question_words:
            if word in query.lower():
                patterns.append(word)

        # Action patterns
        action_words = ["implement", "use", "call", "create", "update", "delete"]
        for word in action_words:
            if word in query.lower():
                patterns.append(word)

        return patterns

    def _expand_pattern(self, pattern: str, entities: Dict[str, List[Entity]]) -> List[str]:
        """Expand based on pattern type."""
        expansions = []

        if pattern in ["what", "where"]:
            for entity in self._get_top_entities(entities, 2):
                expansions.append(f"{pattern} is {entity.text}")
        elif pattern in ["how"]:
            for entity in self._get_top_entities(entities, 2):
                expansions.append(f"{pattern} to use {entity.text}")

        return expansions

    def _extract_concepts(self, query: str) -> List[str]:
        """Extract high-level concepts from query."""
        # Simple concept extraction - could be enhanced with NLP
        words = query.lower().split()
        concepts = []

        # Look for noun phrases
        for i, word in enumerate(words):
            if len(word) > 4:  # Simple heuristic
                if i + 1 < len(words):
                    concept = f"{word} {words[i+1]}"
                    if len(words[i+1]) > 3:
                        concepts.append(concept)
                concepts.append(word)

        return concepts[:3]

    def _rank_expansions(self, expansions: List[str], original: str) -> List[str]:
        """Rank expansions by relevance to original query."""
        scored = []

        for exp in expansions:
            score = self._calculate_relevance(exp, original)
            scored.append((exp, score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [exp for exp, _ in scored]

    def _calculate_relevance(self, expansion: str, original: str) -> float:
        """Calculate relevance score between expansion and original."""
        # Simple word overlap score
        original_words = set(original.lower().split())
        expansion_words = set(expansion.lower().split())

        overlap = len(original_words & expansion_words)
        total = len(original_words | expansion_words)

        return overlap / total if total > 0 else 0

    def _get_top_entities(self, entities: Dict[str, List[Entity]], n: int) -> List[Entity]:
        """Get top N entities by confidence."""
        all_entities = []
        for entity_list in entities.values():
            all_entities.extend(entity_list)

        all_entities.sort(key=lambda e: e.confidence, reverse=True)
        return all_entities[:n]

# --- Advanced Search and Reranking ---
class SearchOrchestrator:
    """Orchestrates complex search operations with multiple strategies."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.entity_extractor = EntityExtractor(config)
        self.query_processor = QueryProcessor(config)
        self._executor = ThreadPoolExecutor(max_workers=config.max_workers)

    def search(self, repo_id: str, query: str,
               strategy: SearchStrategy = SearchStrategy.HYBRID,
               graph: Optional[nx.DiGraph] = None) -> Dict[str, Any]:
        """Perform advanced search with specified strategy."""
        start_time = time.time()

        # Check cache
        cache_key = self._generate_cache_key(repo_id, query, strategy)
        cached_result = search_cache.get(cache_key)
        if cached_result:
            logger.info(f"Cache hit for query: {query}")
            return cached_result

        # Extract entities
        entities = self.entity_extractor.extract(query)

        # Choose search strategy
        if strategy == SearchStrategy.SIMPLE:
            results = self._simple_search(repo_id, query)
        elif strategy == SearchStrategy.EXPANDED:
            results = self._expanded_search(repo_id, query, entities)
        elif strategy == SearchStrategy.MULTI_HOP:
            results = self._multi_hop_search(repo_id, query, entities)
        elif strategy == SearchStrategy.GRAPH_GUIDED and graph:
            results = self._graph_guided_search(repo_id, query, entities, graph)
        else:
            results = self._hybrid_search(repo_id, query, entities, graph)

        # Post-process results
        results = self._post_process_results(results, query, entities)

        # Add metadata
        results['metadata'] = {
            'strategy': strategy.value,
            'entities_found': {k: len(v) for k, v in entities.items()},
            'search_time': time.time() - start_time,
            'total_results': len(results.get('chunks', []))
        }

        # Cache results
        search_cache.put(cache_key, results)

        return results

    def _simple_search(self, repo_id: str, query: str) -> Dict[str, Any]:
        """Perform simple single-query search."""
        try:
            chunks = ollama.search_index(
                query,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=self.config.default_k
            )
            return {'chunks': chunks or []}
        except Exception as e:
            logger.error(f"Simple search failed: {e}")
            return {'chunks': [], 'error': str(e)}

    def _expanded_search(self, repo_id: str, query: str,
                        entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Perform expanded search with query variations."""
        expansions = self.query_processor.process(query, entities, SearchStrategy.EXPANDED)

        all_chunks = []
        futures = []

        for expansion in expansions:
            future = self._executor.submit(
                ollama.search_index,
                expansion,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=self.config.default_k // len(expansions)
            )
            futures.append((future, expansion))

        for future, expansion in futures:
            try:
                chunks = future.result(timeout=self.config.timeout_seconds)
                if chunks:
                    for chunk in chunks:
                        chunk['query_source'] = expansion
                    all_chunks.extend(chunks)
            except Exception as e:
                logger.warning(f"Search failed for expansion '{expansion}': {e}")

        return {'chunks': all_chunks, 'expansions': expansions}

    def _multi_hop_search(self, repo_id: str, query: str,
                         entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Perform multi-hop search following references."""
        all_chunks = []
        hop_results = []
        visited_queries = set()

        current_queries = [query]
        current_entities = entities

        for hop in range(self.config.max_hops):
            if not current_queries:
                break

            hop_chunks = []
            next_queries = []
            next_entities = defaultdict(list)

            # Search current queries
            for q in current_queries:
                if q in visited_queries:
                    continue

                visited_queries.add(q)

                # Search
                result = self._expanded_search(repo_id, q, current_entities)
                chunks = result.get('chunks', [])

                if chunks:
                    # Apply hop decay to scores
                    decay = self.config.hop_decay ** hop
                    for chunk in chunks:
                        chunk['hop'] = hop
                        chunk['relevance_score'] = chunk.get('relevance_score', 1.0) * decay

                    hop_chunks.extend(chunks)

                    # Extract new entities for next hop
                    for chunk in chunks[:5]:  # Analyze top chunks
                        text = chunk.get('text', '')
                        new_entities = self.entity_extractor.extract(text, use_llm=False)
                        for entity_type, entity_list in new_entities.items():
                            next_entities[entity_type].extend(entity_list)

            if hop_chunks:
                all_chunks.extend(hop_chunks)
                hop_results.append({
                    'hop': hop,
                    'queries': current_queries,
                    'chunks_found': len(hop_chunks)
                })

            # Generate next hop queries
            if hop < self.config.max_hops - 1:
                for entity_type, entity_list in next_entities.items():
                    for entity in entity_list[:2]:  # Limit expansion
                        if entity.confidence > self.config.min_hop_relevance:
                            next_queries.append(entity.text)

            current_queries = list(set(next_queries))[:self.config.max_expansions]
            current_entities = dict(next_entities)

        return {
            'chunks': all_chunks,
            'hop_results': hop_results,
            'total_hops': len(hop_results)
        }

    def _graph_guided_search(self, repo_id: str, query: str,
                           entities: Dict[str, List[Entity]],
                           graph: nx.DiGraph) -> Dict[str, Any]:
        """Use graph structure to guide search."""
        # Find relevant nodes in graph
        relevant_nodes = self._find_relevant_nodes(graph, entities)

        if not relevant_nodes:
            # Fallback to expanded search
            return self._expanded_search(repo_id, query, entities)

        all_chunks = []
        node_results = []

        # Search for each relevant node and its neighbors
        for node_id, relevance_score in relevant_nodes[:10]:
            node_data = graph.nodes[node_id]

            # Generate targeted query
            node_query = self._generate_node_query(node_id, node_data, query)

            # Search
            chunks = ollama.search_index(
                node_query,
                "snowflake-arctic-embed2:latest",
                repo_id,
                k=5
            )

            if chunks:
                for chunk in chunks:
                    chunk['graph_node'] = node_id
                    chunk['node_relevance'] = relevance_score

                all_chunks.extend(chunks)

                node_results.append({
                    'node': node_id,
                    'query': node_query,
                    'chunks_found': len(chunks)
                })

        return {
            'chunks': all_chunks,
            'node_results': node_results,
            'graph_nodes_searched': len(relevant_nodes)
        }

    def _hybrid_search(self, repo_id: str, query: str,
                      entities: Dict[str, List[Entity]],
                      graph: Optional[nx.DiGraph] = None) -> Dict[str, Any]:
        """Combine multiple search strategies."""
        all_results = []

        # Parallel execution of different strategies
        futures = [
            (self._executor.submit(self._expanded_search, repo_id, query, entities), 'expanded'),
            (self._executor.submit(self._multi_hop_search, repo_id, query, entities), 'multi_hop')
        ]

        if graph:
            futures.append(
                (self._executor.submit(self._graph_guided_search, repo_id, query, entities, graph), 'graph_guided')
            )

        strategy_results = {}
        all_chunks = []

        for future, strategy_name in futures:
            try:
                result = future.result(timeout=self.config.timeout_seconds)
                strategy_results[strategy_name] = result
                all_chunks.extend(result.get('chunks', []))
            except Exception as e:
                logger.warning(f"Strategy {strategy_name} failed: {e}")

        # Merge and deduplicate results
        unique_chunks = self._deduplicate_chunks(all_chunks)

        return {
            'chunks': unique_chunks,
            'strategy_results': strategy_results,
            'strategies_used': list(strategy_results.keys())
        }

    def _find_relevant_nodes(self, graph: nx.DiGraph,
                           entities: Dict[str, List[Entity]]) -> List[Tuple[str, float]]:
        """Find nodes in graph relevant to entities."""
        node_scores = []

        for node_id, node_data in graph.nodes(data=True):
            score = 0.0

            # Score based on entity matches
            for entity_type, entity_list in entities.items():
                for entity in entity_list:
                    if entity.text.lower() in node_id.lower():
                        score += entity.confidence * 2
                    if entity.text.lower() in str(node_data.get('name', '')).lower():
                        score += entity.confidence

                    # Check aliases
                    for alias in entity.aliases:
                        if alias.lower() in node_id.lower():
                            score += entity.confidence * 0.5

            if score > 0:
                node_scores.append((node_id, score))

        # Sort by score
        node_scores.sort(key=lambda x: x[1], reverse=True)
        return node_scores

    def _generate_node_query(self, node_id: str, node_data: Dict[str, Any],
                           original_query: str) -> str:
        """Generate a query specific to a graph node."""
        node_name = node_data.get('name', node_id)
        node_type = node_data.get('type', '')
        file_path = node_data.get('file', '')

        # Combine node info with original query intent
        if file_path:
            return f"{node_name} in {file_path} {original_query}"
        else:
            return f"{node_type} {node_name} {original_query}"

    def _post_process_results(self, results: Dict[str, Any], query: str,
                            entities: Dict[str, List[Entity]]) -> Dict[str, Any]:
        """Post-process search results with reranking and clustering."""
        chunks = results.get('chunks', [])

        if not chunks:
            return results

        # Deduplicate
        chunks = self._deduplicate_chunks(chunks)

        # Rerank
        chunks = self._rerank_chunks(chunks, query, entities)

        # Cluster if enabled
        if self.config.use_semantic_clustering and len(chunks) > 10:
            clusters = self._cluster_chunks(chunks)
            results['clusters'] = clusters

        # Select top chunks with diversity
        final_chunks = self._select_diverse_chunks(chunks, self.config.default_k)

        results['chunks'] = final_chunks
        results['total_before_filtering'] = len(chunks)

        return results

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Advanced deduplication using content similarity."""
        if not chunks:
            return []

        unique_chunks = []
        seen_hashes = set()
        seen_content = []

        for chunk in chunks:
            # Quick hash check
            chunk_hash = self._calculate_chunk_hash(chunk)
            if chunk_hash in seen_hashes:
                continue

            # Semantic similarity check
            if self._is_duplicate(chunk, seen_content):
                continue

            seen_hashes.add(chunk_hash)
            seen_content.append(chunk)
            unique_chunks.append(chunk)

        return unique_chunks

    def _calculate_chunk_hash(self, chunk: Dict[str, Any]) -> str:
        """Calculate hash for chunk."""
        text = chunk.get('text', '')
        file_path = chunk.get('file_path', '')
        # Use more of the content for better deduplication
        content = f"{file_path}:{text[:200]}"
        return hashlib.md5(content.encode()).hexdigest()

    def _is_duplicate(self, chunk: Dict[str, Any],
                     seen_content: List[Dict[str, Any]]) -> bool:
        """Check if chunk is semantically similar to seen content."""
        chunk_text = chunk.get('text', '').lower()
        chunk_words = set(chunk_text.split())

        for seen_chunk in seen_content:
            seen_text = seen_chunk.get('text', '').lower()
            seen_words = set(seen_text.split())

            # Calculate Jaccard similarity
            intersection = len(chunk_words & seen_words)
            union = len(chunk_words | seen_words)

            if union > 0:
                similarity = intersection / union
                if similarity > self.config.similarity_threshold:
                    return True

        return False

    def _rerank_chunks(self, chunks: List[Dict[str, Any]], query: str,
                      entities: Dict[str, List[Entity]]) -> List[Dict[str, Any]]:
        """Advanced reranking with multiple signals."""
        # Extract ranking features
        for chunk in chunks:
            features = self._extract_ranking_features(chunk, query, entities)

            # Calculate composite score
            score = (
                features['text_relevance'] * 0.3 +
                features['entity_coverage'] * 0.3 +
                features['code_quality'] * 0.2 +
                features['freshness'] * 0.1 +
                features['structural_relevance'] * 0.1
            )

            # Apply existing scores
            if 'relevance_score' in chunk:
                score = score * 0.7 + chunk['relevance_score'] * 0.3

            chunk['final_score'] = score
            chunk['ranking_features'] = features

        # Sort by final score
        chunks.sort(key=lambda x: x.get('final_score', 0), reverse=True)

        return chunks

    def _extract_ranking_features(self, chunk: Dict[str, Any], query: str,
                                entities: Dict[str, List[Entity]]) -> Dict[str, float]:
        """Extract features for ranking."""
        text = chunk.get('text', '').lower()
        file_path = chunk.get('file_path', '').lower()

        features = {
            'text_relevance': self._calculate_text_relevance(text, query),
            'entity_coverage': self._calculate_entity_coverage(text, entities),
            'code_quality': self._estimate_code_quality(text),
            'freshness': self._calculate_freshness(chunk),
            'structural_relevance': self._calculate_structural_relevance(chunk)
        }

        return features

    def _calculate_text_relevance(self, text: str, query: str) -> float:
        """Calculate text relevance to query."""
        query_terms = set(query.lower().split())
        text_terms = set(text.split())

        # Term frequency
        term_freq = sum(1 for term in query_terms if term in text_terms)

        # Normalize by query length
        relevance = term_freq / len(query_terms) if query_terms else 0

        # Boost for exact phrase match
        if query.lower() in text:
            relevance += 0.5

        return min(relevance, 1.0)

    def _calculate_entity_coverage(self, text: str,
                                 entities: Dict[str, List[Entity]]) -> float:
        """Calculate how many entities are covered in text."""
        total_entities = sum(len(v) for v in entities.values())
        if total_entities == 0:
            return 0.5  # Neutral score

        covered = 0
        for entity_list in entities.values():
            for entity in entity_list:
                if entity.text.lower() in text:
                    covered += entity.confidence
                # Check aliases
                for alias in entity.aliases:
                    if alias.lower() in text:
                        covered += entity.confidence * 0.5
                        break

        return min(covered / total_entities, 1.0)

    def _estimate_code_quality(self, text: str) -> float:
        """Estimate code quality based on heuristics."""
        quality_score = 0.5  # Base score

        # Check for code indicators
        code_indicators = [
            (r'def\s+\w+\s*\(', 0.1),  # Functions
            (r'class\s+\w+', 0.1),      # Classes
            (r'import\s+\w+', 0.05),    # Imports
            (r'"""[\s\S]+?"""', 0.1),   # Docstrings
            (r'#\s*\w+', 0.05),         # Comments
        ]

        for pattern, score in code_indicators:
            if re.search(pattern, text):
                quality_score += score

        # Penalty for too long or too short
        length = len(text)
        if length < 50:
            quality_score *= 0.5
        elif length > 2000:
            quality_score *= 0.8

        return min(quality_score, 1.0)

    def _calculate_freshness(self, chunk: Dict[str, Any]) -> float:
        """Calculate freshness score (placeholder for timestamp-based scoring)."""
        # Could be enhanced with actual file timestamps
        return 0.5

    def _calculate_structural_relevance(self, chunk: Dict[str, Any]) -> float:
        """Calculate relevance based on structural properties."""
        score = 0.5

        # Boost for certain file types
        file_path = chunk.get('file_path', '').lower()
        if file_path.endswith(('.py', '.js', '.java')):
            score += 0.2
        elif file_path.endswith(('.md', '.txt', '.rst')):
            score += 0.1

        # Boost for specific directories
        important_dirs = ['src', 'core', 'api', 'routes', 'models', 'services']
        for dir_name in important_dirs:
            if dir_name in file_path:
                score += 0.1
                break

        return min(score, 1.0)

    def _cluster_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Cluster chunks by semantic similarity."""
        # Simple clustering based on shared entities
        clusters = []
        clustered = set()

        for i, chunk in enumerate(chunks):
            if i in clustered:
                continue

            cluster = {
                'id': f'cluster_{i}',
                'chunks': [chunk],
                'representative': chunk,
                'size': 1
            }

            # Find similar chunks
            for j, other_chunk in enumerate(chunks[i+1:], i+1):
                if j in clustered:
                    continue

                if self._chunks_similar(chunk, other_chunk):
                    cluster['chunks'].append(other_chunk)
                    cluster['size'] += 1
                    clustered.add(j)

            clusters.append(cluster)

        return clusters

    def _chunks_similar(self, chunk1: Dict[str, Any],
                       chunk2: Dict[str, Any]) -> bool:
        """Check if two chunks are similar enough to cluster."""
        # File-based clustering
        if chunk1.get('file_path') == chunk2.get('file_path'):
            return True

        # Content-based clustering
        text1 = set(chunk1.get('text', '').lower().split())
        text2 = set(chunk2.get('text', '').lower().split())

        if len(text1) == 0 or len(text2) == 0:
            return False

        overlap = len(text1 & text2)
        smaller = min(len(text1), len(text2))

        return (overlap / smaller) > 0.5

    def _select_diverse_chunks(self, chunks: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """Select top k chunks with diversity."""
        if len(chunks) <= k:
            return chunks

        selected = []
        remaining = chunks.copy()

        # Select best chunk
        if remaining:
            selected.append(remaining.pop(0))

        # Select diverse chunks
        while len(selected) < k and remaining:
            # Find chunk most different from selected
            best_chunk = None
            best_diversity = -1

            for chunk in remaining:
                diversity = self._calculate_diversity(chunk, selected)
                if diversity > best_diversity:
                    best_diversity = diversity
                    best_chunk = chunk

            if best_chunk:
                selected.append(best_chunk)
                remaining.remove(best_chunk)

        return selected

    def _calculate_diversity(self, chunk: Dict[str, Any],
                           selected: List[Dict[str, Any]]) -> float:
        """Calculate diversity of chunk compared to selected chunks."""
        if not selected:
            return 1.0

        chunk_text = set(chunk.get('text', '').lower().split())

        min_similarity = 1.0
        for sel_chunk in selected:
            sel_text = set(sel_chunk.get('text', '').lower().split())

            if len(chunk_text) == 0 or len(sel_text) == 0:
                similarity = 0
            else:
                overlap = len(chunk_text & sel_text)
                union = len(chunk_text | sel_text)
                similarity = overlap / union if union > 0 else 0

            min_similarity = min(min_similarity, similarity)

        return 1.0 - min_similarity

    def _generate_cache_key(self, repo_id: str, query: str,
                          strategy: SearchStrategy) -> str:
        """Generate cache key for search results."""
        content = f"{repo_id}:{query}:{strategy.value}"
        return hashlib.md5(content.encode()).hexdigest()

    def close(self):
        """Clean up resources."""
        self._executor.shutdown(wait=True)

# --- Graph Analysis ---
def _build_knowledge_graph(graph_data: Dict[str, Any]) -> nx.DiGraph:
    """Builds a networkx DiGraph from the architectural graph data."""
    if not isinstance(graph_data, dict):
        raise GraphBuildingError(f"Graph data must be a dictionary, got {type(graph_data)}")

    try:
        G = nx.DiGraph()
        nodes = graph_data.get("nodes", {})
        edges = graph_data.get("edges", [])

        if not isinstance(nodes, dict):
            logger.warning(f"Nodes data is not a dictionary, got {type(nodes)}. Using empty dict.")
            nodes = {}

        if not isinstance(edges, list):
            logger.warning(f"Edges data is not a list, got {type(edges)}. Using empty list.")
            edges = []

        # Add nodes with validation
        nodes_added = 0
        for node_id, node_data in nodes.items():
            try:
                if not isinstance(node_id, str):
                    logger.warning(f"Skipping node with invalid ID type: {type(node_id)}")
                    continue

                if not isinstance(node_data, dict):
                    logger.warning(f"Node {node_id} has invalid data type: {type(node_data)}. Using empty dict.")
                    node_data = {}

                G.add_node(node_id, **node_data)
                nodes_added += 1
            except Exception as e:
                logger.warning(f"Failed to add node {node_id}: {e}")
                continue

        # Add edges with validation
        edges_added = 0
        for edge in edges:
            try:
                if not isinstance(edge, dict):
                    logger.warning(f"Skipping invalid edge: {edge}")
                    continue

                source = edge.get("source")
                target = edge.get("target")

                if not source or not target:
                    logger.warning(f"Edge missing source or target: {edge}")
                    continue

                if not isinstance(source, str) or not isinstance(target, str):
                    logger.warning(f"Edge source/target must be strings: {edge}")
                    continue

                # Only add edge if source exists in graph
                if source in G:
                    edge_type = edge.get("type", "unknown")
                    G.add_edge(source, target, type=edge_type)
                    edges_added += 1
                else:
                    logger.warning(f"Source node {source} not found in graph for edge to {target}")
            except Exception as e:
                logger.warning(f"Failed to add edge {edge}: {e}")
                continue

        logger.info(f"Built knowledge graph with {nodes_added} nodes and {edges_added} edges.")

        if nodes_added == 0:
            raise GraphBuildingError("No valid nodes were added to the graph")

        return G

    except Exception as e:
        if isinstance(e, GraphBuildingError):
            raise
        raise GraphBuildingError(f"Failed to build knowledge graph: {e}")

# --- Context Building ---
class ContextBuilder:
    """Builds context for LLM synthesis from search results."""

    def __init__(self, config: RAGConfig):
        self.config = config

    def build(self, search_results: Dict[str, Any],
              question: str,
              is_specific: bool = False) -> str:
        """Build context string from search results."""
        chunks = search_results.get('chunks', [])

        if not chunks:
            return "No relevant code context was found for your question."

        # Group chunks by file for better organization
        chunks_by_file = defaultdict(list)
        for chunk in chunks:
            file_path = chunk.get('file_path', 'unknown')
            chunks_by_file[file_path].append(chunk)

        # Build context with structure
        context_parts = []

        # Add metadata if available
        if 'metadata' in search_results:
            meta = search_results['metadata']
            context_parts.append(f"[Search performed using {meta.get('strategy', 'unknown')} strategy, found {meta.get('total_results', 0)} results in {meta.get('search_time', 0):.2f}s]")

        # Add clustered results if available
        if 'clusters' in search_results:
            context_parts.append(self._format_clusters(search_results['clusters']))
        else:
            # Format chunks by file
            for file_path, file_chunks in chunks_by_file.items():
                context_parts.append(self._format_file_chunks(file_path, file_chunks))

        # Add hop information if multi-hop search
        if 'hop_results' in search_results:
            context_parts.append(self._format_hop_results(search_results['hop_results']))

        return "\n\n".join(context_parts)

    def _format_file_chunks(self, file_path: str, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks from a single file."""
        parts = [f"=== File: {file_path} ==="]

        for i, chunk in enumerate(chunks[:5]):  # Limit chunks per file
            text = chunk.get('text', '').strip()
            if text:
                parts.append(f"\n[Chunk {i+1}]")
                if 'final_score' in chunk:
                    parts.append(f"Relevance: {chunk['final_score']:.2f}")
                parts.append(text)

        return "\n".join(parts)

    def _format_clusters(self, clusters: List[Dict[str, Any]]) -> str:
        """Format clustered results."""
        parts = ["=== Clustered Results ==="]

        for cluster in clusters[:5]:  # Limit clusters
            parts.append(f"\n[Cluster: {cluster['id']} - {cluster['size']} related chunks]")

            # Show representative chunk
            rep = cluster['representative']
            parts.append(f"File: {rep.get('file_path', 'unknown')}")
            parts.append(rep.get('text', '')[:500])  # Truncate

            if cluster['size'] > 1:
                parts.append(f"... and {cluster['size'] - 1} similar chunks")

        return "\n".join(parts)

    def _format_hop_results(self, hop_results: List[Dict[str, Any]]) -> str:
        """Format multi-hop search results."""
        parts = ["=== Multi-hop Search Path ==="]

        for hop in hop_results:
            parts.append(f"\nHop {hop['hop'] + 1}:")
            parts.append(f"Queries: {', '.join(hop['queries'][:3])}")
            parts.append(f"Found: {hop['chunks_found']} results")

        return "\n".join(parts)

# --- Main Orchestration ---
class OracleService:
    """Enhanced Oracle service with PhD-level RAG capabilities."""

    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or rag_config
        self.search_orchestrator = SearchOrchestrator(self.config)
        self.context_builder = ContextBuilder(self.config)
        self.entity_extractor = EntityExtractor(self.config)

    def answer_question(self, repo_id: str, question: str,
                       use_enhanced_rag: bool = True) -> Dict[str, Any]:
        """
        Main entry point for answering questions with enhanced RAG.
        Maintains backward compatibility while adding advanced features.
        """
        # Input validation
        if not repo_id or not isinstance(repo_id, str):
            return {"error": "Invalid repository ID provided"}

        if not question or not isinstance(question, str):
            return {"error": "Invalid question provided"}

        logger.info(f"The Oracle received a question for repo '{repo_id}': '{question}'")

        # Load repository data
        try:
            cortex_data = cortex_service.load_cortex_data(repo_id)
            if not isinstance(cortex_data, dict):
                return {"error": "Invalid cortex data format"}

            graph_data = cortex_data.get("architectural_graph")
            if not graph_data:
                return {"error": "Architectural graph not found for this repository."}

        except cortex_service.CortexFileNotFound:
            return {"error": f"Cortex file for repo '{repo_id}' not found. Please ingest the repo first."}
        except cortex_service.CortexFileMalformed:
            return {"error": f"Cortex file for repo '{repo_id}' is corrupted. Please re-ingest the repo."}
        except Exception as e:
            logger.error(f"Error loading cortex data for repo {repo_id}: {e}")
            return {"error": "Failed to load repository data. Please try again later."}

        # Build graph
        try:
            graph = _build_knowledge_graph(graph_data)
        except GraphBuildingError as e:
            logger.error(f"Graph building failed: {e}")
            return {"error": "Failed to analyze repository structure. The repository data may be corrupted."}
        except Exception as e:
            logger.error(f"Unexpected error building graph: {e}")
            return {"error": "An unexpected error occurred while analyzing the repository structure."}

        # Extract entities
        try:
            entities_dict = self.entity_extractor.extract(question)
            # Convert to old format for compatibility
            entities = self._convert_entities_format(entities_dict)
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            entities = self._get_default_entities()

        # Determine search strategy
        strategy = self._determine_strategy(question, entities, use_enhanced_rag)

        # Perform search
        try:
            search_results = self.search_orchestrator.search(
                repo_id, question, strategy, graph
            )
        except Exception as e:
            logger.error(f"Search failed: {e}")
            search_results = {'chunks': [], 'error': str(e)}

        # Build context
        context_string = self.context_builder.build(
            search_results, question,
            is_specific=self._is_specific_question(entities)
        )

        # Determine prompt heading
        if self._is_specific_question(entities):
            prompt_heading = "ARCHITECTURAL IMPACT ANALYSIS"
        else:
            prompt_heading = "RELEVANT CODE & COMMENTARY"

        # Generate final answer
        try:
            answer = self._synthesize_answer(
                question, context_string, prompt_heading,
                entities, search_results
            )

            response = {"answer": answer}

            # Add metadata if enhanced RAG was used
            if use_enhanced_rag and 'metadata' in search_results:
                response["search_metadata"] = search_results['metadata']

            return response

        except Exception as e:
            logger.error(f"Final synthesis failed: {e}")
            return {"error": "An unexpected error occurred while generating the final answer."}

    def _determine_strategy(self, question: str, entities: Dict[str, List[str]],
                          use_enhanced: bool) -> SearchStrategy:
        """Determine the best search strategy for the question."""
        if not use_enhanced:
            return SearchStrategy.SIMPLE

        # Check question complexity
        question_lower = question.lower()

        # Multi-hop indicators
        if any(word in question_lower for word in ["how does", "workflow", "process", "trace"]):
            return SearchStrategy.MULTI_HOP

        # Graph-guided indicators
        if any(word in question_lower for word in ["impact", "depends", "affects", "uses"]):
            return SearchStrategy.GRAPH_GUIDED

        # Complex questions need hybrid approach
        if len(question.split()) > 15 or len(entities.get('general_terms', [])) > 3:
            return SearchStrategy.HYBRID

        # Default to expanded search
        return SearchStrategy.EXPANDED

    def _convert_entities_format(self, entities_dict: Dict[str, List[Entity]]) -> Dict[str, List[str]]:
        """Convert new entity format to old format for compatibility."""
        old_format = {}
        for entity_type, entity_list in entities_dict.items():
            old_format[entity_type] = [e.text for e in entity_list]
        return old_format

    def _get_default_entities(self) -> Dict[str, List[str]]:
        """Returns default empty entity structure."""
        return {
            "functions": [],
            "classes": [],
            "files": [],
            "general_terms": [],
            "code_patterns": [],
            "technologies": []
        }

    def _is_specific_question(self, entities: Dict[str, List[str]]) -> bool:
        """Check if question is about specific code entities."""
        return any(entities.get(k) for k in ["functions", "classes", "files"])

    def _synthesize_answer(self, question: str, context: str,
                         prompt_heading: str, entities: Dict[str, List[str]],
                         search_results: Dict[str, Any]) -> str:
        """Synthesize final answer using LLM."""
        # Self-reflection prompt if enabled
        reflection = ""
        if self.config.enable_self_reflection and search_results.get('error'):
            reflection = f"""
**Search Challenges:**
The search encountered some difficulties: {search_results.get('error')}
This may affect the completeness of the answer.
"""

        synthesis_prompt = f"""You are The Oracle, an advanced AI system that understands code architecture at a PhD level.
Your purpose is to provide deep, insightful answers by combining architectural graph analysis with comprehensive source code understanding.

**DEVELOPER'S QUESTION:**
{question}

**{prompt_heading}:**
{context}

**EXTRACTED ENTITIES:**
- Functions: {entities.get('functions', [])}
- Classes: {entities.get('classes', [])}
- Files: {entities.get('files', [])}
- Technologies: {entities.get('technologies', [])}
- Patterns: {entities.get('code_patterns', [])}

{reflection}

---
Based on ALL the evidence above, provide a comprehensive answer that:
1. **Directly answers** the developer's question with specific details
2. **Cites evidence** from the code snippets to support your answer
3. **Explains architectural implications** and design patterns observed
4. **Identifies potential issues** or areas for improvement
5. **Suggests next steps** for deeper investigation if needed

Use clear markdown formatting with headers, code blocks, and lists where appropriate.
Be specific and technical, but also explain the "why" behind the code structure.
"""

        answer = llm_service.generate_text(synthesis_prompt, task_type=TaskType.COMPLEX_REASONING)
        return answer

    def close(self):
        """Clean up resources."""
        self.search_orchestrator.close()

# --- Global Oracle Instance ---
_oracle_instance = None

def get_oracle() -> OracleService:
    """Get or create the global Oracle instance."""
    global _oracle_instance
    if _oracle_instance is None:
        _oracle_instance = OracleService()
    return _oracle_instance

# --- Public API (Backward Compatible) ---
def answer_question(repo_id: str, question: str, use_enhanced_rag: bool = True) -> Dict[str, Any]:
    """
    Public API for answering questions. Maintains backward compatibility.
    """
    oracle = get_oracle()
    return oracle.answer_question(repo_id, question, use_enhanced_rag)

def answer_question_legacy(repo_id: str, question: str) -> Dict[str, Any]:
    """
    Legacy wrapper that uses the original RAG behavior.
    For backward compatibility with existing code.
    """
    return answer_question(repo_id, question, use_enhanced_rag=False)

def perform_semantic_search(repo_id: str, query: str, search_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Public interface for performing semantic search with enhanced RAG capabilities.
    """
    oracle = get_oracle()
    config = search_config or {}

    # Determine strategy from config
    strategy = SearchStrategy.EXPANDED
    if config.get('use_multi_hop'):
        strategy = SearchStrategy.MULTI_HOP
    elif config.get('use_graph'):
        strategy = SearchStrategy.GRAPH_GUIDED

    # Perform search
    results = oracle.search_orchestrator.search(repo_id, query, strategy)

    # Format for API
    return {
        "results": results.get('chunks', [])[:config.get('k', 15)],
        "total": len(results.get('chunks', [])),
        "query": query,
        "metadata": results.get('metadata', {})
    }

def analyze_code_impact(repo_id: str, file_path: str, function_or_class: str) -> Dict[str, Any]:
    """
    Analyzes the impact of changes to a specific code entity.
    """
    question = f"What is the impact of changing {function_or_class} in {file_path}?"
    oracle = get_oracle()

    # Use graph-guided search for impact analysis
    result = oracle.answer_question(repo_id, question, use_enhanced_rag=True)

    # Extract specific impact information
    return {
        "entity": function_or_class,
        "file": file_path,
        "analysis": result.get("answer", ""),
        "metadata": result.get("search_metadata", {})
    }

# --- Configuration Management ---
def update_config(new_config: Dict[str, Any]) -> None:
    """Update RAG configuration."""
    global rag_config
    for key, value in new_config.items():
        if hasattr(rag_config, key):
            setattr(rag_config, key, value)

    # Clear cache on config change
    search_cache.clear()

def get_config() -> RAGConfig:
    """Get current RAG configuration."""
    return rag_config

# --- Cleanup ---
def cleanup():
    """Clean up resources on shutdown."""
    global _oracle_instance
    if _oracle_instance:
        _oracle_instance.close()
        _oracle_instance = None

# Register cleanup
import atexit
atexit.register(cleanup)

--- FILE_END: backend/lumiere_core/services/oracle_service.py ---

--- FILE_START: backend/lumiere_core/services/ollama_service.py ---
# In backend/lumiere_core/services/ollama_service.py

import ollama
import requests 
from typing import Dict, List

def generate_text(prompt: str, model_name: str = 'qwen3:4b') -> str:
    """
    Sends a prompt to a local Ollama model and returns the response.

    Args:
        prompt: The full prompt to send to the model.
        model_name: The name of the Ollama model to use for generation.

    Returns:
        The generated text content from the model.
    """
    print(f"Sending prompt to local Ollama model: '{model_name}'...")
    try:
        client = ollama.Client()
        response = client.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response['message']['content']
    except Exception as e:
        return (f"An error occurred while communicating with the Ollama server: {e}\n"
                f"Please ensure the Ollama server is running and the model '{model_name}' is available.")

# === REPLACE THE ENTIRE list_models FUNCTION WITH THIS NEW VERSION ===
def list_models() -> List[Dict[str, str]]:
    """
    Fetches the list of locally available Ollama models by calling the API directly.
    This is more robust than relying on the library's internal list() parsing.
    """
    print("Fetching available local Ollama models via direct API call...")
    try:
        # Use requests to call the /api/tags endpoint directly
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        data = response.json()

        available = []
        # The structure from the API is {"models": [...]}, so we access the 'models' key
        for model_data in data.get('models', []):
            # The model's full name (e.g., "qwen3:4b") is in the 'name' key
            full_name = model_data.get('name')
            if not full_name:
                continue  # Skip if a model entry is malformed

            model_id = f"ollama/{full_name}"
            available.append({
                "id": model_id,
                "provider": "ollama",
                "name": full_name,  # Use the full name for display as well
            })

        if not available:
            print("Ollama API responded, but no local models were found.")
        else:
            print(f"✓ Found {len(available)} local Ollama models.")

        return available

    except requests.exceptions.ConnectionError:
        print("Could not fetch Ollama models. Is the Ollama server running?")
        return []
    except requests.exceptions.RequestException as e:
        print(f"An error occurred while calling the Ollama API: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/ollama_service.py ---

--- FILE_START: backend/lumiere_core/services/graph_differ.py ---
# backend/lumiere_core/services/graph_differ.py

import networkx as nx
from typing import Dict, List, Tuple, Iterable


def compare_graphs(base_graph: nx.DiGraph, head_graph: nx.DiGraph) -> Dict[str, List[str]]:
    """
    Compares two architectural graphs (base vs. head) and identifies changes.

    Args:
        base_graph (nx.DiGraph): Graph from the base branch.
        head_graph (nx.DiGraph): Graph from the head branch (e.g. a PR).

    Returns:
        Dict[str, List[str]]: A dictionary with:
            - 'nodes_added': New nodes in the head graph.
            - 'nodes_removed': Nodes missing from the head graph.
            - 'edges_added': New edges (formatted as 'A -> B').
            - 'edges_removed': Missing edges (formatted as 'A -> B').
    """
    def format_edges(edges: Iterable[Tuple[str, str]]) -> List[str]:
        return [f"{u} -> {v}" for u, v in sorted(edges)]

    base_nodes = set(base_graph.nodes)
    head_nodes = set(head_graph.nodes)

    base_edges = set(base_graph.edges)
    head_edges = set(head_graph.edges)

    return {
        "nodes_added": sorted(head_nodes - base_nodes),
        "nodes_removed": sorted(base_nodes - head_nodes),
        "edges_added": format_edges(head_edges - base_edges),
        "edges_removed": format_edges(base_edges - head_edges),
    }

--- FILE_END: backend/lumiere_core/services/graph_differ.py ---

--- FILE_START: backend/lumiere_core/services/crucible.py ---
# In backend/lumiere_core/services/crucible.py

import os
import uuid
import json
import time
from pathlib import Path
from typing import Dict, Tuple, Optional, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime

import docker
from docker.errors import BuildError, ContainerError, APIError, DockerException

from .utils import clean_llm_code_output
from ingestion.crawler import IntelligentCrawler

# --- Enhanced Data Structures ---
@dataclass
class ValidationResult:
    """Enhanced result structure with detailed metrics"""
    status: str
    logs: str
    execution_time: float = 0.0
    build_time: float = 0.0
    test_time: float = 0.0
    project_type: str = "unknown"
    image_size: Optional[int] = None
    warnings: List[str] = None
    detected_languages: List[str] = None
    build_tools: List[str] = None

    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
        if self.detected_languages is None:
            self.detected_languages = []
        if self.build_tools is None:
            self.build_tools = []

    def to_dict(self) -> Dict:
        """Convert to dictionary for backward compatibility"""
        result = {"status": self.status, "logs": self.logs}
        # Add enhanced fields only if they have meaningful values
        if self.execution_time > 0:
            result["execution_time"] = self.execution_time
        if self.build_time > 0:
            result["build_time"] = self.build_time
        if self.test_time > 0:
            result["test_time"] = self.test_time
        if self.project_type != "unknown":
            result["project_type"] = self.project_type
        if self.image_size:
            result["image_size_mb"] = round(self.image_size / (1024 * 1024), 2)
        if self.warnings:
            result["warnings"] = self.warnings
        if self.detected_languages:
            result["detected_languages"] = self.detected_languages
        if self.build_tools:
            result["build_tools"] = self.build_tools
        return result

# --- Enhanced Environment Detection (Polyglot Support) ---
def _detect_project_ecosystem(repo_path: Path) -> Tuple[str, List[str], List[str]]:
    """
    Enhanced project detection supporting multiple languages and ecosystems.
    Returns: (primary_project_type, detected_languages, build_tools)
    """
    detected_languages = []
    build_tools = []
    project_types = []

    # File-based detection patterns
    detection_patterns = {
        # Python ecosystem
        "python": {
            "files": ["requirements.txt", "pyproject.toml", "setup.py", "Pipfile",
                     "poetry.lock", "conda.yml", "environment.yml", "setup.cfg"],
            "extensions": [".py"],
            "build_tools": ["pip", "poetry", "pipenv", "conda"]
        },
        # Node.js ecosystem
        "node": {
            "files": ["package.json"],
            "extensions": [".js", ".ts", ".jsx", ".tsx"],
            "build_tools": ["npm", "yarn", "pnpm"]
        },
        # Java ecosystem
        "java": {
            "files": ["pom.xml", "build.gradle", "build.gradle.kts", "gradle.properties"],
            "extensions": [".java"],
            "build_tools": ["maven", "gradle"]
        },
        # .NET ecosystem
        "dotnet": {
            "files": ["*.csproj", "*.vbproj", "*.fsproj", "*.sln", "global.json"],
            "extensions": [".cs", ".vb", ".fs"],
            "build_tools": ["dotnet"]
        },
        # Go ecosystem
        "go": {
            "files": ["go.mod", "go.sum"],
            "extensions": [".go"],
            "build_tools": ["go"]
        },
        # Rust ecosystem
        "rust": {
            "files": ["Cargo.toml", "Cargo.lock"],
            "extensions": [".rs"],
            "build_tools": ["cargo"]
        },
        # C/C++ ecosystem
        "cpp": {
            "files": ["CMakeLists.txt", "Makefile", "makefile", "meson.build", "conanfile.txt"],
            "extensions": [".c", ".cpp", ".cxx", ".cc", ".h", ".hpp"],
            "build_tools": ["cmake", "make", "meson", "conan"]
        },
        # Swift ecosystem
        "swift": {
            "files": ["Package.swift", "*.xcodeproj", "*.xcworkspace"],
            "extensions": [".swift"],
            "build_tools": ["swift", "xcodebuild"]
        },
        # Ruby ecosystem
        "ruby": {
            "files": ["Gemfile", "Gemfile.lock", "Rakefile", "*.gemspec"],
            "extensions": [".rb"],
            "build_tools": ["bundler", "rake", "gem"]
        },
        # PHP ecosystem
        "php": {
            "files": ["composer.json", "composer.lock"],
            "extensions": [".php"],
            "build_tools": ["composer"]
        },
        # Dart/Flutter ecosystem
        "dart": {
            "files": ["pubspec.yaml", "pubspec.lock"],
            "extensions": [".dart"],
            "build_tools": ["pub", "flutter"]
        },
        # Kotlin ecosystem
        "kotlin": {
            "files": ["build.gradle.kts"],
            "extensions": [".kt", ".kts"],
            "build_tools": ["gradle"]
        },
        # Scala ecosystem
        "scala": {
            "files": ["build.sbt", "project/build.properties"],
            "extensions": [".scala"],
            "build_tools": ["sbt"]
        },
        # Clojure ecosystem
        "clojure": {
            "files": ["project.clj", "deps.edn", "build.boot"],
            "extensions": [".clj", ".cljs", ".cljc"],
            "build_tools": ["leiningen", "clojure"]
        },
        # Haskell ecosystem
        "haskell": {
            "files": ["*.cabal", "stack.yaml", "cabal.project"],
            "extensions": [".hs", ".lhs"],
            "build_tools": ["cabal", "stack"]
        },
        # Elixir ecosystem
        "elixir": {
            "files": ["mix.exs", "mix.lock"],
            "extensions": [".ex", ".exs"],
            "build_tools": ["mix"]
        },
        # R ecosystem
        "r": {
            "files": ["DESCRIPTION", "renv.lock", ".Rprofile"],
            "extensions": [".r", ".R"],
            "build_tools": ["R"]
        }
    }

    # Check for file indicators
    for ecosystem, config in detection_patterns.items():
        found_files = []
        for pattern in config["files"]:
            if "*" in pattern:
                found_files.extend(repo_path.glob(pattern))
            else:
                if (repo_path / pattern).exists():
                    found_files.append(pattern)

        if found_files:
            detected_languages.append(ecosystem)
            build_tools.extend(config["build_tools"])
            project_types.append(ecosystem)

    # Check for source code files if no config files found
    if not detected_languages:
        for ecosystem, config in detection_patterns.items():
            for ext in config["extensions"]:
                if list(repo_path.rglob(f"*{ext}")):
                    detected_languages.append(ecosystem)
                    build_tools.extend(config["build_tools"])
                    project_types.append(ecosystem)
                    break

    # Determine primary project type
    if project_types:
        # Priority order for mixed projects
        priority_order = ["python", "node", "java", "dotnet", "go", "rust", "cpp", "swift"]
        for priority_type in priority_order:
            if priority_type in project_types:
                primary_type = priority_type
                break
        else:
            primary_type = project_types[0]
    else:
        primary_type = "unknown"

    print(f"   -> Detected ecosystem: {primary_type}")
    print(f"   -> Languages found: {detected_languages}")
    print(f"   -> Build tools: {build_tools}")

    return primary_type, detected_languages, build_tools

def _get_enhanced_project_commands(project_type: str, repo_path: Path, detected_languages: List[str]) -> Tuple[str, str, str, Optional[str]]:
    """
    Enhanced command generation supporting polyglot projects.
    Returns: (install_command, test_command, base_image, dependency_file)
    """
    print(f"   -> Configuring build for project type: {project_type}")

    # Multi-stage build support for polyglot projects
    base_images = {
        "python": "python:3.11-slim",
        "node": "node:18-alpine",
        "java": "openjdk:17-jdk-slim",
        "dotnet": "mcr.microsoft.com/dotnet/sdk:7.0",
        "go": "golang:1.21-alpine",
        "rust": "rust:1.75-slim",
        "cpp": "gcc:12",
        "swift": "swift:5.9",
        "ruby": "ruby:3.2-alpine",
        "php": "php:8.2-cli",
        "dart": "dart:stable",
        "kotlin": "openjdk:17-jdk-slim",
        "scala": "openjdk:17-jdk-slim",
        "clojure": "clojure:lein-alpine",
        "haskell": "haskell:9.4",
        "elixir": "elixir:1.15-alpine",
        "r": "r-base:4.3.0"
    }

    # Primary language configuration
    if project_type == "python":
        install_cmd, dep_file = _configure_python_build(repo_path)
        test_cmd = _detect_python_test_runner(repo_path)
        base_image = base_images["python"]

    elif project_type == "node":
        install_cmd, dep_file = _configure_node_build(repo_path)
        test_cmd = _detect_node_test_runner(repo_path)
        base_image = base_images["node"]

    elif project_type == "java":
        install_cmd, dep_file = _configure_java_build(repo_path)
        test_cmd = _detect_java_test_runner(repo_path)
        base_image = base_images["java"]

    elif project_type == "dotnet":
        install_cmd, dep_file = _configure_dotnet_build(repo_path)
        test_cmd = _detect_dotnet_test_runner(repo_path)
        base_image = base_images["dotnet"]

    elif project_type == "go":
        install_cmd, dep_file = _configure_go_build(repo_path)
        test_cmd = _detect_go_test_runner(repo_path)
        base_image = base_images["go"]

    elif project_type == "rust":
        install_cmd, dep_file = _configure_rust_build(repo_path)
        test_cmd = _detect_rust_test_runner(repo_path)
        base_image = base_images["rust"]

    elif project_type == "cpp":
        install_cmd, dep_file = _configure_cpp_build(repo_path)
        test_cmd = _detect_cpp_test_runner(repo_path)
        base_image = base_images["cpp"]

    elif project_type == "swift":
        install_cmd, dep_file = _configure_swift_build(repo_path)
        test_cmd = _detect_swift_test_runner(repo_path)
        base_image = base_images["swift"]

    elif project_type == "ruby":
        install_cmd, dep_file = _configure_ruby_build(repo_path)
        test_cmd = _detect_ruby_test_runner(repo_path)
        base_image = base_images["ruby"]

    elif project_type == "php":
        install_cmd, dep_file = _configure_php_build(repo_path)
        test_cmd = _detect_php_test_runner(repo_path)
        base_image = base_images["php"]

    elif project_type == "dart":
        install_cmd, dep_file = _configure_dart_build(repo_path)
        test_cmd = _detect_dart_test_runner(repo_path)
        base_image = base_images["dart"]

    else:
        # Fallback for unknown or unsupported languages
        install_cmd = "echo 'Unknown project type, attempting generic build'"
        test_cmd = "echo 'No test runner detected for this project type'"
        base_image = "alpine:latest"
        dep_file = None

    return install_cmd, test_cmd, base_image, dep_file

# Language-specific build configuration functions
def _configure_python_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Python build commands and detect dependency file."""
    if (repo_path / "poetry.lock").exists():
        return "pip install poetry && poetry install", "pyproject.toml"
    elif (repo_path / "Pipfile").exists():
        return "pip install pipenv && pipenv install --system", "Pipfile"
    elif (repo_path / "requirements.txt").exists():
        return "pip install -r requirements.txt", "requirements.txt"
    elif (repo_path / "pyproject.toml").exists():
        return "pip install .", "pyproject.toml"
    elif (repo_path / "setup.py").exists():
        return "pip install -e .", "setup.py"
    elif (repo_path / "conda.yml").exists() or (repo_path / "environment.yml").exists():
        env_file = "conda.yml" if (repo_path / "conda.yml").exists() else "environment.yml"
        return f"conda env create -f {env_file} && conda activate $(head -1 {env_file} | cut -d' ' -f2)", env_file
    else:
        return "echo 'No Python dependencies to install'", None

def _detect_python_test_runner(repo_path: Path) -> str:
    """Detect appropriate Python test runner."""
    if (repo_path / "pytest.ini").exists() or (repo_path / "pyproject.toml").exists():
        content = ""
        if (repo_path / "pyproject.toml").exists():
            content = (repo_path / "pyproject.toml").read_text()
        if "pytest" in content or (repo_path / "pytest.ini").exists():
            return "python -m pytest"

    if (repo_path / "tox.ini").exists():
        return "tox"

    if any(repo_path.rglob("test_*.py")) or any(repo_path.rglob("*_test.py")):
        return "python -m pytest"

    if (repo_path / "tests").is_dir() or (repo_path / "test").is_dir():
        return "python -m unittest discover"

    return "python -m unittest discover"

def _configure_node_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Node.js build commands."""
    if (repo_path / "pnpm-lock.yaml").exists():
        return "pnpm install", "package.json"
    elif (repo_path / "yarn.lock").exists():
        return "yarn install --frozen-lockfile", "package.json"
    elif (repo_path / "package.json").exists():
        return "npm ci", "package.json"
    else:
        return "echo 'No Node.js dependencies found'", None

def _detect_node_test_runner(repo_path: Path) -> str:
    """Detect appropriate Node.js test runner."""
    if (repo_path / "package.json").exists():
        try:
            package_json = json.loads((repo_path / "package.json").read_text())
            scripts = package_json.get("scripts", {})

            if "test" in scripts:
                if (repo_path / "pnpm-lock.yaml").exists():
                    return "pnpm test"
                elif (repo_path / "yarn.lock").exists():
                    return "yarn test"
                else:
                    return "npm test"
        except:
            pass

    # Check for specific test framework files
    if (repo_path / "jest.config.js").exists() or (repo_path / "jest.config.ts").exists():
        return "npx jest"
    elif (repo_path / "vitest.config.js").exists() or (repo_path / "vitest.config.ts").exists():
        return "npx vitest run"
    elif (repo_path / "cypress.config.js").exists():
        return "npx cypress run"
    elif (repo_path / "playwright.config.js").exists():
        return "npx playwright test"

    return "npm test"

def _configure_java_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Java build commands."""
    if (repo_path / "pom.xml").exists():
        return "mvn compile", "pom.xml"
    elif (repo_path / "build.gradle").exists() or (repo_path / "build.gradle.kts").exists():
        gradle_file = "build.gradle.kts" if (repo_path / "build.gradle.kts").exists() else "build.gradle"
        return "./gradlew build", gradle_file
    else:
        return "echo 'No Java build file found'", None

def _detect_java_test_runner(repo_path: Path) -> str:
    """Detect appropriate Java test runner."""
    if (repo_path / "pom.xml").exists():
        return "mvn test"
    elif (repo_path / "build.gradle").exists() or (repo_path / "build.gradle.kts").exists():
        return "./gradlew test"
    else:
        return "echo 'No Java test runner found'"

def _configure_dotnet_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure .NET build commands."""
    sln_files = list(repo_path.glob("*.sln"))
    csproj_files = list(repo_path.glob("*.csproj"))

    if sln_files:
        return "dotnet restore && dotnet build", sln_files[0].name
    elif csproj_files:
        return "dotnet restore && dotnet build", csproj_files[0].name
    else:
        return "dotnet build", None

def _detect_dotnet_test_runner(repo_path: Path) -> str:
    """Detect appropriate .NET test runner."""
    return "dotnet test"

def _configure_go_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Go build commands."""
    if (repo_path / "go.mod").exists():
        return "go mod download && go build ./...", "go.mod"
    else:
        return "go build ./...", None

def _detect_go_test_runner(repo_path: Path) -> str:
    """Detect appropriate Go test runner."""
    return "go test ./..."

def _configure_rust_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Rust build commands."""
    if (repo_path / "Cargo.toml").exists():
        return "cargo build --release", "Cargo.toml"
    else:
        return "echo 'No Cargo.toml found'", None

def _detect_rust_test_runner(repo_path: Path) -> str:
    """Detect appropriate Rust test runner."""
    return "cargo test --release"

def _configure_cpp_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure C++ build commands."""
    if (repo_path / "CMakeLists.txt").exists():
        return "mkdir -p build && cd build && cmake .. && make", "CMakeLists.txt"
    elif (repo_path / "Makefile").exists() or (repo_path / "makefile").exists():
        makefile = "Makefile" if (repo_path / "Makefile").exists() else "makefile"
        return "make", makefile
    elif (repo_path / "meson.build").exists():
        return "meson setup builddir && meson compile -C builddir", "meson.build"
    else:
        return "echo 'No C++ build system found'", None

def _detect_cpp_test_runner(repo_path: Path) -> str:
    """Detect appropriate C++ test runner."""
    if (repo_path / "CMakeLists.txt").exists():
        return "cd build && ctest"
    elif (repo_path / "Makefile").exists():
        return "make test"
    else:
        return "echo 'No C++ test runner found'"

def _configure_swift_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Swift build commands."""
    if (repo_path / "Package.swift").exists():
        return "swift build", "Package.swift"
    else:
        return "echo 'No Package.swift found'", None

def _detect_swift_test_runner(repo_path: Path) -> str:
    """Detect appropriate Swift test runner."""
    return "swift test"

def _configure_ruby_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Ruby build commands."""
    if (repo_path / "Gemfile").exists():
        return "bundle install", "Gemfile"
    else:
        return "echo 'No Gemfile found'", None

def _detect_ruby_test_runner(repo_path: Path) -> str:
    """Detect appropriate Ruby test runner."""
    if (repo_path / "Rakefile").exists():
        return "bundle exec rake test"
    elif any(repo_path.rglob("*_spec.rb")):
        return "bundle exec rspec"
    elif any(repo_path.rglob("test_*.rb")):
        return "bundle exec ruby -Itest test/test_*.rb"
    else:
        return "bundle exec rake test"

def _configure_php_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure PHP build commands."""
    if (repo_path / "composer.json").exists():
        return "composer install", "composer.json"
    else:
        return "echo 'No composer.json found'", None

def _detect_php_test_runner(repo_path: Path) -> str:
    """Detect appropriate PHP test runner."""
    if (repo_path / "phpunit.xml").exists() or (repo_path / "phpunit.xml.dist").exists():
        return "vendor/bin/phpunit"
    elif (repo_path / "composer.json").exists():
        try:
            composer_json = json.loads((repo_path / "composer.json").read_text())
            scripts = composer_json.get("scripts", {})
            if "test" in scripts:
                return "composer test"
        except:
            pass
    return "vendor/bin/phpunit"

def _configure_dart_build(repo_path: Path) -> Tuple[str, Optional[str]]:
    """Configure Dart build commands."""
    if (repo_path / "pubspec.yaml").exists():
        # Check if it's a Flutter project
        pubspec_content = (repo_path / "pubspec.yaml").read_text()
        if "flutter:" in pubspec_content:
            return "flutter pub get && flutter build apk --debug", "pubspec.yaml"
        else:
            return "dart pub get", "pubspec.yaml"
    else:
        return "echo 'No pubspec.yaml found'", None

def _detect_dart_test_runner(repo_path: Path) -> str:
    """Detect appropriate Dart test runner."""
    if (repo_path / "pubspec.yaml").exists():
        pubspec_content = (repo_path / "pubspec.yaml").read_text()
        if "flutter:" in pubspec_content:
            return "flutter test"
        else:
            return "dart test"
    return "dart test"

def _generate_polyglot_dockerfile(install_command: str, test_command: str, base_image: str,
                                project_type: str = "unknown", detected_languages: List[str] = None) -> str:
    """
    Generate optimized Dockerfile with polyglot support and multi-stage builds when needed.
    """
    print("   -> Generating enhanced polyglot Dockerfile...")

    dockerfile_content = f"FROM {base_image}\n"
    dockerfile_content += "WORKDIR /app\n\n"

    # Enhanced security and optimization
    dockerfile_content += "# Security: Create non-root user\n"
    dockerfile_content += "RUN groupadd -r appuser && useradd -r -g appuser -d /app -s /sbin/nologin appuser\n\n"

    # Language-specific optimizations
    if project_type == "node":
        dockerfile_content += "# Node.js optimizations\n"
        dockerfile_content += "ENV NODE_ENV=production\n"
        dockerfile_content += "ENV CI=true\n"
        dockerfile_content += "COPY package*.json yarn.lock* pnpm-lock.yaml* ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "# Copy source code after dependency installation for better caching\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "python":
        dockerfile_content += "# Python optimizations\n"
        dockerfile_content += "ENV PYTHONUNBUFFERED=1\n"
        dockerfile_content += "ENV PYTHONDONTWRITEBYTECODE=1\n"
        dockerfile_content += "ENV PIP_NO_CACHE_DIR=1\n"
        dockerfile_content += "ENV PIP_DISABLE_PIP_VERSION_CHECK=1\n"

        # Copy dependency files first for better caching
        dependency_files = ["requirements*.txt", "pyproject.toml", "setup.py", "setup.cfg",
                          "Pipfile", "Pipfile.lock", "poetry.lock", "conda.yml", "environment.yml"]
        dockerfile_content += "# Copy dependency files\n"
        for dep_file in dependency_files:
            dockerfile_content += f"COPY {dep_file} ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "# Copy source code\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "java":
        dockerfile_content += "# Java optimizations\n"
        dockerfile_content += "ENV JAVA_OPTS=\"-Xmx512m -XX:+UseContainerSupport\"\n"
        dockerfile_content += "COPY pom.xml build.gradle* build.gradle.kts* gradle.properties* gradlew* ./\n"
        dockerfile_content += "COPY gradle/ gradle/ 2>/dev/null || true\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "go":
        dockerfile_content += "# Go optimizations\n"
        dockerfile_content += "ENV CGO_ENABLED=0\n"
        dockerfile_content += "ENV GOOS=linux\n"
        dockerfile_content += "COPY go.mod go.sum ./\n"
        dockerfile_content += f"RUN {install_command}\n\n"
        dockerfile_content += "COPY . .\n\n"

    elif project_type == "rust":
        dockerfile_content += "# Rust optimizations\n"
        dockerfile_content += "ENV CARGO_NET_GIT_FETCH_WITH_CLI=true\n"
        dockerfile_content += "COPY Cargo.toml Cargo.lock ./\n"
        dockerfile_content += "# Pre-build dependencies for better caching\n"
        dockerfile_content += "RUN mkdir src && echo 'fn main() {}' > src/main.rs\n"
        dockerfile_content += "RUN cargo build --release && rm -rf src\n"
        dockerfile_content += "COPY . .\n"
        dockerfile_content += "RUN touch src/main.rs\n"
        dockerfile_content += "RUN cargo build --release\n\n"

    else:
        # Generic approach for other languages
        dockerfile_content += "# Copy source code\n"
        dockerfile_content += "COPY . .\n\n"
        dockerfile_content += f"# Install dependencies\n"
        dockerfile_content += f"RUN {install_command}\n\n"

    # Set proper ownership and switch to non-root user
    dockerfile_content += "# Set ownership and switch to non-root user\n"
    dockerfile_content += "RUN chown -R appuser:appuser /app\n"
    dockerfile_content += "USER appuser\n\n"

    # Health check for long-running tests
    dockerfile_content += "# Health check\n"
    dockerfile_content += "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n"
    dockerfile_content += "  CMD echo 'Health check: Tests running'\n\n"

    # Final command
    cmd_parts = test_command.split()
    cmd_json = ', '.join([f'"{part}"' for part in cmd_parts])
    dockerfile_content += f"# Run tests\n"
    dockerfile_content += f"CMD [{cmd_json}]\n"

    print(f"      - Enhanced polyglot Dockerfile generated for {project_type}")
    return dockerfile_content

def _analyze_polyglot_warnings(repo_path: Path, project_type: str, detected_languages: List[str]) -> List[str]:
    """Enhanced warning analysis for polyglot projects."""
    warnings = []

    # Multi-language project warnings
    if len(detected_languages) > 1:
        warnings.append(f"Multi-language project detected: {', '.join(detected_languages)}. Build complexity may be higher.")

    # Language-specific warnings
    for language in detected_languages:
        if language == "python":
            req_file = repo_path / "requirements.txt"
            if req_file.exists():
                content = req_file.read_text()
                if "==" not in content and ">=" not in content:
                    warnings.append("Python: Consider pinning package versions for reproducible builds")

        elif language == "node":
            package_json = repo_path / "package.json"
            if package_json.exists() and not (repo_path / "package-lock.json").exists() and not (repo_path / "yarn.lock").exists():
                warnings.append("Node.js: No lock file found. Consider using npm ci or yarn for reproducible builds")

        elif language == "java":
            if not any((repo_path / f).exists() for f in ["pom.xml", "build.gradle", "build.gradle.kts"]):
                warnings.append("Java: No standard build file found. Manual compilation may be required")

    # Security warnings
    security_files = [".env", "config.json", "secrets.yml"]
    for sec_file in security_files:
        if (repo_path / sec_file).exists():
            warnings.append(f"Security: {sec_file} found. Ensure sensitive data is not included in container")

    # Performance warnings
    large_files = []
    for file_path in repo_path.rglob("*"):
        if file_path.is_file() and file_path.stat().st_size > 50 * 1024 * 1024:  # 50MB
            large_files.append(file_path.name)

    if large_files:
        warnings.append(f"Performance: Large files detected: {', '.join(large_files[:3])}. Consider .dockerignore")

    # Dockerfile optimization warnings
    if not (repo_path / ".dockerignore").exists():
        warnings.append("Optimization: Consider adding .dockerignore to exclude unnecessary files")

    return warnings

# --- Enhanced Main Service Orchestrator ---
def validate_fix(repo_url: str, target_file: str, modified_code: str,
                 enhanced_output: bool = False) -> Dict[str, str]:
    """
    Enhanced Crucible agent with comprehensive polyglot support.

    Args:
        repo_url: Repository URL to validate
        target_file: Target file path to modify
        modified_code: Modified code content
        enhanced_output: Whether to return enhanced output with metrics

    Returns:
        Dictionary with validation results
    """
    print("\n--- ENHANCED POLYGLOT CRUCIBLE ACTIVATED ---")
    print(f"Validating fix for '{target_file}' in '{repo_url}'")

    start_time = time.time()
    result = ValidationResult(status="error", logs="Initialization failed")

    try:
        client = docker.from_env()
        client.ping()
        print("✓ Successfully connected to Docker daemon.")
    except DockerException:
        error_msg = "Crucible Error: Could not connect to the Docker daemon. Please ensure Docker Desktop is running."
        print(f"✗ {error_msg}")
        result.logs = error_msg
        return result.to_dict() if enhanced_output else {"status": result.status, "logs": result.logs}

    with IntelligentCrawler(repo_url=repo_url) as crawler:
        repo_path = crawler.repo_path
        print("[Step 1/7] Patching file in local clone...")
        full_target_path = repo_path / target_file
        if not full_target_path.exists():
            full_target_path.parent.mkdir(parents=True, exist_ok=True)
        full_target_path.write_text(modified_code, encoding='utf-8')
        print("✓ File patched.")

        print("[Step 2/7] Analyzing polyglot project environment...")
        project_type, detected_languages, build_tools = _detect_project_ecosystem(repo_path)
        result.project_type = project_type
        result.detected_languages = detected_languages
        result.build_tools = build_tools

        install_cmd, test_cmd, base_image, dep_file = _get_enhanced_project_commands(
            project_type, repo_path, detected_languages
        )

        # Generate enhanced warnings
        result.warnings = _analyze_polyglot_warnings(repo_path, project_type, detected_languages)

        dockerfile_str = _generate_polyglot_dockerfile(
            install_cmd, test_cmd, base_image, project_type, detected_languages
        )
        (repo_path / "Dockerfile.lumiere").write_text(dockerfile_str, encoding='utf-8')
        print("✓ Enhanced polyglot environment analysis complete.")

        print("[Step 3/7] Building polyglot validation image...")
        build_start = time.time()
        image_tag = f"lumiere-crucible-polyglot/{uuid.uuid4()}"
        image = None
        try:
            image, build_logs = client.images.build(
                path=str(repo_path),
                dockerfile="Dockerfile.lumiere",
                tag=image_tag,
                rm=True,
                forcerm=True,
                pull=True,
                platform="linux/amd64"  # Ensure consistent platform
            )
            result.build_time = time.time() - build_start
            result.image_size = image.attrs.get('Size', 0)
            print(f"✓ Polyglot image '{image_tag}' built in {result.build_time:.2f}s")
        except BuildError as e:
            print(f"✗ Build Failed: {e}")
            logs = "\n".join([log.get('stream', '').strip() for log in e.build_log if 'stream' in log])
            result.status = "failed"
            result.logs = f"Docker image build failed:\n{logs}"
            result.execution_time = time.time() - start_time
            return result.to_dict() if enhanced_output else {"status": result.status, "logs": result.logs}

        try:
            print(f"[Step 4/7] Running polyglot tests in container '{image_tag}'...")
            test_start = time.time()

            # Enhanced container run with better resource limits
            container_output = client.containers.run(
                image_tag,
                remove=True,
                mem_limit="2g",  # Increased memory for polyglot builds
                cpu_count=4,     # More CPU for complex builds
                network_disabled=True,
                security_opt=["no-new-privileges:true"],  # Enhanced security
                read_only=False,  # Some builds need write access
                tmpfs={"/tmp": "size=512m,noexec"}  # Secure tmp
            )

            result.test_time = time.time() - test_start
            print(f"✓ Polyglot tests PASSED in {result.test_time:.2f}s")
            result.status = "passed"
            result.logs = container_output.decode('utf-8')

        except ContainerError as e:
            result.test_time = time.time() - test_start if 'test_start' in locals() else 0
            print(f"✗ Polyglot tests FAILED. Exit code: {e.exit_status}")
            logs = e.stderr.decode('utf-8') if e.stderr else e.stdout.decode('utf-8')
            result.status = "failed"
            result.logs = logs

        finally:
            print("[Step 5/7] Cleaning up polyglot validation image...")
            if image:
                try:
                    client.images.remove(image.id, force=True)
                    print(f"✓ Image '{image_tag}' removed.")
                except APIError as e:
                    print(f"Warning: Could not remove image '{image_tag}'. Error: {e}")
                    result.warnings.append(f"Failed to cleanup image: {image_tag}")

    result.execution_time = time.time() - start_time
    print(f"[Step 6/7] Polyglot validation completed in {result.execution_time:.2f}s")
    print(f"[Step 7/7] Languages processed: {', '.join(result.detected_languages)}")
    print("--- ENHANCED POLYGLOT CRUCIBLE MISSION COMPLETE ---")

    # Return backward compatible format by default
    if enhanced_output:
        return result.to_dict()
    else:
        return {"status": result.status, "logs": result.logs}

# --- Enhanced API Functions ---
def validate_fix_enhanced(repo_url: str, target_file: str, modified_code: str) -> Dict:
    """Enhanced version that returns detailed polyglot metrics and warnings"""
    return validate_fix(repo_url, target_file, modified_code, enhanced_output=True)

def get_supported_project_types() -> List[str]:
    """Returns list of supported project types with enhanced polyglot support"""
    return [
        "python", "node", "java", "dotnet", "go", "rust", "cpp", "swift",
        "ruby", "php", "dart", "kotlin", "scala", "clojure", "haskell",
        "elixir", "r"
    ]

def analyze_project_ecosystem(repo_url: str) -> Dict[str, Any]:
    """Enhanced project analysis with comprehensive ecosystem detection"""
    with IntelligentCrawler(repo_url=repo_url) as crawler:
        repo_path = crawler.repo_path
        project_type, detected_languages, build_tools = _detect_project_ecosystem(repo_path)
        install_cmd, test_cmd, base_image, dep_file = _get_enhanced_project_commands(
            project_type, repo_path, detected_languages
        )
        warnings = _analyze_polyglot_warnings(repo_path, project_type, detected_languages)

        return {
            "primary_project_type": project_type,
            "detected_languages": detected_languages,
            "build_tools": build_tools,
            "install_command": install_cmd,
            "test_command": test_cmd,
            "base_image": base_image,
            "dependency_file": dep_file,
            "warnings": warnings,
            "polyglot_support": True,
            "supported": project_type in get_supported_project_types()
        }

# --- Backward Compatibility Aliases ---
def crucible_validate(repo_url: str, target_file: str, modified_code: str) -> Dict[str, str]:
    """Backward compatibility alias for validate_fix"""
    return validate_fix(repo_url, target_file, modified_code)

def analyze_project(repo_url: str) -> Dict[str, str]:
    """Backward compatibility alias for analyze_project_ecosystem"""
    result = analyze_project_ecosystem(repo_url)
    # Convert to old format for backward compatibility
    return {
        "project_type": result["primary_project_type"],
        "install_command": result["install_command"],
        "test_command": result["test_command"],
        "base_image": result["base_image"],
        "dependency_file": result["dependency_file"],
        "supported": result["supported"]
    }

--- FILE_END: backend/lumiere_core/services/crucible.py ---

--- FILE_START: backend/lumiere_core/services/strategist.py ---
# In backend/lumiere_core/services/strategist.py

import json
import re
from typing import Dict, List, Any

from . import github

from . import llm_service
from .llm_service import TaskType

def analyze_and_prioritize(repo_url: str) -> Dict[str, Any]:
    """
    The core logic for The Strategist agent.
    Fetches all open issues and uses an LLM to prioritize them.
    The 'model_identifier' is no longer passed in.
    """
    print("--- STRATEGIST AGENT ACTIVATED ---")
    print(f"Analyzing repository: {repo_url}")
    # The specific model used is now decided by the Task Router.

    # Step 1: Fetch and Enrich All Open Issues (Unchanged)
    print("\n[Step 1/3] Fetching and enriching all open issues...")
    match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
    if not match:
        return {"error": f"Could not parse repository name from URL: {repo_url}"}
    repo_full_name = f"{match.group(1)}/{match.group(2)}"
    raw_issues = github.list_open_issues(repo_full_name)
    if not raw_issues:
        return {"analysis_summary": "No open issues found for this repository.", "prioritized_issues": []}
    enriched_issues = []
    issues_for_prompt = ""
    for issue_stub in raw_issues:
        issue_details = github.scrape_github_issue(issue_stub['url'])
        if issue_details:
            enriched_issue_data = {**issue_stub, **issue_details}
            enriched_issues.append(enriched_issue_data)
            description = issue_details.get('description') or ""
            issues_for_prompt += f"### Issue #{issue_stub['number']}: {issue_stub['title']}\n{description}\n\n---\n\n"
    print(f"✓ Found and enriched {len(enriched_issues)} open issues.")

    # Step 2: Use LLM to score and justify prioritization
    print("\n[Step 2/3] Submitting issues to LLM for prioritization analysis...")
    prompt = f"""You are "The Strategist", an expert engineering manager. Your mission is to analyze a list of open GitHub issues and prioritize them.
You MUST produce a valid JSON array as your output. For each issue, create a JSON object with these exact fields:
- "issue_number": The integer issue number.
- "score": An integer from 0 to 100, where 100 is most critical.
- "justification": A concise, one-sentence explanation for your score.
SCORING CRITERIA:
- Critical (90-100): Crashes, data corruption, security vulnerabilities.
- High (70-89): Major feature bugs, performance problems.
- Medium (40-69): Minor bugs, UI/UX issues.
- Low (0-39): Feature requests, documentation, refactoring.
Analyze the following issues and provide ONLY the JSON array as your response.
--- START OF ISSUES ---
{issues_for_prompt}
--- END OF ISSUES ---
"""
    # --- THE CHANGE IS HERE ---
    llm_response_str = llm_service.generate_text(
        prompt,
        task_type=TaskType.COMPLEX_REASONING
    )

    try:
        cleaned_str = re.sub(r'<think>.*?</think>', '', llm_response_str, flags=re.DOTALL)
        json_str_match = re.search(r'\[.*\]', cleaned_str, re.DOTALL)
        if not json_str_match:
            raise json.JSONDecodeError("No JSON array found in the LLM's cleaned response.", llm_response_str, 0)
        prioritization_data = json.loads(json_str_match.group(0))
        priority_map = {item['issue_number']: item for item in prioritization_data}
    except (json.JSONDecodeError, KeyError) as e:
        print(f"Error parsing LLM response: {e}\nLLM Response was:\n{llm_response_str}")
        return {"error": "Failed to parse prioritization data from LLM.", "llm_response": llm_response_str}

    print("✓ LLM analysis complete.")

    # Step 3: Merge data and sort (Unchanged)
    print("\n[Step 3/3] Finalizing report...")
    final_ranked_list = []
    for issue in enriched_issues:
        issue_number = issue['number']
        if issue_number in priority_map:
            issue.update(priority_map[issue_number])
            final_ranked_list.append(issue)
    final_ranked_list.sort(key=lambda x: x.get('score', 0), reverse=True)
    summary = f"Analyzed {len(final_ranked_list)} open issues."
    for i, issue in enumerate(final_ranked_list):
        issue['rank'] = i + 1
    return {
        "repository": repo_full_name,
        "analysis_summary": summary,
        "prioritized_issues": final_ranked_list
    }

--- FILE_END: backend/lumiere_core/services/strategist.py ---

--- FILE_START: backend/lumiere_core/services/github.py ---
# In backend/lumiere_core/services/github.py

import os
import re
from datetime import datetime, timezone
from typing import Dict, Optional, Tuple, List, Any
from github import Github, GithubException, PaginatedList
from dotenv import load_dotenv

load_dotenv()

GITHUB_TOKEN = os.getenv("GITHUB_ACCESS_TOKEN")
if not GITHUB_TOKEN:
    print("WARNING: GITHUB_ACCESS_TOKEN not found. API calls will be heavily rate-limited.")
    g = Github()
else:
    g = Github(GITHUB_TOKEN)


def _paginated_to_list(paginated_list: PaginatedList, max_items: int = 10) -> List[Dict[str, Any]]:
    items = []
    for i, item in enumerate(paginated_list):
        if i >= max_items:
            break
        item_data = {
            "name": item.name,
            "full_name": item.full_name,
            "description": item.description,
            "html_url": item.html_url,
            "language": item.language,
            "stargazers_count": item.stargazers_count
        }
        items.append(item_data)
    return items


def get_user_profile(username: str) -> Optional[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return {
            "login": user.login, "name": user.name, "bio": user.bio,
            "html_url": user.html_url, "public_repos": user.public_repos,
            "followers": user.followers, "following": user.following,
        }
    except GithubException:
        return None

def get_user_repos(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_repos(sort='updated'), max_items=10)
    except GithubException:
        return []

def get_user_starred(username: str) -> List[Dict[str, Any]]:
    try:
        user = g.get_user(username)
        return _paginated_to_list(user.get_starred(), max_items=10)
    except GithubException:
        return []

def get_user_comment_threads(username: str) -> List[Dict[str, Any]]:
    threads = []
    try:
        user = g.get_user(username)
        events = user.get_events()
        # Increase check limit to ensure we find comment events
        max_events_to_check = 50
        comment_events_found = 0
        max_comments_to_process = 5

        for i, event in enumerate(events):
            if i >= max_events_to_check or comment_events_found >= max_comments_to_process:
                break

            if event.type in ['IssueCommentEvent', 'PullRequestReviewCommentEvent']:
                payload = event.payload
                comment_data = payload.get('comment')
                issue_data = payload.get('issue', payload.get('pull_request'))

                if not comment_data or not issue_data or comment_data['user']['login'] != username:
                    continue

                comment_events_found += 1
                repo_name, issue_number = event.repo.name, issue_data['number']

                try:
                    repo_obj = g.get_repo(repo_name)
                    issue_obj = repo_obj.get_issue(number=issue_number)

                    created_at_str = comment_data.get('created_at')
                    if not created_at_str: continue

                    # Correctly parse the ISO 8601 string into a timezone-aware datetime object
                    created_at_dt = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))

                    user_comment = {"id": comment_data['id'], "body": comment_data['body'], "html_url": comment_data['html_url']}

                    replies = []
                    # Fetch comments created *after* the user's comment
                    for reply_comment in issue_obj.get_comments(since=created_at_dt):
                        if reply_comment.user.login != username and reply_comment.id != user_comment['id']:
                            replies.append({"user": reply_comment.user.login, "body": reply_comment.body, "html_url": reply_comment.html_url})

                    threads.append({
                        "repo_name": repo_name, "issue_number": issue_number, "issue_title": issue_data['title'],
                        "issue_url": issue_data['html_url'], "user_comment": user_comment, "replies": replies
                    })
                except GithubException as ge:
                    print(f"Warning: Could not fully process event for {repo_name}#{issue_number}. Skipping. Reason: {ge}")
                    continue
        return threads
    except GithubException as e:
        print(f"GitHub API Error while fetching comment threads: {e}")
        return []

def _parse_github_issue_url(issue_url: str) -> Optional[Tuple[str, str, int]]:
    match = re.match(r"https://github\.com/([^/]+)/([^/]+)/(?:issues|pull)/(\d+)", issue_url)
    if match:
        owner, repo_name, issue_number_str = match.groups()
        return owner, repo_name, int(issue_number_str)
    return None

def scrape_github_issue(issue_url: str) -> Optional[Dict[str, str]]:
    print(f"Fetching GitHub issue via API: {issue_url}")
    parsed_url = _parse_github_issue_url(issue_url)
    if not parsed_url:
        print(f"Error: Could not parse GitHub issue URL: {issue_url}")
        return None
    owner, repo_name, issue_number = parsed_url
    repo_full_name = f"{owner}/{repo_name}"
    try:
        repo = g.get_repo(repo_full_name)
        issue = repo.get_issue(number=issue_number)
        title, description = issue.title, issue.body if issue.body else ""
        full_text_query, repo_url = f"Issue Title: {title}\n\nDescription:\n{description}", f"https://github.com/{owner}/{repo_name}"
        return {"title": title, "description": description, "full_text_query": full_text_query, "repo_url": repo_url}
    except GithubException as e:
        print(f"GitHub API Error: {e.status}, {e.data}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during GitHub API call: {e}")
        return None

def list_open_issues(repo_full_name: str) -> List[Dict[str, Any]]:
    """
    Fetches a list of all open issues for a given repository.
    """
    print(f"Fetching open issues for repository: {repo_full_name}")
    try:
        repo = g.get_repo(repo_full_name)
        open_issues = repo.get_issues(state='open')
        issues_list = []
        for issue in open_issues:
            if not issue.pull_request:
                issues_list.append({
                    "number": issue.number,
                    "title": issue.title,
                    "url": issue.html_url,
                    "author": issue.user.login,
                })
        return issues_list
    except GithubException as e:
        print(f"GitHub API Error while listing issues: {e}")
        return []

--- FILE_END: backend/lumiere_core/services/github.py ---

--- FILE_START: backend/lumiere_core/services/testing.py ---
# backend/lumiere_core/services/testing.py
import logging
from typing import Dict, List, Optional, Any, Set
from . import llm_service
from .ollama import search_index
from .utils import clean_llm_code_output

# Set up logging for better debugging
logger = logging.getLogger(__name__)

def generate_tests_for_code(repo_id: str, new_code: str, instruction: str) -> Dict[str, str]:
    """
    The core logic for the Test Generation Agent.
    It finds existing test files in the repository to learn the project's testing
    style and then generates a new test function consistent with that style.

    Args:
        repo_id: The unique identifier for the repository.
        new_code: The new code for which tests need to be generated.
        instruction: A natural language description of the code's purpose.

    Returns:
        A dictionary containing the generated test code or an error message.
        Format: {"generated_tests": str, "error": str (optional)}
    """
    logger.info(f"Initiating Test Generation Agent for repo '{repo_id}'")

    # Input validation
    if not all([repo_id, new_code]):
        error_msg = "Missing required parameters: repo_id and new_code."
        logger.error(error_msg)
        return {"generated_tests": "", "error": error_msg}

    if not instruction:
        logger.warning("No instruction provided, using default description")
        instruction = "Generate appropriate tests for the given code"

    try:
        # --- Step 1: Find existing test patterns with RAG ---
        logger.info("Step 1: Finding existing test patterns with RAG...")
        test_context_string = _find_existing_test_patterns(repo_id, instruction)

        # --- Step 2: Construct the Reinforced Prompt ---
        logger.info("Step 2: Constructing reinforced test generation prompt...")
        prompt = _build_test_generation_prompt(test_context_string, new_code)

        # --- Step 3: Generate the Test Code ---
        logger.info("Step 3: Generating test code...")
        raw_generated_tests = _generate_test_code(prompt)

        # --- Step 4: Clean and validate the output ---
        logger.info("Step 4: Cleaning and validating the generated test code...")
        final_tests = clean_llm_code_output(raw_generated_tests)

        # Validate the generated tests
        validation_result = validate_test_code(final_tests)
        if not validation_result["is_valid"]:
            logger.warning(f"Generated test code has validation issues: {validation_result['syntax_errors']}")
            # Still return the code but include warning in logs

        logger.info("✓ Test generation completed successfully.")
        return {"generated_tests": final_tests}

    except Exception as e:
        error_msg = f"An unexpected error occurred during test generation: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {"generated_tests": "", "error": error_msg}


def _find_existing_test_patterns(repo_id: str, instruction: str) -> str:
    """
    Find existing test patterns using RAG search.

    Args:
        repo_id: Repository identifier
        instruction: Code description for search context

    Returns:
        String containing formatted test examples or default message
    """
    search_query = f"Example test cases for python code like this: {instruction}"

    try:
        # CORRECTED CALL: Pass repo_id directly to the centralized search function.
        context_chunks = search_index(
            query_text=search_query,
            model_name='snowflake-arctic-embed2:latest',
            repo_id=repo_id,
            k=7  # Look for up to 7 relevant chunks
        )
    except Exception as e:
        logger.warning(f"RAG search failed for test generation in repo '{repo_id}': {e}")
        context_chunks = []

    test_context_string = ""
    found_test_files: Set[str] = set()

    for chunk in context_chunks:
        file_path = chunk.get('file_path', '')
        chunk_text = chunk.get('text', '')

        # Heuristic to identify test files and avoid duplicates
        if (_is_test_file(file_path) and
            file_path not in found_test_files and
            chunk_text and
            len(chunk_text.strip()) > 10):  # Ensure meaningful content

            test_context_string += f"--- Example test from file `{file_path}` ---\n```python\n{chunk_text}\n```\n\n"
            found_test_files.add(file_path)

    if not test_context_string:
        test_context_string = "No specific test patterns found. Please generate a standard `pytest` function using `assert`."
        logger.info("Warning: No existing test files found via RAG. Will generate a generic test.")
    else:
        logger.info(f"Found test patterns from files: {list(found_test_files)}")

    return test_context_string


def _is_test_file(file_path: str) -> bool:
    """Check if a file path indicates a test file."""
    if not file_path:
        return False

    file_path_lower = file_path.lower()
    return (
        'test' in file_path_lower or
        file_path_lower.endswith('_test.py') or
        file_path_lower.startswith('test_') or
        '/tests/' in file_path_lower
    )


def _build_test_generation_prompt(test_context_string: str, new_code: str) -> str:
    """
    Build the prompt for test generation.

    Args:
        test_context_string: Existing test examples
        new_code: Code to generate tests for

    Returns:
        Formatted prompt string
    """
    return f"""You are an expert QA Engineer and Python programmer. Your task is to write a new unit test for a piece of code.

**YOUR INSTRUCTIONS:**
1. **Analyze "EXISTING TEST EXAMPLES"** to understand the project's testing style. Pay close attention to:
   * Imports (e.g., `unittest`, `pytest`).
   * Structure: Are tests inside a `class`?
   * Assertions: Do they use `self.assertEqual` or plain `assert`?
   * Setup: Are there fixtures or `setUp` methods?

2. **Analyze the "NEW CODE TO BE TESTED"** to understand its functionality.

3. **Write a complete test function.** It is CRITICAL that you exactly match the style of the examples.
   If the examples are standalone functions (e.g., `def test_...():`), your test MUST also be a standalone function.
   DO NOT invent a class if the examples do not use one.

4. **Output ONLY raw Python code.** Do not include any explanations, commentary, or Markdown fences like ```python.

---
### EXISTING TEST EXAMPLES
{test_context_string}

---
### NEW CODE TO BE TESTED
```python
{new_code}
```

Now, generate ONLY the new, stylistically-consistent test function."""


def _generate_test_code(prompt: str) -> str:
    """
    Generate test code using the LLM service.

    Args:
        prompt: The formatted prompt for test generation

    Returns:
        Generated test code

    Raises:
        Exception: If LLM generation fails
    """
    model_to_use = "ollama/qwen2.5-coder:3b"
    logger.info(f"Sending request to code generation model '{model_to_use}'...")

    try:
        # Use the master LLM service and pass the full model identifier
        raw_generated_tests = llm_service.generate_text(prompt, model_identifier=model_to_use)

        if not raw_generated_tests or not raw_generated_tests.strip():
            raise ValueError("LLM returned empty response")

        return raw_generated_tests

    except Exception as e:
        logger.error(f"LLM generation failed for tests: {e}")
        raise Exception(f"LLM generation failed: {str(e)}")


def validate_test_code(test_code: str) -> Dict[str, Any]:
    """
    Validates the generated test code for basic syntax and structure.

    Args:
        test_code: The generated test code to validate

    Returns:
        Dictionary with validation results containing:
        - is_valid: bool
        - has_test_function: bool
        - syntax_errors: List[str]
    """
    validation_result = {
        "is_valid": False,
        "has_test_function": False,
        "syntax_errors": [],
    }

    if not test_code or not test_code.strip():
        validation_result["syntax_errors"].append("Test code is empty")
        return validation_result

    # Check for test function presence
    if "def test_" in test_code:
        validation_result["has_test_function"] = True
    else:
        validation_result["syntax_errors"].append("No test function found (should start with 'def test_')")

    # Validate syntax
    try:
        compile(test_code, '<string>', 'exec')
        validation_result["is_valid"] = True
    except SyntaxError as e:
        error_msg = f"Syntax error: {e.msg}"
        if e.lineno:
            error_msg += f" on line {e.lineno}"
        validation_result["syntax_errors"].append(error_msg)
    except Exception as e:
        validation_result["syntax_errors"].append(f"Unexpected compilation error: {str(e)}")

    return validation_result


def generate_test_suggestions(code_snippet: str) -> List[str]:
    """
    Generates high-level suggestions for what types of tests should be written.

    Args:
        code_snippet: The code to analyze for test suggestions

    Returns:
        List of test case suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return ["No code provided for analysis"]

    suggestions = []
    code_lower = code_snippet.lower()

    # Function-based suggestions
    if "def " in code_lower:
        suggestions.extend([
            "Test the happy path with valid inputs",
            "Test edge cases (e.g., empty strings, zero, None)",
            "Test with invalid inputs to verify error handling"
        ])

    # Control flow suggestions
    if "if " in code_lower or "elif " in code_lower:
        suggestions.append("Ensure all conditional branches are tested")

    # Loop suggestions
    if "for " in code_lower or "while " in code_lower:
        suggestions.append("Test loop behavior (e.g., zero, one, and multiple iterations)")

    # Exception handling suggestions
    if "try:" in code_lower and "except" in code_lower:
        suggestions.extend([
            "Verify that expected exceptions are raised correctly",
            "Verify behavior when no exception occurs"
        ])

    # Class-based suggestions
    if "class " in code_lower:
        suggestions.extend([
            "Test object initialization",
            "Test all public methods",
            "Test method interactions and state changes"
        ])

    # Data structure suggestions
    if any(keyword in code_lower for keyword in ["list", "dict", "set", "tuple"]):
        suggestions.append("Test with different data structure sizes and types")

    # Return default suggestions if none found
    if not suggestions:
        suggestions = [
            "Test basic functionality",
            "Test edge cases",
            "Test error conditions"
        ]

    return suggestions


def get_test_coverage_suggestions(code_snippet: str) -> Dict[str, List[str]]:
    """
    Analyze code and provide comprehensive test coverage suggestions.

    Args:
        code_snippet: Code to analyze

    Returns:
        Dictionary categorizing different types of test suggestions
    """
    if not code_snippet or not code_snippet.strip():
        return {"error": ["No code provided for analysis"]}

    suggestions = {
        "unit_tests": generate_test_suggestions(code_snippet),
        "integration_tests": [],
        "edge_cases": [],
        "performance_tests": []
    }

    code_lower = code_snippet.lower()

    # Integration test suggestions
    if any(keyword in code_lower for keyword in ["import", "from", "api", "database", "http"]):
        suggestions["integration_tests"].extend([
            "Test external API interactions",
            "Test database operations if applicable",
            "Test module integration"
        ])

    # Edge case suggestions
    suggestions["edge_cases"].extend([
        "Test with None values",
        "Test with empty collections",
        "Test with maximum/minimum values",
        "Test with malformed input"
    ])

    # Performance test suggestions
    if any(keyword in code_lower for keyword in ["for", "while", "sort", "search"]):
        suggestions["performance_tests"].extend([
            "Test with large datasets",
            "Test execution time constraints",
            "Test memory usage"
        ])

    return suggestions

--- FILE_END: backend/lumiere_core/services/testing.py ---

--- FILE_START: backend/lumiere_core/services/profile_service.py ---
# In backend/lumiere_core/services/profile_service.py
from typing import Dict, Any
from . import github
from . import llm_service

def _format_data_for_llm(profile_data: Dict[str, Any]) -> str:
    """Formats the aggregated GitHub data into a text block for the LLM."""

    user = profile_data['user_profile']
    text = f"""
# GitHub User Profile Analysis for: {user['login']} ({user.get('name', 'N/A')})
Bio: {user.get('bio', 'N/A')}
Followers: {user.get('followers', 0)} | Following: {user.get('following', 0)}
Public Repos: {user.get('public_repos', 0)}

---
## Owned Repositories (Sample)
"""
    if profile_data['repositories']:
        for repo in profile_data['repositories'][:5]:
            text += f"- **{repo['name']}**: {repo.get('language', 'N/A')} | ☆{repo['stargazers_count']} | {repo.get('description', 'No description')}\n"
    else:
        text += "No public repositories found.\n"

    text += "\n---"
    text += "\n## Starred Repositories (Sample)\n"
    if profile_data['starred_repositories']:
        for repo in profile_data['starred_repositories'][:5]:
             text += f"- **{repo['full_name']}**: {repo.get('description', 'No description')}\n"
    else:
        text += "No starred repositories found.\n"

    text += "\n---"
    text += f"\n## Recent Issue/PR Comments & Replies by {user['login']}\n"
    if profile_data['comment_threads']:
        for thread in profile_data['comment_threads']:
            text += f"\nOn repo `{thread['repo_name']}` (Issue/PR #{thread['issue_number']}):\n"
            text += f"  - **Their Comment**: \"{thread['user_comment']['body']}\"\n"
            if thread['replies']:
                for reply in thread['replies']:
                    text += f"    - **Reply from {reply['user']}**: \"{reply['body']}\"\n"
            else:
                text += "    - No replies to this comment found.\n"
    else:
        text += "No recent comments found.\n"

    return text

def generate_profile_review(username: str, model_identifier: str) -> Dict[str, Any]:
    """
    The core logic for the Chronicler Agent.
    Fetches a user's GitHub activity and generates a narrative summary.
    """
    print(f"Initiating Chronicler Agent for user '{username}'")
    print(f"Using model: {model_identifier}")

    print("   -> Step 1: Fetching profile data from GitHub API...")
    user_profile = github.get_user_profile(username)
    if not user_profile:
        raise FileNotFoundError(f"User '{username}' not found on GitHub.")

    repositories = github.get_user_repos(username)
    starred = github.get_user_starred(username)
    comment_threads = github.get_user_comment_threads(username)

    raw_data = {
        "user_profile": user_profile, "repositories": repositories,
        "starred_repositories": starred, "comment_threads": comment_threads,
    }

    print("   -> Step 2: Formatting data and constructing FINAL prompt for LLM...")
    context_string = _format_data_for_llm(raw_data)

    prompt = f"""You are an expert GitHub profile analyst. Your task is to analyze the user '{username}' based ONLY on the provided data.

**CRITICAL INSTRUCTION: Your entire analysis MUST be about the user '{username}'. Do NOT summarize the technical problems in the comments. Instead, use the comments to understand the USER'S BEHAVIOR.**

Generate a "Developer Profile Briefing" in Markdown with these exact sections:

### 1. Identity & Technical Focus
*   Based on their bio, owned repos, and starred repos, what are '{username}'s primary technical interests?
*   What are their main programming languages? (e.g., JavaScript, C++, Python)

### 2. Community Engagement Style
*   Based on their comments, what is '{username}'s role in the community? Are they reporting bugs, asking for help, or providing solutions?
*   Analyze the tone and content of THEIR comments. For example: `The user provides detailed debugging reports ("Debugging Report: itzzzme/anime-api Integration Issues") suggesting a methodical approach to problem-solving.`

### 3. Community Reception
*   Look at the replies to '{username}'s comments. Are others engaging with them? Are they receiving help and feedback?
*   Briefly summarize the nature of the replies they receive (e.g., "The user receives helpful replies from other developers, who offer suggestions and updated decryption keys.").

---
### RAW GITHUB DATA FOR {username}
{context_string}
---

Now, generate the Developer Profile Briefing about the user '{username}'.
"""

    print("   -> Step 3: Sending request to LLM for narrative generation...")
    summary = llm_service.generate_text(prompt, model_identifier=model_identifier)

    final_response = { "profile_summary": summary, "raw_data": raw_data }

    return final_response

--- FILE_END: backend/lumiere_core/services/profile_service.py ---

--- FILE_START: backend/lumiere_core/services/ollama.py ---
# In lumiere_core/services/ollama.py

import ollama
from tqdm import tqdm
from typing import List
import faiss
import numpy as np
import json
from pathlib import Path # <--- ADD THIS

def get_ollama_embeddings(chunks: List[str], model_name: str) -> List[List[float]]:
    """
    Generates embeddings for a list of text chunks using a local Ollama model.

    Args:
        chunks: A list of strings to be embedded.
        model_name: The name of the Ollama model to use (e.g., 'snowflake-arctic-embed').

    Returns:
        A list of embeddings, where each embedding is a list of floats.
    """
    embeddings = []
    # The ollama client automatically connects to http://localhost:11434
    client = ollama.Client()

    # Show a progress bar because this can take time
    for text in tqdm(chunks, desc="Generating Ollama Embeddings"):
        response = client.embeddings(model=model_name, prompt=text)
        embeddings.append(response['embedding'])

    return embeddings

def search_index(
    query_text: str,
    model_name: str,
    repo_id: str, # <--- MODIFIED: Take repo_id directly
    k: int = 10,
    **kwargs # <--- MODIFIED: Accept and ignore old path args for compatibility
) -> List[dict]:
    """
    Searches the Faiss index for the top k most similar chunks to a query for a given repo_id.

    Args:
        query_text: The user's search query.
        model_name: The name of the Ollama model used to create the index.
        repo_id: The unique ID of the repository whose index should be searched.
        k: The number of results to return.

    Returns:
        A list of dictionaries, where each dictionary contains the chunk_id,
        file_path, and the original text of the matching chunk.
    """
    # --- THIS IS THE FIX ---
    # Centralize path construction based on repo_id.
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_dir = backend_dir / "cloned_repositories" / repo_id
    index_path = artifacts_dir / f"{repo_id}_faiss.index"
    map_path = artifacts_dir / f"{repo_id}_id_map.json"

    print(f"Loading index '{index_path}' and map '{map_path}'...")
    # Load the Faiss index
    index = faiss.read_index(str(index_path))

    # Load the ID mapping files
    with open(map_path, 'r', encoding='utf-8') as f:
        id_maps = json.load(f)
    faiss_id_to_chunk_id = id_maps['faiss_id_to_chunk_id']
    chunk_id_to_data = id_maps['chunk_id_to_data']

    print(f"Generating embedding for query: '{query_text}'...")
    # 1. Embed the query using the same Ollama model
    client = ollama.Client()
    response = client.embeddings(model=model_name, prompt=query_text)
    query_vector = np.array([response['embedding']]).astype('float32')

    print(f"Searching index for top {k} results...")
    # 2. Search the Faiss index
    distances, indices = index.search(query_vector, k)

    # 3. Retrieve the results
    results = []
    for i in range(min(k, len(indices[0]))): # Ensure we don't go out of bounds
        faiss_id = indices[0][i]
        chunk_id = faiss_id_to_chunk_id[faiss_id]
        chunk_data = chunk_id_to_data[chunk_id]

        results.append({
            "chunk_id": chunk_id,
            "file_path": chunk_data['file_path'],
            "text": chunk_data['text'],
            "distance": float(distances[0][i])
        })

    return results

--- FILE_END: backend/lumiere_core/services/ollama.py ---

--- FILE_START: backend/lumiere_core/services/ingestion_service.py ---
# backend/lumiere_core/services/ingestion_service.py

import json
import traceback
import os
import re
from pathlib import Path
from typing import Dict, Any

from ingestion.crawler import IntelligentCrawler
from ingestion.jsonifier import Jsonifier
from ingestion.indexing import EmbeddingIndexer
from . import sentinel_service

def generate_repo_id(repo_url: str, max_length: int = 100) -> str:
    """
    Generate a safe repository ID from a GitHub URL with intelligent truncation.

    Args:
        repo_url: The GitHub repository URL
        max_length: Maximum length for the generated repo_id (default: 100)

    Returns:
        A safe, filesystem-friendly repository identifier
    """
    # Extract the repo path (owner/repo-name)
    repo_path = repo_url.replace("https://github.com/", "").replace("/", "_")

    # If it's already within limits, return as-is (backward compatibility)
    if len(repo_path) <= max_length:
        return repo_path

    # Split into owner and repo name for intelligent truncation
    original_path = repo_url.replace("https://github.com/", "")
    parts = original_path.split("/", 1)

    if len(parts) != 2:
        # Fallback: just truncate the whole thing
        return repo_path[:max_length]

    owner, repo_name = parts

    # Calculate available space for repo name after owner and underscore
    available_space = max_length - len(owner) - 1  # -1 for the underscore

    if available_space <= 10:  # If owner name is too long, truncate both
        # Use first 40 chars for owner, rest for repo (with underscore)
        truncated_owner = owner[:40]
        available_for_repo = max_length - len(truncated_owner) - 1
        truncated_repo = repo_name[:available_for_repo] if available_for_repo > 0 else ""
        return f"{truncated_owner}_{truncated_repo}".rstrip("_")

    # Truncate repo name intelligently
    if len(repo_name) > available_space:
        # Try to keep meaningful parts of the repo name
        # Remove common words and separators, keep important parts
        cleaned_repo = re.sub(r'[-_\s]+', '-', repo_name)
        words = cleaned_repo.split('-')

        if len(words) > 1:
            # Keep first and last words, add middle words until we hit the limit
            truncated_parts = [words[0]]
            remaining_space = available_space - len(words[0])

            # Add words from the end working backwards
            for word in reversed(words[1:]):
                if remaining_space >= len(word) + 1:  # +1 for separator
                    truncated_parts.insert(-1, word)
                    remaining_space -= len(word) + 1
                else:
                    break

            truncated_repo = '-'.join(truncated_parts)
        else:
            # Single word, just truncate
            truncated_repo = repo_name[:available_space]

        return f"{owner}_{truncated_repo}"

    return repo_path


def clone_and_embed_repository(repo_url: str, embedding_model: str = 'snowflake-arctic-embed2:latest') -> Dict[str, Any]:
    """
    Orchestrates the entire ingestion pipeline, including the new Sentinel metrics capture.
    """
    repo_id = generate_repo_id(repo_url)
    backend_dir = Path(__file__).resolve().parent.parent.parent
    artifacts_base_dir = backend_dir / "cloned_repositories"
    repo_output_dir = artifacts_base_dir / repo_id
    repo_output_dir.mkdir(parents=True, exist_ok=True)

    output_cortex_path = repo_output_dir / f"{repo_id}_cortex.json"
    metrics_path = repo_output_dir / "metrics.json" # <-- DEFINE METRICS FILE PATH

    print(f"--- INGESTION SERVICE: Starting for {repo_id} ---")
    print(f"   -> Artifacts will be saved to: {repo_output_dir}")

    try:
        project_cortex = None # Initialize
        # --- Step 1: Crawl & Jsonify ---
        print(f"[1/4] Cloning repository and generating Project Cortex file...")
        with IntelligentCrawler(repo_url=repo_url) as crawler:
            files_to_process = crawler.get_file_paths()
            if not files_to_process:
                return {"status": "failed", "error": "No files found to process in the repository."}

            jsonifier = Jsonifier(
                file_paths=files_to_process,
                repo_root=crawler.repo_path,
                repo_id=repo_id
            )
            project_cortex = jsonifier.generate_cortex()

            with open(output_cortex_path, 'w', encoding='utf-8') as f:
                json.dump(project_cortex, f, indent=2)
            print(f"✓ Project Cortex created successfully: {output_cortex_path}")

            # --- NEW STEP 2: Calculate Sentinel Metrics ---
            print(f"[2/4] Sentinel: Calculating health metrics...")
            graph_data = project_cortex.get("architectural_graph", {})
            latest_metrics = sentinel_service.calculate_snapshot_metrics(crawler.repo_path, graph_data)

            # Load existing metrics and append the new snapshot
            if metrics_path.exists():
                with open(metrics_path, 'r', encoding='utf-8') as f:
                    historical_metrics = json.load(f)
            else:
                historical_metrics = []

            historical_metrics.append(latest_metrics)

            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(historical_metrics, f, indent=2)
            print("✓ Sentinel: Health metrics saved.")

        # --- Step 3: Index --- (Now step 3)
        print(f"[3/4] Starting vector indexing with model '{embedding_model}'...")
        indexer = EmbeddingIndexer(model_name=embedding_model)
        indexer.process_cortex(str(output_cortex_path))
        print(f"✓ Vector indexing complete.")

        print(f"[4/4] Ingestion complete.")

        return {
            "status": "success",
            "message": f"Repository '{repo_id}' was successfully cloned, embedded, and indexed.",
            "repo_id": repo_id,
            "original_url": repo_url
        }

    except Exception as e:
        print(f"--- INGESTION FAILED for {repo_id} ---")
        traceback.print_exc()
        if output_cortex_path.exists():
            os.remove(output_cortex_path)
        return {"status": "failed", "error": str(e), "details": traceback.format_exc(), "repo_id": repo_id}

--- FILE_END: backend/lumiere_core/services/ingestion_service.py ---

--- FILE_START: backend/lumiere_core/wsgi.py ---
# In ~/lumiere_semantique/backend/lumiere_core/wsgi.py
"""
WSGI config for backend project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")

application = get_wsgi_application()

--- FILE_END: backend/lumiere_core/wsgi.py ---

--- FILE_START: backend/.env ---

GEMINI_API_KEY='AIzaSyAVvfR4hTMO15HP0Skk0y3A3Bmsvg9ivmI'

--- FILE_END: backend/.env ---

--- FILE_START: backend/api/migrations/__init__.py ---

--- FILE_END: backend/api/migrations/__init__.py ---

--- FILE_START: backend/api/models.py ---
from django.db import models

# Create your models here.

--- FILE_END: backend/api/models.py ---

--- FILE_START: backend/api/__init__.py ---

--- FILE_END: backend/api/__init__.py ---

--- FILE_START: backend/api/apps.py ---
from django.apps import AppConfig


class ApiConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "api"

--- FILE_END: backend/api/apps.py ---

--- FILE_START: backend/api/admin.py ---
from django.contrib import admin

# Register your models here.

--- FILE_END: backend/api/admin.py ---

--- FILE_START: backend/api/tests.py ---
from django.test import TestCase

# Create your tests here.

--- FILE_END: backend/api/tests.py ---

--- FILE_START: backend/api/urls.py ---
# In backend/api/urls.py

from django.urls import path
from .views import (
    BriefingView, ScaffoldView, TestGenerationView, RcaView,
    DocstringGenerationView,
    ProfileReviewView, AmbassadorDispatchView, IssueListView,
    StrategistPrioritizeView, FileContentView, DiplomatView,
    CrucibleValidateView, ListModelsView, HealthCheckView,
    IngestRepositoryView, GraphDataView, OracleView,
    AdjudicateView, HarmonizeView, SentinelBriefingView,
    SuggestActionsView,
)

urlpatterns = [
    # --- HEALTH CHECK ENDPOINT ---
    path('health/', HealthCheckView.as_view(), name='health_check'),

    # --- SUGGESTER ENDPOINT ---
    path('suggest-actions/', SuggestActionsView.as_view(), name='suggest_actions'),

    # --- MODEL MANAGEMENT ENDPOINT ---
    path('models/list/', ListModelsView.as_view(), name='list_models'),

     # --- INGESTION ENDPOINT ---
    path('ingest/', IngestRepositoryView.as_view(), name='ingest_repository'),

     # --- ORACLE (Q&A) ENDPOINT ---
    path('oracle/ask/', OracleView.as_view(), name='oracle_ask'),

    # --- Graph ENDPOINT ---
    path('graph/', GraphDataView.as_view(), name='graph_data'),

    # --- "Triage & Strategy" ENDPOINTS ---
    path('issues/list/', IssueListView.as_view(), name='list_issues'),
    path('strategist/prioritize/', StrategistPrioritizeView.as_view(), name='strategist_prioritize'),
    path('diplomat/find-similar-issues/', DiplomatView.as_view(), name='diplomat_find_similar'),

    # --- "Execution & Validation" ENDPOINTS ---
    path('briefing/', BriefingView.as_view(), name='briefing'),
    path('scaffold/', ScaffoldView.as_view(), name='scaffold'),
    path('crucible/validate/', CrucibleValidateView.as_view(), name='crucible_validate'),
    path('file-content/', FileContentView.as_view(), name='file_content'),
    path('generate-tests/', TestGenerationView.as_view(), name='generate_tests'),
    path('generate-docstring/', DocstringGenerationView.as_view(), name='generate_docstring'),
    path('rca/', RcaView.as_view(), name='rca'),
    path('ambassador/dispatch/', AmbassadorDispatchView.as_view(), name='ambassador_dispatch'),

    # --- "Review" ENDPOINTS ---
    path('profile/review/', ProfileReviewView.as_view(), name='profile_review'),
    path('review/adjudicate/', AdjudicateView.as_view(), name='adjudicate_pr'),
    path('review/harmonize/', HarmonizeView.as_view(), name='harmonize_pr_fix'),

    # --- SENTINEL ENDPOINT ---
    path('sentinel/briefing/', SentinelBriefingView.as_view(), name='sentinel_briefing'),
]

--- FILE_END: backend/api/urls.py ---

--- FILE_START: backend/api/views.py ---
# In backend/api/views.py

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
import os
import re
import traceback
import json
from pathlib import Path

# --- Correctly import services from lumiere_core ---
from lumiere_core.services import (
    llm_service, github, ambassador, crucible, diplomat,
    documentation, profile_service, rca_service, scaffolding, strategist,
    testing, review_service, ingestion_service, cortex_service, oracle_service,
    suggester_service
)
from lumiere_core.services.llm_service import TaskType

# A sensible default model for old workflows, though it's now mostly unused.
DEFAULT_MODEL = "ollama/qwen3:4b"

class HealthCheckView(APIView):
    """A simple view to confirm the server is running."""
    def get(self, request, *args, **kwargs):
        return Response({"status": "ok"}, status=status.HTTP_200_OK)

class ListModelsView(APIView):
    """Returns a list of all available LLM models from configured providers."""
    def get(self, request, *args, **kwargs):
        try:
            models = llm_service.list_available_models()
            return Response(models, status=status.HTTP_200_OK)
        except Exception as e:
            return Response(
                {"error": "Failed to list models.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class StrategistPrioritizeView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # The 'model' parameter is no longer needed. The Task Router handles it.
            result = strategist.analyze_and_prioritize(repo_url)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DiplomatView(APIView):
    def post(self, request, *args, **kwargs):
        issue_title = request.data.get('issue_title')
        issue_body = request.data.get('issue_body')
        if not all([issue_title, issue_body]):
            return Response(
                {"error": "'issue_title' and 'issue_body' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # This service should also be updated to not require model_identifier
            # Assuming diplomat_service is updated internally to use the Task Router
            result = diplomat.find_similar_solved_issues(issue_title, issue_body)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class BriefingView(APIView):
    def post(self, request, *args, **kwargs):
        issue_url = request.data.get('issue_url')
        if not issue_url:
            return Response(
                {"error": "'issue_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Assuming rca_service.generate_briefing is updated to use the Task Router
            result = rca_service.generate_briefing(issue_url)
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class RcaView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        bug_description = request.data.get('bug_description')
        advanced_analysis = request.data.get('advanced_analysis', False)
        confidence_threshold = request.data.get('confidence_threshold', 0.7)

        if not all([repo_url, bug_description]):
            return Response(
                {"error": "'repo_url' and 'bug_description' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Assuming rca_service.perform_rca is updated for the Task Router
            result = rca_service.perform_rca(
                repo_url=repo_url,
                bug_description=bug_description,
                advanced_analysis=advanced_analysis,
                confidence_threshold=confidence_threshold
            )
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class ScaffoldView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        target_files = request.data.get('target_files')
        instruction = request.data.get('instruction')
        rca_report = request.data.get('rca_report')
        refinement_history = request.data.get('refinement_history')

        if not all([repo_id, target_files, instruction, rca_report]):
            return Response(
                {"error": "'repo_id', 'target_files' (list), 'instruction', and 'rca_report' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not isinstance(target_files, list):
            return Response(
                {"error": "'target_files' must be a list of strings."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Scaffolding service uses the Task Router internally now
            result = scaffolding.generate_scaffold(
                repo_id, target_files, instruction, rca_report, refinement_history
            )
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class CrucibleValidateView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        target_file = request.data.get('target_file')
        modified_code = request.data.get('modified_code')
        if not all([repo_url, target_file, modified_code]):
            return Response(
                {"error": "'repo_url', 'target_file', 'modified_code' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            result = crucible.validate_fix(repo_url, target_file, modified_code)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            error_details = traceback.format_exc()
            return Response(
                {
                    "error": "An unexpected internal server error occurred in The Crucible.",
                    "details": str(e),
                    "traceback": error_details
                },
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AmbassadorDispatchView(APIView):
    def post(self, request, *args, **kwargs):
        issue_url = request.data.get('issue_url')
        modified_files = request.data.get('modified_files')

        if not all([issue_url, modified_files]):
            return Response(
                {"error": "'issue_url' and 'modified_files' (dict) are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        if not isinstance(modified_files, dict):
            return Response(
                {"error": "'modified_files' must be a dictionary."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Ambassador uses the Task Router internally now
            result = ambassador.dispatch_pr(issue_url, modified_files)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class ProfileReviewView(APIView):
    def post(self, request, *args, **kwargs):
        username = request.data.get('username')
        if not username:
            return Response(
                {"error": "'username' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Profile service uses the Task Router internally
            result = profile_service.generate_profile_review(username)
            if "error" in result:
                return Response(result, status=status.HTTP_404_NOT_FOUND)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class IssueListView(APIView):
    def get(self, request, *args, **kwargs):
        repo_url = request.query_params.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        match = re.search(r"github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            return Response(
                {"error": "Invalid 'repo_url' format."},
                status=status.HTTP_400_BAD_REQUEST
            )
        repo_full_name = f"{match.group(1)}/{match.group(2)}"
        try:
            issues = github.list_open_issues(repo_full_name)
            return Response(issues, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileContentView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        file_path = request.data.get('file_path')
        if not all([repo_id, file_path]):
            return Response(
                {"error": "'repo_id' and 'file_path' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            content = cortex_service.get_file_content(repo_id, file_path)
            if content is None:
                return Response(
                    {"error": f"File '{file_path}' not found for repo '{repo_id}'."},
                    status=status.HTTP_404_NOT_FOUND
                )
            return Response({"content": content}, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TestGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        if not all([repo_id, new_code, instruction]):
            return Response(
                {"error": "'repo_id', 'new_code', and 'instruction' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Testing service uses the Task Router internally
            result = testing.generate_tests_for_code(repo_id, new_code, instruction)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DocstringGenerationView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        new_code = request.data.get('new_code')
        instruction = request.data.get('instruction')
        if not all([repo_id, new_code, instruction]):
            return Response(
                {"error": "'repo_id', 'new_code', and 'instruction' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Documentation service uses the Task Router internally
            result = documentation.generate_docstring_for_code(repo_id, new_code, instruction)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class IngestRepositoryView(APIView):
    def post(self, request, *args, **kwargs):
        repo_url = request.data.get('repo_url')
        if not repo_url:
            return Response(
                {"error": "'repo_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            result = ingestion_service.clone_and_embed_repository(repo_url)
            if result.get("status") == "failed":
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_201_CREATED)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An unexpected internal server error occurred during ingestion.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class GraphDataView(APIView):
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )

        backend_dir = Path(__file__).resolve().parent.parent
        cortex_file = backend_dir / "cloned_repositories" / repo_id / f"{repo_id}_cortex.json"

        if not cortex_file.exists():
            return Response(
                {"error": f"Cortex file for repo '{repo_id}' not found."},
                status=status.HTTP_404_NOT_FOUND
            )

        try:
            with open(cortex_file, 'r', encoding='utf-8') as f:
                cortex_data = json.load(f)

            graph_data = cortex_data.get('architectural_graph')
            if not graph_data:
                new_message = ("Architectural graph is not available. The Cartographer feature currently only supports "
                             "Python projects (.py files). This repository does not appear to contain Python code suitable for graphing.")
                return Response({"message": new_message, "graph": None}, status=status.HTTP_200_OK)

            return Response({"graph": graph_data, "repo_id": repo_id}, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to read or parse graph data.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class OracleView(APIView):
    def post(self, request, *args, **kwargs):
        repo_id = request.data.get('repo_id')
        question = request.data.get('question')
        if not all([repo_id, question]):
            return Response(
                {"error": "'repo_id' and 'question' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Oracle service uses the Task Router internally
            result = oracle_service.answer_question(repo_id, question)
            if "error" in result:
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Oracle.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AdjudicateView(APIView):
    def post(self, request, *args, **kwargs):
        pr_url = request.data.get('pr_url')
        if not pr_url:
            return Response(
                {"error": "'pr_url' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Inquire service uses the Task Router internally
            result = review_service.inquire_pr(pr_url)
            if "error" in result:
                # Safe check for ingestion error with proper string handling
                if result.get("error") and "ingested" in str(result.get("error", "")):
                    return Response(result, status=status.HTTP_412_PRECONDITION_FAILED)
                return Response(result, status=status.HTTP_400_BAD_REQUEST)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Inquisitor.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class HarmonizeView(APIView):
    def post(self, request, *args, **kwargs):
        pr_url = request.data.get('pr_url')
        review_text = request.data.get('review_text')
        if not all([pr_url, review_text]):
            return Response(
                {"error": "'pr_url' and 'review_text' are required."},
                status=status.HTTP_400_BAD_REQUEST
            )
        try:
            # Harmonizer uses services that use the Task Router
            result = review_service.harmonize_pr_fix(pr_url, review_text)
            if "error" in result:
                return Response(result, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            return Response(result, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "An internal server error occurred in The Harmonizer.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class SentinelBriefingView(APIView):
    def get(self, request, *args, **kwargs):
        repo_id = request.query_params.get('repo_id')
        if not repo_id:
            return Response(
                {"error": "'repo_id' query parameter is required."},
                status=status.HTTP_400_BAD_REQUEST
            )

        backend_dir = Path(__file__).resolve().parent.parent
        metrics_path = backend_dir / "cloned_repositories" / repo_id / "metrics.json"

        if not metrics_path.exists():
            return Response(
                {"error": f"No metrics file found for repo '{repo_id}'. Please run ingestion/analysis first."},
                status=status.HTTP_404_NOT_FOUND
            )

        try:
            with open(metrics_path, 'r') as f:
                metrics_data = json.load(f)

            if len(metrics_data) < 2:
                return Response({
                    "briefing": "Not enough historical data to generate a trend analysis. At least two data points are needed."
                })

            latest = metrics_data[-1]
            previous = metrics_data[-2]
            trends_str = "Key Trends:\n"

            for key, value in latest.items():
                if isinstance(value, (int, float)) and key in previous:
                    prev_val = previous[key]
                    if prev_val != 0:
                        change = ((value - prev_val) / prev_val) * 100
                        trends_str += f"- {key.replace('_', ' ').title()}: {value:.2f} ({change:+.1f}% change)\n"

            # Note: The prompt is truncated for brevity but should include the full Sentinel prompt
            prompt = f"You are The Sentinel, analyzing repository metrics trends...\n{trends_str}"
            briefing = llm_service.generate_text(prompt, task_type=TaskType.SIMPLE)

            return Response({"briefing": briefing, "latest_metrics": latest})
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to generate Sentinel briefing.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

# --- NEW VIEW FOR THE MISSION CONTROLLER ---
class SuggestActionsView(APIView):
    """
    Takes context from the last action and suggests next steps.
    """
    def post(self, request, *args, **kwargs):
        last_action = request.data.get("last_action")
        result_data = request.data.get("result_data", {})

        if not last_action:
            return Response(
                {"error": "'last_action' is required."},
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            suggestions = suggester_service.suggest_next_actions(last_action, result_data)
            return Response(suggestions, status=status.HTTP_200_OK)
        except Exception as e:
            traceback.print_exc()
            return Response(
                {"error": "Failed to generate suggestions.", "details": str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

--- FILE_END: backend/api/views.py ---

--- FILE_START: backend/manage.py ---
# In ~/lumiere_semantique/backend/manage.py
#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys


def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lumiere_core.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


if __name__ == "__main__":
    main()

--- FILE_END: backend/manage.py ---

--- FILE_START: README.md ---
# Lumière Sémantique

--- FILE_END: README.md ---

--- FILE_START: .gitignore ---
# In ~/lumiere_semantique/.gitignore

# --- Python / Django ---
__pycache__/
*.pyc

# --- Virtual Environments ---
# This will ignore venv, venv_broken, etc.
venv/
venv_broken/
*.env
.env

# --- OS / IDE Files ---
.DS_Store
.idea/
.vscode/

# --- Database Files ---
db.sqlite3
*.sqlite3-journal

# --- Lumière Sémantique Generated Artifacts ---
# We don't want to commit the large index and JSON files.
# These should be generated by anyone who clones the repo.
*.json
*.index

# --- Django Media/Static Files ---
media/
static/

# --- Build Artifacts ---
build/
dist/
*.egg-info/

--- FILE_END: .gitignore ---

--- FILE_START: manage.py ---
#!/usr/bin/env python
"""
Lumière Sémantique Project - Root Management Utility.

This script acts as a proxy to the real Django manage.py script
located inside the 'backend' directory. This allows you to run
Django commands from the project root.
"""
import os
import sys
from pathlib import Path

def main():
    # 1. Find the project root and the backend directory
    project_root = Path(__file__).resolve().parent
    backend_dir = project_root / 'backend'

    # 2. Add the backend directory to Python's path
    # This is crucial so that Python can find 'lumiere_core.settings'
    sys.path.insert(0, str(backend_dir))

    # 3. Change the current working directory to the backend
    # This ensures that files like 'db.sqlite3' are found correctly
    os.chdir(backend_dir)

    # 4. Set the DJANGO_SETTINGS_MODULE environment variable
    # This is what the original manage.py does.
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'lumiere_core.settings')

    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc

    # 5. Execute the command that was passed to this script
    execute_from_command_line(sys.argv)

if __name__ == "__main__":
    main()

--- FILE_END: manage.py ---

